{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import xavier_initializer_conv2d as xavier_conv2d\n",
    "from tensorflow.contrib.layers import xavier_initializer as xavier\n",
    "import time\n",
    "\n",
    "import os\n",
    "import math\n",
    "\n",
    "import skimage as ski\n",
    "import skimage.io\n",
    "\n",
    "from im2col_cython import col2im_cython, im2col_cython\n",
    "\n",
    "import _pickle as pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ZADATAK\n",
    "\n",
    "Dovr≈°ite implementacije potpuno povezanog sloja, sloja nelinearnosti te funkcije gubitka u razredima FC, ReLU i SoftmaxCrossEntropyWithLogits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zero_init = np.zeros\n",
    "\n",
    "def variance_scaling_initializer(shape, fan_in, factor=2.0, seed=None):\n",
    "    sigma = np.sqrt(factor / fan_in)\n",
    "    return stats.truncnorm(-2, 2, loc=0, scale=sigma).rvs(shape)\n",
    "\n",
    "\n",
    "# -- ABSTRACT CLASS DEFINITION --\n",
    "class Layer(metaclass = ABCMeta):\n",
    "    \"Interface for layers\"\n",
    "    # See documentation of abstract base classes (ABC): https://docs.python.org/3/library/abc.html\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          inputs: ndarray tensor.\n",
    "        Returns:\n",
    "          ndarray tensor, result of the forward pass.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward_inputs(self, grads):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          grads: gradient of the loss with respect to the output of the layer.\n",
    "        Returns:\n",
    "          Gradient of the loss with respect to the input of the layer.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def backward_params(self, grads):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          grads: gradient of the loss with respect to the output of the layer.\n",
    "        Returns:\n",
    "          Gradient of the loss with respect to all the parameters of the layer as a list\n",
    "          [[w0, g0], ..., [wk, gk], self.name] where w are parameter weights and g their gradient.\n",
    "          Note that wk and gk must have the same shape.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "# -- CONVOLUTION LAYER --\n",
    "class Convolution(Layer):\n",
    "    \"N-dimensional convolution layer\"\n",
    "\n",
    "    def __init__(self, input_layer, num_filters, kernel_size, name, padding='SAME',\n",
    "               weights_initializer_fn=variance_scaling_initializer,\n",
    "               bias_initializer_fn=zero_init):\n",
    "        self.input_shape = input_layer.shape\n",
    "        N, C, H, W = input_layer.shape\n",
    "        self.C = C\n",
    "        self.N = N\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        assert kernel_size % 2 == 1\n",
    "\n",
    "        self.padding = padding\n",
    "\n",
    "        if padding == 'SAME':\n",
    "            # with zero padding\n",
    "            self.shape = (N, num_filters, H, W)\n",
    "            self.pad = (kernel_size - 1) // 2\n",
    "        else:\n",
    "            # without padding\n",
    "            self.shape = (N, num_filters, H - kernel_size + 1, W - kernel_size + 1)\n",
    "            self.pad = 0\n",
    "\n",
    "        fan_in = C * kernel_size**2\n",
    "        self.weights = weights_initializer_fn([num_filters, kernel_size**2 * C], fan_in)\n",
    "        self.bias = bias_initializer_fn([num_filters])\n",
    "        \n",
    "        self.stride = 1\n",
    "        self.name = name\n",
    "        self.has_params = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        k = self.kernel_size\n",
    "        self.x_cols = im2col_cython(x, k, k, self.pad, self.stride)\n",
    "        res = self.weights.dot(self.x_cols) + self.bias.reshape(-1, 1)\n",
    "        N, C, H, W = x.shape\n",
    "        out = res.reshape(self.num_filters, self.shape[2], self.shape[3], N)\n",
    "        return out.transpose(3, 0, 1, 2)\n",
    "\n",
    "    def backward_inputs(self, grad_out):\n",
    "        # nice trick from CS231n, backward pass can be done with just matrix mul and col2im\n",
    "        grad_out = grad_out.transpose(1, 2, 3, 0).reshape(self.num_filters, -1)\n",
    "        grad_x_cols = self.weights.T.dot(grad_out)\n",
    "        N, C, H, W = self.input_shape\n",
    "        k = self.kernel_size\n",
    "        grad_x = col2im_cython(grad_x_cols, N, C, H, W, k, k, self.pad, self.stride)\n",
    "        return grad_x\n",
    "\n",
    "    def backward_params(self, grad_out):\n",
    "        grad_bias = np.sum(grad_out, axis=(0, 2, 3))\n",
    "        grad_out = grad_out.transpose(1, 2, 3, 0).reshape(self.num_filters, -1)\n",
    "        grad_weights = grad_out.dot(self.x_cols.T).reshape(self.weights.shape)\n",
    "        return [[self.weights, grad_weights], [self.bias, grad_bias], self.name]\n",
    "\n",
    "\n",
    "class MaxPooling(Layer):\n",
    "    def __init__(self, input_layer, name, pool_size=2, stride=2):\n",
    "        self.name = name\n",
    "        self.input_shape = input_layer.shape\n",
    "        N, C, H, W = self.input_shape\n",
    "        self.stride = stride\n",
    "        self.shape = (N, C, H // stride, W // stride)\n",
    "        self.pool_size = pool_size\n",
    "        assert pool_size == stride, 'Invalid pooling params'\n",
    "        assert H % pool_size == 0\n",
    "        assert W % pool_size == 0\n",
    "        self.has_params = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        self.input_shape = x.shape\n",
    "        # with this clever reshaping we can implement pooling where pool_size == stride\n",
    "        self.x = x.reshape(N, C, H // self.pool_size, self.pool_size,\n",
    "                           W // self.pool_size, self.pool_size)\n",
    "        self.out = self.x.max(axis=3).max(axis=4)\n",
    "        # if you are returning class member be sure to return a copy\n",
    "        return self.out.copy()\n",
    "\n",
    "    def backward_inputs(self, grad_out):\n",
    "        grad_x = np.zeros_like(self.x)\n",
    "        out_newaxis = self.out[:, :, :, np.newaxis, :, np.newaxis]\n",
    "        mask = (self.x == out_newaxis)\n",
    "        dout_newaxis = grad_out[:, :, :, np.newaxis, :, np.newaxis]\n",
    "        dout_broadcast, _ = np.broadcast_arrays(dout_newaxis, grad_x)\n",
    "        # this is almost the same as the real backward pass\n",
    "        grad_x[mask] = dout_broadcast[mask]\n",
    "        # in the very rare case that more then one input have the same max value\n",
    "        # we can aprox the real gradient routing by evenly distributing across multiple inputs\n",
    "        # but in almost all cases this sum will be 1\n",
    "        grad_x /= np.sum(mask, axis=(3, 5), keepdims=True)\n",
    "        grad_x = grad_x.reshape(self.input_shape)\n",
    "        return grad_x\n",
    "\n",
    "\n",
    "class Flatten(Layer):\n",
    "    def __init__(self, input_layer, name):\n",
    "        self.input_shape = input_layer.shape\n",
    "        self.N = self.input_shape[0]\n",
    "        self.num_outputs = 1\n",
    "        for i in range(1, len(self.input_shape)):\n",
    "            self.num_outputs *= self.input_shape[i]\n",
    "        self.shape = (self.N, self.num_outputs)\n",
    "        self.has_params = False\n",
    "        self.name = name\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.input_shape = inputs.shape\n",
    "        inputs_flat = inputs.reshape(self.input_shape[0], -1)\n",
    "        self.shape = inputs_flat.shape\n",
    "        return inputs_flat\n",
    "\n",
    "    def backward_inputs(self, grads):\n",
    "        return grads.reshape(self.input_shape)\n",
    "\n",
    "\n",
    "class FC(Layer):\n",
    "    def __init__(self, input_layer, num_outputs, name,\n",
    "               weights_initializer_fn=variance_scaling_initializer,\n",
    "               bias_initializer_fn=zero_init):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          input_layer: layer below\n",
    "          num_outputs: number of neurons in this layer\n",
    "          weights_initializer_fn: initializer function for weights,\n",
    "          bias_initializer_fn: initializer function for biases\n",
    "        \"\"\"\n",
    "\n",
    "        self.input_shape = input_layer.shape\n",
    "        self.N = self.input_shape[0]\n",
    "        self.shape = (self.N, num_outputs)\n",
    "        self.num_outputs = num_outputs\n",
    "\n",
    "        self.num_inputs = 1\n",
    "        for i in range(1, len(self.input_shape)):\n",
    "            self.num_inputs *= self.input_shape[i]\n",
    "\n",
    "        self.weights = weights_initializer_fn([num_outputs, self.num_inputs], fan_in=self.num_inputs)\n",
    "        self.bias = bias_initializer_fn([num_outputs])\n",
    "        self.name = name\n",
    "        self.has_params = True\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          inputs: ndarray of shape (N, num_inputs)\n",
    "        Returns:\n",
    "          An ndarray of shape (N, num_outputs)\n",
    "        \"\"\"\n",
    "        self.inputs = inputs\n",
    "        return inputs.dot(self.weights.T) + self.bias\n",
    "\n",
    "    def backward_inputs(self, grads):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          grads: ndarray of shape (N, num_outputs)\n",
    "        Returns:\n",
    "          An ndarray of shape (N, num_inputs)\n",
    "        \"\"\"\n",
    "        return grads.dot(self.weights)\n",
    "\n",
    "    def backward_params(self, grads):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          grads: ndarray of shape (N, num_outputs)\n",
    "        Returns:\n",
    "          List of params and gradient pairs.\n",
    "        \"\"\"\n",
    "        grad_weights = grads.T.dot(self.inputs)\n",
    "        grad_bias = grads.sum(axis = 0)\n",
    "        return [[self.weights, grad_weights], [self.bias, grad_bias], self.name]\n",
    "\n",
    "\n",
    "\n",
    "class ReLU(Layer):\n",
    "    def __init__(self, input_layer, name):\n",
    "        self.shape = input_layer.shape\n",
    "        self.name = name\n",
    "        self.has_params = False\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          inputs: ndarray of shape (N, C, H, W).\n",
    "        Returns:\n",
    "          ndarray of shape (N, C, H, W).\n",
    "        \"\"\"\n",
    "        self.inputs = inputs\n",
    "        return np.maximum(0, inputs)\n",
    "\n",
    "    def backward_inputs(self, grads):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          grads: ndarray of shape (N, C, H, W).\n",
    "        Returns:\n",
    "          ndarray of shape (N, C, H, W).\n",
    "        \"\"\"\n",
    "        grads[self.inputs < 0] = 0\n",
    "        return grads\n",
    "\n",
    "def softmax(x):\n",
    "    x -= np.max(x)\n",
    "    logits_exp = np.exp(x)\n",
    "    return logits_exp / np.sum(logits_exp, axis=1, keepdims=True)\n",
    "\n",
    "class SoftmaxCrossEntropyWithLogits():\n",
    "    def __init__(self):\n",
    "        self.has_params = False\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x: ndarray of shape (N, num_classes).\n",
    "          y: ndarray of shape (N, num_classes).\n",
    "        Returns:\n",
    "          Scalar, average loss over N examples.\n",
    "          It is better to compute average loss here instead of just sum\n",
    "          because then learning rate and weight decay won't depend on batch size.\n",
    "\n",
    "        \"\"\"\n",
    "        return (-np.log(softmax(x)) * y).sum(axis=1).mean()\n",
    "\n",
    "    def backward_inputs(self, x, y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x: ndarray of shape (N, num_classes).\n",
    "          y: ndarray of shape (N, num_classes).\n",
    "        Returns:\n",
    "          Gradient with respect to the x, ndarray of shape (N, num_classes).\n",
    "        \"\"\"\n",
    "        # Hint: don't forget that we took the average in the forward pass\n",
    "        N = len(x)\n",
    "        return (softmax(x) - y) / N\n",
    "\n",
    "\n",
    "class L2Regularizer():\n",
    "    def __init__(self, weights, weight_decay, name):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          weights: parameters which will be regularizerized\n",
    "          weight_decay: lambda, regularization strength\n",
    "          name: layer name\n",
    "        \"\"\"\n",
    "        # this is still a reference to original tensor so don't change self.weights\n",
    "        self.weights = weights\n",
    "        self.weight_decay = weight_decay\n",
    "        self.name = name\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "         Returns:\n",
    "          Scalar, loss due to the L2 regularization.\n",
    "        \"\"\"\n",
    "        return 1 / 2 * self.weight_decay * np.sum(self.weights * self.weights)\n",
    "\n",
    "    def backward_params(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "          Gradient of the L2 loss with respect to the regularized weights.\n",
    "        \"\"\"\n",
    "        grad_weights = self.weight_decay * self.weights\n",
    "        return [[self.weights, grad_weights], self.name]\n",
    "\n",
    "\n",
    "class RegularizedLoss():\n",
    "    def __init__(self, data_loss, regularizer_losses):\n",
    "        self.data_loss = data_loss\n",
    "        self.regularizer_losses = regularizer_losses\n",
    "        self.has_params = True\n",
    "        self.name = 'RegularizedLoss'\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        loss_val = self.data_loss.forward(x, y)\n",
    "        for loss in self.regularizer_losses:\n",
    "            loss_val += loss.forward()\n",
    "        return loss_val\n",
    "\n",
    "    def backward_inputs(self, x, y):\n",
    "        return self.data_loss.backward_inputs(x, y)\n",
    "\n",
    "    def backward_params(self):\n",
    "        grads = []\n",
    "        for loss in self.regularizer_losses:\n",
    "            grads += [loss.backward_params()]\n",
    "        return grads\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolution\n",
      "Check grad wrt input\n",
      "Relative error =  4.34037350521e-08\n",
      "Error norm =  3.03166983177e-10\n",
      "Check grad wrt params\n",
      "Check weights:\n",
      "Relative error =  2.05308084484e-10\n",
      "Error norm =  3.24746247413e-10\n",
      "Check biases:\n",
      "Relative error =  5.98901687008e-12\n",
      "Error norm =  7.554444292e-11\n",
      "\n",
      "MaxPooling\n",
      "Check grad wrt input\n",
      "Relative error =  3.27563498338e-12\n",
      "Error norm =  9.03555812612e-11\n",
      "\n",
      "ReLU\n",
      "Check grad wrt input\n",
      "Relative error =  3.27561389578e-12\n",
      "Error norm =  5.54668785433e-11\n",
      "\n",
      "FC\n",
      "Check grad wrt input\n",
      "Relative error =  5.32355357345e-06\n",
      "Error norm =  7.87698199114e-10\n",
      "Check grad wrt params\n",
      "Check weights:\n",
      "Relative error =  1.55163851315e-09\n",
      "Error norm =  8.1923090186e-10\n",
      "Check biases:\n",
      "Relative error =  1.49794455459e-10\n",
      "Error norm =  1.25695890228e-10\n",
      "\n",
      "SoftmaxCrossEntropyWithLogits\n",
      "Relative error =  2.71679466205e-07\n",
      "Error norm =  5.13073954141e-10\n",
      "\n",
      "L2Regularizer\n",
      "Check grad wrt params\n",
      "Relative error =  3.87171066729e-06\n",
      "Error norm =  9.82378385361e-10\n"
     ]
    }
   ],
   "source": [
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "def eval_numerical_gradient(f, x, df, h=1e-5):\n",
    "    \"\"\"\n",
    "    Evaluate a numeric gradient for a function that accepts a numpy\n",
    "    array and returns a numpy array.\n",
    "    - f should be a function that takes a single argument\n",
    "    - x is the point (numpy array) to evaluate the gradient at\n",
    "    \"\"\"\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h\n",
    "        # evaluate f(x + h)\n",
    "        pos = f(x.copy()).copy()\n",
    "        x[ix] = oldval - h\n",
    "        # evaluate f(x - h)\n",
    "        neg = f(x.copy()).copy()\n",
    "        x[ix] = oldval\n",
    "\n",
    "        # compute the partial derivative with centered formula\n",
    "        grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n",
    "        # step to next dimension\n",
    "        it.iternext()\n",
    "    return grad\n",
    "\n",
    "def check_grad_inputs(layer, x, grad_out):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    layer: Layer object\n",
    "    x: ndarray tensor input data\n",
    "    grad_out: ndarray tensor gradient from the next layer\n",
    "    \"\"\"\n",
    "    grad_x_num = eval_numerical_gradient(layer.forward, x, grad_out)\n",
    "    grad_x = layer.backward_inputs(grad_out)\n",
    "    print(\"Relative error = \", rel_error(grad_x_num, grad_x))\n",
    "    print(\"Error norm = \", np.linalg.norm(grad_x_num - grad_x))\n",
    "\n",
    "def check_grad_params(layer, x, w, b, grad_out):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    layer: Layer object\n",
    "    x: ndarray tensor input data\n",
    "    w: ndarray tensor layer weights\n",
    "    b: ndarray tensor layer biases\n",
    "    grad_out: ndarray tensor gradient from the next layer\n",
    "    \"\"\"\n",
    "    func = lambda params: layer.forward(x)\n",
    "    grad_w_num = eval_numerical_gradient(func, w, grad_out)\n",
    "    grad_b_num = eval_numerical_gradient(func, b, grad_out)\n",
    "    grads = layer.backward_params(grad_out)\n",
    "    grad_w = grads[0][1]\n",
    "    grad_b = grads[1][1]\n",
    "    print(\"Check weights:\")\n",
    "    print(\"Relative error = \", rel_error(grad_w_num, grad_w))\n",
    "    print(\"Error norm = \", np.linalg.norm(grad_w_num - grad_w))\n",
    "    print(\"Check biases:\")\n",
    "    print(\"Relative error = \", rel_error(grad_b_num, grad_b))\n",
    "    print(\"Error norm = \", np.linalg.norm(grad_b_num - grad_b))\n",
    "\n",
    "print(\"Convolution\")\n",
    "x = np.random.randn(4, 3, 5, 5)\n",
    "grad_out = np.random.randn(4, 2, 5, 5)\n",
    "conv = Convolution(x, 2, 3, \"conv1\")\n",
    "print(\"Check grad wrt input\")\n",
    "check_grad_inputs(conv, x, grad_out)\n",
    "print(\"Check grad wrt params\")\n",
    "check_grad_params(conv, x, conv.weights, conv.bias, grad_out)\n",
    "\n",
    "print(\"\\nMaxPooling\")\n",
    "x = np.random.randn(5, 4, 8, 8)\n",
    "grad_out = np.random.randn(5, 4, 4, 4)\n",
    "pool = MaxPooling(x, \"pool\", 2, 2)\n",
    "print(\"Check grad wrt input\")\n",
    "check_grad_inputs(pool, x, grad_out)\n",
    "\n",
    "print(\"\\nReLU\")\n",
    "x = np.random.randn(4, 3, 5, 5)\n",
    "grad_out = np.random.randn(4, 3, 5, 5)\n",
    "relu = ReLU(x, \"relu\")\n",
    "print(\"Check grad wrt input\")\n",
    "check_grad_inputs(relu, x, grad_out)\n",
    "\n",
    "print(\"\\nFC\")\n",
    "x = np.random.randn(20, 40)\n",
    "grad_out = np.random.randn(20, 30)\n",
    "fc = FC(x, 30, \"fc\")\n",
    "print(\"Check grad wrt input\")\n",
    "check_grad_inputs(fc, x, grad_out)\n",
    "print(\"Check grad wrt params\")\n",
    "check_grad_params(fc, x, fc.weights, fc.bias, grad_out)\n",
    "\n",
    "print(\"\\nSoftmaxCrossEntropyWithLogits\")\n",
    "x = np.random.randn(50, 20)\n",
    "y = np.zeros([50, 20])\n",
    "y[:,0] = 1\n",
    "loss = SoftmaxCrossEntropyWithLogits()\n",
    "grad_x_num = eval_numerical_gradient(lambda x: loss.forward(x, y), x, 1)\n",
    "out = loss.forward(x, y)\n",
    "grad_x = loss.backward_inputs(x, y)\n",
    "print(\"Relative error = \", rel_error(grad_x_num, grad_x))\n",
    "print(\"Error norm = \", np.linalg.norm(grad_x_num - grad_x))\n",
    "\n",
    "print(\"\\nL2Regularizer\")\n",
    "x = np.random.randn(5, 4, 8, 8)\n",
    "grad_out = np.random.randn(5, 4, 4, 4)\n",
    "l2reg = L2Regularizer(x, 1e-2, 'L2reg')\n",
    "print(\"Check grad wrt params\")\n",
    "func = lambda params: l2reg.forward()\n",
    "grad_num = eval_numerical_gradient(func, l2reg.weights, 1)\n",
    "grads = l2reg.backward_params()\n",
    "grad = grads[0][1]\n",
    "print(\"Relative error = \", rel_error(grad_num, grad))\n",
    "print(\"Error norm = \", np.linalg.norm(grad_num - grad))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_pass(net, inputs):\n",
    "    output = inputs\n",
    "    for layer in net:\n",
    "        output = layer.forward(output)\n",
    "    return output\n",
    "\n",
    "\n",
    "def backward_pass(net, loss, x, y):\n",
    "    grads = []\n",
    "    grad_out = loss.backward_inputs(x, y)\n",
    "    if loss.has_params:\n",
    "        grads += loss.backward_params()\n",
    "    for layer in reversed(net):\n",
    "        grad_inputs = layer.backward_inputs(grad_out)\n",
    "        if layer.has_params:\n",
    "            grads += [layer.backward_params(grad_out)]\n",
    "        grad_out = grad_inputs\n",
    "    return grads\n",
    "\n",
    "def sgd_update_params(grads, config):\n",
    "    lr = config['lr']\n",
    "    for layer_grads in grads:\n",
    "        for i in range(len(layer_grads) - 1):\n",
    "            params = layer_grads[i][0]\n",
    "            grads = layer_grads[i][1]\n",
    "            params -= lr * grads\n",
    "\n",
    "\n",
    "def draw_conv_filters(epoch, step, layer, save_dir):\n",
    "    C = layer.C\n",
    "    w = layer.weights.copy()\n",
    "    num_filters = w.shape[0]\n",
    "    k = int(np.sqrt(w.shape[1] / C))\n",
    "    w = w.reshape(num_filters, C, k, k)\n",
    "    w -= w.min()\n",
    "    w /= w.max()\n",
    "    border = 1\n",
    "    cols = 8\n",
    "    rows = math.ceil(num_filters / cols)\n",
    "    width = cols * k + (cols-1) * border\n",
    "    height = rows * k + (rows-1) * border\n",
    "    for i in range(1):\n",
    "        img = np.zeros([height, width])\n",
    "        \n",
    "        for j in range(num_filters):\n",
    "            r = int(j / cols) * (k + border)\n",
    "            c = int(j % cols) * (k + border)\n",
    "            img[r:r+k,c:c+k] = w[j,i]\n",
    "            \n",
    "        filename = '%s_epoch_%02d_step_%06d_input_%03d.png' % (layer.name, epoch, step, i)\n",
    "        ski.io.imsave(os.path.join(save_dir, filename), img)\n",
    "\n",
    "\n",
    "def train(train_x, train_y, valid_x, valid_y, net, loss, config):\n",
    "    lr_policy = config['lr_policy']\n",
    "    batch_size = config['batch_size']\n",
    "    max_epochs = config['max_epochs']\n",
    "    save_dir = config['save_dir']\n",
    "    num_examples = train_x.shape[0]\n",
    "    \n",
    "    assert num_examples % batch_size == 0\n",
    "    num_batches = num_examples // batch_size\n",
    "    \n",
    "    for epoch in range(1, max_epochs+1):\n",
    "        if epoch in lr_policy:\n",
    "            solver_config = lr_policy[epoch]\n",
    "            \n",
    "        cnt_correct = 0\n",
    "        \n",
    "        # shuffle the data at the beggining of each epoch\n",
    "        permutation_idx = np.random.permutation(num_examples)\n",
    "        train_x = train_x[permutation_idx]\n",
    "        train_y = train_y[permutation_idx]\n",
    "        for i in range(num_batches):\n",
    "            # store mini-batch to ndarray\n",
    "            batch_x = train_x[i*batch_size:(i+1)*batch_size, :]\n",
    "            batch_y = train_y[i*batch_size:(i+1)*batch_size, :]\n",
    "            \n",
    "            logits = forward_pass(net, batch_x)\n",
    "            loss_val = loss.forward(logits, batch_y)\n",
    "            \n",
    "            # compute classification accuracy\n",
    "            yp = np.argmax(logits, 1)\n",
    "            yt = np.argmax(batch_y, 1)\n",
    "            \n",
    "            cnt_correct += (yp == yt).sum()\n",
    "            \n",
    "            grads = backward_pass(net, loss, logits, batch_y)\n",
    "            sgd_update_params(grads, solver_config)\n",
    "            \n",
    "            if i % 5 == 0:\n",
    "                print(\"epoch %d, step %d/%d, batch loss = %.2f\" % \n",
    "                      (epoch, i*batch_size, num_examples, loss_val))\n",
    "            if i % 100 == 0:\n",
    "                draw_conv_filters(epoch, i*batch_size, net[0], save_dir)\n",
    "            if i > 0 and i % 50 == 0:\n",
    "                print(\"Train accuracy = %.2f\" % \n",
    "                      (cnt_correct / ((i+1)*batch_size) * 100))\n",
    "                \n",
    "        print(\"Train accuracy = %.2f\" % (cnt_correct / num_examples * 100))\n",
    "        evaluate(\"Validation\", valid_x, valid_y, net, loss, config)\n",
    "        \n",
    "    return net\n",
    "\n",
    "def evaluate(name, x, y, net, loss, config):\n",
    "    print(\"\\nRunning evaluation: \", name)\n",
    "    \n",
    "    batch_size = config['batch_size']\n",
    "    num_examples = x.shape[0]\n",
    "    assert num_examples % batch_size == 0\n",
    "    num_batches = num_examples // batch_size\n",
    "    \n",
    "    cnt_correct = 0\n",
    "    loss_avg = 0\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        batch_x = x[i*batch_size:(i+1)*batch_size, :]\n",
    "        batch_y = y[i*batch_size:(i+1)*batch_size, :]\n",
    "        logits = forward_pass(net, batch_x)\n",
    "        yp = np.argmax(logits, 1)\n",
    "        yt = np.argmax(batch_y, 1)\n",
    "        cnt_correct += (yp == yt).sum()\n",
    "        loss_val = loss.forward(logits, batch_y)\n",
    "        loss_avg += loss_val\n",
    "        \n",
    "    valid_acc = cnt_correct / num_examples * 100\n",
    "    loss_avg /= num_batches\n",
    "    print(name + \" accuracy = %.2f\" % valid_acc)\n",
    "    print(name + \" avg loss = %.2f\\n\" % loss_avg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Training without regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "tf.app.flags.DEFINE_string('data_dir', \n",
    "  '/tmp/data/', 'Directory for storing data')\n",
    "mnist = input_data.read_data_sets(\n",
    "  tf.app.flags.FLAGS.data_dir, one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, step 0/55000, batch loss = 2.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/Anaconda/anaconda/lib/python3.5/site-packages/skimage/util/dtype.py:110: UserWarning: Possible precision loss when converting from float64 to uint16\n",
      "  \"%s to %s\" % (dtypeobj_in, dtypeobj))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, step 250/55000, batch loss = 1.96\n",
      "epoch 1, step 500/55000, batch loss = 1.45\n",
      "epoch 1, step 750/55000, batch loss = 0.94\n",
      "epoch 1, step 1000/55000, batch loss = 0.58\n",
      "epoch 1, step 1250/55000, batch loss = 0.64\n",
      "epoch 1, step 1500/55000, batch loss = 0.47\n",
      "epoch 1, step 1750/55000, batch loss = 0.71\n",
      "epoch 1, step 2000/55000, batch loss = 0.29\n",
      "epoch 1, step 2250/55000, batch loss = 0.44\n",
      "epoch 1, step 2500/55000, batch loss = 0.40\n",
      "Train accuracy = 70.39\n",
      "epoch 1, step 2750/55000, batch loss = 0.18\n",
      "epoch 1, step 3000/55000, batch loss = 0.33\n",
      "epoch 1, step 3250/55000, batch loss = 0.31\n",
      "epoch 1, step 3500/55000, batch loss = 0.38\n",
      "epoch 1, step 3750/55000, batch loss = 0.43\n",
      "epoch 1, step 4000/55000, batch loss = 0.15\n",
      "epoch 1, step 4250/55000, batch loss = 0.50\n",
      "epoch 1, step 4500/55000, batch loss = 0.42\n",
      "epoch 1, step 4750/55000, batch loss = 0.16\n",
      "epoch 1, step 5000/55000, batch loss = 0.30\n",
      "Train accuracy = 80.83\n",
      "epoch 1, step 5250/55000, batch loss = 0.18\n",
      "epoch 1, step 5500/55000, batch loss = 0.13\n",
      "epoch 1, step 5750/55000, batch loss = 0.15\n",
      "epoch 1, step 6000/55000, batch loss = 0.24\n",
      "epoch 1, step 6250/55000, batch loss = 0.13\n",
      "epoch 1, step 6500/55000, batch loss = 0.14\n",
      "epoch 1, step 6750/55000, batch loss = 0.10\n",
      "epoch 1, step 7000/55000, batch loss = 0.11\n",
      "epoch 1, step 7250/55000, batch loss = 0.25\n",
      "epoch 1, step 7500/55000, batch loss = 0.16\n",
      "Train accuracy = 85.36\n",
      "epoch 1, step 7750/55000, batch loss = 0.14\n",
      "epoch 1, step 8000/55000, batch loss = 0.15\n",
      "epoch 1, step 8250/55000, batch loss = 0.08\n",
      "epoch 1, step 8500/55000, batch loss = 0.24\n",
      "epoch 1, step 8750/55000, batch loss = 0.21\n",
      "epoch 1, step 9000/55000, batch loss = 0.18\n",
      "epoch 1, step 9250/55000, batch loss = 0.05\n",
      "epoch 1, step 9500/55000, batch loss = 0.15\n",
      "epoch 1, step 9750/55000, batch loss = 0.25\n",
      "epoch 1, step 10000/55000, batch loss = 0.07\n",
      "Train accuracy = 87.90\n",
      "epoch 1, step 10250/55000, batch loss = 0.24\n",
      "epoch 1, step 10500/55000, batch loss = 0.21\n",
      "epoch 1, step 10750/55000, batch loss = 0.14\n",
      "epoch 1, step 11000/55000, batch loss = 0.11\n",
      "epoch 1, step 11250/55000, batch loss = 0.22\n",
      "epoch 1, step 11500/55000, batch loss = 0.05\n",
      "epoch 1, step 11750/55000, batch loss = 0.15\n",
      "epoch 1, step 12000/55000, batch loss = 0.06\n",
      "epoch 1, step 12250/55000, batch loss = 0.45\n",
      "epoch 1, step 12500/55000, batch loss = 0.06\n",
      "Train accuracy = 89.63\n",
      "epoch 1, step 12750/55000, batch loss = 0.08\n",
      "epoch 1, step 13000/55000, batch loss = 0.18\n",
      "epoch 1, step 13250/55000, batch loss = 0.09\n",
      "epoch 1, step 13500/55000, batch loss = 0.19\n",
      "epoch 1, step 13750/55000, batch loss = 0.17\n",
      "epoch 1, step 14000/55000, batch loss = 0.29\n",
      "epoch 1, step 14250/55000, batch loss = 0.05\n",
      "epoch 1, step 14500/55000, batch loss = 0.11\n",
      "epoch 1, step 14750/55000, batch loss = 0.01\n",
      "epoch 1, step 15000/55000, batch loss = 0.16\n",
      "Train accuracy = 90.78\n",
      "epoch 1, step 15250/55000, batch loss = 0.06\n",
      "epoch 1, step 15500/55000, batch loss = 0.18\n",
      "epoch 1, step 15750/55000, batch loss = 0.08\n",
      "epoch 1, step 16000/55000, batch loss = 0.03\n",
      "epoch 1, step 16250/55000, batch loss = 0.20\n",
      "epoch 1, step 16500/55000, batch loss = 0.08\n",
      "epoch 1, step 16750/55000, batch loss = 0.24\n",
      "epoch 1, step 17000/55000, batch loss = 0.03\n",
      "epoch 1, step 17250/55000, batch loss = 0.16\n",
      "epoch 1, step 17500/55000, batch loss = 0.21\n",
      "Train accuracy = 91.69\n",
      "epoch 1, step 17750/55000, batch loss = 0.09\n",
      "epoch 1, step 18000/55000, batch loss = 0.16\n",
      "epoch 1, step 18250/55000, batch loss = 0.04\n",
      "epoch 1, step 18500/55000, batch loss = 0.12\n",
      "epoch 1, step 18750/55000, batch loss = 0.08\n",
      "epoch 1, step 19000/55000, batch loss = 0.09\n",
      "epoch 1, step 19250/55000, batch loss = 0.21\n",
      "epoch 1, step 19500/55000, batch loss = 0.15\n",
      "epoch 1, step 19750/55000, batch loss = 0.03\n",
      "epoch 1, step 20000/55000, batch loss = 0.04\n",
      "Train accuracy = 92.34\n",
      "epoch 1, step 20250/55000, batch loss = 0.05\n",
      "epoch 1, step 20500/55000, batch loss = 0.08\n",
      "epoch 1, step 20750/55000, batch loss = 0.07\n",
      "epoch 1, step 21000/55000, batch loss = 0.12\n",
      "epoch 1, step 21250/55000, batch loss = 0.05\n",
      "epoch 1, step 21500/55000, batch loss = 0.22\n",
      "epoch 1, step 21750/55000, batch loss = 0.03\n",
      "epoch 1, step 22000/55000, batch loss = 0.08\n",
      "epoch 1, step 22250/55000, batch loss = 0.13\n",
      "epoch 1, step 22500/55000, batch loss = 0.07\n",
      "Train accuracy = 92.87\n",
      "epoch 1, step 22750/55000, batch loss = 0.08\n",
      "epoch 1, step 23000/55000, batch loss = 0.11\n",
      "epoch 1, step 23250/55000, batch loss = 0.05\n",
      "epoch 1, step 23500/55000, batch loss = 0.12\n",
      "epoch 1, step 23750/55000, batch loss = 0.22\n",
      "epoch 1, step 24000/55000, batch loss = 0.14\n",
      "epoch 1, step 24250/55000, batch loss = 0.15\n",
      "epoch 1, step 24500/55000, batch loss = 0.09\n",
      "epoch 1, step 24750/55000, batch loss = 0.19\n",
      "epoch 1, step 25000/55000, batch loss = 0.12\n",
      "Train accuracy = 93.26\n",
      "epoch 1, step 25250/55000, batch loss = 0.10\n",
      "epoch 1, step 25500/55000, batch loss = 0.11\n",
      "epoch 1, step 25750/55000, batch loss = 0.02\n",
      "epoch 1, step 26000/55000, batch loss = 0.26\n",
      "epoch 1, step 26250/55000, batch loss = 0.01\n",
      "epoch 1, step 26500/55000, batch loss = 0.03\n",
      "epoch 1, step 26750/55000, batch loss = 0.03\n",
      "epoch 1, step 27000/55000, batch loss = 0.01\n",
      "epoch 1, step 27250/55000, batch loss = 0.09\n",
      "epoch 1, step 27500/55000, batch loss = 0.11\n",
      "Train accuracy = 93.67\n",
      "epoch 1, step 27750/55000, batch loss = 0.06\n",
      "epoch 1, step 28000/55000, batch loss = 0.16\n",
      "epoch 1, step 28250/55000, batch loss = 0.30\n",
      "epoch 1, step 28500/55000, batch loss = 0.18\n",
      "epoch 1, step 28750/55000, batch loss = 0.01\n",
      "epoch 1, step 29000/55000, batch loss = 0.08\n",
      "epoch 1, step 29250/55000, batch loss = 0.04\n",
      "epoch 1, step 29500/55000, batch loss = 0.02\n",
      "epoch 1, step 29750/55000, batch loss = 0.16\n",
      "epoch 1, step 30000/55000, batch loss = 0.09\n",
      "Train accuracy = 93.96\n",
      "epoch 1, step 30250/55000, batch loss = 0.03\n",
      "epoch 1, step 30500/55000, batch loss = 0.08\n",
      "epoch 1, step 30750/55000, batch loss = 0.01\n",
      "epoch 1, step 31000/55000, batch loss = 0.09\n",
      "epoch 1, step 31250/55000, batch loss = 0.09\n",
      "epoch 1, step 31500/55000, batch loss = 0.16\n",
      "epoch 1, step 31750/55000, batch loss = 0.07\n",
      "epoch 1, step 32000/55000, batch loss = 0.02\n",
      "epoch 1, step 32250/55000, batch loss = 0.03\n",
      "epoch 1, step 32500/55000, batch loss = 0.08\n",
      "Train accuracy = 94.23\n",
      "epoch 1, step 32750/55000, batch loss = 0.13\n",
      "epoch 1, step 33000/55000, batch loss = 0.06\n",
      "epoch 1, step 33250/55000, batch loss = 0.03\n",
      "epoch 1, step 33500/55000, batch loss = 0.05\n",
      "epoch 1, step 33750/55000, batch loss = 0.08\n",
      "epoch 1, step 34000/55000, batch loss = 0.23\n",
      "epoch 1, step 34250/55000, batch loss = 0.02\n",
      "epoch 1, step 34500/55000, batch loss = 0.07\n",
      "epoch 1, step 34750/55000, batch loss = 0.06\n",
      "epoch 1, step 35000/55000, batch loss = 0.21\n",
      "Train accuracy = 94.47\n",
      "epoch 1, step 35250/55000, batch loss = 0.02\n",
      "epoch 1, step 35500/55000, batch loss = 0.18\n",
      "epoch 1, step 35750/55000, batch loss = 0.04\n",
      "epoch 1, step 36000/55000, batch loss = 0.01\n",
      "epoch 1, step 36250/55000, batch loss = 0.04\n",
      "epoch 1, step 36500/55000, batch loss = 0.02\n",
      "epoch 1, step 36750/55000, batch loss = 0.05\n",
      "epoch 1, step 37000/55000, batch loss = 0.08\n",
      "epoch 1, step 37250/55000, batch loss = 0.01\n",
      "epoch 1, step 37500/55000, batch loss = 0.04\n",
      "Train accuracy = 94.67\n",
      "epoch 1, step 37750/55000, batch loss = 0.19\n",
      "epoch 1, step 38000/55000, batch loss = 0.08\n",
      "epoch 1, step 38250/55000, batch loss = 0.06\n",
      "epoch 1, step 38500/55000, batch loss = 0.03\n",
      "epoch 1, step 38750/55000, batch loss = 0.13\n",
      "epoch 1, step 39000/55000, batch loss = 0.02\n",
      "epoch 1, step 39250/55000, batch loss = 0.03\n",
      "epoch 1, step 39500/55000, batch loss = 0.09\n",
      "epoch 1, step 39750/55000, batch loss = 0.07\n",
      "epoch 1, step 40000/55000, batch loss = 0.05\n",
      "Train accuracy = 94.93\n",
      "epoch 1, step 40250/55000, batch loss = 0.15\n",
      "epoch 1, step 40500/55000, batch loss = 0.10\n",
      "epoch 1, step 40750/55000, batch loss = 0.09\n",
      "epoch 1, step 41000/55000, batch loss = 0.05\n",
      "epoch 1, step 41250/55000, batch loss = 0.08\n",
      "epoch 1, step 41500/55000, batch loss = 0.20\n",
      "epoch 1, step 41750/55000, batch loss = 0.06\n",
      "epoch 1, step 42000/55000, batch loss = 0.19\n",
      "epoch 1, step 42250/55000, batch loss = 0.06\n",
      "epoch 1, step 42500/55000, batch loss = 0.15\n",
      "Train accuracy = 95.11\n",
      "epoch 1, step 42750/55000, batch loss = 0.00\n",
      "epoch 1, step 43000/55000, batch loss = 0.03\n",
      "epoch 1, step 43250/55000, batch loss = 0.07\n",
      "epoch 1, step 43500/55000, batch loss = 0.02\n",
      "epoch 1, step 43750/55000, batch loss = 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, step 44000/55000, batch loss = 0.02\n",
      "epoch 1, step 44250/55000, batch loss = 0.01\n",
      "epoch 1, step 44500/55000, batch loss = 0.02\n",
      "epoch 1, step 44750/55000, batch loss = 0.02\n",
      "epoch 1, step 45000/55000, batch loss = 0.00\n",
      "Train accuracy = 95.28\n",
      "epoch 1, step 45250/55000, batch loss = 0.02\n",
      "epoch 1, step 45500/55000, batch loss = 0.04\n",
      "epoch 1, step 45750/55000, batch loss = 0.02\n",
      "epoch 1, step 46000/55000, batch loss = 0.03\n",
      "epoch 1, step 46250/55000, batch loss = 0.04\n",
      "epoch 1, step 46500/55000, batch loss = 0.07\n",
      "epoch 1, step 46750/55000, batch loss = 0.02\n",
      "epoch 1, step 47000/55000, batch loss = 0.02\n",
      "epoch 1, step 47250/55000, batch loss = 0.10\n",
      "epoch 1, step 47500/55000, batch loss = 0.01\n",
      "Train accuracy = 95.40\n",
      "epoch 1, step 47750/55000, batch loss = 0.08\n",
      "epoch 1, step 48000/55000, batch loss = 0.08\n",
      "epoch 1, step 48250/55000, batch loss = 0.05\n",
      "epoch 1, step 48500/55000, batch loss = 0.02\n",
      "epoch 1, step 48750/55000, batch loss = 0.07\n",
      "epoch 1, step 49000/55000, batch loss = 0.24\n",
      "epoch 1, step 49250/55000, batch loss = 0.03\n",
      "epoch 1, step 49500/55000, batch loss = 0.05\n",
      "epoch 1, step 49750/55000, batch loss = 0.10\n",
      "epoch 1, step 50000/55000, batch loss = 0.04\n",
      "Train accuracy = 95.49\n",
      "epoch 1, step 50250/55000, batch loss = 0.11\n",
      "epoch 1, step 50500/55000, batch loss = 0.08\n",
      "epoch 1, step 50750/55000, batch loss = 0.04\n",
      "epoch 1, step 51000/55000, batch loss = 0.09\n",
      "epoch 1, step 51250/55000, batch loss = 0.04\n",
      "epoch 1, step 51500/55000, batch loss = 0.16\n",
      "epoch 1, step 51750/55000, batch loss = 0.05\n",
      "epoch 1, step 52000/55000, batch loss = 0.14\n",
      "epoch 1, step 52250/55000, batch loss = 0.07\n",
      "epoch 1, step 52500/55000, batch loss = 0.11\n",
      "Train accuracy = 95.60\n",
      "epoch 1, step 52750/55000, batch loss = 0.03\n",
      "epoch 1, step 53000/55000, batch loss = 0.02\n",
      "epoch 1, step 53250/55000, batch loss = 0.10\n",
      "epoch 1, step 53500/55000, batch loss = 0.07\n",
      "epoch 1, step 53750/55000, batch loss = 0.11\n",
      "epoch 1, step 54000/55000, batch loss = 0.00\n",
      "epoch 1, step 54250/55000, batch loss = 0.10\n",
      "epoch 1, step 54500/55000, batch loss = 0.05\n",
      "epoch 1, step 54750/55000, batch loss = 0.03\n",
      "Train accuracy = 95.72\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 98.10\n",
      "Validation avg loss = 0.06\n",
      "\n",
      "epoch 2, step 0/55000, batch loss = 0.02\n",
      "epoch 2, step 250/55000, batch loss = 0.01\n",
      "epoch 2, step 500/55000, batch loss = 0.07\n",
      "epoch 2, step 750/55000, batch loss = 0.18\n",
      "epoch 2, step 1000/55000, batch loss = 0.08\n",
      "epoch 2, step 1250/55000, batch loss = 0.01\n",
      "epoch 2, step 1500/55000, batch loss = 0.07\n",
      "epoch 2, step 1750/55000, batch loss = 0.09\n",
      "epoch 2, step 2000/55000, batch loss = 0.08\n",
      "epoch 2, step 2250/55000, batch loss = 0.06\n",
      "epoch 2, step 2500/55000, batch loss = 0.01\n",
      "Train accuracy = 98.59\n",
      "epoch 2, step 2750/55000, batch loss = 0.02\n",
      "epoch 2, step 3000/55000, batch loss = 0.00\n",
      "epoch 2, step 3250/55000, batch loss = 0.01\n",
      "epoch 2, step 3500/55000, batch loss = 0.13\n",
      "epoch 2, step 3750/55000, batch loss = 0.05\n",
      "epoch 2, step 4000/55000, batch loss = 0.01\n",
      "epoch 2, step 4250/55000, batch loss = 0.02\n",
      "epoch 2, step 4500/55000, batch loss = 0.03\n",
      "epoch 2, step 4750/55000, batch loss = 0.04\n",
      "epoch 2, step 5000/55000, batch loss = 0.02\n",
      "Train accuracy = 98.53\n",
      "epoch 2, step 5250/55000, batch loss = 0.06\n",
      "epoch 2, step 5500/55000, batch loss = 0.02\n",
      "epoch 2, step 5750/55000, batch loss = 0.00\n",
      "epoch 2, step 6000/55000, batch loss = 0.04\n",
      "epoch 2, step 6250/55000, batch loss = 0.03\n",
      "epoch 2, step 6500/55000, batch loss = 0.05\n",
      "epoch 2, step 6750/55000, batch loss = 0.04\n",
      "epoch 2, step 7000/55000, batch loss = 0.01\n",
      "epoch 2, step 7250/55000, batch loss = 0.03\n",
      "epoch 2, step 7500/55000, batch loss = 0.08\n",
      "Train accuracy = 98.60\n",
      "epoch 2, step 7750/55000, batch loss = 0.05\n",
      "epoch 2, step 8000/55000, batch loss = 0.09\n",
      "epoch 2, step 8250/55000, batch loss = 0.01\n",
      "epoch 2, step 8500/55000, batch loss = 0.03\n",
      "epoch 2, step 8750/55000, batch loss = 0.01\n",
      "epoch 2, step 9000/55000, batch loss = 0.00\n",
      "epoch 2, step 9250/55000, batch loss = 0.03\n",
      "epoch 2, step 9500/55000, batch loss = 0.02\n",
      "epoch 2, step 9750/55000, batch loss = 0.01\n",
      "epoch 2, step 10000/55000, batch loss = 0.02\n",
      "Train accuracy = 98.64\n",
      "epoch 2, step 10250/55000, batch loss = 0.17\n",
      "epoch 2, step 10500/55000, batch loss = 0.01\n",
      "epoch 2, step 10750/55000, batch loss = 0.05\n",
      "epoch 2, step 11000/55000, batch loss = 0.02\n",
      "epoch 2, step 11250/55000, batch loss = 0.07\n",
      "epoch 2, step 11500/55000, batch loss = 0.01\n",
      "epoch 2, step 11750/55000, batch loss = 0.02\n",
      "epoch 2, step 12000/55000, batch loss = 0.01\n",
      "epoch 2, step 12250/55000, batch loss = 0.01\n",
      "epoch 2, step 12500/55000, batch loss = 0.02\n",
      "Train accuracy = 98.68\n",
      "epoch 2, step 12750/55000, batch loss = 0.04\n",
      "epoch 2, step 13000/55000, batch loss = 0.01\n",
      "epoch 2, step 13250/55000, batch loss = 0.12\n",
      "epoch 2, step 13500/55000, batch loss = 0.00\n",
      "epoch 2, step 13750/55000, batch loss = 0.00\n",
      "epoch 2, step 14000/55000, batch loss = 0.09\n",
      "epoch 2, step 14250/55000, batch loss = 0.05\n",
      "epoch 2, step 14500/55000, batch loss = 0.01\n",
      "epoch 2, step 14750/55000, batch loss = 0.01\n",
      "epoch 2, step 15000/55000, batch loss = 0.05\n",
      "Train accuracy = 98.64\n",
      "epoch 2, step 15250/55000, batch loss = 0.00\n",
      "epoch 2, step 15500/55000, batch loss = 0.04\n",
      "epoch 2, step 15750/55000, batch loss = 0.12\n",
      "epoch 2, step 16000/55000, batch loss = 0.07\n",
      "epoch 2, step 16250/55000, batch loss = 0.01\n",
      "epoch 2, step 16500/55000, batch loss = 0.01\n",
      "epoch 2, step 16750/55000, batch loss = 0.03\n",
      "epoch 2, step 17000/55000, batch loss = 0.03\n",
      "epoch 2, step 17250/55000, batch loss = 0.06\n",
      "epoch 2, step 17500/55000, batch loss = 0.01\n",
      "Train accuracy = 98.64\n",
      "epoch 2, step 17750/55000, batch loss = 0.00\n",
      "epoch 2, step 18000/55000, batch loss = 0.04\n",
      "epoch 2, step 18250/55000, batch loss = 0.03\n",
      "epoch 2, step 18500/55000, batch loss = 0.06\n",
      "epoch 2, step 18750/55000, batch loss = 0.01\n",
      "epoch 2, step 19000/55000, batch loss = 0.00\n",
      "epoch 2, step 19250/55000, batch loss = 0.05\n",
      "epoch 2, step 19500/55000, batch loss = 0.01\n",
      "epoch 2, step 19750/55000, batch loss = 0.05\n",
      "epoch 2, step 20000/55000, batch loss = 0.14\n",
      "Train accuracy = 98.62\n",
      "epoch 2, step 20250/55000, batch loss = 0.08\n",
      "epoch 2, step 20500/55000, batch loss = 0.07\n",
      "epoch 2, step 20750/55000, batch loss = 0.06\n",
      "epoch 2, step 21000/55000, batch loss = 0.10\n",
      "epoch 2, step 21250/55000, batch loss = 0.04\n",
      "epoch 2, step 21500/55000, batch loss = 0.01\n",
      "epoch 2, step 21750/55000, batch loss = 0.07\n",
      "epoch 2, step 22000/55000, batch loss = 0.12\n",
      "epoch 2, step 22250/55000, batch loss = 0.00\n",
      "epoch 2, step 22500/55000, batch loss = 0.02\n",
      "Train accuracy = 98.59\n",
      "epoch 2, step 22750/55000, batch loss = 0.07\n",
      "epoch 2, step 23000/55000, batch loss = 0.05\n",
      "epoch 2, step 23250/55000, batch loss = 0.00\n",
      "epoch 2, step 23500/55000, batch loss = 0.05\n",
      "epoch 2, step 23750/55000, batch loss = 0.03\n",
      "epoch 2, step 24000/55000, batch loss = 0.07\n",
      "epoch 2, step 24250/55000, batch loss = 0.05\n",
      "epoch 2, step 24500/55000, batch loss = 0.01\n",
      "epoch 2, step 24750/55000, batch loss = 0.09\n",
      "epoch 2, step 25000/55000, batch loss = 0.09\n",
      "Train accuracy = 98.55\n",
      "epoch 2, step 25250/55000, batch loss = 0.01\n",
      "epoch 2, step 25500/55000, batch loss = 0.01\n",
      "epoch 2, step 25750/55000, batch loss = 0.01\n",
      "epoch 2, step 26000/55000, batch loss = 0.00\n",
      "epoch 2, step 26250/55000, batch loss = 0.07\n",
      "epoch 2, step 26500/55000, batch loss = 0.07\n",
      "epoch 2, step 26750/55000, batch loss = 0.06\n",
      "epoch 2, step 27000/55000, batch loss = 0.03\n",
      "epoch 2, step 27250/55000, batch loss = 0.06\n",
      "epoch 2, step 27500/55000, batch loss = 0.01\n",
      "Train accuracy = 98.58\n",
      "epoch 2, step 27750/55000, batch loss = 0.01\n",
      "epoch 2, step 28000/55000, batch loss = 0.03\n",
      "epoch 2, step 28250/55000, batch loss = 0.04\n",
      "epoch 2, step 28500/55000, batch loss = 0.06\n",
      "epoch 2, step 28750/55000, batch loss = 0.05\n",
      "epoch 2, step 29000/55000, batch loss = 0.01\n",
      "epoch 2, step 29250/55000, batch loss = 0.02\n",
      "epoch 2, step 29500/55000, batch loss = 0.04\n",
      "epoch 2, step 29750/55000, batch loss = 0.05\n",
      "epoch 2, step 30000/55000, batch loss = 0.03\n",
      "Train accuracy = 98.58\n",
      "epoch 2, step 30250/55000, batch loss = 0.00\n",
      "epoch 2, step 30500/55000, batch loss = 0.02\n",
      "epoch 2, step 30750/55000, batch loss = 0.00\n",
      "epoch 2, step 31000/55000, batch loss = 0.01\n",
      "epoch 2, step 31250/55000, batch loss = 0.07\n",
      "epoch 2, step 31500/55000, batch loss = 0.18\n",
      "epoch 2, step 31750/55000, batch loss = 0.01\n",
      "epoch 2, step 32000/55000, batch loss = 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, step 32250/55000, batch loss = 0.01\n",
      "epoch 2, step 32500/55000, batch loss = 0.00\n",
      "Train accuracy = 98.61\n",
      "epoch 2, step 32750/55000, batch loss = 0.02\n",
      "epoch 2, step 33000/55000, batch loss = 0.00\n",
      "epoch 2, step 33250/55000, batch loss = 0.08\n",
      "epoch 2, step 33500/55000, batch loss = 0.03\n",
      "epoch 2, step 33750/55000, batch loss = 0.02\n",
      "epoch 2, step 34000/55000, batch loss = 0.07\n",
      "epoch 2, step 34250/55000, batch loss = 0.06\n",
      "epoch 2, step 34500/55000, batch loss = 0.04\n",
      "epoch 2, step 34750/55000, batch loss = 0.03\n",
      "epoch 2, step 35000/55000, batch loss = 0.02\n",
      "Train accuracy = 98.63\n",
      "epoch 2, step 35250/55000, batch loss = 0.05\n",
      "epoch 2, step 35500/55000, batch loss = 0.04\n",
      "epoch 2, step 35750/55000, batch loss = 0.03\n",
      "epoch 2, step 36000/55000, batch loss = 0.04\n",
      "epoch 2, step 36250/55000, batch loss = 0.20\n",
      "epoch 2, step 36500/55000, batch loss = 0.06\n",
      "epoch 2, step 36750/55000, batch loss = 0.05\n",
      "epoch 2, step 37000/55000, batch loss = 0.03\n",
      "epoch 2, step 37250/55000, batch loss = 0.06\n",
      "epoch 2, step 37500/55000, batch loss = 0.02\n",
      "Train accuracy = 98.60\n",
      "epoch 2, step 37750/55000, batch loss = 0.02\n",
      "epoch 2, step 38000/55000, batch loss = 0.09\n",
      "epoch 2, step 38250/55000, batch loss = 0.07\n",
      "epoch 2, step 38500/55000, batch loss = 0.05\n",
      "epoch 2, step 38750/55000, batch loss = 0.00\n",
      "epoch 2, step 39000/55000, batch loss = 0.02\n",
      "epoch 2, step 39250/55000, batch loss = 0.00\n",
      "epoch 2, step 39500/55000, batch loss = 0.01\n",
      "epoch 2, step 39750/55000, batch loss = 0.00\n",
      "epoch 2, step 40000/55000, batch loss = 0.03\n",
      "Train accuracy = 98.61\n",
      "epoch 2, step 40250/55000, batch loss = 0.11\n",
      "epoch 2, step 40500/55000, batch loss = 0.06\n",
      "epoch 2, step 40750/55000, batch loss = 0.01\n",
      "epoch 2, step 41000/55000, batch loss = 0.07\n",
      "epoch 2, step 41250/55000, batch loss = 0.00\n",
      "epoch 2, step 41500/55000, batch loss = 0.02\n",
      "epoch 2, step 41750/55000, batch loss = 0.00\n",
      "epoch 2, step 42000/55000, batch loss = 0.10\n",
      "epoch 2, step 42250/55000, batch loss = 0.05\n",
      "epoch 2, step 42500/55000, batch loss = 0.05\n",
      "Train accuracy = 98.59\n",
      "epoch 2, step 42750/55000, batch loss = 0.10\n",
      "epoch 2, step 43000/55000, batch loss = 0.01\n",
      "epoch 2, step 43250/55000, batch loss = 0.05\n",
      "epoch 2, step 43500/55000, batch loss = 0.00\n",
      "epoch 2, step 43750/55000, batch loss = 0.09\n",
      "epoch 2, step 44000/55000, batch loss = 0.03\n",
      "epoch 2, step 44250/55000, batch loss = 0.01\n",
      "epoch 2, step 44500/55000, batch loss = 0.01\n",
      "epoch 2, step 44750/55000, batch loss = 0.04\n",
      "epoch 2, step 45000/55000, batch loss = 0.01\n",
      "Train accuracy = 98.59\n",
      "epoch 2, step 45250/55000, batch loss = 0.00\n",
      "epoch 2, step 45500/55000, batch loss = 0.06\n",
      "epoch 2, step 45750/55000, batch loss = 0.01\n",
      "epoch 2, step 46000/55000, batch loss = 0.01\n",
      "epoch 2, step 46250/55000, batch loss = 0.08\n",
      "epoch 2, step 46500/55000, batch loss = 0.06\n",
      "epoch 2, step 46750/55000, batch loss = 0.06\n",
      "epoch 2, step 47000/55000, batch loss = 0.01\n",
      "epoch 2, step 47250/55000, batch loss = 0.05\n",
      "epoch 2, step 47500/55000, batch loss = 0.11\n",
      "Train accuracy = 98.59\n",
      "epoch 2, step 47750/55000, batch loss = 0.09\n",
      "epoch 2, step 48000/55000, batch loss = 0.02\n",
      "epoch 2, step 48250/55000, batch loss = 0.01\n",
      "epoch 2, step 48500/55000, batch loss = 0.06\n",
      "epoch 2, step 48750/55000, batch loss = 0.03\n",
      "epoch 2, step 49000/55000, batch loss = 0.01\n",
      "epoch 2, step 49250/55000, batch loss = 0.01\n",
      "epoch 2, step 49500/55000, batch loss = 0.05\n",
      "epoch 2, step 49750/55000, batch loss = 0.03\n",
      "epoch 2, step 50000/55000, batch loss = 0.02\n",
      "Train accuracy = 98.59\n",
      "epoch 2, step 50250/55000, batch loss = 0.05\n",
      "epoch 2, step 50500/55000, batch loss = 0.02\n",
      "epoch 2, step 50750/55000, batch loss = 0.01\n",
      "epoch 2, step 51000/55000, batch loss = 0.00\n",
      "epoch 2, step 51250/55000, batch loss = 0.01\n",
      "epoch 2, step 51500/55000, batch loss = 0.05\n",
      "epoch 2, step 51750/55000, batch loss = 0.05\n",
      "epoch 2, step 52000/55000, batch loss = 0.00\n",
      "epoch 2, step 52250/55000, batch loss = 0.02\n",
      "epoch 2, step 52500/55000, batch loss = 0.01\n",
      "Train accuracy = 98.60\n",
      "epoch 2, step 52750/55000, batch loss = 0.01\n",
      "epoch 2, step 53000/55000, batch loss = 0.02\n",
      "epoch 2, step 53250/55000, batch loss = 0.01\n",
      "epoch 2, step 53500/55000, batch loss = 0.03\n",
      "epoch 2, step 53750/55000, batch loss = 0.05\n",
      "epoch 2, step 54000/55000, batch loss = 0.08\n",
      "epoch 2, step 54250/55000, batch loss = 0.01\n",
      "epoch 2, step 54500/55000, batch loss = 0.02\n",
      "epoch 2, step 54750/55000, batch loss = 0.00\n",
      "Train accuracy = 98.61\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 98.74\n",
      "Validation avg loss = 0.04\n",
      "\n",
      "epoch 3, step 0/55000, batch loss = 0.01\n",
      "epoch 3, step 250/55000, batch loss = 0.03\n",
      "epoch 3, step 500/55000, batch loss = 0.02\n",
      "epoch 3, step 750/55000, batch loss = 0.02\n",
      "epoch 3, step 1000/55000, batch loss = 0.04\n",
      "epoch 3, step 1250/55000, batch loss = 0.04\n",
      "epoch 3, step 1500/55000, batch loss = 0.01\n",
      "epoch 3, step 1750/55000, batch loss = 0.00\n",
      "epoch 3, step 2000/55000, batch loss = 0.01\n",
      "epoch 3, step 2250/55000, batch loss = 0.00\n",
      "epoch 3, step 2500/55000, batch loss = 0.01\n",
      "Train accuracy = 99.06\n",
      "epoch 3, step 2750/55000, batch loss = 0.03\n",
      "epoch 3, step 3000/55000, batch loss = 0.04\n",
      "epoch 3, step 3250/55000, batch loss = 0.02\n",
      "epoch 3, step 3500/55000, batch loss = 0.09\n",
      "epoch 3, step 3750/55000, batch loss = 0.01\n",
      "epoch 3, step 4000/55000, batch loss = 0.03\n",
      "epoch 3, step 4250/55000, batch loss = 0.05\n",
      "epoch 3, step 4500/55000, batch loss = 0.01\n",
      "epoch 3, step 4750/55000, batch loss = 0.01\n",
      "epoch 3, step 5000/55000, batch loss = 0.03\n",
      "Train accuracy = 99.05\n",
      "epoch 3, step 5250/55000, batch loss = 0.01\n",
      "epoch 3, step 5500/55000, batch loss = 0.07\n",
      "epoch 3, step 5750/55000, batch loss = 0.04\n",
      "epoch 3, step 6000/55000, batch loss = 0.01\n",
      "epoch 3, step 6250/55000, batch loss = 0.01\n",
      "epoch 3, step 6500/55000, batch loss = 0.02\n",
      "epoch 3, step 6750/55000, batch loss = 0.04\n",
      "epoch 3, step 7000/55000, batch loss = 0.00\n",
      "epoch 3, step 7250/55000, batch loss = 0.02\n",
      "epoch 3, step 7500/55000, batch loss = 0.00\n",
      "Train accuracy = 99.23\n",
      "epoch 3, step 7750/55000, batch loss = 0.03\n",
      "epoch 3, step 8000/55000, batch loss = 0.01\n",
      "epoch 3, step 8250/55000, batch loss = 0.02\n",
      "epoch 3, step 8500/55000, batch loss = 0.01\n",
      "epoch 3, step 8750/55000, batch loss = 0.00\n",
      "epoch 3, step 9000/55000, batch loss = 0.01\n",
      "epoch 3, step 9250/55000, batch loss = 0.00\n",
      "epoch 3, step 9500/55000, batch loss = 0.13\n",
      "epoch 3, step 9750/55000, batch loss = 0.02\n",
      "epoch 3, step 10000/55000, batch loss = 0.02\n",
      "Train accuracy = 99.34\n",
      "epoch 3, step 10250/55000, batch loss = 0.02\n",
      "epoch 3, step 10500/55000, batch loss = 0.01\n",
      "epoch 3, step 10750/55000, batch loss = 0.00\n",
      "epoch 3, step 11000/55000, batch loss = 0.00\n",
      "epoch 3, step 11250/55000, batch loss = 0.00\n",
      "epoch 3, step 11500/55000, batch loss = 0.05\n",
      "epoch 3, step 11750/55000, batch loss = 0.01\n",
      "epoch 3, step 12000/55000, batch loss = 0.01\n",
      "epoch 3, step 12250/55000, batch loss = 0.01\n",
      "epoch 3, step 12500/55000, batch loss = 0.02\n",
      "Train accuracy = 99.36\n",
      "epoch 3, step 12750/55000, batch loss = 0.01\n",
      "epoch 3, step 13000/55000, batch loss = 0.01\n",
      "epoch 3, step 13250/55000, batch loss = 0.02\n",
      "epoch 3, step 13500/55000, batch loss = 0.01\n",
      "epoch 3, step 13750/55000, batch loss = 0.01\n",
      "epoch 3, step 14000/55000, batch loss = 0.01\n",
      "epoch 3, step 14250/55000, batch loss = 0.09\n",
      "epoch 3, step 14500/55000, batch loss = 0.10\n",
      "epoch 3, step 14750/55000, batch loss = 0.03\n",
      "epoch 3, step 15000/55000, batch loss = 0.00\n",
      "Train accuracy = 99.37\n",
      "epoch 3, step 15250/55000, batch loss = 0.09\n",
      "epoch 3, step 15500/55000, batch loss = 0.01\n",
      "epoch 3, step 15750/55000, batch loss = 0.04\n",
      "epoch 3, step 16000/55000, batch loss = 0.01\n",
      "epoch 3, step 16250/55000, batch loss = 0.02\n",
      "epoch 3, step 16500/55000, batch loss = 0.00\n",
      "epoch 3, step 16750/55000, batch loss = 0.01\n",
      "epoch 3, step 17000/55000, batch loss = 0.01\n",
      "epoch 3, step 17250/55000, batch loss = 0.01\n",
      "epoch 3, step 17500/55000, batch loss = 0.00\n",
      "Train accuracy = 99.38\n",
      "epoch 3, step 17750/55000, batch loss = 0.03\n",
      "epoch 3, step 18000/55000, batch loss = 0.01\n",
      "epoch 3, step 18250/55000, batch loss = 0.01\n",
      "epoch 3, step 18500/55000, batch loss = 0.04\n",
      "epoch 3, step 18750/55000, batch loss = 0.01\n",
      "epoch 3, step 19000/55000, batch loss = 0.02\n",
      "epoch 3, step 19250/55000, batch loss = 0.00\n",
      "epoch 3, step 19500/55000, batch loss = 0.01\n",
      "epoch 3, step 19750/55000, batch loss = 0.01\n",
      "epoch 3, step 20000/55000, batch loss = 0.08\n",
      "Train accuracy = 99.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, step 20250/55000, batch loss = 0.02\n",
      "epoch 3, step 20500/55000, batch loss = 0.01\n",
      "epoch 3, step 20750/55000, batch loss = 0.03\n",
      "epoch 3, step 21000/55000, batch loss = 0.05\n",
      "epoch 3, step 21250/55000, batch loss = 0.02\n",
      "epoch 3, step 21500/55000, batch loss = 0.00\n",
      "epoch 3, step 21750/55000, batch loss = 0.11\n",
      "epoch 3, step 22000/55000, batch loss = 0.01\n",
      "epoch 3, step 22250/55000, batch loss = 0.01\n",
      "epoch 3, step 22500/55000, batch loss = 0.02\n",
      "Train accuracy = 99.37\n",
      "epoch 3, step 22750/55000, batch loss = 0.00\n",
      "epoch 3, step 23000/55000, batch loss = 0.02\n",
      "epoch 3, step 23250/55000, batch loss = 0.00\n",
      "epoch 3, step 23500/55000, batch loss = 0.01\n",
      "epoch 3, step 23750/55000, batch loss = 0.00\n",
      "epoch 3, step 24000/55000, batch loss = 0.02\n",
      "epoch 3, step 24250/55000, batch loss = 0.01\n",
      "epoch 3, step 24500/55000, batch loss = 0.02\n",
      "epoch 3, step 24750/55000, batch loss = 0.00\n",
      "epoch 3, step 25000/55000, batch loss = 0.01\n",
      "Train accuracy = 99.40\n",
      "epoch 3, step 25250/55000, batch loss = 0.01\n",
      "epoch 3, step 25500/55000, batch loss = 0.01\n",
      "epoch 3, step 25750/55000, batch loss = 0.01\n",
      "epoch 3, step 26000/55000, batch loss = 0.01\n",
      "epoch 3, step 26250/55000, batch loss = 0.18\n",
      "epoch 3, step 26500/55000, batch loss = 0.01\n",
      "epoch 3, step 26750/55000, batch loss = 0.00\n",
      "epoch 3, step 27000/55000, batch loss = 0.05\n",
      "epoch 3, step 27250/55000, batch loss = 0.14\n",
      "epoch 3, step 27500/55000, batch loss = 0.02\n",
      "Train accuracy = 99.42\n",
      "epoch 3, step 27750/55000, batch loss = 0.00\n",
      "epoch 3, step 28000/55000, batch loss = 0.01\n",
      "epoch 3, step 28250/55000, batch loss = 0.01\n",
      "epoch 3, step 28500/55000, batch loss = 0.04\n",
      "epoch 3, step 28750/55000, batch loss = 0.04\n",
      "epoch 3, step 29000/55000, batch loss = 0.08\n",
      "epoch 3, step 29250/55000, batch loss = 0.02\n",
      "epoch 3, step 29500/55000, batch loss = 0.00\n",
      "epoch 3, step 29750/55000, batch loss = 0.07\n",
      "epoch 3, step 30000/55000, batch loss = 0.01\n",
      "Train accuracy = 99.41\n",
      "epoch 3, step 30250/55000, batch loss = 0.02\n",
      "epoch 3, step 30500/55000, batch loss = 0.12\n",
      "epoch 3, step 30750/55000, batch loss = 0.02\n",
      "epoch 3, step 31000/55000, batch loss = 0.06\n",
      "epoch 3, step 31250/55000, batch loss = 0.02\n",
      "epoch 3, step 31500/55000, batch loss = 0.01\n",
      "epoch 3, step 31750/55000, batch loss = 0.07\n",
      "epoch 3, step 32000/55000, batch loss = 0.03\n",
      "epoch 3, step 32250/55000, batch loss = 0.00\n",
      "epoch 3, step 32500/55000, batch loss = 0.00\n",
      "Train accuracy = 99.41\n",
      "epoch 3, step 32750/55000, batch loss = 0.01\n",
      "epoch 3, step 33000/55000, batch loss = 0.01\n",
      "epoch 3, step 33250/55000, batch loss = 0.04\n",
      "epoch 3, step 33500/55000, batch loss = 0.00\n",
      "epoch 3, step 33750/55000, batch loss = 0.03\n",
      "epoch 3, step 34000/55000, batch loss = 0.00\n",
      "epoch 3, step 34250/55000, batch loss = 0.00\n",
      "epoch 3, step 34500/55000, batch loss = 0.01\n",
      "epoch 3, step 34750/55000, batch loss = 0.07\n",
      "epoch 3, step 35000/55000, batch loss = 0.00\n",
      "Train accuracy = 99.41\n",
      "epoch 3, step 35250/55000, batch loss = 0.00\n",
      "epoch 3, step 35500/55000, batch loss = 0.01\n",
      "epoch 3, step 35750/55000, batch loss = 0.01\n",
      "epoch 3, step 36000/55000, batch loss = 0.04\n",
      "epoch 3, step 36250/55000, batch loss = 0.01\n",
      "epoch 3, step 36500/55000, batch loss = 0.00\n",
      "epoch 3, step 36750/55000, batch loss = 0.00\n",
      "epoch 3, step 37000/55000, batch loss = 0.05\n",
      "epoch 3, step 37250/55000, batch loss = 0.01\n",
      "epoch 3, step 37500/55000, batch loss = 0.00\n",
      "Train accuracy = 99.43\n",
      "epoch 3, step 37750/55000, batch loss = 0.04\n",
      "epoch 3, step 38000/55000, batch loss = 0.08\n",
      "epoch 3, step 38250/55000, batch loss = 0.00\n",
      "epoch 3, step 38500/55000, batch loss = 0.01\n",
      "epoch 3, step 38750/55000, batch loss = 0.02\n",
      "epoch 3, step 39000/55000, batch loss = 0.00\n",
      "epoch 3, step 39250/55000, batch loss = 0.00\n",
      "epoch 3, step 39500/55000, batch loss = 0.01\n",
      "epoch 3, step 39750/55000, batch loss = 0.06\n",
      "epoch 3, step 40000/55000, batch loss = 0.03\n",
      "Train accuracy = 99.43\n",
      "epoch 3, step 40250/55000, batch loss = 0.00\n",
      "epoch 3, step 40500/55000, batch loss = 0.01\n",
      "epoch 3, step 40750/55000, batch loss = 0.06\n",
      "epoch 3, step 41000/55000, batch loss = 0.01\n",
      "epoch 3, step 41250/55000, batch loss = 0.01\n",
      "epoch 3, step 41500/55000, batch loss = 0.03\n",
      "epoch 3, step 41750/55000, batch loss = 0.00\n",
      "epoch 3, step 42000/55000, batch loss = 0.00\n",
      "epoch 3, step 42250/55000, batch loss = 0.03\n",
      "epoch 3, step 42500/55000, batch loss = 0.00\n",
      "Train accuracy = 99.42\n",
      "epoch 3, step 42750/55000, batch loss = 0.01\n",
      "epoch 3, step 43000/55000, batch loss = 0.02\n",
      "epoch 3, step 43250/55000, batch loss = 0.01\n",
      "epoch 3, step 43500/55000, batch loss = 0.00\n",
      "epoch 3, step 43750/55000, batch loss = 0.01\n",
      "epoch 3, step 44000/55000, batch loss = 0.00\n",
      "epoch 3, step 44250/55000, batch loss = 0.01\n",
      "epoch 3, step 44500/55000, batch loss = 0.01\n",
      "epoch 3, step 44750/55000, batch loss = 0.00\n",
      "epoch 3, step 45000/55000, batch loss = 0.01\n",
      "Train accuracy = 99.43\n",
      "epoch 3, step 45250/55000, batch loss = 0.01\n",
      "epoch 3, step 45500/55000, batch loss = 0.01\n",
      "epoch 3, step 45750/55000, batch loss = 0.01\n",
      "epoch 3, step 46000/55000, batch loss = 0.00\n",
      "epoch 3, step 46250/55000, batch loss = 0.01\n",
      "epoch 3, step 46500/55000, batch loss = 0.01\n",
      "epoch 3, step 46750/55000, batch loss = 0.00\n",
      "epoch 3, step 47000/55000, batch loss = 0.04\n",
      "epoch 3, step 47250/55000, batch loss = 0.01\n",
      "epoch 3, step 47500/55000, batch loss = 0.01\n",
      "Train accuracy = 99.43\n",
      "epoch 3, step 47750/55000, batch loss = 0.01\n",
      "epoch 3, step 48000/55000, batch loss = 0.00\n",
      "epoch 3, step 48250/55000, batch loss = 0.00\n",
      "epoch 3, step 48500/55000, batch loss = 0.05\n",
      "epoch 3, step 48750/55000, batch loss = 0.00\n",
      "epoch 3, step 49000/55000, batch loss = 0.00\n",
      "epoch 3, step 49250/55000, batch loss = 0.01\n",
      "epoch 3, step 49500/55000, batch loss = 0.03\n",
      "epoch 3, step 49750/55000, batch loss = 0.03\n",
      "epoch 3, step 50000/55000, batch loss = 0.01\n",
      "Train accuracy = 99.43\n",
      "epoch 3, step 50250/55000, batch loss = 0.03\n",
      "epoch 3, step 50500/55000, batch loss = 0.01\n",
      "epoch 3, step 50750/55000, batch loss = 0.00\n",
      "epoch 3, step 51000/55000, batch loss = 0.02\n",
      "epoch 3, step 51250/55000, batch loss = 0.05\n",
      "epoch 3, step 51500/55000, batch loss = 0.00\n",
      "epoch 3, step 51750/55000, batch loss = 0.02\n",
      "epoch 3, step 52000/55000, batch loss = 0.03\n",
      "epoch 3, step 52250/55000, batch loss = 0.05\n",
      "epoch 3, step 52500/55000, batch loss = 0.00\n",
      "Train accuracy = 99.44\n",
      "epoch 3, step 52750/55000, batch loss = 0.01\n",
      "epoch 3, step 53000/55000, batch loss = 0.12\n",
      "epoch 3, step 53250/55000, batch loss = 0.02\n",
      "epoch 3, step 53500/55000, batch loss = 0.00\n",
      "epoch 3, step 53750/55000, batch loss = 0.01\n",
      "epoch 3, step 54000/55000, batch loss = 0.01\n",
      "epoch 3, step 54250/55000, batch loss = 0.00\n",
      "epoch 3, step 54500/55000, batch loss = 0.02\n",
      "epoch 3, step 54750/55000, batch loss = 0.00\n",
      "Train accuracy = 99.44\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 99.16\n",
      "Validation avg loss = 0.03\n",
      "\n",
      "epoch 4, step 0/55000, batch loss = 0.00\n",
      "epoch 4, step 250/55000, batch loss = 0.01\n",
      "epoch 4, step 500/55000, batch loss = 0.02\n",
      "epoch 4, step 750/55000, batch loss = 0.00\n",
      "epoch 4, step 1000/55000, batch loss = 0.09\n",
      "epoch 4, step 1250/55000, batch loss = 0.00\n",
      "epoch 4, step 1500/55000, batch loss = 0.04\n",
      "epoch 4, step 1750/55000, batch loss = 0.01\n",
      "epoch 4, step 2000/55000, batch loss = 0.03\n",
      "epoch 4, step 2250/55000, batch loss = 0.01\n",
      "epoch 4, step 2500/55000, batch loss = 0.03\n",
      "Train accuracy = 99.37\n",
      "epoch 4, step 2750/55000, batch loss = 0.02\n",
      "epoch 4, step 3000/55000, batch loss = 0.01\n",
      "epoch 4, step 3250/55000, batch loss = 0.00\n",
      "epoch 4, step 3500/55000, batch loss = 0.00\n",
      "epoch 4, step 3750/55000, batch loss = 0.00\n",
      "epoch 4, step 4000/55000, batch loss = 0.02\n",
      "epoch 4, step 4250/55000, batch loss = 0.01\n",
      "epoch 4, step 4500/55000, batch loss = 0.01\n",
      "epoch 4, step 4750/55000, batch loss = 0.01\n",
      "epoch 4, step 5000/55000, batch loss = 0.00\n",
      "Train accuracy = 99.49\n",
      "epoch 4, step 5250/55000, batch loss = 0.01\n",
      "epoch 4, step 5500/55000, batch loss = 0.00\n",
      "epoch 4, step 5750/55000, batch loss = 0.01\n",
      "epoch 4, step 6000/55000, batch loss = 0.01\n",
      "epoch 4, step 6250/55000, batch loss = 0.00\n",
      "epoch 4, step 6500/55000, batch loss = 0.00\n",
      "epoch 4, step 6750/55000, batch loss = 0.01\n",
      "epoch 4, step 7000/55000, batch loss = 0.00\n",
      "epoch 4, step 7250/55000, batch loss = 0.00\n",
      "epoch 4, step 7500/55000, batch loss = 0.02\n",
      "Train accuracy = 99.60\n",
      "epoch 4, step 7750/55000, batch loss = 0.00\n",
      "epoch 4, step 8000/55000, batch loss = 0.04\n",
      "epoch 4, step 8250/55000, batch loss = 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, step 8500/55000, batch loss = 0.00\n",
      "epoch 4, step 8750/55000, batch loss = 0.00\n",
      "epoch 4, step 9000/55000, batch loss = 0.03\n",
      "epoch 4, step 9250/55000, batch loss = 0.01\n",
      "epoch 4, step 9500/55000, batch loss = 0.00\n",
      "epoch 4, step 9750/55000, batch loss = 0.00\n",
      "epoch 4, step 10000/55000, batch loss = 0.01\n",
      "Train accuracy = 99.53\n",
      "epoch 4, step 10250/55000, batch loss = 0.00\n",
      "epoch 4, step 10500/55000, batch loss = 0.01\n",
      "epoch 4, step 10750/55000, batch loss = 0.01\n",
      "epoch 4, step 11000/55000, batch loss = 0.00\n",
      "epoch 4, step 11250/55000, batch loss = 0.01\n",
      "epoch 4, step 11500/55000, batch loss = 0.01\n",
      "epoch 4, step 11750/55000, batch loss = 0.02\n",
      "epoch 4, step 12000/55000, batch loss = 0.00\n",
      "epoch 4, step 12250/55000, batch loss = 0.02\n",
      "epoch 4, step 12500/55000, batch loss = 0.01\n",
      "Train accuracy = 99.52\n",
      "epoch 4, step 12750/55000, batch loss = 0.00\n",
      "epoch 4, step 13000/55000, batch loss = 0.03\n",
      "epoch 4, step 13250/55000, batch loss = 0.00\n",
      "epoch 4, step 13500/55000, batch loss = 0.01\n",
      "epoch 4, step 13750/55000, batch loss = 0.04\n",
      "epoch 4, step 14000/55000, batch loss = 0.02\n",
      "epoch 4, step 14250/55000, batch loss = 0.02\n",
      "epoch 4, step 14500/55000, batch loss = 0.04\n",
      "epoch 4, step 14750/55000, batch loss = 0.00\n",
      "epoch 4, step 15000/55000, batch loss = 0.01\n",
      "Train accuracy = 99.50\n",
      "epoch 4, step 15250/55000, batch loss = 0.03\n",
      "epoch 4, step 15500/55000, batch loss = 0.00\n",
      "epoch 4, step 15750/55000, batch loss = 0.04\n",
      "epoch 4, step 16000/55000, batch loss = 0.02\n",
      "epoch 4, step 16250/55000, batch loss = 0.01\n",
      "epoch 4, step 16500/55000, batch loss = 0.00\n",
      "epoch 4, step 16750/55000, batch loss = 0.02\n",
      "epoch 4, step 17000/55000, batch loss = 0.04\n",
      "epoch 4, step 17250/55000, batch loss = 0.00\n",
      "epoch 4, step 17500/55000, batch loss = 0.02\n",
      "Train accuracy = 99.46\n",
      "epoch 4, step 17750/55000, batch loss = 0.06\n",
      "epoch 4, step 18000/55000, batch loss = 0.03\n",
      "epoch 4, step 18250/55000, batch loss = 0.03\n",
      "epoch 4, step 18500/55000, batch loss = 0.13\n",
      "epoch 4, step 18750/55000, batch loss = 0.02\n",
      "epoch 4, step 19000/55000, batch loss = 0.02\n",
      "epoch 4, step 19250/55000, batch loss = 0.00\n",
      "epoch 4, step 19500/55000, batch loss = 0.01\n",
      "epoch 4, step 19750/55000, batch loss = 0.02\n",
      "epoch 4, step 20000/55000, batch loss = 0.00\n",
      "Train accuracy = 99.46\n",
      "epoch 4, step 20250/55000, batch loss = 0.02\n",
      "epoch 4, step 20500/55000, batch loss = 0.00\n",
      "epoch 4, step 20750/55000, batch loss = 0.03\n",
      "epoch 4, step 21000/55000, batch loss = 0.00\n",
      "epoch 4, step 21250/55000, batch loss = 0.02\n",
      "epoch 4, step 21500/55000, batch loss = 0.01\n",
      "epoch 4, step 21750/55000, batch loss = 0.02\n",
      "epoch 4, step 22000/55000, batch loss = 0.00\n",
      "epoch 4, step 22250/55000, batch loss = 0.00\n",
      "epoch 4, step 22500/55000, batch loss = 0.00\n",
      "Train accuracy = 99.48\n",
      "epoch 4, step 22750/55000, batch loss = 0.00\n",
      "epoch 4, step 23000/55000, batch loss = 0.01\n",
      "epoch 4, step 23250/55000, batch loss = 0.01\n",
      "epoch 4, step 23500/55000, batch loss = 0.02\n",
      "epoch 4, step 23750/55000, batch loss = 0.05\n",
      "epoch 4, step 24000/55000, batch loss = 0.01\n",
      "epoch 4, step 24250/55000, batch loss = 0.01\n",
      "epoch 4, step 24500/55000, batch loss = 0.00\n",
      "epoch 4, step 24750/55000, batch loss = 0.04\n",
      "epoch 4, step 25000/55000, batch loss = 0.00\n",
      "Train accuracy = 99.48\n",
      "epoch 4, step 25250/55000, batch loss = 0.01\n",
      "epoch 4, step 25500/55000, batch loss = 0.00\n",
      "epoch 4, step 25750/55000, batch loss = 0.01\n",
      "epoch 4, step 26000/55000, batch loss = 0.01\n",
      "epoch 4, step 26250/55000, batch loss = 0.01\n",
      "epoch 4, step 26500/55000, batch loss = 0.02\n",
      "epoch 4, step 26750/55000, batch loss = 0.01\n",
      "epoch 4, step 27000/55000, batch loss = 0.02\n",
      "epoch 4, step 27250/55000, batch loss = 0.02\n",
      "epoch 4, step 27500/55000, batch loss = 0.00\n",
      "Train accuracy = 99.49\n",
      "epoch 4, step 27750/55000, batch loss = 0.03\n",
      "epoch 4, step 28000/55000, batch loss = 0.01\n",
      "epoch 4, step 28250/55000, batch loss = 0.01\n",
      "epoch 4, step 28500/55000, batch loss = 0.00\n",
      "epoch 4, step 28750/55000, batch loss = 0.02\n",
      "epoch 4, step 29000/55000, batch loss = 0.18\n",
      "epoch 4, step 29250/55000, batch loss = 0.00\n",
      "epoch 4, step 29500/55000, batch loss = 0.00\n",
      "epoch 4, step 29750/55000, batch loss = 0.01\n",
      "epoch 4, step 30000/55000, batch loss = 0.07\n",
      "Train accuracy = 99.48\n",
      "epoch 4, step 30250/55000, batch loss = 0.05\n",
      "epoch 4, step 30500/55000, batch loss = 0.01\n",
      "epoch 4, step 30750/55000, batch loss = 0.00\n",
      "epoch 4, step 31000/55000, batch loss = 0.03\n",
      "epoch 4, step 31250/55000, batch loss = 0.00\n",
      "epoch 4, step 31500/55000, batch loss = 0.00\n",
      "epoch 4, step 31750/55000, batch loss = 0.01\n",
      "epoch 4, step 32000/55000, batch loss = 0.00\n",
      "epoch 4, step 32250/55000, batch loss = 0.00\n",
      "epoch 4, step 32500/55000, batch loss = 0.01\n",
      "Train accuracy = 99.49\n",
      "epoch 4, step 32750/55000, batch loss = 0.00\n",
      "epoch 4, step 33000/55000, batch loss = 0.01\n",
      "epoch 4, step 33250/55000, batch loss = 0.02\n",
      "epoch 4, step 33500/55000, batch loss = 0.01\n",
      "epoch 4, step 33750/55000, batch loss = 0.01\n",
      "epoch 4, step 34000/55000, batch loss = 0.00\n",
      "epoch 4, step 34250/55000, batch loss = 0.01\n",
      "epoch 4, step 34500/55000, batch loss = 0.01\n",
      "epoch 4, step 34750/55000, batch loss = 0.00\n",
      "epoch 4, step 35000/55000, batch loss = 0.00\n",
      "Train accuracy = 99.50\n",
      "epoch 4, step 35250/55000, batch loss = 0.01\n",
      "epoch 4, step 35500/55000, batch loss = 0.00\n",
      "epoch 4, step 35750/55000, batch loss = 0.00\n",
      "epoch 4, step 36000/55000, batch loss = 0.01\n",
      "epoch 4, step 36250/55000, batch loss = 0.00\n",
      "epoch 4, step 36500/55000, batch loss = 0.01\n",
      "epoch 4, step 36750/55000, batch loss = 0.01\n",
      "epoch 4, step 37000/55000, batch loss = 0.00\n",
      "epoch 4, step 37250/55000, batch loss = 0.00\n",
      "epoch 4, step 37500/55000, batch loss = 0.08\n",
      "Train accuracy = 99.51\n",
      "epoch 4, step 37750/55000, batch loss = 0.02\n",
      "epoch 4, step 38000/55000, batch loss = 0.00\n",
      "epoch 4, step 38250/55000, batch loss = 0.01\n",
      "epoch 4, step 38500/55000, batch loss = 0.09\n",
      "epoch 4, step 38750/55000, batch loss = 0.00\n",
      "epoch 4, step 39000/55000, batch loss = 0.02\n",
      "epoch 4, step 39250/55000, batch loss = 0.00\n",
      "epoch 4, step 39500/55000, batch loss = 0.03\n",
      "epoch 4, step 39750/55000, batch loss = 0.00\n",
      "epoch 4, step 40000/55000, batch loss = 0.00\n",
      "Train accuracy = 99.51\n",
      "epoch 4, step 40250/55000, batch loss = 0.01\n",
      "epoch 4, step 40500/55000, batch loss = 0.01\n",
      "epoch 4, step 40750/55000, batch loss = 0.06\n",
      "epoch 4, step 41000/55000, batch loss = 0.07\n",
      "epoch 4, step 41250/55000, batch loss = 0.00\n",
      "epoch 4, step 41500/55000, batch loss = 0.00\n",
      "epoch 4, step 41750/55000, batch loss = 0.01\n",
      "epoch 4, step 42000/55000, batch loss = 0.14\n",
      "epoch 4, step 42250/55000, batch loss = 0.01\n",
      "epoch 4, step 42500/55000, batch loss = 0.00\n",
      "Train accuracy = 99.52\n",
      "epoch 4, step 42750/55000, batch loss = 0.03\n",
      "epoch 4, step 43000/55000, batch loss = 0.00\n",
      "epoch 4, step 43250/55000, batch loss = 0.00\n",
      "epoch 4, step 43500/55000, batch loss = 0.01\n",
      "epoch 4, step 43750/55000, batch loss = 0.03\n",
      "epoch 4, step 44000/55000, batch loss = 0.00\n",
      "epoch 4, step 44250/55000, batch loss = 0.00\n",
      "epoch 4, step 44500/55000, batch loss = 0.00\n",
      "epoch 4, step 44750/55000, batch loss = 0.05\n",
      "epoch 4, step 45000/55000, batch loss = 0.06\n",
      "Train accuracy = 99.51\n",
      "epoch 4, step 45250/55000, batch loss = 0.04\n",
      "epoch 4, step 45500/55000, batch loss = 0.01\n",
      "epoch 4, step 45750/55000, batch loss = 0.03\n",
      "epoch 4, step 46000/55000, batch loss = 0.01\n",
      "epoch 4, step 46250/55000, batch loss = 0.01\n",
      "epoch 4, step 46500/55000, batch loss = 0.01\n",
      "epoch 4, step 46750/55000, batch loss = 0.00\n",
      "epoch 4, step 47000/55000, batch loss = 0.00\n",
      "epoch 4, step 47250/55000, batch loss = 0.01\n",
      "epoch 4, step 47500/55000, batch loss = 0.01\n",
      "Train accuracy = 99.53\n",
      "epoch 4, step 47750/55000, batch loss = 0.09\n",
      "epoch 4, step 48000/55000, batch loss = 0.00\n",
      "epoch 4, step 48250/55000, batch loss = 0.00\n",
      "epoch 4, step 48500/55000, batch loss = 0.02\n",
      "epoch 4, step 48750/55000, batch loss = 0.00\n",
      "epoch 4, step 49000/55000, batch loss = 0.02\n",
      "epoch 4, step 49250/55000, batch loss = 0.00\n",
      "epoch 4, step 49500/55000, batch loss = 0.00\n",
      "epoch 4, step 49750/55000, batch loss = 0.01\n",
      "epoch 4, step 50000/55000, batch loss = 0.02\n",
      "Train accuracy = 99.52\n",
      "epoch 4, step 50250/55000, batch loss = 0.00\n",
      "epoch 4, step 50500/55000, batch loss = 0.03\n",
      "epoch 4, step 50750/55000, batch loss = 0.01\n",
      "epoch 4, step 51000/55000, batch loss = 0.04\n",
      "epoch 4, step 51250/55000, batch loss = 0.06\n",
      "epoch 4, step 51500/55000, batch loss = 0.01\n",
      "epoch 4, step 51750/55000, batch loss = 0.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, step 52000/55000, batch loss = 0.08\n",
      "epoch 4, step 52250/55000, batch loss = 0.01\n",
      "epoch 4, step 52500/55000, batch loss = 0.00\n",
      "Train accuracy = 99.52\n",
      "epoch 4, step 52750/55000, batch loss = 0.05\n",
      "epoch 4, step 53000/55000, batch loss = 0.01\n",
      "epoch 4, step 53250/55000, batch loss = 0.01\n",
      "epoch 4, step 53500/55000, batch loss = 0.05\n",
      "epoch 4, step 53750/55000, batch loss = 0.00\n",
      "epoch 4, step 54000/55000, batch loss = 0.00\n",
      "epoch 4, step 54250/55000, batch loss = 0.00\n",
      "epoch 4, step 54500/55000, batch loss = 0.00\n",
      "epoch 4, step 54750/55000, batch loss = 0.04\n",
      "Train accuracy = 99.51\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 99.18\n",
      "Validation avg loss = 0.03\n",
      "\n",
      "epoch 5, step 0/55000, batch loss = 0.00\n",
      "epoch 5, step 250/55000, batch loss = 0.00\n",
      "epoch 5, step 500/55000, batch loss = 0.02\n",
      "epoch 5, step 750/55000, batch loss = 0.00\n",
      "epoch 5, step 1000/55000, batch loss = 0.01\n",
      "epoch 5, step 1250/55000, batch loss = 0.00\n",
      "epoch 5, step 1500/55000, batch loss = 0.03\n",
      "epoch 5, step 1750/55000, batch loss = 0.01\n",
      "epoch 5, step 2000/55000, batch loss = 0.00\n",
      "epoch 5, step 2250/55000, batch loss = 0.00\n",
      "epoch 5, step 2500/55000, batch loss = 0.00\n",
      "Train accuracy = 99.73\n",
      "epoch 5, step 2750/55000, batch loss = 0.01\n",
      "epoch 5, step 3000/55000, batch loss = 0.00\n",
      "epoch 5, step 3250/55000, batch loss = 0.01\n",
      "epoch 5, step 3500/55000, batch loss = 0.01\n",
      "epoch 5, step 3750/55000, batch loss = 0.00\n",
      "epoch 5, step 4000/55000, batch loss = 0.02\n",
      "epoch 5, step 4250/55000, batch loss = 0.00\n",
      "epoch 5, step 4500/55000, batch loss = 0.01\n",
      "epoch 5, step 4750/55000, batch loss = 0.02\n",
      "epoch 5, step 5000/55000, batch loss = 0.00\n",
      "Train accuracy = 99.68\n",
      "epoch 5, step 5250/55000, batch loss = 0.00\n",
      "epoch 5, step 5500/55000, batch loss = 0.04\n",
      "epoch 5, step 5750/55000, batch loss = 0.03\n",
      "epoch 5, step 6000/55000, batch loss = 0.02\n",
      "epoch 5, step 6250/55000, batch loss = 0.01\n",
      "epoch 5, step 6500/55000, batch loss = 0.00\n",
      "epoch 5, step 6750/55000, batch loss = 0.01\n",
      "epoch 5, step 7000/55000, batch loss = 0.00\n",
      "epoch 5, step 7250/55000, batch loss = 0.01\n",
      "epoch 5, step 7500/55000, batch loss = 0.03\n",
      "Train accuracy = 99.64\n",
      "epoch 5, step 7750/55000, batch loss = 0.03\n",
      "epoch 5, step 8000/55000, batch loss = 0.03\n",
      "epoch 5, step 8250/55000, batch loss = 0.00\n",
      "epoch 5, step 8500/55000, batch loss = 0.00\n",
      "epoch 5, step 8750/55000, batch loss = 0.00\n",
      "epoch 5, step 9000/55000, batch loss = 0.01\n",
      "epoch 5, step 9250/55000, batch loss = 0.00\n",
      "epoch 5, step 9500/55000, batch loss = 0.01\n",
      "epoch 5, step 9750/55000, batch loss = 0.11\n",
      "epoch 5, step 10000/55000, batch loss = 0.02\n",
      "Train accuracy = 99.66\n",
      "epoch 5, step 10250/55000, batch loss = 0.00\n",
      "epoch 5, step 10500/55000, batch loss = 0.01\n",
      "epoch 5, step 10750/55000, batch loss = 0.01\n",
      "epoch 5, step 11000/55000, batch loss = 0.00\n",
      "epoch 5, step 11250/55000, batch loss = 0.02\n",
      "epoch 5, step 11500/55000, batch loss = 0.01\n",
      "epoch 5, step 11750/55000, batch loss = 0.00\n",
      "epoch 5, step 12000/55000, batch loss = 0.02\n",
      "epoch 5, step 12250/55000, batch loss = 0.01\n",
      "epoch 5, step 12500/55000, batch loss = 0.00\n",
      "Train accuracy = 99.70\n",
      "epoch 5, step 12750/55000, batch loss = 0.01\n",
      "epoch 5, step 13000/55000, batch loss = 0.00\n",
      "epoch 5, step 13250/55000, batch loss = 0.00\n",
      "epoch 5, step 13500/55000, batch loss = 0.00\n",
      "epoch 5, step 13750/55000, batch loss = 0.00\n",
      "epoch 5, step 14000/55000, batch loss = 0.00\n",
      "epoch 5, step 14250/55000, batch loss = 0.00\n",
      "epoch 5, step 14500/55000, batch loss = 0.00\n",
      "epoch 5, step 14750/55000, batch loss = 0.00\n",
      "epoch 5, step 15000/55000, batch loss = 0.02\n",
      "Train accuracy = 99.67\n",
      "epoch 5, step 15250/55000, batch loss = 0.01\n",
      "epoch 5, step 15500/55000, batch loss = 0.01\n",
      "epoch 5, step 15750/55000, batch loss = 0.00\n",
      "epoch 5, step 16000/55000, batch loss = 0.01\n",
      "epoch 5, step 16250/55000, batch loss = 0.02\n",
      "epoch 5, step 16500/55000, batch loss = 0.02\n",
      "epoch 5, step 16750/55000, batch loss = 0.02\n",
      "epoch 5, step 17000/55000, batch loss = 0.00\n",
      "epoch 5, step 17250/55000, batch loss = 0.00\n",
      "epoch 5, step 17500/55000, batch loss = 0.00\n",
      "Train accuracy = 99.66\n",
      "epoch 5, step 17750/55000, batch loss = 0.02\n",
      "epoch 5, step 18000/55000, batch loss = 0.00\n",
      "epoch 5, step 18250/55000, batch loss = 0.04\n",
      "epoch 5, step 18500/55000, batch loss = 0.03\n",
      "epoch 5, step 18750/55000, batch loss = 0.01\n",
      "epoch 5, step 19000/55000, batch loss = 0.02\n",
      "epoch 5, step 19250/55000, batch loss = 0.01\n",
      "epoch 5, step 19500/55000, batch loss = 0.00\n",
      "epoch 5, step 19750/55000, batch loss = 0.06\n",
      "epoch 5, step 20000/55000, batch loss = 0.02\n",
      "Train accuracy = 99.63\n",
      "epoch 5, step 20250/55000, batch loss = 0.04\n",
      "epoch 5, step 20500/55000, batch loss = 0.00\n",
      "epoch 5, step 20750/55000, batch loss = 0.05\n",
      "epoch 5, step 21000/55000, batch loss = 0.01\n",
      "epoch 5, step 21250/55000, batch loss = 0.01\n",
      "epoch 5, step 21500/55000, batch loss = 0.03\n",
      "epoch 5, step 21750/55000, batch loss = 0.02\n",
      "epoch 5, step 22000/55000, batch loss = 0.00\n",
      "epoch 5, step 22250/55000, batch loss = 0.06\n",
      "epoch 5, step 22500/55000, batch loss = 0.03\n",
      "Train accuracy = 99.57\n",
      "epoch 5, step 22750/55000, batch loss = 0.00\n",
      "epoch 5, step 23000/55000, batch loss = 0.05\n",
      "epoch 5, step 23250/55000, batch loss = 0.01\n",
      "epoch 5, step 23500/55000, batch loss = 0.02\n",
      "epoch 5, step 23750/55000, batch loss = 0.00\n",
      "epoch 5, step 24000/55000, batch loss = 0.06\n",
      "epoch 5, step 24250/55000, batch loss = 0.00\n",
      "epoch 5, step 24500/55000, batch loss = 0.00\n",
      "epoch 5, step 24750/55000, batch loss = 0.00\n",
      "epoch 5, step 25000/55000, batch loss = 0.04\n",
      "Train accuracy = 99.59\n",
      "epoch 5, step 25250/55000, batch loss = 0.00\n",
      "epoch 5, step 25500/55000, batch loss = 0.01\n",
      "epoch 5, step 25750/55000, batch loss = 0.00\n",
      "epoch 5, step 26000/55000, batch loss = 0.00\n",
      "epoch 5, step 26250/55000, batch loss = 0.00\n",
      "epoch 5, step 26500/55000, batch loss = 0.00\n",
      "epoch 5, step 26750/55000, batch loss = 0.00\n",
      "epoch 5, step 27000/55000, batch loss = 0.02\n",
      "epoch 5, step 27250/55000, batch loss = 0.01\n",
      "epoch 5, step 27500/55000, batch loss = 0.01\n",
      "Train accuracy = 99.60\n",
      "epoch 5, step 27750/55000, batch loss = 0.01\n",
      "epoch 5, step 28000/55000, batch loss = 0.04\n",
      "epoch 5, step 28250/55000, batch loss = 0.00\n",
      "epoch 5, step 28500/55000, batch loss = 0.02\n",
      "epoch 5, step 28750/55000, batch loss = 0.01\n",
      "epoch 5, step 29000/55000, batch loss = 0.00\n",
      "epoch 5, step 29250/55000, batch loss = 0.00\n",
      "epoch 5, step 29500/55000, batch loss = 0.01\n",
      "epoch 5, step 29750/55000, batch loss = 0.00\n",
      "epoch 5, step 30000/55000, batch loss = 0.01\n",
      "Train accuracy = 99.60\n",
      "epoch 5, step 30250/55000, batch loss = 0.00\n",
      "epoch 5, step 30500/55000, batch loss = 0.01\n",
      "epoch 5, step 30750/55000, batch loss = 0.01\n",
      "epoch 5, step 31000/55000, batch loss = 0.00\n",
      "epoch 5, step 31250/55000, batch loss = 0.04\n",
      "epoch 5, step 31500/55000, batch loss = 0.00\n",
      "epoch 5, step 31750/55000, batch loss = 0.03\n",
      "epoch 5, step 32000/55000, batch loss = 0.02\n",
      "epoch 5, step 32250/55000, batch loss = 0.12\n",
      "epoch 5, step 32500/55000, batch loss = 0.00\n",
      "Train accuracy = 99.61\n",
      "epoch 5, step 32750/55000, batch loss = 0.01\n",
      "epoch 5, step 33000/55000, batch loss = 0.00\n",
      "epoch 5, step 33250/55000, batch loss = 0.01\n",
      "epoch 5, step 33500/55000, batch loss = 0.01\n",
      "epoch 5, step 33750/55000, batch loss = 0.01\n",
      "epoch 5, step 34000/55000, batch loss = 0.02\n",
      "epoch 5, step 34250/55000, batch loss = 0.00\n",
      "epoch 5, step 34500/55000, batch loss = 0.01\n",
      "epoch 5, step 34750/55000, batch loss = 0.00\n",
      "epoch 5, step 35000/55000, batch loss = 0.01\n",
      "Train accuracy = 99.62\n",
      "epoch 5, step 35250/55000, batch loss = 0.01\n",
      "epoch 5, step 35500/55000, batch loss = 0.00\n",
      "epoch 5, step 35750/55000, batch loss = 0.00\n",
      "epoch 5, step 36000/55000, batch loss = 0.01\n",
      "epoch 5, step 36250/55000, batch loss = 0.00\n",
      "epoch 5, step 36500/55000, batch loss = 0.00\n",
      "epoch 5, step 36750/55000, batch loss = 0.00\n",
      "epoch 5, step 37000/55000, batch loss = 0.00\n",
      "epoch 5, step 37250/55000, batch loss = 0.02\n",
      "epoch 5, step 37500/55000, batch loss = 0.00\n",
      "Train accuracy = 99.61\n",
      "epoch 5, step 37750/55000, batch loss = 0.00\n",
      "epoch 5, step 38000/55000, batch loss = 0.02\n",
      "epoch 5, step 38250/55000, batch loss = 0.00\n",
      "epoch 5, step 38500/55000, batch loss = 0.00\n",
      "epoch 5, step 38750/55000, batch loss = 0.00\n",
      "epoch 5, step 39000/55000, batch loss = 0.00\n",
      "epoch 5, step 39250/55000, batch loss = 0.05\n",
      "epoch 5, step 39500/55000, batch loss = 0.02\n",
      "epoch 5, step 39750/55000, batch loss = 0.02\n",
      "epoch 5, step 40000/55000, batch loss = 0.01\n",
      "Train accuracy = 99.61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, step 40250/55000, batch loss = 0.00\n",
      "epoch 5, step 40500/55000, batch loss = 0.02\n",
      "epoch 5, step 40750/55000, batch loss = 0.01\n",
      "epoch 5, step 41000/55000, batch loss = 0.01\n",
      "epoch 5, step 41250/55000, batch loss = 0.04\n",
      "epoch 5, step 41500/55000, batch loss = 0.01\n",
      "epoch 5, step 41750/55000, batch loss = 0.01\n",
      "epoch 5, step 42000/55000, batch loss = 0.00\n",
      "epoch 5, step 42250/55000, batch loss = 0.19\n",
      "epoch 5, step 42500/55000, batch loss = 0.00\n",
      "Train accuracy = 99.61\n",
      "epoch 5, step 42750/55000, batch loss = 0.01\n",
      "epoch 5, step 43000/55000, batch loss = 0.01\n",
      "epoch 5, step 43250/55000, batch loss = 0.17\n",
      "epoch 5, step 43500/55000, batch loss = 0.11\n",
      "epoch 5, step 43750/55000, batch loss = 0.00\n",
      "epoch 5, step 44000/55000, batch loss = 0.05\n",
      "epoch 5, step 44250/55000, batch loss = 0.00\n",
      "epoch 5, step 44500/55000, batch loss = 0.02\n",
      "epoch 5, step 44750/55000, batch loss = 0.00\n",
      "epoch 5, step 45000/55000, batch loss = 0.01\n",
      "Train accuracy = 99.60\n",
      "epoch 5, step 45250/55000, batch loss = 0.00\n",
      "epoch 5, step 45500/55000, batch loss = 0.03\n",
      "epoch 5, step 45750/55000, batch loss = 0.03\n",
      "epoch 5, step 46000/55000, batch loss = 0.00\n",
      "epoch 5, step 46250/55000, batch loss = 0.01\n",
      "epoch 5, step 46500/55000, batch loss = 0.11\n",
      "epoch 5, step 46750/55000, batch loss = 0.00\n",
      "epoch 5, step 47000/55000, batch loss = 0.00\n",
      "epoch 5, step 47250/55000, batch loss = 0.03\n",
      "epoch 5, step 47500/55000, batch loss = 0.00\n",
      "Train accuracy = 99.60\n",
      "epoch 5, step 47750/55000, batch loss = 0.00\n",
      "epoch 5, step 48000/55000, batch loss = 0.01\n",
      "epoch 5, step 48250/55000, batch loss = 0.00\n",
      "epoch 5, step 48500/55000, batch loss = 0.00\n",
      "epoch 5, step 48750/55000, batch loss = 0.00\n",
      "epoch 5, step 49000/55000, batch loss = 0.00\n",
      "epoch 5, step 49250/55000, batch loss = 0.03\n",
      "epoch 5, step 49500/55000, batch loss = 0.07\n",
      "epoch 5, step 49750/55000, batch loss = 0.01\n",
      "epoch 5, step 50000/55000, batch loss = 0.02\n",
      "Train accuracy = 99.60\n",
      "epoch 5, step 50250/55000, batch loss = 0.00\n",
      "epoch 5, step 50500/55000, batch loss = 0.03\n",
      "epoch 5, step 50750/55000, batch loss = 0.00\n",
      "epoch 5, step 51000/55000, batch loss = 0.01\n",
      "epoch 5, step 51250/55000, batch loss = 0.00\n",
      "epoch 5, step 51500/55000, batch loss = 0.10\n",
      "epoch 5, step 51750/55000, batch loss = 0.15\n",
      "epoch 5, step 52000/55000, batch loss = 0.02\n",
      "epoch 5, step 52250/55000, batch loss = 0.01\n",
      "epoch 5, step 52500/55000, batch loss = 0.03\n",
      "Train accuracy = 99.60\n",
      "epoch 5, step 52750/55000, batch loss = 0.00\n",
      "epoch 5, step 53000/55000, batch loss = 0.00\n",
      "epoch 5, step 53250/55000, batch loss = 0.04\n",
      "epoch 5, step 53500/55000, batch loss = 0.02\n",
      "epoch 5, step 53750/55000, batch loss = 0.07\n",
      "epoch 5, step 54000/55000, batch loss = 0.00\n",
      "epoch 5, step 54250/55000, batch loss = 0.00\n",
      "epoch 5, step 54500/55000, batch loss = 0.01\n",
      "epoch 5, step 54750/55000, batch loss = 0.07\n",
      "Train accuracy = 99.59\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 99.20\n",
      "Validation avg loss = 0.03\n",
      "\n",
      "epoch 6, step 0/55000, batch loss = 0.04\n",
      "epoch 6, step 250/55000, batch loss = 0.00\n",
      "epoch 6, step 500/55000, batch loss = 0.00\n",
      "epoch 6, step 750/55000, batch loss = 0.01\n",
      "epoch 6, step 1000/55000, batch loss = 0.00\n",
      "epoch 6, step 1250/55000, batch loss = 0.00\n",
      "epoch 6, step 1500/55000, batch loss = 0.00\n",
      "epoch 6, step 1750/55000, batch loss = 0.02\n",
      "epoch 6, step 2000/55000, batch loss = 0.01\n",
      "epoch 6, step 2250/55000, batch loss = 0.01\n",
      "epoch 6, step 2500/55000, batch loss = 0.04\n",
      "Train accuracy = 99.37\n",
      "epoch 6, step 2750/55000, batch loss = 0.01\n",
      "epoch 6, step 3000/55000, batch loss = 0.00\n",
      "epoch 6, step 3250/55000, batch loss = 0.05\n",
      "epoch 6, step 3500/55000, batch loss = 0.00\n",
      "epoch 6, step 3750/55000, batch loss = 0.00\n",
      "epoch 6, step 4000/55000, batch loss = 0.04\n",
      "epoch 6, step 4250/55000, batch loss = 0.00\n",
      "epoch 6, step 4500/55000, batch loss = 0.00\n",
      "epoch 6, step 4750/55000, batch loss = 0.00\n",
      "epoch 6, step 5000/55000, batch loss = 0.03\n",
      "Train accuracy = 99.47\n",
      "epoch 6, step 5250/55000, batch loss = 0.00\n",
      "epoch 6, step 5500/55000, batch loss = 0.00\n",
      "epoch 6, step 5750/55000, batch loss = 0.00\n",
      "epoch 6, step 6000/55000, batch loss = 0.01\n",
      "epoch 6, step 6250/55000, batch loss = 0.00\n",
      "epoch 6, step 6500/55000, batch loss = 0.01\n",
      "epoch 6, step 6750/55000, batch loss = 0.05\n",
      "epoch 6, step 7000/55000, batch loss = 0.01\n",
      "epoch 6, step 7250/55000, batch loss = 0.01\n",
      "epoch 6, step 7500/55000, batch loss = 0.00\n",
      "Train accuracy = 99.48\n",
      "epoch 6, step 7750/55000, batch loss = 0.00\n",
      "epoch 6, step 8000/55000, batch loss = 0.00\n",
      "epoch 6, step 8250/55000, batch loss = 0.01\n",
      "epoch 6, step 8500/55000, batch loss = 0.04\n",
      "epoch 6, step 8750/55000, batch loss = 0.01\n",
      "epoch 6, step 9000/55000, batch loss = 0.01\n",
      "epoch 6, step 9250/55000, batch loss = 0.00\n",
      "epoch 6, step 9500/55000, batch loss = 0.18\n",
      "epoch 6, step 9750/55000, batch loss = 0.00\n",
      "epoch 6, step 10000/55000, batch loss = 0.02\n",
      "Train accuracy = 99.48\n",
      "epoch 6, step 10250/55000, batch loss = 0.00\n",
      "epoch 6, step 10500/55000, batch loss = 0.00\n",
      "epoch 6, step 10750/55000, batch loss = 0.00\n",
      "epoch 6, step 11000/55000, batch loss = 0.00\n",
      "epoch 6, step 11250/55000, batch loss = 0.01\n",
      "epoch 6, step 11500/55000, batch loss = 0.00\n",
      "epoch 6, step 11750/55000, batch loss = 0.00\n",
      "epoch 6, step 12000/55000, batch loss = 0.01\n",
      "epoch 6, step 12250/55000, batch loss = 0.08\n",
      "epoch 6, step 12500/55000, batch loss = 0.00\n",
      "Train accuracy = 99.50\n",
      "epoch 6, step 12750/55000, batch loss = 0.04\n",
      "epoch 6, step 13000/55000, batch loss = 0.01\n",
      "epoch 6, step 13250/55000, batch loss = 0.02\n",
      "epoch 6, step 13500/55000, batch loss = 0.00\n",
      "epoch 6, step 13750/55000, batch loss = 0.01\n",
      "epoch 6, step 14000/55000, batch loss = 0.03\n",
      "epoch 6, step 14250/55000, batch loss = 0.03\n",
      "epoch 6, step 14500/55000, batch loss = 0.01\n",
      "epoch 6, step 14750/55000, batch loss = 0.01\n",
      "epoch 6, step 15000/55000, batch loss = 0.00\n",
      "Train accuracy = 99.50\n",
      "epoch 6, step 15250/55000, batch loss = 0.01\n",
      "epoch 6, step 15500/55000, batch loss = 0.00\n",
      "epoch 6, step 15750/55000, batch loss = 0.02\n",
      "epoch 6, step 16000/55000, batch loss = 0.02\n",
      "epoch 6, step 16250/55000, batch loss = 0.01\n",
      "epoch 6, step 16500/55000, batch loss = 0.00\n",
      "epoch 6, step 16750/55000, batch loss = 0.05\n",
      "epoch 6, step 17000/55000, batch loss = 0.02\n",
      "epoch 6, step 17250/55000, batch loss = 0.01\n",
      "epoch 6, step 17500/55000, batch loss = 0.00\n",
      "Train accuracy = 99.49\n",
      "epoch 6, step 17750/55000, batch loss = 0.01\n",
      "epoch 6, step 18000/55000, batch loss = 0.01\n",
      "epoch 6, step 18250/55000, batch loss = 0.00\n",
      "epoch 6, step 18500/55000, batch loss = 0.06\n",
      "epoch 6, step 18750/55000, batch loss = 0.00\n",
      "epoch 6, step 19000/55000, batch loss = 0.01\n",
      "epoch 6, step 19250/55000, batch loss = 0.01\n",
      "epoch 6, step 19500/55000, batch loss = 0.00\n",
      "epoch 6, step 19750/55000, batch loss = 0.01\n",
      "epoch 6, step 20000/55000, batch loss = 0.01\n",
      "Train accuracy = 99.52\n",
      "epoch 6, step 20250/55000, batch loss = 0.01\n",
      "epoch 6, step 20500/55000, batch loss = 0.02\n",
      "epoch 6, step 20750/55000, batch loss = 0.01\n",
      "epoch 6, step 21000/55000, batch loss = 0.00\n",
      "epoch 6, step 21250/55000, batch loss = 0.07\n",
      "epoch 6, step 21500/55000, batch loss = 0.02\n",
      "epoch 6, step 21750/55000, batch loss = 0.00\n",
      "epoch 6, step 22000/55000, batch loss = 0.01\n",
      "epoch 6, step 22250/55000, batch loss = 0.01\n",
      "epoch 6, step 22500/55000, batch loss = 0.03\n",
      "Train accuracy = 99.51\n",
      "epoch 6, step 22750/55000, batch loss = 0.00\n",
      "epoch 6, step 23000/55000, batch loss = 0.00\n",
      "epoch 6, step 23250/55000, batch loss = 0.01\n",
      "epoch 6, step 23500/55000, batch loss = 0.00\n",
      "epoch 6, step 23750/55000, batch loss = 0.01\n",
      "epoch 6, step 24000/55000, batch loss = 0.07\n",
      "epoch 6, step 24250/55000, batch loss = 0.00\n",
      "epoch 6, step 24500/55000, batch loss = 0.01\n",
      "epoch 6, step 24750/55000, batch loss = 0.02\n",
      "epoch 6, step 25000/55000, batch loss = 0.01\n",
      "Train accuracy = 99.52\n",
      "epoch 6, step 25250/55000, batch loss = 0.00\n",
      "epoch 6, step 25500/55000, batch loss = 0.01\n",
      "epoch 6, step 25750/55000, batch loss = 0.00\n",
      "epoch 6, step 26000/55000, batch loss = 0.04\n",
      "epoch 6, step 26250/55000, batch loss = 0.00\n",
      "epoch 6, step 26500/55000, batch loss = 0.00\n",
      "epoch 6, step 26750/55000, batch loss = 0.01\n",
      "epoch 6, step 27000/55000, batch loss = 0.00\n",
      "epoch 6, step 27250/55000, batch loss = 0.02\n",
      "epoch 6, step 27500/55000, batch loss = 0.05\n",
      "Train accuracy = 99.52\n",
      "epoch 6, step 27750/55000, batch loss = 0.00\n",
      "epoch 6, step 28000/55000, batch loss = 0.01\n",
      "epoch 6, step 28250/55000, batch loss = 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6, step 28500/55000, batch loss = 0.00\n",
      "epoch 6, step 28750/55000, batch loss = 0.03\n",
      "epoch 6, step 29000/55000, batch loss = 0.01\n",
      "epoch 6, step 29250/55000, batch loss = 0.00\n",
      "epoch 6, step 29500/55000, batch loss = 0.02\n",
      "epoch 6, step 29750/55000, batch loss = 0.00\n",
      "epoch 6, step 30000/55000, batch loss = 0.02\n",
      "Train accuracy = 99.53\n",
      "epoch 6, step 30250/55000, batch loss = 0.01\n",
      "epoch 6, step 30500/55000, batch loss = 0.00\n",
      "epoch 6, step 30750/55000, batch loss = 0.02\n",
      "epoch 6, step 31000/55000, batch loss = 0.03\n",
      "epoch 6, step 31250/55000, batch loss = 0.01\n",
      "epoch 6, step 31500/55000, batch loss = 0.05\n",
      "epoch 6, step 31750/55000, batch loss = 0.00\n",
      "epoch 6, step 32000/55000, batch loss = 0.01\n",
      "epoch 6, step 32250/55000, batch loss = 0.00\n",
      "epoch 6, step 32500/55000, batch loss = 0.00\n",
      "Train accuracy = 99.54\n",
      "epoch 6, step 32750/55000, batch loss = 0.01\n",
      "epoch 6, step 33000/55000, batch loss = 0.01\n",
      "epoch 6, step 33250/55000, batch loss = 0.00\n",
      "epoch 6, step 33500/55000, batch loss = 0.03\n",
      "epoch 6, step 33750/55000, batch loss = 0.00\n",
      "epoch 6, step 34000/55000, batch loss = 0.00\n",
      "epoch 6, step 34250/55000, batch loss = 0.01\n",
      "epoch 6, step 34500/55000, batch loss = 0.02\n",
      "epoch 6, step 34750/55000, batch loss = 0.00\n",
      "epoch 6, step 35000/55000, batch loss = 0.00\n",
      "Train accuracy = 99.56\n",
      "epoch 6, step 35250/55000, batch loss = 0.01\n",
      "epoch 6, step 35500/55000, batch loss = 0.01\n",
      "epoch 6, step 35750/55000, batch loss = 0.01\n",
      "epoch 6, step 36000/55000, batch loss = 0.02\n",
      "epoch 6, step 36250/55000, batch loss = 0.03\n",
      "epoch 6, step 36500/55000, batch loss = 0.00\n",
      "epoch 6, step 36750/55000, batch loss = 0.01\n",
      "epoch 6, step 37000/55000, batch loss = 0.03\n",
      "epoch 6, step 37250/55000, batch loss = 0.00\n",
      "epoch 6, step 37500/55000, batch loss = 0.01\n",
      "Train accuracy = 99.55\n",
      "epoch 6, step 37750/55000, batch loss = 0.02\n",
      "epoch 6, step 38000/55000, batch loss = 0.00\n",
      "epoch 6, step 38250/55000, batch loss = 0.00\n",
      "epoch 6, step 38500/55000, batch loss = 0.01\n",
      "epoch 6, step 38750/55000, batch loss = 0.00\n",
      "epoch 6, step 39000/55000, batch loss = 0.01\n",
      "epoch 6, step 39250/55000, batch loss = 0.07\n",
      "epoch 6, step 39500/55000, batch loss = 0.02\n",
      "epoch 6, step 39750/55000, batch loss = 0.01\n",
      "epoch 6, step 40000/55000, batch loss = 0.06\n",
      "Train accuracy = 99.56\n",
      "epoch 6, step 40250/55000, batch loss = 0.01\n",
      "epoch 6, step 40500/55000, batch loss = 0.03\n",
      "epoch 6, step 40750/55000, batch loss = 0.00\n",
      "epoch 6, step 41000/55000, batch loss = 0.01\n",
      "epoch 6, step 41250/55000, batch loss = 0.01\n",
      "epoch 6, step 41500/55000, batch loss = 0.00\n",
      "epoch 6, step 41750/55000, batch loss = 0.00\n",
      "epoch 6, step 42000/55000, batch loss = 0.01\n",
      "epoch 6, step 42250/55000, batch loss = 0.01\n",
      "epoch 6, step 42500/55000, batch loss = 0.03\n",
      "Train accuracy = 99.56\n",
      "epoch 6, step 42750/55000, batch loss = 0.02\n",
      "epoch 6, step 43000/55000, batch loss = 0.02\n",
      "epoch 6, step 43250/55000, batch loss = 0.01\n",
      "epoch 6, step 43500/55000, batch loss = 0.00\n",
      "epoch 6, step 43750/55000, batch loss = 0.00\n",
      "epoch 6, step 44000/55000, batch loss = 0.00\n",
      "epoch 6, step 44250/55000, batch loss = 0.00\n",
      "epoch 6, step 44500/55000, batch loss = 0.03\n",
      "epoch 6, step 44750/55000, batch loss = 0.06\n",
      "epoch 6, step 45000/55000, batch loss = 0.01\n",
      "Train accuracy = 99.56\n",
      "epoch 6, step 45250/55000, batch loss = 0.01\n",
      "epoch 6, step 45500/55000, batch loss = 0.01\n",
      "epoch 6, step 45750/55000, batch loss = 0.01\n",
      "epoch 6, step 46000/55000, batch loss = 0.01\n",
      "epoch 6, step 46250/55000, batch loss = 0.00\n",
      "epoch 6, step 46500/55000, batch loss = 0.00\n",
      "epoch 6, step 46750/55000, batch loss = 0.00\n",
      "epoch 6, step 47000/55000, batch loss = 0.00\n",
      "epoch 6, step 47250/55000, batch loss = 0.00\n",
      "epoch 6, step 47500/55000, batch loss = 0.00\n",
      "Train accuracy = 99.58\n",
      "epoch 6, step 47750/55000, batch loss = 0.01\n",
      "epoch 6, step 48000/55000, batch loss = 0.03\n",
      "epoch 6, step 48250/55000, batch loss = 0.00\n",
      "epoch 6, step 48500/55000, batch loss = 0.02\n",
      "epoch 6, step 48750/55000, batch loss = 0.01\n",
      "epoch 6, step 49000/55000, batch loss = 0.01\n",
      "epoch 6, step 49250/55000, batch loss = 0.01\n",
      "epoch 6, step 49500/55000, batch loss = 0.01\n",
      "epoch 6, step 49750/55000, batch loss = 0.00\n",
      "epoch 6, step 50000/55000, batch loss = 0.00\n",
      "Train accuracy = 99.58\n",
      "epoch 6, step 50250/55000, batch loss = 0.01\n",
      "epoch 6, step 50500/55000, batch loss = 0.03\n",
      "epoch 6, step 50750/55000, batch loss = 0.01\n",
      "epoch 6, step 51000/55000, batch loss = 0.00\n",
      "epoch 6, step 51250/55000, batch loss = 0.01\n",
      "epoch 6, step 51500/55000, batch loss = 0.00\n",
      "epoch 6, step 51750/55000, batch loss = 0.00\n",
      "epoch 6, step 52000/55000, batch loss = 0.01\n",
      "epoch 6, step 52250/55000, batch loss = 0.00\n",
      "epoch 6, step 52500/55000, batch loss = 0.01\n",
      "Train accuracy = 99.58\n",
      "epoch 6, step 52750/55000, batch loss = 0.00\n",
      "epoch 6, step 53000/55000, batch loss = 0.01\n",
      "epoch 6, step 53250/55000, batch loss = 0.01\n",
      "epoch 6, step 53500/55000, batch loss = 0.01\n",
      "epoch 6, step 53750/55000, batch loss = 0.00\n",
      "epoch 6, step 54000/55000, batch loss = 0.00\n",
      "epoch 6, step 54250/55000, batch loss = 0.02\n",
      "epoch 6, step 54500/55000, batch loss = 0.05\n",
      "epoch 6, step 54750/55000, batch loss = 0.03\n",
      "Train accuracy = 99.58\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 99.18\n",
      "Validation avg loss = 0.03\n",
      "\n",
      "epoch 7, step 0/55000, batch loss = 0.00\n",
      "epoch 7, step 250/55000, batch loss = 0.06\n",
      "epoch 7, step 500/55000, batch loss = 0.06\n",
      "epoch 7, step 750/55000, batch loss = 0.01\n",
      "epoch 7, step 1000/55000, batch loss = 0.01\n",
      "epoch 7, step 1250/55000, batch loss = 0.03\n",
      "epoch 7, step 1500/55000, batch loss = 0.00\n",
      "epoch 7, step 1750/55000, batch loss = 0.01\n",
      "epoch 7, step 2000/55000, batch loss = 0.01\n",
      "epoch 7, step 2250/55000, batch loss = 0.00\n",
      "epoch 7, step 2500/55000, batch loss = 0.00\n",
      "Train accuracy = 99.53\n",
      "epoch 7, step 2750/55000, batch loss = 0.00\n",
      "epoch 7, step 3000/55000, batch loss = 0.00\n",
      "epoch 7, step 3250/55000, batch loss = 0.00\n",
      "epoch 7, step 3500/55000, batch loss = 0.00\n",
      "epoch 7, step 3750/55000, batch loss = 0.00\n",
      "epoch 7, step 4000/55000, batch loss = 0.01\n",
      "epoch 7, step 4250/55000, batch loss = 0.00\n",
      "epoch 7, step 4500/55000, batch loss = 0.01\n",
      "epoch 7, step 4750/55000, batch loss = 0.00\n",
      "epoch 7, step 5000/55000, batch loss = 0.00\n",
      "Train accuracy = 99.54\n",
      "epoch 7, step 5250/55000, batch loss = 0.00\n",
      "epoch 7, step 5500/55000, batch loss = 0.03\n",
      "epoch 7, step 5750/55000, batch loss = 0.02\n",
      "epoch 7, step 6000/55000, batch loss = 0.01\n",
      "epoch 7, step 6250/55000, batch loss = 0.00\n",
      "epoch 7, step 6500/55000, batch loss = 0.00\n",
      "epoch 7, step 6750/55000, batch loss = 0.01\n",
      "epoch 7, step 7000/55000, batch loss = 0.03\n",
      "epoch 7, step 7250/55000, batch loss = 0.00\n",
      "epoch 7, step 7500/55000, batch loss = 0.00\n",
      "Train accuracy = 99.58\n",
      "epoch 7, step 7750/55000, batch loss = 0.02\n",
      "epoch 7, step 8000/55000, batch loss = 0.00\n",
      "epoch 7, step 8250/55000, batch loss = 0.02\n",
      "epoch 7, step 8500/55000, batch loss = 0.00\n",
      "epoch 7, step 8750/55000, batch loss = 0.00\n",
      "epoch 7, step 9000/55000, batch loss = 0.02\n",
      "epoch 7, step 9250/55000, batch loss = 0.00\n",
      "epoch 7, step 9500/55000, batch loss = 0.05\n",
      "epoch 7, step 9750/55000, batch loss = 0.05\n",
      "epoch 7, step 10000/55000, batch loss = 0.01\n",
      "Train accuracy = 99.55\n",
      "epoch 7, step 10250/55000, batch loss = 0.00\n",
      "epoch 7, step 10500/55000, batch loss = 0.02\n",
      "epoch 7, step 10750/55000, batch loss = 0.00\n",
      "epoch 7, step 11000/55000, batch loss = 0.02\n",
      "epoch 7, step 11250/55000, batch loss = 0.03\n",
      "epoch 7, step 11500/55000, batch loss = 0.00\n",
      "epoch 7, step 11750/55000, batch loss = 0.02\n",
      "epoch 7, step 12000/55000, batch loss = 0.03\n",
      "epoch 7, step 12250/55000, batch loss = 0.01\n",
      "epoch 7, step 12500/55000, batch loss = 0.01\n",
      "Train accuracy = 99.59\n",
      "epoch 7, step 12750/55000, batch loss = 0.00\n",
      "epoch 7, step 13000/55000, batch loss = 0.00\n",
      "epoch 7, step 13250/55000, batch loss = 0.00\n",
      "epoch 7, step 13500/55000, batch loss = 0.09\n",
      "epoch 7, step 13750/55000, batch loss = 0.02\n",
      "epoch 7, step 14000/55000, batch loss = 0.01\n",
      "epoch 7, step 14250/55000, batch loss = 0.01\n",
      "epoch 7, step 14500/55000, batch loss = 0.03\n",
      "epoch 7, step 14750/55000, batch loss = 0.02\n",
      "epoch 7, step 15000/55000, batch loss = 0.00\n",
      "Train accuracy = 99.59\n",
      "epoch 7, step 15250/55000, batch loss = 0.01\n",
      "epoch 7, step 15500/55000, batch loss = 0.00\n",
      "epoch 7, step 15750/55000, batch loss = 0.00\n",
      "epoch 7, step 16000/55000, batch loss = 0.00\n",
      "epoch 7, step 16250/55000, batch loss = 0.00\n",
      "epoch 7, step 16500/55000, batch loss = 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7, step 16750/55000, batch loss = 0.00\n",
      "epoch 7, step 17000/55000, batch loss = 0.00\n",
      "epoch 7, step 17250/55000, batch loss = 0.00\n",
      "epoch 7, step 17500/55000, batch loss = 0.01\n",
      "Train accuracy = 99.57\n",
      "epoch 7, step 17750/55000, batch loss = 0.01\n",
      "epoch 7, step 18000/55000, batch loss = 0.19\n",
      "epoch 7, step 18250/55000, batch loss = 0.00\n",
      "epoch 7, step 18500/55000, batch loss = 0.06\n",
      "epoch 7, step 18750/55000, batch loss = 0.00\n",
      "epoch 7, step 19000/55000, batch loss = 0.00\n",
      "epoch 7, step 19250/55000, batch loss = 0.01\n",
      "epoch 7, step 19500/55000, batch loss = 0.00\n",
      "epoch 7, step 19750/55000, batch loss = 0.00\n",
      "epoch 7, step 20000/55000, batch loss = 0.01\n",
      "Train accuracy = 99.56\n",
      "epoch 7, step 20250/55000, batch loss = 0.00\n",
      "epoch 7, step 20500/55000, batch loss = 0.01\n",
      "epoch 7, step 20750/55000, batch loss = 0.00\n",
      "epoch 7, step 21000/55000, batch loss = 0.01\n",
      "epoch 7, step 21250/55000, batch loss = 0.01\n",
      "epoch 7, step 21500/55000, batch loss = 0.04\n",
      "epoch 7, step 21750/55000, batch loss = 0.00\n",
      "epoch 7, step 22000/55000, batch loss = 0.01\n",
      "epoch 7, step 22250/55000, batch loss = 0.00\n",
      "epoch 7, step 22500/55000, batch loss = 0.00\n",
      "Train accuracy = 99.54\n",
      "epoch 7, step 22750/55000, batch loss = 0.01\n",
      "epoch 7, step 23000/55000, batch loss = 0.00\n",
      "epoch 7, step 23250/55000, batch loss = 0.00\n",
      "epoch 7, step 23500/55000, batch loss = 0.00\n",
      "epoch 7, step 23750/55000, batch loss = 0.01\n",
      "epoch 7, step 24000/55000, batch loss = 0.01\n",
      "epoch 7, step 24250/55000, batch loss = 0.00\n",
      "epoch 7, step 24500/55000, batch loss = 0.01\n",
      "epoch 7, step 24750/55000, batch loss = 0.01\n",
      "epoch 7, step 25000/55000, batch loss = 0.00\n",
      "Train accuracy = 99.57\n",
      "epoch 7, step 25250/55000, batch loss = 0.00\n",
      "epoch 7, step 25500/55000, batch loss = 0.02\n",
      "epoch 7, step 25750/55000, batch loss = 0.00\n",
      "epoch 7, step 26000/55000, batch loss = 0.00\n",
      "epoch 7, step 26250/55000, batch loss = 0.01\n",
      "epoch 7, step 26500/55000, batch loss = 0.00\n",
      "epoch 7, step 26750/55000, batch loss = 0.00\n",
      "epoch 7, step 27000/55000, batch loss = 0.00\n",
      "epoch 7, step 27250/55000, batch loss = 0.02\n",
      "epoch 7, step 27500/55000, batch loss = 0.00\n",
      "Train accuracy = 99.59\n",
      "epoch 7, step 27750/55000, batch loss = 0.04\n",
      "epoch 7, step 28000/55000, batch loss = 0.01\n",
      "epoch 7, step 28250/55000, batch loss = 0.01\n",
      "epoch 7, step 28500/55000, batch loss = 0.01\n",
      "epoch 7, step 28750/55000, batch loss = 0.00\n",
      "epoch 7, step 29000/55000, batch loss = 0.00\n",
      "epoch 7, step 29250/55000, batch loss = 0.00\n",
      "epoch 7, step 29500/55000, batch loss = 0.00\n",
      "epoch 7, step 29750/55000, batch loss = 0.01\n",
      "epoch 7, step 30000/55000, batch loss = 0.02\n",
      "Train accuracy = 99.60\n",
      "epoch 7, step 30250/55000, batch loss = 0.03\n",
      "epoch 7, step 30500/55000, batch loss = 0.00\n",
      "epoch 7, step 30750/55000, batch loss = 0.02\n",
      "epoch 7, step 31000/55000, batch loss = 0.04\n",
      "epoch 7, step 31250/55000, batch loss = 0.04\n",
      "epoch 7, step 31500/55000, batch loss = 0.00\n",
      "epoch 7, step 31750/55000, batch loss = 0.00\n",
      "epoch 7, step 32000/55000, batch loss = 0.02\n",
      "epoch 7, step 32250/55000, batch loss = 0.00\n",
      "epoch 7, step 32500/55000, batch loss = 0.02\n",
      "Train accuracy = 99.60\n",
      "epoch 7, step 32750/55000, batch loss = 0.00\n",
      "epoch 7, step 33000/55000, batch loss = 0.02\n",
      "epoch 7, step 33250/55000, batch loss = 0.01\n",
      "epoch 7, step 33500/55000, batch loss = 0.00\n",
      "epoch 7, step 33750/55000, batch loss = 0.00\n",
      "epoch 7, step 34000/55000, batch loss = 0.00\n",
      "epoch 7, step 34250/55000, batch loss = 0.02\n",
      "epoch 7, step 34500/55000, batch loss = 0.00\n",
      "epoch 7, step 34750/55000, batch loss = 0.02\n",
      "epoch 7, step 35000/55000, batch loss = 0.01\n",
      "Train accuracy = 99.58\n",
      "epoch 7, step 35250/55000, batch loss = 0.01\n",
      "epoch 7, step 35500/55000, batch loss = 0.00\n",
      "epoch 7, step 35750/55000, batch loss = 0.01\n",
      "epoch 7, step 36000/55000, batch loss = 0.01\n",
      "epoch 7, step 36250/55000, batch loss = 0.06\n",
      "epoch 7, step 36500/55000, batch loss = 0.00\n",
      "epoch 7, step 36750/55000, batch loss = 0.00\n",
      "epoch 7, step 37000/55000, batch loss = 0.02\n",
      "epoch 7, step 37250/55000, batch loss = 0.00\n",
      "epoch 7, step 37500/55000, batch loss = 0.00\n",
      "Train accuracy = 99.59\n",
      "epoch 7, step 37750/55000, batch loss = 0.00\n",
      "epoch 7, step 38000/55000, batch loss = 0.00\n",
      "epoch 7, step 38250/55000, batch loss = 0.01\n",
      "epoch 7, step 38500/55000, batch loss = 0.02\n",
      "epoch 7, step 38750/55000, batch loss = 0.01\n",
      "epoch 7, step 39000/55000, batch loss = 0.03\n",
      "epoch 7, step 39250/55000, batch loss = 0.01\n",
      "epoch 7, step 39500/55000, batch loss = 0.01\n",
      "epoch 7, step 39750/55000, batch loss = 0.09\n",
      "epoch 7, step 40000/55000, batch loss = 0.04\n",
      "Train accuracy = 99.60\n",
      "epoch 7, step 40250/55000, batch loss = 0.03\n",
      "epoch 7, step 40500/55000, batch loss = 0.02\n",
      "epoch 7, step 40750/55000, batch loss = 0.00\n",
      "epoch 7, step 41000/55000, batch loss = 0.00\n",
      "epoch 7, step 41250/55000, batch loss = 0.03\n",
      "epoch 7, step 41500/55000, batch loss = 0.01\n",
      "epoch 7, step 41750/55000, batch loss = 0.03\n",
      "epoch 7, step 42000/55000, batch loss = 0.01\n",
      "epoch 7, step 42250/55000, batch loss = 0.00\n",
      "epoch 7, step 42500/55000, batch loss = 0.01\n",
      "Train accuracy = 99.59\n",
      "epoch 7, step 42750/55000, batch loss = 0.00\n",
      "epoch 7, step 43000/55000, batch loss = 0.02\n",
      "epoch 7, step 43250/55000, batch loss = 0.02\n",
      "epoch 7, step 43500/55000, batch loss = 0.00\n",
      "epoch 7, step 43750/55000, batch loss = 0.01\n",
      "epoch 7, step 44000/55000, batch loss = 0.01\n",
      "epoch 7, step 44250/55000, batch loss = 0.01\n",
      "epoch 7, step 44500/55000, batch loss = 0.00\n",
      "epoch 7, step 44750/55000, batch loss = 0.01\n",
      "epoch 7, step 45000/55000, batch loss = 0.01\n",
      "Train accuracy = 99.60\n",
      "epoch 7, step 45250/55000, batch loss = 0.01\n",
      "epoch 7, step 45500/55000, batch loss = 0.01\n",
      "epoch 7, step 45750/55000, batch loss = 0.02\n",
      "epoch 7, step 46000/55000, batch loss = 0.00\n",
      "epoch 7, step 46250/55000, batch loss = 0.03\n",
      "epoch 7, step 46500/55000, batch loss = 0.02\n",
      "epoch 7, step 46750/55000, batch loss = 0.00\n",
      "epoch 7, step 47000/55000, batch loss = 0.01\n",
      "epoch 7, step 47250/55000, batch loss = 0.03\n",
      "epoch 7, step 47500/55000, batch loss = 0.00\n",
      "Train accuracy = 99.60\n",
      "epoch 7, step 47750/55000, batch loss = 0.00\n",
      "epoch 7, step 48000/55000, batch loss = 0.02\n",
      "epoch 7, step 48250/55000, batch loss = 0.02\n",
      "epoch 7, step 48500/55000, batch loss = 0.00\n",
      "epoch 7, step 48750/55000, batch loss = 0.08\n",
      "epoch 7, step 49000/55000, batch loss = 0.02\n",
      "epoch 7, step 49250/55000, batch loss = 0.01\n",
      "epoch 7, step 49500/55000, batch loss = 0.01\n",
      "epoch 7, step 49750/55000, batch loss = 0.01\n",
      "epoch 7, step 50000/55000, batch loss = 0.00\n",
      "Train accuracy = 99.60\n",
      "epoch 7, step 50250/55000, batch loss = 0.01\n",
      "epoch 7, step 50500/55000, batch loss = 0.04\n",
      "epoch 7, step 50750/55000, batch loss = 0.02\n",
      "epoch 7, step 51000/55000, batch loss = 0.05\n",
      "epoch 7, step 51250/55000, batch loss = 0.00\n",
      "epoch 7, step 51500/55000, batch loss = 0.01\n",
      "epoch 7, step 51750/55000, batch loss = 0.00\n",
      "epoch 7, step 52000/55000, batch loss = 0.00\n",
      "epoch 7, step 52250/55000, batch loss = 0.00\n",
      "epoch 7, step 52500/55000, batch loss = 0.01\n",
      "Train accuracy = 99.60\n",
      "epoch 7, step 52750/55000, batch loss = 0.00\n",
      "epoch 7, step 53000/55000, batch loss = 0.01\n",
      "epoch 7, step 53250/55000, batch loss = 0.01\n",
      "epoch 7, step 53500/55000, batch loss = 0.06\n",
      "epoch 7, step 53750/55000, batch loss = 0.00\n",
      "epoch 7, step 54000/55000, batch loss = 0.01\n",
      "epoch 7, step 54250/55000, batch loss = 0.01\n",
      "epoch 7, step 54500/55000, batch loss = 0.00\n",
      "epoch 7, step 54750/55000, batch loss = 0.00\n",
      "Train accuracy = 99.60\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 99.18\n",
      "Validation avg loss = 0.03\n",
      "\n",
      "epoch 8, step 0/55000, batch loss = 0.01\n",
      "epoch 8, step 250/55000, batch loss = 0.02\n",
      "epoch 8, step 500/55000, batch loss = 0.03\n",
      "epoch 8, step 750/55000, batch loss = 0.01\n",
      "epoch 8, step 1000/55000, batch loss = 0.01\n",
      "epoch 8, step 1250/55000, batch loss = 0.00\n",
      "epoch 8, step 1500/55000, batch loss = 0.00\n",
      "epoch 8, step 1750/55000, batch loss = 0.02\n",
      "epoch 8, step 2000/55000, batch loss = 0.01\n",
      "epoch 8, step 2250/55000, batch loss = 0.01\n",
      "epoch 8, step 2500/55000, batch loss = 0.01\n",
      "Train accuracy = 99.57\n",
      "epoch 8, step 2750/55000, batch loss = 0.01\n",
      "epoch 8, step 3000/55000, batch loss = 0.01\n",
      "epoch 8, step 3250/55000, batch loss = 0.00\n",
      "epoch 8, step 3500/55000, batch loss = 0.00\n",
      "epoch 8, step 3750/55000, batch loss = 0.01\n",
      "epoch 8, step 4000/55000, batch loss = 0.01\n",
      "epoch 8, step 4250/55000, batch loss = 0.00\n",
      "epoch 8, step 4500/55000, batch loss = 0.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8, step 4750/55000, batch loss = 0.16\n",
      "epoch 8, step 5000/55000, batch loss = 0.00\n",
      "Train accuracy = 99.66\n",
      "epoch 8, step 5250/55000, batch loss = 0.00\n",
      "epoch 8, step 5500/55000, batch loss = 0.03\n",
      "epoch 8, step 5750/55000, batch loss = 0.00\n",
      "epoch 8, step 6000/55000, batch loss = 0.00\n",
      "epoch 8, step 6250/55000, batch loss = 0.01\n",
      "epoch 8, step 6500/55000, batch loss = 0.01\n",
      "epoch 8, step 6750/55000, batch loss = 0.00\n",
      "epoch 8, step 7000/55000, batch loss = 0.04\n",
      "epoch 8, step 7250/55000, batch loss = 0.00\n",
      "epoch 8, step 7500/55000, batch loss = 0.00\n",
      "Train accuracy = 99.66\n",
      "epoch 8, step 7750/55000, batch loss = 0.02\n",
      "epoch 8, step 8000/55000, batch loss = 0.00\n",
      "epoch 8, step 8250/55000, batch loss = 0.01\n",
      "epoch 8, step 8500/55000, batch loss = 0.00\n",
      "epoch 8, step 8750/55000, batch loss = 0.01\n",
      "epoch 8, step 9000/55000, batch loss = 0.01\n",
      "epoch 8, step 9250/55000, batch loss = 0.00\n",
      "epoch 8, step 9500/55000, batch loss = 0.01\n",
      "epoch 8, step 9750/55000, batch loss = 0.01\n",
      "epoch 8, step 10000/55000, batch loss = 0.00\n",
      "Train accuracy = 99.66\n",
      "epoch 8, step 10250/55000, batch loss = 0.00\n",
      "epoch 8, step 10500/55000, batch loss = 0.00\n",
      "epoch 8, step 10750/55000, batch loss = 0.00\n",
      "epoch 8, step 11000/55000, batch loss = 0.02\n",
      "epoch 8, step 11250/55000, batch loss = 0.08\n",
      "epoch 8, step 11500/55000, batch loss = 0.06\n",
      "epoch 8, step 11750/55000, batch loss = 0.00\n",
      "epoch 8, step 12000/55000, batch loss = 0.00\n",
      "epoch 8, step 12250/55000, batch loss = 0.00\n",
      "epoch 8, step 12500/55000, batch loss = 0.01\n",
      "Train accuracy = 99.63\n",
      "epoch 8, step 12750/55000, batch loss = 0.01\n",
      "epoch 8, step 13000/55000, batch loss = 0.05\n",
      "epoch 8, step 13250/55000, batch loss = 0.06\n",
      "epoch 8, step 13500/55000, batch loss = 0.01\n",
      "epoch 8, step 13750/55000, batch loss = 0.00\n",
      "epoch 8, step 14000/55000, batch loss = 0.00\n",
      "epoch 8, step 14250/55000, batch loss = 0.00\n",
      "epoch 8, step 14500/55000, batch loss = 0.02\n",
      "epoch 8, step 14750/55000, batch loss = 0.02\n",
      "epoch 8, step 15000/55000, batch loss = 0.01\n",
      "Train accuracy = 99.63\n",
      "epoch 8, step 15250/55000, batch loss = 0.00\n",
      "epoch 8, step 15500/55000, batch loss = 0.01\n",
      "epoch 8, step 15750/55000, batch loss = 0.00\n",
      "epoch 8, step 16000/55000, batch loss = 0.06\n",
      "epoch 8, step 16250/55000, batch loss = 0.03\n",
      "epoch 8, step 16500/55000, batch loss = 0.00\n",
      "epoch 8, step 16750/55000, batch loss = 0.02\n",
      "epoch 8, step 17000/55000, batch loss = 0.10\n",
      "epoch 8, step 17250/55000, batch loss = 0.00\n",
      "epoch 8, step 17500/55000, batch loss = 0.01\n",
      "Train accuracy = 99.58\n",
      "epoch 8, step 17750/55000, batch loss = 0.00\n",
      "epoch 8, step 18000/55000, batch loss = 0.00\n",
      "epoch 8, step 18250/55000, batch loss = 0.01\n",
      "epoch 8, step 18500/55000, batch loss = 0.01\n",
      "epoch 8, step 18750/55000, batch loss = 0.01\n",
      "epoch 8, step 19000/55000, batch loss = 0.00\n",
      "epoch 8, step 19250/55000, batch loss = 0.01\n",
      "epoch 8, step 19500/55000, batch loss = 0.01\n",
      "epoch 8, step 19750/55000, batch loss = 0.00\n",
      "epoch 8, step 20000/55000, batch loss = 0.01\n",
      "Train accuracy = 99.60\n",
      "epoch 8, step 20250/55000, batch loss = 0.00\n",
      "epoch 8, step 20500/55000, batch loss = 0.08\n",
      "epoch 8, step 20750/55000, batch loss = 0.01\n",
      "epoch 8, step 21000/55000, batch loss = 0.00\n",
      "epoch 8, step 21250/55000, batch loss = 0.01\n",
      "epoch 8, step 21500/55000, batch loss = 0.00\n",
      "epoch 8, step 21750/55000, batch loss = 0.04\n",
      "epoch 8, step 22000/55000, batch loss = 0.04\n",
      "epoch 8, step 22250/55000, batch loss = 0.00\n",
      "epoch 8, step 22500/55000, batch loss = 0.00\n",
      "Train accuracy = 99.61\n",
      "epoch 8, step 22750/55000, batch loss = 0.00\n",
      "epoch 8, step 23000/55000, batch loss = 0.12\n",
      "epoch 8, step 23250/55000, batch loss = 0.00\n",
      "epoch 8, step 23500/55000, batch loss = 0.02\n",
      "epoch 8, step 23750/55000, batch loss = 0.03\n",
      "epoch 8, step 24000/55000, batch loss = 0.01\n",
      "epoch 8, step 24250/55000, batch loss = 0.00\n",
      "epoch 8, step 24500/55000, batch loss = 0.01\n",
      "epoch 8, step 24750/55000, batch loss = 0.01\n",
      "epoch 8, step 25000/55000, batch loss = 0.03\n",
      "Train accuracy = 99.60\n",
      "epoch 8, step 25250/55000, batch loss = 0.00\n",
      "epoch 8, step 25500/55000, batch loss = 0.00\n",
      "epoch 8, step 25750/55000, batch loss = 0.04\n",
      "epoch 8, step 26000/55000, batch loss = 0.00\n",
      "epoch 8, step 26250/55000, batch loss = 0.01\n",
      "epoch 8, step 26500/55000, batch loss = 0.01\n",
      "epoch 8, step 26750/55000, batch loss = 0.00\n",
      "epoch 8, step 27000/55000, batch loss = 0.01\n",
      "epoch 8, step 27250/55000, batch loss = 0.00\n",
      "epoch 8, step 27500/55000, batch loss = 0.02\n",
      "Train accuracy = 99.59\n",
      "epoch 8, step 27750/55000, batch loss = 0.00\n",
      "epoch 8, step 28000/55000, batch loss = 0.03\n",
      "epoch 8, step 28250/55000, batch loss = 0.00\n",
      "epoch 8, step 28500/55000, batch loss = 0.01\n",
      "epoch 8, step 28750/55000, batch loss = 0.01\n",
      "epoch 8, step 29000/55000, batch loss = 0.01\n",
      "epoch 8, step 29250/55000, batch loss = 0.02\n",
      "epoch 8, step 29500/55000, batch loss = 0.00\n",
      "epoch 8, step 29750/55000, batch loss = 0.01\n",
      "epoch 8, step 30000/55000, batch loss = 0.00\n",
      "Train accuracy = 99.60\n",
      "epoch 8, step 30250/55000, batch loss = 0.01\n",
      "epoch 8, step 30500/55000, batch loss = 0.03\n",
      "epoch 8, step 30750/55000, batch loss = 0.00\n",
      "epoch 8, step 31000/55000, batch loss = 0.01\n",
      "epoch 8, step 31250/55000, batch loss = 0.00\n",
      "epoch 8, step 31500/55000, batch loss = 0.02\n",
      "epoch 8, step 31750/55000, batch loss = 0.00\n",
      "epoch 8, step 32000/55000, batch loss = 0.01\n",
      "epoch 8, step 32250/55000, batch loss = 0.00\n",
      "epoch 8, step 32500/55000, batch loss = 0.00\n",
      "Train accuracy = 99.61\n",
      "epoch 8, step 32750/55000, batch loss = 0.01\n",
      "epoch 8, step 33000/55000, batch loss = 0.01\n",
      "epoch 8, step 33250/55000, batch loss = 0.00\n",
      "epoch 8, step 33500/55000, batch loss = 0.01\n",
      "epoch 8, step 33750/55000, batch loss = 0.02\n",
      "epoch 8, step 34000/55000, batch loss = 0.02\n",
      "epoch 8, step 34250/55000, batch loss = 0.00\n",
      "epoch 8, step 34500/55000, batch loss = 0.00\n",
      "epoch 8, step 34750/55000, batch loss = 0.02\n",
      "epoch 8, step 35000/55000, batch loss = 0.00\n",
      "Train accuracy = 99.61\n",
      "epoch 8, step 35250/55000, batch loss = 0.00\n",
      "epoch 8, step 35500/55000, batch loss = 0.01\n",
      "epoch 8, step 35750/55000, batch loss = 0.01\n",
      "epoch 8, step 36000/55000, batch loss = 0.00\n",
      "epoch 8, step 36250/55000, batch loss = 0.00\n",
      "epoch 8, step 36500/55000, batch loss = 0.00\n",
      "epoch 8, step 36750/55000, batch loss = 0.01\n",
      "epoch 8, step 37000/55000, batch loss = 0.01\n",
      "epoch 8, step 37250/55000, batch loss = 0.02\n",
      "epoch 8, step 37500/55000, batch loss = 0.05\n",
      "Train accuracy = 99.61\n",
      "epoch 8, step 37750/55000, batch loss = 0.01\n",
      "epoch 8, step 38000/55000, batch loss = 0.04\n",
      "epoch 8, step 38250/55000, batch loss = 0.05\n",
      "epoch 8, step 38500/55000, batch loss = 0.03\n",
      "epoch 8, step 38750/55000, batch loss = 0.00\n",
      "epoch 8, step 39000/55000, batch loss = 0.01\n",
      "epoch 8, step 39250/55000, batch loss = 0.01\n",
      "epoch 8, step 39500/55000, batch loss = 0.00\n",
      "epoch 8, step 39750/55000, batch loss = 0.15\n",
      "epoch 8, step 40000/55000, batch loss = 0.05\n",
      "Train accuracy = 99.59\n",
      "epoch 8, step 40250/55000, batch loss = 0.01\n",
      "epoch 8, step 40500/55000, batch loss = 0.01\n",
      "epoch 8, step 40750/55000, batch loss = 0.02\n",
      "epoch 8, step 41000/55000, batch loss = 0.01\n",
      "epoch 8, step 41250/55000, batch loss = 0.00\n",
      "epoch 8, step 41500/55000, batch loss = 0.01\n",
      "epoch 8, step 41750/55000, batch loss = 0.01\n",
      "epoch 8, step 42000/55000, batch loss = 0.02\n",
      "epoch 8, step 42250/55000, batch loss = 0.02\n",
      "epoch 8, step 42500/55000, batch loss = 0.01\n",
      "Train accuracy = 99.59\n",
      "epoch 8, step 42750/55000, batch loss = 0.02\n",
      "epoch 8, step 43000/55000, batch loss = 0.00\n",
      "epoch 8, step 43250/55000, batch loss = 0.03\n",
      "epoch 8, step 43500/55000, batch loss = 0.09\n",
      "epoch 8, step 43750/55000, batch loss = 0.00\n",
      "epoch 8, step 44000/55000, batch loss = 0.00\n",
      "epoch 8, step 44250/55000, batch loss = 0.02\n",
      "epoch 8, step 44500/55000, batch loss = 0.01\n",
      "epoch 8, step 44750/55000, batch loss = 0.02\n",
      "epoch 8, step 45000/55000, batch loss = 0.01\n",
      "Train accuracy = 99.59\n",
      "epoch 8, step 45250/55000, batch loss = 0.00\n",
      "epoch 8, step 45500/55000, batch loss = 0.00\n",
      "epoch 8, step 45750/55000, batch loss = 0.01\n",
      "epoch 8, step 46000/55000, batch loss = 0.00\n",
      "epoch 8, step 46250/55000, batch loss = 0.00\n",
      "epoch 8, step 46500/55000, batch loss = 0.00\n",
      "epoch 8, step 46750/55000, batch loss = 0.00\n",
      "epoch 8, step 47000/55000, batch loss = 0.03\n",
      "epoch 8, step 47250/55000, batch loss = 0.01\n",
      "epoch 8, step 47500/55000, batch loss = 0.00\n",
      "Train accuracy = 99.59\n",
      "epoch 8, step 47750/55000, batch loss = 0.01\n",
      "epoch 8, step 48000/55000, batch loss = 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8, step 48250/55000, batch loss = 0.00\n",
      "epoch 8, step 48500/55000, batch loss = 0.00\n",
      "epoch 8, step 48750/55000, batch loss = 0.01\n",
      "epoch 8, step 49000/55000, batch loss = 0.00\n",
      "epoch 8, step 49250/55000, batch loss = 0.04\n",
      "epoch 8, step 49500/55000, batch loss = 0.03\n",
      "epoch 8, step 49750/55000, batch loss = 0.00\n",
      "epoch 8, step 50000/55000, batch loss = 0.00\n",
      "Train accuracy = 99.59\n",
      "epoch 8, step 50250/55000, batch loss = 0.02\n",
      "epoch 8, step 50500/55000, batch loss = 0.01\n",
      "epoch 8, step 50750/55000, batch loss = 0.04\n",
      "epoch 8, step 51000/55000, batch loss = 0.01\n",
      "epoch 8, step 51250/55000, batch loss = 0.01\n",
      "epoch 8, step 51500/55000, batch loss = 0.00\n",
      "epoch 8, step 51750/55000, batch loss = 0.00\n",
      "epoch 8, step 52000/55000, batch loss = 0.01\n",
      "epoch 8, step 52250/55000, batch loss = 0.00\n",
      "epoch 8, step 52500/55000, batch loss = 0.00\n",
      "Train accuracy = 99.60\n",
      "epoch 8, step 52750/55000, batch loss = 0.00\n",
      "epoch 8, step 53000/55000, batch loss = 0.04\n",
      "epoch 8, step 53250/55000, batch loss = 0.00\n",
      "epoch 8, step 53500/55000, batch loss = 0.01\n",
      "epoch 8, step 53750/55000, batch loss = 0.01\n",
      "epoch 8, step 54000/55000, batch loss = 0.01\n",
      "epoch 8, step 54250/55000, batch loss = 0.02\n",
      "epoch 8, step 54500/55000, batch loss = 0.00\n",
      "epoch 8, step 54750/55000, batch loss = 0.01\n",
      "Train accuracy = 99.60\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 99.18\n",
      "Validation avg loss = 0.03\n",
      "\n",
      "\n",
      "Running evaluation:  Test\n",
      "Test accuracy = 99.12\n",
      "Test avg loss = 0.02\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SAVE_DIR = \"/Users/goran/Documents/III_Semestar/DU/labosi/2_lab/zad1_images/\"\n",
    "\n",
    "config = {}\n",
    "config['max_epochs'] = 8\n",
    "config['batch_size'] = 50\n",
    "config['save_dir'] = SAVE_DIR\n",
    "config['lr_policy'] = {1:{'lr':1e-1}, 3:{'lr':1e-2}, 5:{'lr':1e-3}, 7:{'lr':1e-4}}\n",
    " \n",
    "np.random.seed(int(time.time() * 1e6) % 2**31)\n",
    "train_x = mnist.train.images\n",
    "train_x = train_x.reshape([-1, 1, 28, 28])\n",
    "train_y = mnist.train.labels\n",
    "valid_x = mnist.validation.images\n",
    "valid_x = valid_x.reshape([-1, 1, 28, 28])\n",
    "valid_y = mnist.validation.labels\n",
    "test_x = mnist.test.images\n",
    "test_x = test_x.reshape([-1, 1, 28, 28])\n",
    "test_y = mnist.test.labels\n",
    "train_mean = train_x.mean()\n",
    "train_x -= train_mean\n",
    "valid_x -= train_mean\n",
    "test_x -= train_mean\n",
    "\n",
    "\n",
    "net = []\n",
    "inputs = np.random.randn(config['batch_size'], 1, 28, 28)\n",
    "net += [Convolution(inputs, 16, 5, \"conv1\")]\n",
    "net += [MaxPooling(net[-1], \"pool1\")]\n",
    "net += [ReLU(net[-1], \"relu1\")]\n",
    "net += [Convolution(net[-1], 32, 5, \"conv2\")]\n",
    "net += [MaxPooling(net[-1], \"pool2\")]\n",
    "net += [ReLU(net[-1], \"relu2\")]\n",
    "# out = 7x7\n",
    "net += [Flatten(net[-1], \"flatten3\")]\n",
    "net += [FC(net[-1], 512, \"fc3\")]\n",
    "net += [ReLU(net[-1], \"relu3\")]\n",
    "net += [FC(net[-1], 10, \"logits\")]\n",
    "\n",
    "loss = SoftmaxCrossEntropyWithLogits()\n",
    "\n",
    "train(train_x, train_y, valid_x, valid_y, net, loss, config)\n",
    "evaluate(\"Test\", test_x, test_y, net, loss, config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ZADATAK\n",
    "U ovom zadatku trebate dodati podr≈°ku za L2 regularizaciju parametara. Dovr≈°ite implementaciju L2Regularizer sloja te nauƒçite regularizirani model iz prethodnog zadatka koji se nalazi u train_l2reg.py. Igrajte se s regularizacijskim parametrom tako da nauƒçite tri razliƒçite mre≈æe Œª=1e‚àí3, Œª=1e‚àí2, Œª=1e‚àí1 te usporedite nauƒçene filtre u prvom sloju i dobivenu toƒçnost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN training with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAMBDA: 0.1 \n",
      "\n",
      "epoch 1, step 0/55000, batch loss = 45.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/Anaconda/anaconda/lib/python3.5/site-packages/skimage/util/dtype.py:110: UserWarning: Possible precision loss when converting from float64 to uint16\n",
      "  \"%s to %s\" % (dtypeobj_in, dtypeobj))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, step 250/55000, batch loss = 41.27\n",
      "epoch 1, step 500/55000, batch loss = 37.05\n",
      "epoch 1, step 750/55000, batch loss = 33.79\n",
      "epoch 1, step 1000/55000, batch loss = 30.48\n",
      "epoch 1, step 1250/55000, batch loss = 27.33\n",
      "epoch 1, step 1500/55000, batch loss = 24.70\n",
      "epoch 1, step 1750/55000, batch loss = 22.64\n",
      "epoch 1, step 2000/55000, batch loss = 20.31\n",
      "epoch 1, step 2250/55000, batch loss = 18.35\n",
      "epoch 1, step 2500/55000, batch loss = 16.86\n",
      "Train accuracy = 64.35\n",
      "epoch 1, step 2750/55000, batch loss = 15.18\n",
      "epoch 1, step 3000/55000, batch loss = 14.84\n",
      "epoch 1, step 3250/55000, batch loss = 12.87\n",
      "epoch 1, step 3500/55000, batch loss = 11.50\n",
      "epoch 1, step 3750/55000, batch loss = 10.57\n",
      "epoch 1, step 4000/55000, batch loss = 9.74\n",
      "epoch 1, step 4250/55000, batch loss = 8.68\n",
      "epoch 1, step 4500/55000, batch loss = 7.82\n",
      "epoch 1, step 4750/55000, batch loss = 7.34\n",
      "epoch 1, step 5000/55000, batch loss = 6.61\n",
      "Train accuracy = 73.60\n",
      "epoch 1, step 5250/55000, batch loss = 6.15\n",
      "epoch 1, step 5500/55000, batch loss = 6.08\n",
      "epoch 1, step 5750/55000, batch loss = 5.14\n",
      "epoch 1, step 6000/55000, batch loss = 4.76\n",
      "epoch 1, step 6250/55000, batch loss = 4.28\n",
      "epoch 1, step 6500/55000, batch loss = 4.23\n",
      "epoch 1, step 6750/55000, batch loss = 3.82\n",
      "epoch 1, step 7000/55000, batch loss = 3.45\n",
      "epoch 1, step 7250/55000, batch loss = 3.15\n",
      "epoch 1, step 7500/55000, batch loss = 2.90\n",
      "Train accuracy = 77.87\n",
      "epoch 1, step 7750/55000, batch loss = 3.04\n",
      "epoch 1, step 8000/55000, batch loss = 2.70\n",
      "epoch 1, step 8250/55000, batch loss = 2.35\n",
      "epoch 1, step 8500/55000, batch loss = 2.28\n",
      "epoch 1, step 8750/55000, batch loss = 2.33\n",
      "epoch 1, step 9000/55000, batch loss = 2.02\n",
      "epoch 1, step 9250/55000, batch loss = 1.84\n",
      "epoch 1, step 9500/55000, batch loss = 2.41\n",
      "epoch 1, step 9750/55000, batch loss = 1.61\n",
      "epoch 1, step 10000/55000, batch loss = 1.59\n",
      "Train accuracy = 80.55\n",
      "epoch 1, step 10250/55000, batch loss = 1.46\n",
      "epoch 1, step 10500/55000, batch loss = 1.58\n",
      "epoch 1, step 10750/55000, batch loss = 1.33\n",
      "epoch 1, step 11000/55000, batch loss = 1.37\n",
      "epoch 1, step 11250/55000, batch loss = 1.22\n",
      "epoch 1, step 11500/55000, batch loss = 1.37\n",
      "epoch 1, step 11750/55000, batch loss = 1.25\n",
      "epoch 1, step 12000/55000, batch loss = 1.17\n",
      "epoch 1, step 12250/55000, batch loss = 0.92\n",
      "epoch 1, step 12500/55000, batch loss = 1.39\n",
      "Train accuracy = 82.25\n",
      "epoch 1, step 12750/55000, batch loss = 0.86\n",
      "epoch 1, step 13000/55000, batch loss = 1.05\n",
      "epoch 1, step 13250/55000, batch loss = 0.95\n",
      "epoch 1, step 13500/55000, batch loss = 1.00\n",
      "epoch 1, step 13750/55000, batch loss = 1.17\n",
      "epoch 1, step 14000/55000, batch loss = 0.83\n",
      "epoch 1, step 14250/55000, batch loss = 1.00\n",
      "epoch 1, step 14500/55000, batch loss = 0.79\n",
      "epoch 1, step 14750/55000, batch loss = 0.80\n",
      "epoch 1, step 15000/55000, batch loss = 0.97\n",
      "Train accuracy = 83.43\n",
      "epoch 1, step 15250/55000, batch loss = 0.73\n",
      "epoch 1, step 15500/55000, batch loss = 0.94\n",
      "epoch 1, step 15750/55000, batch loss = 0.72\n",
      "epoch 1, step 16000/55000, batch loss = 0.73\n",
      "epoch 1, step 16250/55000, batch loss = 0.88\n",
      "epoch 1, step 16500/55000, batch loss = 0.75\n",
      "epoch 1, step 16750/55000, batch loss = 0.70\n",
      "epoch 1, step 17000/55000, batch loss = 0.66\n",
      "epoch 1, step 17250/55000, batch loss = 0.68\n",
      "epoch 1, step 17500/55000, batch loss = 0.85\n",
      "Train accuracy = 84.56\n",
      "epoch 1, step 17750/55000, batch loss = 0.76\n",
      "epoch 1, step 18000/55000, batch loss = 0.81\n",
      "epoch 1, step 18250/55000, batch loss = 0.90\n",
      "epoch 1, step 18500/55000, batch loss = 0.59\n",
      "epoch 1, step 18750/55000, batch loss = 0.75\n",
      "epoch 1, step 19000/55000, batch loss = 0.76\n",
      "epoch 1, step 19250/55000, batch loss = 0.65\n",
      "epoch 1, step 19500/55000, batch loss = 0.69\n",
      "epoch 1, step 19750/55000, batch loss = 0.79\n",
      "epoch 1, step 20000/55000, batch loss = 0.97\n",
      "Train accuracy = 85.17\n",
      "epoch 1, step 20250/55000, batch loss = 0.81\n",
      "epoch 1, step 20500/55000, batch loss = 0.63\n",
      "epoch 1, step 20750/55000, batch loss = 0.59\n",
      "epoch 1, step 21000/55000, batch loss = 0.64\n",
      "epoch 1, step 21250/55000, batch loss = 0.88\n",
      "epoch 1, step 21500/55000, batch loss = 0.63\n",
      "epoch 1, step 21750/55000, batch loss = 0.59\n",
      "epoch 1, step 22000/55000, batch loss = 0.87\n",
      "epoch 1, step 22250/55000, batch loss = 0.72\n",
      "epoch 1, step 22500/55000, batch loss = 0.72\n",
      "Train accuracy = 85.82\n",
      "epoch 1, step 22750/55000, batch loss = 0.72\n",
      "epoch 1, step 23000/55000, batch loss = 0.80\n",
      "epoch 1, step 23250/55000, batch loss = 1.52\n",
      "epoch 1, step 23500/55000, batch loss = 0.89\n",
      "epoch 1, step 23750/55000, batch loss = 0.95\n",
      "epoch 1, step 24000/55000, batch loss = 0.71\n",
      "epoch 1, step 24250/55000, batch loss = 0.88\n",
      "epoch 1, step 24500/55000, batch loss = 0.64\n",
      "epoch 1, step 24750/55000, batch loss = 0.81\n",
      "epoch 1, step 25000/55000, batch loss = 0.71\n",
      "Train accuracy = 85.98\n",
      "epoch 1, step 25250/55000, batch loss = 0.68\n",
      "epoch 1, step 25500/55000, batch loss = 0.68\n",
      "epoch 1, step 25750/55000, batch loss = 0.71\n",
      "epoch 1, step 26000/55000, batch loss = 0.73\n",
      "epoch 1, step 26250/55000, batch loss = 0.54\n",
      "epoch 1, step 26500/55000, batch loss = 1.32\n",
      "epoch 1, step 26750/55000, batch loss = 0.78\n",
      "epoch 1, step 27000/55000, batch loss = 0.71\n",
      "epoch 1, step 27250/55000, batch loss = 0.75\n",
      "epoch 1, step 27500/55000, batch loss = 0.71\n",
      "Train accuracy = 86.43\n",
      "epoch 1, step 27750/55000, batch loss = 0.79\n",
      "epoch 1, step 28000/55000, batch loss = 0.63\n",
      "epoch 1, step 28250/55000, batch loss = 0.78\n",
      "epoch 1, step 28500/55000, batch loss = 0.76\n",
      "epoch 1, step 28750/55000, batch loss = 0.82\n",
      "epoch 1, step 29000/55000, batch loss = 0.88\n",
      "epoch 1, step 29250/55000, batch loss = 0.57\n",
      "epoch 1, step 29500/55000, batch loss = 0.70\n",
      "epoch 1, step 29750/55000, batch loss = 0.66\n",
      "epoch 1, step 30000/55000, batch loss = 0.86\n",
      "Train accuracy = 86.75\n",
      "epoch 1, step 30250/55000, batch loss = 0.67\n",
      "epoch 1, step 30500/55000, batch loss = 0.64\n",
      "epoch 1, step 30750/55000, batch loss = 0.64\n",
      "epoch 1, step 31000/55000, batch loss = 0.74\n",
      "epoch 1, step 31250/55000, batch loss = 0.61\n",
      "epoch 1, step 31500/55000, batch loss = 0.81\n",
      "epoch 1, step 31750/55000, batch loss = 0.55\n",
      "epoch 1, step 32000/55000, batch loss = 0.68\n",
      "epoch 1, step 32250/55000, batch loss = 0.63\n",
      "epoch 1, step 32500/55000, batch loss = 0.56\n",
      "Train accuracy = 87.15\n",
      "epoch 1, step 32750/55000, batch loss = 0.67\n",
      "epoch 1, step 33000/55000, batch loss = 0.66\n",
      "epoch 1, step 33250/55000, batch loss = 1.17\n",
      "epoch 1, step 33500/55000, batch loss = 0.60\n",
      "epoch 1, step 33750/55000, batch loss = 0.53\n",
      "epoch 1, step 34000/55000, batch loss = 0.63\n",
      "epoch 1, step 34250/55000, batch loss = 0.74\n",
      "epoch 1, step 34500/55000, batch loss = 0.65\n",
      "epoch 1, step 34750/55000, batch loss = 0.59\n",
      "epoch 1, step 35000/55000, batch loss = 0.94\n",
      "Train accuracy = 87.45\n",
      "epoch 1, step 35250/55000, batch loss = 0.61\n",
      "epoch 1, step 35500/55000, batch loss = 1.08\n",
      "epoch 1, step 35750/55000, batch loss = 0.65\n",
      "epoch 1, step 36000/55000, batch loss = 0.76\n",
      "epoch 1, step 36250/55000, batch loss = 0.67\n",
      "epoch 1, step 36500/55000, batch loss = 1.13\n",
      "epoch 1, step 36750/55000, batch loss = 0.86\n",
      "epoch 1, step 37000/55000, batch loss = 0.67\n",
      "epoch 1, step 37250/55000, batch loss = 0.63\n",
      "epoch 1, step 37500/55000, batch loss = 0.80\n",
      "Train accuracy = 87.54\n",
      "epoch 1, step 37750/55000, batch loss = 0.47\n",
      "epoch 1, step 38000/55000, batch loss = 0.52\n",
      "epoch 1, step 38250/55000, batch loss = 0.70\n",
      "epoch 1, step 38500/55000, batch loss = 0.68\n",
      "epoch 1, step 38750/55000, batch loss = 0.58\n",
      "epoch 1, step 39000/55000, batch loss = 0.61\n",
      "epoch 1, step 39250/55000, batch loss = 0.62\n",
      "epoch 1, step 39500/55000, batch loss = 0.67\n",
      "epoch 1, step 39750/55000, batch loss = 0.56\n",
      "epoch 1, step 40000/55000, batch loss = 0.89\n",
      "Train accuracy = 87.84\n",
      "epoch 1, step 40250/55000, batch loss = 0.60\n",
      "epoch 1, step 40500/55000, batch loss = 0.63\n",
      "epoch 1, step 40750/55000, batch loss = 0.51\n",
      "epoch 1, step 41000/55000, batch loss = 0.67\n",
      "epoch 1, step 41250/55000, batch loss = 0.54\n",
      "epoch 1, step 41500/55000, batch loss = 0.71\n",
      "epoch 1, step 41750/55000, batch loss = 0.53\n",
      "epoch 1, step 42000/55000, batch loss = 0.91\n",
      "epoch 1, step 42250/55000, batch loss = 0.66\n",
      "epoch 1, step 42500/55000, batch loss = 0.75\n",
      "Train accuracy = 88.11\n",
      "epoch 1, step 42750/55000, batch loss = 0.64\n",
      "epoch 1, step 43000/55000, batch loss = 0.67\n",
      "epoch 1, step 43250/55000, batch loss = 0.71\n",
      "epoch 1, step 43500/55000, batch loss = 0.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, step 43750/55000, batch loss = 0.61\n",
      "epoch 1, step 44000/55000, batch loss = 0.59\n",
      "epoch 1, step 44250/55000, batch loss = 0.52\n",
      "epoch 1, step 44500/55000, batch loss = 0.63\n",
      "epoch 1, step 44750/55000, batch loss = 0.65\n",
      "epoch 1, step 45000/55000, batch loss = 1.03\n",
      "Train accuracy = 88.29\n",
      "epoch 1, step 45250/55000, batch loss = 0.72\n",
      "epoch 1, step 45500/55000, batch loss = 0.68\n",
      "epoch 1, step 45750/55000, batch loss = 0.63\n",
      "epoch 1, step 46000/55000, batch loss = 0.74\n",
      "epoch 1, step 46250/55000, batch loss = 0.58\n",
      "epoch 1, step 46500/55000, batch loss = 1.14\n",
      "epoch 1, step 46750/55000, batch loss = 0.95\n",
      "epoch 1, step 47000/55000, batch loss = 0.80\n",
      "epoch 1, step 47250/55000, batch loss = 0.61\n",
      "epoch 1, step 47500/55000, batch loss = 0.70\n",
      "Train accuracy = 88.30\n",
      "epoch 1, step 47750/55000, batch loss = 0.59\n",
      "epoch 1, step 48000/55000, batch loss = 1.02\n",
      "epoch 1, step 48250/55000, batch loss = 0.60\n",
      "epoch 1, step 48500/55000, batch loss = 0.61\n",
      "epoch 1, step 48750/55000, batch loss = 0.54\n",
      "epoch 1, step 49000/55000, batch loss = 0.61\n",
      "epoch 1, step 49250/55000, batch loss = 0.59\n",
      "epoch 1, step 49500/55000, batch loss = 0.62\n",
      "epoch 1, step 49750/55000, batch loss = 0.58\n",
      "epoch 1, step 50000/55000, batch loss = 0.83\n",
      "Train accuracy = 88.45\n",
      "epoch 1, step 50250/55000, batch loss = 0.65\n",
      "epoch 1, step 50500/55000, batch loss = 0.74\n",
      "epoch 1, step 50750/55000, batch loss = 0.66\n",
      "epoch 1, step 51000/55000, batch loss = 0.78\n",
      "epoch 1, step 51250/55000, batch loss = 0.62\n",
      "epoch 1, step 51500/55000, batch loss = 0.61\n",
      "epoch 1, step 51750/55000, batch loss = 0.75\n",
      "epoch 1, step 52000/55000, batch loss = 0.66\n",
      "epoch 1, step 52250/55000, batch loss = 0.61\n",
      "epoch 1, step 52500/55000, batch loss = 0.67\n",
      "Train accuracy = 88.58\n",
      "epoch 1, step 52750/55000, batch loss = 0.59\n",
      "epoch 1, step 53000/55000, batch loss = 0.51\n",
      "epoch 1, step 53250/55000, batch loss = 0.53\n",
      "epoch 1, step 53500/55000, batch loss = 0.63\n",
      "epoch 1, step 53750/55000, batch loss = 0.57\n",
      "epoch 1, step 54000/55000, batch loss = 0.60\n",
      "epoch 1, step 54250/55000, batch loss = 2.72\n",
      "epoch 1, step 54500/55000, batch loss = 1.18\n",
      "epoch 1, step 54750/55000, batch loss = 1.07\n",
      "Train accuracy = 88.47\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 88.98\n",
      "Validation avg loss = 0.85\n",
      "\n",
      "epoch 2, step 0/55000, batch loss = 0.78\n",
      "epoch 2, step 250/55000, batch loss = 0.74\n",
      "epoch 2, step 500/55000, batch loss = 0.67\n",
      "epoch 2, step 750/55000, batch loss = 0.70\n",
      "epoch 2, step 1000/55000, batch loss = 0.97\n",
      "epoch 2, step 1250/55000, batch loss = 0.75\n",
      "epoch 2, step 1500/55000, batch loss = 0.59\n",
      "epoch 2, step 1750/55000, batch loss = 0.84\n",
      "epoch 2, step 2000/55000, batch loss = 0.80\n",
      "epoch 2, step 2250/55000, batch loss = 0.51\n",
      "epoch 2, step 2500/55000, batch loss = 0.68\n",
      "Train accuracy = 91.18\n",
      "epoch 2, step 2750/55000, batch loss = 0.68\n",
      "epoch 2, step 3000/55000, batch loss = 0.64\n",
      "epoch 2, step 3250/55000, batch loss = 0.67\n",
      "epoch 2, step 3500/55000, batch loss = 0.58\n",
      "epoch 2, step 3750/55000, batch loss = 0.62\n",
      "epoch 2, step 4000/55000, batch loss = 0.88\n",
      "epoch 2, step 4250/55000, batch loss = 0.52\n",
      "epoch 2, step 4500/55000, batch loss = 0.55\n",
      "epoch 2, step 4750/55000, batch loss = 0.78\n",
      "epoch 2, step 5000/55000, batch loss = 0.58\n",
      "Train accuracy = 91.03\n",
      "epoch 2, step 5250/55000, batch loss = 0.44\n",
      "epoch 2, step 5500/55000, batch loss = 0.62\n",
      "epoch 2, step 5750/55000, batch loss = 0.82\n",
      "epoch 2, step 6000/55000, batch loss = 0.78\n",
      "epoch 2, step 6250/55000, batch loss = 0.62\n",
      "epoch 2, step 6500/55000, batch loss = 0.67\n",
      "epoch 2, step 6750/55000, batch loss = 0.52\n",
      "epoch 2, step 7000/55000, batch loss = 0.84\n",
      "epoch 2, step 7250/55000, batch loss = 0.60\n",
      "epoch 2, step 7500/55000, batch loss = 0.78\n",
      "Train accuracy = 90.91\n",
      "epoch 2, step 7750/55000, batch loss = 0.57\n",
      "epoch 2, step 8000/55000, batch loss = 0.57\n",
      "epoch 2, step 8250/55000, batch loss = 0.96\n",
      "epoch 2, step 8500/55000, batch loss = 0.69\n",
      "epoch 2, step 8750/55000, batch loss = 0.67\n",
      "epoch 2, step 9000/55000, batch loss = 0.70\n",
      "epoch 2, step 9250/55000, batch loss = 0.74\n",
      "epoch 2, step 9500/55000, batch loss = 0.61\n",
      "epoch 2, step 9750/55000, batch loss = 0.61\n",
      "epoch 2, step 10000/55000, batch loss = 0.62\n",
      "Train accuracy = 90.90\n",
      "epoch 2, step 10250/55000, batch loss = 0.52\n",
      "epoch 2, step 10500/55000, batch loss = 0.70\n",
      "epoch 2, step 10750/55000, batch loss = 0.50\n",
      "epoch 2, step 11000/55000, batch loss = 0.56\n",
      "epoch 2, step 11250/55000, batch loss = 0.68\n",
      "epoch 2, step 11500/55000, batch loss = 0.54\n",
      "epoch 2, step 11750/55000, batch loss = 0.79\n",
      "epoch 2, step 12000/55000, batch loss = 0.84\n",
      "epoch 2, step 12250/55000, batch loss = 0.66\n",
      "epoch 2, step 12500/55000, batch loss = 0.70\n",
      "Train accuracy = 90.95\n",
      "epoch 2, step 12750/55000, batch loss = 0.67\n",
      "epoch 2, step 13000/55000, batch loss = 0.49\n",
      "epoch 2, step 13250/55000, batch loss = 0.73\n",
      "epoch 2, step 13500/55000, batch loss = 0.60\n",
      "epoch 2, step 13750/55000, batch loss = 0.47\n",
      "epoch 2, step 14000/55000, batch loss = 0.58\n",
      "epoch 2, step 14250/55000, batch loss = 0.55\n",
      "epoch 2, step 14500/55000, batch loss = 0.84\n",
      "epoch 2, step 14750/55000, batch loss = 0.64\n",
      "epoch 2, step 15000/55000, batch loss = 0.75\n",
      "Train accuracy = 91.08\n",
      "epoch 2, step 15250/55000, batch loss = 0.52\n",
      "epoch 2, step 15500/55000, batch loss = 0.66\n",
      "epoch 2, step 15750/55000, batch loss = 0.59\n",
      "epoch 2, step 16000/55000, batch loss = 0.54\n",
      "epoch 2, step 16250/55000, batch loss = 0.68\n",
      "epoch 2, step 16500/55000, batch loss = 0.50\n",
      "epoch 2, step 16750/55000, batch loss = 0.54\n",
      "epoch 2, step 17000/55000, batch loss = 1.08\n",
      "epoch 2, step 17250/55000, batch loss = 0.66\n",
      "epoch 2, step 17500/55000, batch loss = 0.49\n",
      "Train accuracy = 91.09\n",
      "epoch 2, step 17750/55000, batch loss = 0.48\n",
      "epoch 2, step 18000/55000, batch loss = 0.72\n",
      "epoch 2, step 18250/55000, batch loss = 0.64\n",
      "epoch 2, step 18500/55000, batch loss = 0.67\n",
      "epoch 2, step 18750/55000, batch loss = 0.48\n",
      "epoch 2, step 19000/55000, batch loss = 0.78\n",
      "epoch 2, step 19250/55000, batch loss = 0.56\n",
      "epoch 2, step 19500/55000, batch loss = 0.76\n",
      "epoch 2, step 19750/55000, batch loss = 0.70\n",
      "epoch 2, step 20000/55000, batch loss = 0.56\n",
      "Train accuracy = 91.19\n",
      "epoch 2, step 20250/55000, batch loss = 0.61\n",
      "epoch 2, step 20500/55000, batch loss = 0.49\n",
      "epoch 2, step 20750/55000, batch loss = 0.65\n",
      "epoch 2, step 21000/55000, batch loss = 0.54\n",
      "epoch 2, step 21250/55000, batch loss = 0.84\n",
      "epoch 2, step 21500/55000, batch loss = 0.63\n",
      "epoch 2, step 21750/55000, batch loss = 0.53\n",
      "epoch 2, step 22000/55000, batch loss = 0.62\n",
      "epoch 2, step 22250/55000, batch loss = 0.77\n",
      "epoch 2, step 22500/55000, batch loss = 0.54\n",
      "Train accuracy = 91.35\n",
      "epoch 2, step 22750/55000, batch loss = 0.54\n",
      "epoch 2, step 23000/55000, batch loss = 0.64\n",
      "epoch 2, step 23250/55000, batch loss = 0.80\n",
      "epoch 2, step 23500/55000, batch loss = 0.65\n",
      "epoch 2, step 23750/55000, batch loss = 0.53\n",
      "epoch 2, step 24000/55000, batch loss = 0.53\n",
      "epoch 2, step 24250/55000, batch loss = 0.67\n",
      "epoch 2, step 24500/55000, batch loss = 0.73\n",
      "epoch 2, step 24750/55000, batch loss = 0.61\n",
      "epoch 2, step 25000/55000, batch loss = 0.48\n",
      "Train accuracy = 91.40\n",
      "epoch 2, step 25250/55000, batch loss = 0.81\n",
      "epoch 2, step 25500/55000, batch loss = 0.71\n",
      "epoch 2, step 25750/55000, batch loss = 0.75\n",
      "epoch 2, step 26000/55000, batch loss = 0.54\n",
      "epoch 2, step 26250/55000, batch loss = 0.81\n",
      "epoch 2, step 26500/55000, batch loss = 0.50\n",
      "epoch 2, step 26750/55000, batch loss = 0.53\n",
      "epoch 2, step 27000/55000, batch loss = 1.08\n",
      "epoch 2, step 27250/55000, batch loss = 0.98\n",
      "epoch 2, step 27500/55000, batch loss = 0.64\n",
      "Train accuracy = 91.18\n",
      "epoch 2, step 27750/55000, batch loss = 0.80\n",
      "epoch 2, step 28000/55000, batch loss = 0.53\n",
      "epoch 2, step 28250/55000, batch loss = 0.55\n",
      "epoch 2, step 28500/55000, batch loss = 0.93\n",
      "epoch 2, step 28750/55000, batch loss = 0.63\n",
      "epoch 2, step 29000/55000, batch loss = 0.52\n",
      "epoch 2, step 29250/55000, batch loss = 0.75\n",
      "epoch 2, step 29500/55000, batch loss = 0.75\n",
      "epoch 2, step 29750/55000, batch loss = 0.50\n",
      "epoch 2, step 30000/55000, batch loss = 0.53\n",
      "Train accuracy = 91.27\n",
      "epoch 2, step 30250/55000, batch loss = 0.57\n",
      "epoch 2, step 30500/55000, batch loss = 0.58\n",
      "epoch 2, step 30750/55000, batch loss = 0.64\n",
      "epoch 2, step 31000/55000, batch loss = 0.65\n",
      "epoch 2, step 31250/55000, batch loss = 0.48\n",
      "epoch 2, step 31500/55000, batch loss = 0.55\n",
      "epoch 2, step 31750/55000, batch loss = 0.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, step 32000/55000, batch loss = 0.75\n",
      "epoch 2, step 32250/55000, batch loss = 1.23\n",
      "epoch 2, step 32500/55000, batch loss = 0.93\n",
      "Train accuracy = 91.23\n",
      "epoch 2, step 32750/55000, batch loss = 0.55\n",
      "epoch 2, step 33000/55000, batch loss = 0.54\n",
      "epoch 2, step 33250/55000, batch loss = 0.59\n",
      "epoch 2, step 33500/55000, batch loss = 0.53\n",
      "epoch 2, step 33750/55000, batch loss = 0.61\n",
      "epoch 2, step 34000/55000, batch loss = 0.85\n",
      "epoch 2, step 34250/55000, batch loss = 0.54\n",
      "epoch 2, step 34500/55000, batch loss = 0.63\n",
      "epoch 2, step 34750/55000, batch loss = 0.49\n",
      "epoch 2, step 35000/55000, batch loss = 0.63\n",
      "Train accuracy = 91.36\n",
      "epoch 2, step 35250/55000, batch loss = 0.77\n",
      "epoch 2, step 35500/55000, batch loss = 0.56\n",
      "epoch 2, step 35750/55000, batch loss = 0.49\n",
      "epoch 2, step 36000/55000, batch loss = 0.60\n",
      "epoch 2, step 36250/55000, batch loss = 0.49\n",
      "epoch 2, step 36500/55000, batch loss = 0.70\n",
      "epoch 2, step 36750/55000, batch loss = 0.70\n",
      "epoch 2, step 37000/55000, batch loss = 0.58\n",
      "epoch 2, step 37250/55000, batch loss = 0.69\n",
      "epoch 2, step 37500/55000, batch loss = 0.86\n",
      "Train accuracy = 91.38\n",
      "epoch 2, step 37750/55000, batch loss = 0.78\n",
      "epoch 2, step 38000/55000, batch loss = 0.51\n",
      "epoch 2, step 38250/55000, batch loss = 1.07\n",
      "epoch 2, step 38500/55000, batch loss = 0.54\n",
      "epoch 2, step 38750/55000, batch loss = 0.61\n",
      "epoch 2, step 39000/55000, batch loss = 0.47\n",
      "epoch 2, step 39250/55000, batch loss = 0.52\n",
      "epoch 2, step 39500/55000, batch loss = 0.60\n",
      "epoch 2, step 39750/55000, batch loss = 0.61\n",
      "epoch 2, step 40000/55000, batch loss = 0.90\n",
      "Train accuracy = 91.36\n",
      "epoch 2, step 40250/55000, batch loss = 0.92\n",
      "epoch 2, step 40500/55000, batch loss = 0.56\n",
      "epoch 2, step 40750/55000, batch loss = 0.52\n",
      "epoch 2, step 41000/55000, batch loss = 0.54\n",
      "epoch 2, step 41250/55000, batch loss = 0.76\n",
      "epoch 2, step 41500/55000, batch loss = 0.56\n",
      "epoch 2, step 41750/55000, batch loss = 0.62\n",
      "epoch 2, step 42000/55000, batch loss = 0.71\n",
      "epoch 2, step 42250/55000, batch loss = 0.53\n",
      "epoch 2, step 42500/55000, batch loss = 0.49\n",
      "Train accuracy = 91.42\n",
      "epoch 2, step 42750/55000, batch loss = 0.52\n",
      "epoch 2, step 43000/55000, batch loss = 0.59\n",
      "epoch 2, step 43250/55000, batch loss = 0.72\n",
      "epoch 2, step 43500/55000, batch loss = 0.54\n",
      "epoch 2, step 43750/55000, batch loss = 0.68\n",
      "epoch 2, step 44000/55000, batch loss = 1.71\n",
      "epoch 2, step 44250/55000, batch loss = 0.95\n",
      "epoch 2, step 44500/55000, batch loss = 0.68\n",
      "epoch 2, step 44750/55000, batch loss = 0.86\n",
      "epoch 2, step 45000/55000, batch loss = 0.76\n",
      "Train accuracy = 91.30\n",
      "epoch 2, step 45250/55000, batch loss = 1.10\n",
      "epoch 2, step 45500/55000, batch loss = 0.64\n",
      "epoch 2, step 45750/55000, batch loss = 0.54\n",
      "epoch 2, step 46000/55000, batch loss = 0.55\n",
      "epoch 2, step 46250/55000, batch loss = 0.53\n",
      "epoch 2, step 46500/55000, batch loss = 0.75\n",
      "epoch 2, step 46750/55000, batch loss = 0.61\n",
      "epoch 2, step 47000/55000, batch loss = 0.67\n",
      "epoch 2, step 47250/55000, batch loss = 0.75\n",
      "epoch 2, step 47500/55000, batch loss = 0.59\n",
      "Train accuracy = 91.38\n",
      "epoch 2, step 47750/55000, batch loss = 0.56\n",
      "epoch 2, step 48000/55000, batch loss = 0.78\n",
      "epoch 2, step 48250/55000, batch loss = 0.67\n",
      "epoch 2, step 48500/55000, batch loss = 0.64\n",
      "epoch 2, step 48750/55000, batch loss = 0.61\n",
      "epoch 2, step 49000/55000, batch loss = 0.62\n",
      "epoch 2, step 49250/55000, batch loss = 0.54\n",
      "epoch 2, step 49500/55000, batch loss = 0.55\n",
      "epoch 2, step 49750/55000, batch loss = 0.55\n",
      "epoch 2, step 50000/55000, batch loss = 0.78\n",
      "Train accuracy = 91.35\n",
      "epoch 2, step 50250/55000, batch loss = 0.62\n",
      "epoch 2, step 50500/55000, batch loss = 0.49\n",
      "epoch 2, step 50750/55000, batch loss = 0.61\n",
      "epoch 2, step 51000/55000, batch loss = 0.56\n",
      "epoch 2, step 51250/55000, batch loss = 0.48\n",
      "epoch 2, step 51500/55000, batch loss = 0.45\n",
      "epoch 2, step 51750/55000, batch loss = 0.69\n",
      "epoch 2, step 52000/55000, batch loss = 0.53\n",
      "epoch 2, step 52250/55000, batch loss = 0.49\n",
      "epoch 2, step 52500/55000, batch loss = 0.56\n",
      "Train accuracy = 91.46\n",
      "epoch 2, step 52750/55000, batch loss = 0.56\n",
      "epoch 2, step 53000/55000, batch loss = 0.51\n",
      "epoch 2, step 53250/55000, batch loss = 0.64\n",
      "epoch 2, step 53500/55000, batch loss = 0.63\n",
      "epoch 2, step 53750/55000, batch loss = 0.60\n",
      "epoch 2, step 54000/55000, batch loss = 0.56\n",
      "epoch 2, step 54250/55000, batch loss = 0.67\n",
      "epoch 2, step 54500/55000, batch loss = 0.58\n",
      "epoch 2, step 54750/55000, batch loss = 0.48\n",
      "Train accuracy = 91.51\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 89.80\n",
      "Validation avg loss = 0.67\n",
      "\n",
      "epoch 3, step 0/55000, batch loss = 0.71\n",
      "epoch 3, step 250/55000, batch loss = 0.55\n",
      "epoch 3, step 500/55000, batch loss = 0.49\n",
      "epoch 3, step 750/55000, batch loss = 0.51\n",
      "epoch 3, step 1000/55000, batch loss = 0.57\n",
      "epoch 3, step 1250/55000, batch loss = 0.48\n",
      "epoch 3, step 1500/55000, batch loss = 0.47\n",
      "epoch 3, step 1750/55000, batch loss = 0.47\n",
      "epoch 3, step 2000/55000, batch loss = 0.52\n",
      "epoch 3, step 2250/55000, batch loss = 0.39\n",
      "epoch 3, step 2500/55000, batch loss = 0.60\n",
      "Train accuracy = 95.41\n",
      "epoch 3, step 2750/55000, batch loss = 0.61\n",
      "epoch 3, step 3000/55000, batch loss = 0.44\n",
      "epoch 3, step 3250/55000, batch loss = 0.44\n",
      "epoch 3, step 3500/55000, batch loss = 0.40\n",
      "epoch 3, step 3750/55000, batch loss = 0.39\n",
      "epoch 3, step 4000/55000, batch loss = 0.54\n",
      "epoch 3, step 4250/55000, batch loss = 0.54\n",
      "epoch 3, step 4500/55000, batch loss = 0.53\n",
      "epoch 3, step 4750/55000, batch loss = 0.45\n",
      "epoch 3, step 5000/55000, batch loss = 0.46\n",
      "Train accuracy = 95.68\n",
      "epoch 3, step 5250/55000, batch loss = 0.38\n",
      "epoch 3, step 5500/55000, batch loss = 0.48\n",
      "epoch 3, step 5750/55000, batch loss = 0.61\n",
      "epoch 3, step 6000/55000, batch loss = 0.51\n",
      "epoch 3, step 6250/55000, batch loss = 0.47\n",
      "epoch 3, step 6500/55000, batch loss = 0.50\n",
      "epoch 3, step 6750/55000, batch loss = 0.46\n",
      "epoch 3, step 7000/55000, batch loss = 0.54\n",
      "epoch 3, step 7250/55000, batch loss = 0.50\n",
      "epoch 3, step 7500/55000, batch loss = 0.47\n",
      "Train accuracy = 95.64\n",
      "epoch 3, step 7750/55000, batch loss = 0.46\n",
      "epoch 3, step 8000/55000, batch loss = 0.43\n",
      "epoch 3, step 8250/55000, batch loss = 0.36\n",
      "epoch 3, step 8500/55000, batch loss = 0.44\n",
      "epoch 3, step 8750/55000, batch loss = 0.36\n",
      "epoch 3, step 9000/55000, batch loss = 0.39\n",
      "epoch 3, step 9250/55000, batch loss = 0.43\n",
      "epoch 3, step 9500/55000, batch loss = 0.44\n",
      "epoch 3, step 9750/55000, batch loss = 0.40\n",
      "epoch 3, step 10000/55000, batch loss = 0.49\n",
      "Train accuracy = 95.87\n",
      "epoch 3, step 10250/55000, batch loss = 0.36\n",
      "epoch 3, step 10500/55000, batch loss = 0.51\n",
      "epoch 3, step 10750/55000, batch loss = 0.48\n",
      "epoch 3, step 11000/55000, batch loss = 0.51\n",
      "epoch 3, step 11250/55000, batch loss = 0.50\n",
      "epoch 3, step 11500/55000, batch loss = 0.40\n",
      "epoch 3, step 11750/55000, batch loss = 0.41\n",
      "epoch 3, step 12000/55000, batch loss = 0.42\n",
      "epoch 3, step 12250/55000, batch loss = 0.40\n",
      "epoch 3, step 12500/55000, batch loss = 0.58\n",
      "Train accuracy = 95.91\n",
      "epoch 3, step 12750/55000, batch loss = 0.44\n",
      "epoch 3, step 13000/55000, batch loss = 0.55\n",
      "epoch 3, step 13250/55000, batch loss = 0.45\n",
      "epoch 3, step 13500/55000, batch loss = 0.49\n",
      "epoch 3, step 13750/55000, batch loss = 0.51\n",
      "epoch 3, step 14000/55000, batch loss = 0.40\n",
      "epoch 3, step 14250/55000, batch loss = 0.44\n",
      "epoch 3, step 14500/55000, batch loss = 0.49\n",
      "epoch 3, step 14750/55000, batch loss = 0.43\n",
      "epoch 3, step 15000/55000, batch loss = 0.46\n",
      "Train accuracy = 95.83\n",
      "epoch 3, step 15250/55000, batch loss = 0.40\n",
      "epoch 3, step 15500/55000, batch loss = 0.42\n",
      "epoch 3, step 15750/55000, batch loss = 0.43\n",
      "epoch 3, step 16000/55000, batch loss = 0.40\n",
      "epoch 3, step 16250/55000, batch loss = 0.36\n",
      "epoch 3, step 16500/55000, batch loss = 0.48\n",
      "epoch 3, step 16750/55000, batch loss = 0.44\n",
      "epoch 3, step 17000/55000, batch loss = 0.48\n",
      "epoch 3, step 17250/55000, batch loss = 0.41\n",
      "epoch 3, step 17500/55000, batch loss = 0.46\n",
      "Train accuracy = 95.82\n",
      "epoch 3, step 17750/55000, batch loss = 0.42\n",
      "epoch 3, step 18000/55000, batch loss = 0.38\n",
      "epoch 3, step 18250/55000, batch loss = 0.46\n",
      "epoch 3, step 18500/55000, batch loss = 0.44\n",
      "epoch 3, step 18750/55000, batch loss = 0.41\n",
      "epoch 3, step 19000/55000, batch loss = 0.43\n",
      "epoch 3, step 19250/55000, batch loss = 0.45\n",
      "epoch 3, step 19500/55000, batch loss = 0.40\n",
      "epoch 3, step 19750/55000, batch loss = 0.45\n",
      "epoch 3, step 20000/55000, batch loss = 0.69\n",
      "Train accuracy = 95.83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, step 20250/55000, batch loss = 0.61\n",
      "epoch 3, step 20500/55000, batch loss = 0.37\n",
      "epoch 3, step 20750/55000, batch loss = 0.41\n",
      "epoch 3, step 21000/55000, batch loss = 0.54\n",
      "epoch 3, step 21250/55000, batch loss = 0.48\n",
      "epoch 3, step 21500/55000, batch loss = 0.52\n",
      "epoch 3, step 21750/55000, batch loss = 0.41\n",
      "epoch 3, step 22000/55000, batch loss = 0.47\n",
      "epoch 3, step 22250/55000, batch loss = 0.41\n",
      "epoch 3, step 22500/55000, batch loss = 0.41\n",
      "Train accuracy = 95.82\n",
      "epoch 3, step 22750/55000, batch loss = 0.43\n",
      "epoch 3, step 23000/55000, batch loss = 0.43\n",
      "epoch 3, step 23250/55000, batch loss = 0.41\n",
      "epoch 3, step 23500/55000, batch loss = 0.46\n",
      "epoch 3, step 23750/55000, batch loss = 0.43\n",
      "epoch 3, step 24000/55000, batch loss = 0.45\n",
      "epoch 3, step 24250/55000, batch loss = 0.39\n",
      "epoch 3, step 24500/55000, batch loss = 0.40\n",
      "epoch 3, step 24750/55000, batch loss = 0.42\n",
      "epoch 3, step 25000/55000, batch loss = 0.61\n",
      "Train accuracy = 95.80\n",
      "epoch 3, step 25250/55000, batch loss = 0.42\n",
      "epoch 3, step 25500/55000, batch loss = 0.51\n",
      "epoch 3, step 25750/55000, batch loss = 0.43\n",
      "epoch 3, step 26000/55000, batch loss = 0.44\n",
      "epoch 3, step 26250/55000, batch loss = 0.47\n",
      "epoch 3, step 26500/55000, batch loss = 0.60\n",
      "epoch 3, step 26750/55000, batch loss = 0.48\n",
      "epoch 3, step 27000/55000, batch loss = 0.51\n",
      "epoch 3, step 27250/55000, batch loss = 0.46\n",
      "epoch 3, step 27500/55000, batch loss = 0.47\n",
      "Train accuracy = 95.79\n",
      "epoch 3, step 27750/55000, batch loss = 0.49\n",
      "epoch 3, step 28000/55000, batch loss = 0.58\n",
      "epoch 3, step 28250/55000, batch loss = 0.52\n",
      "epoch 3, step 28500/55000, batch loss = 0.40\n",
      "epoch 3, step 28750/55000, batch loss = 0.39\n",
      "epoch 3, step 29000/55000, batch loss = 0.39\n",
      "epoch 3, step 29250/55000, batch loss = 0.43\n",
      "epoch 3, step 29500/55000, batch loss = 0.44\n",
      "epoch 3, step 29750/55000, batch loss = 0.40\n",
      "epoch 3, step 30000/55000, batch loss = 0.43\n",
      "Train accuracy = 95.77\n",
      "epoch 3, step 30250/55000, batch loss = 0.40\n",
      "epoch 3, step 30500/55000, batch loss = 0.49\n",
      "epoch 3, step 30750/55000, batch loss = 0.45\n",
      "epoch 3, step 31000/55000, batch loss = 0.41\n",
      "epoch 3, step 31250/55000, batch loss = 0.51\n",
      "epoch 3, step 31500/55000, batch loss = 0.55\n",
      "epoch 3, step 31750/55000, batch loss = 0.46\n",
      "epoch 3, step 32000/55000, batch loss = 0.41\n",
      "epoch 3, step 32250/55000, batch loss = 0.55\n",
      "epoch 3, step 32500/55000, batch loss = 0.48\n",
      "Train accuracy = 95.77\n",
      "epoch 3, step 32750/55000, batch loss = 0.40\n",
      "epoch 3, step 33000/55000, batch loss = 0.38\n",
      "epoch 3, step 33250/55000, batch loss = 0.37\n",
      "epoch 3, step 33500/55000, batch loss = 0.54\n",
      "epoch 3, step 33750/55000, batch loss = 0.46\n",
      "epoch 3, step 34000/55000, batch loss = 0.54\n",
      "epoch 3, step 34250/55000, batch loss = 0.38\n",
      "epoch 3, step 34500/55000, batch loss = 0.39\n",
      "epoch 3, step 34750/55000, batch loss = 0.43\n",
      "epoch 3, step 35000/55000, batch loss = 0.37\n",
      "Train accuracy = 95.81\n",
      "epoch 3, step 35250/55000, batch loss = 0.47\n",
      "epoch 3, step 35500/55000, batch loss = 0.40\n",
      "epoch 3, step 35750/55000, batch loss = 0.39\n",
      "epoch 3, step 36000/55000, batch loss = 0.53\n",
      "epoch 3, step 36250/55000, batch loss = 0.41\n",
      "epoch 3, step 36500/55000, batch loss = 0.42\n",
      "epoch 3, step 36750/55000, batch loss = 0.54\n",
      "epoch 3, step 37000/55000, batch loss = 0.35\n",
      "epoch 3, step 37250/55000, batch loss = 0.51\n",
      "epoch 3, step 37500/55000, batch loss = 0.46\n",
      "Train accuracy = 95.86\n",
      "epoch 3, step 37750/55000, batch loss = 0.38\n",
      "epoch 3, step 38000/55000, batch loss = 0.46\n",
      "epoch 3, step 38250/55000, batch loss = 0.45\n",
      "epoch 3, step 38500/55000, batch loss = 0.49\n",
      "epoch 3, step 38750/55000, batch loss = 0.45\n",
      "epoch 3, step 39000/55000, batch loss = 0.38\n",
      "epoch 3, step 39250/55000, batch loss = 0.39\n",
      "epoch 3, step 39500/55000, batch loss = 0.44\n",
      "epoch 3, step 39750/55000, batch loss = 0.45\n",
      "epoch 3, step 40000/55000, batch loss = 0.51\n",
      "Train accuracy = 95.85\n",
      "epoch 3, step 40250/55000, batch loss = 0.43\n",
      "epoch 3, step 40500/55000, batch loss = 0.48\n",
      "epoch 3, step 40750/55000, batch loss = 0.46\n",
      "epoch 3, step 41000/55000, batch loss = 0.39\n",
      "epoch 3, step 41250/55000, batch loss = 0.43\n",
      "epoch 3, step 41500/55000, batch loss = 0.44\n",
      "epoch 3, step 41750/55000, batch loss = 0.46\n",
      "epoch 3, step 42000/55000, batch loss = 0.43\n",
      "epoch 3, step 42250/55000, batch loss = 0.39\n",
      "epoch 3, step 42500/55000, batch loss = 0.33\n",
      "Train accuracy = 95.88\n",
      "epoch 3, step 42750/55000, batch loss = 0.54\n",
      "epoch 3, step 43000/55000, batch loss = 0.48\n",
      "epoch 3, step 43250/55000, batch loss = 0.45\n",
      "epoch 3, step 43500/55000, batch loss = 0.42\n",
      "epoch 3, step 43750/55000, batch loss = 0.50\n",
      "epoch 3, step 44000/55000, batch loss = 0.42\n",
      "epoch 3, step 44250/55000, batch loss = 0.44\n",
      "epoch 3, step 44500/55000, batch loss = 0.51\n",
      "epoch 3, step 44750/55000, batch loss = 0.53\n",
      "epoch 3, step 45000/55000, batch loss = 0.44\n",
      "Train accuracy = 95.88\n",
      "epoch 3, step 45250/55000, batch loss = 0.50\n",
      "epoch 3, step 45500/55000, batch loss = 0.41\n",
      "epoch 3, step 45750/55000, batch loss = 0.43\n",
      "epoch 3, step 46000/55000, batch loss = 0.38\n",
      "epoch 3, step 46250/55000, batch loss = 0.43\n",
      "epoch 3, step 46500/55000, batch loss = 0.45\n",
      "epoch 3, step 46750/55000, batch loss = 0.51\n",
      "epoch 3, step 47000/55000, batch loss = 0.37\n",
      "epoch 3, step 47250/55000, batch loss = 0.52\n",
      "epoch 3, step 47500/55000, batch loss = 0.44\n",
      "Train accuracy = 95.86\n",
      "epoch 3, step 47750/55000, batch loss = 0.44\n",
      "epoch 3, step 48000/55000, batch loss = 0.45\n",
      "epoch 3, step 48250/55000, batch loss = 0.41\n",
      "epoch 3, step 48500/55000, batch loss = 0.36\n",
      "epoch 3, step 48750/55000, batch loss = 0.40\n",
      "epoch 3, step 49000/55000, batch loss = 0.44\n",
      "epoch 3, step 49250/55000, batch loss = 0.45\n",
      "epoch 3, step 49500/55000, batch loss = 0.41\n",
      "epoch 3, step 49750/55000, batch loss = 0.35\n",
      "epoch 3, step 50000/55000, batch loss = 0.44\n",
      "Train accuracy = 95.88\n",
      "epoch 3, step 50250/55000, batch loss = 0.41\n",
      "epoch 3, step 50500/55000, batch loss = 0.51\n",
      "epoch 3, step 50750/55000, batch loss = 0.42\n",
      "epoch 3, step 51000/55000, batch loss = 0.49\n",
      "epoch 3, step 51250/55000, batch loss = 0.41\n",
      "epoch 3, step 51500/55000, batch loss = 0.41\n",
      "epoch 3, step 51750/55000, batch loss = 0.44\n",
      "epoch 3, step 52000/55000, batch loss = 0.55\n",
      "epoch 3, step 52250/55000, batch loss = 0.43\n",
      "epoch 3, step 52500/55000, batch loss = 0.46\n",
      "Train accuracy = 95.85\n",
      "epoch 3, step 52750/55000, batch loss = 0.42\n",
      "epoch 3, step 53000/55000, batch loss = 0.40\n",
      "epoch 3, step 53250/55000, batch loss = 0.40\n",
      "epoch 3, step 53500/55000, batch loss = 0.45\n",
      "epoch 3, step 53750/55000, batch loss = 0.52\n",
      "epoch 3, step 54000/55000, batch loss = 0.47\n",
      "epoch 3, step 54250/55000, batch loss = 0.48\n",
      "epoch 3, step 54500/55000, batch loss = 0.49\n",
      "epoch 3, step 54750/55000, batch loss = 0.56\n",
      "Train accuracy = 95.80\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 96.44\n",
      "Validation avg loss = 0.43\n",
      "\n",
      "epoch 4, step 0/55000, batch loss = 0.39\n",
      "epoch 4, step 250/55000, batch loss = 0.38\n",
      "epoch 4, step 500/55000, batch loss = 0.43\n",
      "epoch 4, step 750/55000, batch loss = 0.49\n",
      "epoch 4, step 1000/55000, batch loss = 0.65\n",
      "epoch 4, step 1250/55000, batch loss = 0.43\n",
      "epoch 4, step 1500/55000, batch loss = 0.46\n",
      "epoch 4, step 1750/55000, batch loss = 0.38\n",
      "epoch 4, step 2000/55000, batch loss = 0.46\n",
      "epoch 4, step 2250/55000, batch loss = 0.50\n",
      "epoch 4, step 2500/55000, batch loss = 0.45\n",
      "Train accuracy = 95.29\n",
      "epoch 4, step 2750/55000, batch loss = 0.46\n",
      "epoch 4, step 3000/55000, batch loss = 0.45\n",
      "epoch 4, step 3250/55000, batch loss = 0.34\n",
      "epoch 4, step 3500/55000, batch loss = 0.46\n",
      "epoch 4, step 3750/55000, batch loss = 0.55\n",
      "epoch 4, step 4000/55000, batch loss = 0.45\n",
      "epoch 4, step 4250/55000, batch loss = 0.36\n",
      "epoch 4, step 4500/55000, batch loss = 0.34\n",
      "epoch 4, step 4750/55000, batch loss = 0.39\n",
      "epoch 4, step 5000/55000, batch loss = 0.43\n",
      "Train accuracy = 95.78\n",
      "epoch 4, step 5250/55000, batch loss = 0.43\n",
      "epoch 4, step 5500/55000, batch loss = 0.48\n",
      "epoch 4, step 5750/55000, batch loss = 0.44\n",
      "epoch 4, step 6000/55000, batch loss = 0.46\n",
      "epoch 4, step 6250/55000, batch loss = 0.50\n",
      "epoch 4, step 6500/55000, batch loss = 0.41\n",
      "epoch 4, step 6750/55000, batch loss = 0.37\n",
      "epoch 4, step 7000/55000, batch loss = 0.37\n",
      "epoch 4, step 7250/55000, batch loss = 0.51\n",
      "epoch 4, step 7500/55000, batch loss = 0.36\n",
      "Train accuracy = 95.95\n",
      "epoch 4, step 7750/55000, batch loss = 0.38\n",
      "epoch 4, step 8000/55000, batch loss = 0.63\n",
      "epoch 4, step 8250/55000, batch loss = 0.41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, step 8500/55000, batch loss = 0.37\n",
      "epoch 4, step 8750/55000, batch loss = 0.43\n",
      "epoch 4, step 9000/55000, batch loss = 0.43\n",
      "epoch 4, step 9250/55000, batch loss = 0.42\n",
      "epoch 4, step 9500/55000, batch loss = 0.49\n",
      "epoch 4, step 9750/55000, batch loss = 0.43\n",
      "epoch 4, step 10000/55000, batch loss = 0.43\n",
      "Train accuracy = 96.08\n",
      "epoch 4, step 10250/55000, batch loss = 0.52\n",
      "epoch 4, step 10500/55000, batch loss = 0.35\n",
      "epoch 4, step 10750/55000, batch loss = 0.43\n",
      "epoch 4, step 11000/55000, batch loss = 0.36\n",
      "epoch 4, step 11250/55000, batch loss = 0.44\n",
      "epoch 4, step 11500/55000, batch loss = 0.46\n",
      "epoch 4, step 11750/55000, batch loss = 0.48\n",
      "epoch 4, step 12000/55000, batch loss = 0.44\n",
      "epoch 4, step 12250/55000, batch loss = 0.41\n",
      "epoch 4, step 12500/55000, batch loss = 0.45\n",
      "Train accuracy = 96.06\n",
      "epoch 4, step 12750/55000, batch loss = 0.43\n",
      "epoch 4, step 13000/55000, batch loss = 0.43\n",
      "epoch 4, step 13250/55000, batch loss = 0.50\n",
      "epoch 4, step 13500/55000, batch loss = 0.46\n",
      "epoch 4, step 13750/55000, batch loss = 0.40\n",
      "epoch 4, step 14000/55000, batch loss = 0.41\n",
      "epoch 4, step 14250/55000, batch loss = 0.45\n",
      "epoch 4, step 14500/55000, batch loss = 0.50\n",
      "epoch 4, step 14750/55000, batch loss = 0.48\n",
      "epoch 4, step 15000/55000, batch loss = 0.42\n",
      "Train accuracy = 96.03\n",
      "epoch 4, step 15250/55000, batch loss = 0.41\n",
      "epoch 4, step 15500/55000, batch loss = 0.42\n",
      "epoch 4, step 15750/55000, batch loss = 0.49\n",
      "epoch 4, step 16000/55000, batch loss = 0.43\n",
      "epoch 4, step 16250/55000, batch loss = 0.48\n",
      "epoch 4, step 16500/55000, batch loss = 0.37\n",
      "epoch 4, step 16750/55000, batch loss = 0.42\n",
      "epoch 4, step 17000/55000, batch loss = 0.47\n",
      "epoch 4, step 17250/55000, batch loss = 0.57\n",
      "epoch 4, step 17500/55000, batch loss = 0.42\n",
      "Train accuracy = 95.99\n",
      "epoch 4, step 17750/55000, batch loss = 0.37\n",
      "epoch 4, step 18000/55000, batch loss = 0.43\n",
      "epoch 4, step 18250/55000, batch loss = 0.39\n",
      "epoch 4, step 18500/55000, batch loss = 0.36\n",
      "epoch 4, step 18750/55000, batch loss = 0.42\n",
      "epoch 4, step 19000/55000, batch loss = 0.38\n",
      "epoch 4, step 19250/55000, batch loss = 0.39\n",
      "epoch 4, step 19500/55000, batch loss = 0.57\n",
      "epoch 4, step 19750/55000, batch loss = 0.41\n",
      "epoch 4, step 20000/55000, batch loss = 0.40\n",
      "Train accuracy = 96.07\n",
      "epoch 4, step 20250/55000, batch loss = 0.50\n",
      "epoch 4, step 20500/55000, batch loss = 0.43\n",
      "epoch 4, step 20750/55000, batch loss = 0.47\n",
      "epoch 4, step 21000/55000, batch loss = 0.49\n",
      "epoch 4, step 21250/55000, batch loss = 0.34\n",
      "epoch 4, step 21500/55000, batch loss = 0.42\n",
      "epoch 4, step 21750/55000, batch loss = 0.40\n",
      "epoch 4, step 22000/55000, batch loss = 0.36\n",
      "epoch 4, step 22250/55000, batch loss = 0.53\n",
      "epoch 4, step 22500/55000, batch loss = 0.42\n",
      "Train accuracy = 96.02\n",
      "epoch 4, step 22750/55000, batch loss = 0.37\n",
      "epoch 4, step 23000/55000, batch loss = 0.39\n",
      "epoch 4, step 23250/55000, batch loss = 0.41\n",
      "epoch 4, step 23500/55000, batch loss = 0.39\n",
      "epoch 4, step 23750/55000, batch loss = 0.63\n",
      "epoch 4, step 24000/55000, batch loss = 0.50\n",
      "epoch 4, step 24250/55000, batch loss = 0.42\n",
      "epoch 4, step 24500/55000, batch loss = 0.45\n",
      "epoch 4, step 24750/55000, batch loss = 0.50\n",
      "epoch 4, step 25000/55000, batch loss = 0.58\n",
      "Train accuracy = 95.97\n",
      "epoch 4, step 25250/55000, batch loss = 0.48\n",
      "epoch 4, step 25500/55000, batch loss = 0.40\n",
      "epoch 4, step 25750/55000, batch loss = 0.42\n",
      "epoch 4, step 26000/55000, batch loss = 0.40\n",
      "epoch 4, step 26250/55000, batch loss = 0.41\n",
      "epoch 4, step 26500/55000, batch loss = 0.33\n",
      "epoch 4, step 26750/55000, batch loss = 0.43\n",
      "epoch 4, step 27000/55000, batch loss = 0.39\n",
      "epoch 4, step 27250/55000, batch loss = 0.40\n",
      "epoch 4, step 27500/55000, batch loss = 0.39\n",
      "Train accuracy = 95.95\n",
      "epoch 4, step 27750/55000, batch loss = 0.46\n",
      "epoch 4, step 28000/55000, batch loss = 0.44\n",
      "epoch 4, step 28250/55000, batch loss = 0.47\n",
      "epoch 4, step 28500/55000, batch loss = 0.41\n",
      "epoch 4, step 28750/55000, batch loss = 0.41\n",
      "epoch 4, step 29000/55000, batch loss = 0.44\n",
      "epoch 4, step 29250/55000, batch loss = 0.47\n",
      "epoch 4, step 29500/55000, batch loss = 0.62\n",
      "epoch 4, step 29750/55000, batch loss = 0.49\n",
      "epoch 4, step 30000/55000, batch loss = 0.47\n",
      "Train accuracy = 95.91\n",
      "epoch 4, step 30250/55000, batch loss = 0.52\n",
      "epoch 4, step 30500/55000, batch loss = 0.47\n",
      "epoch 4, step 30750/55000, batch loss = 0.44\n",
      "epoch 4, step 31000/55000, batch loss = 0.50\n",
      "epoch 4, step 31250/55000, batch loss = 0.36\n",
      "epoch 4, step 31500/55000, batch loss = 0.49\n",
      "epoch 4, step 31750/55000, batch loss = 0.45\n",
      "epoch 4, step 32000/55000, batch loss = 0.44\n",
      "epoch 4, step 32250/55000, batch loss = 0.43\n",
      "epoch 4, step 32500/55000, batch loss = 0.50\n",
      "Train accuracy = 95.92\n",
      "epoch 4, step 32750/55000, batch loss = 0.56\n",
      "epoch 4, step 33000/55000, batch loss = 0.52\n",
      "epoch 4, step 33250/55000, batch loss = 0.54\n",
      "epoch 4, step 33500/55000, batch loss = 0.48\n",
      "epoch 4, step 33750/55000, batch loss = 0.52\n",
      "epoch 4, step 34000/55000, batch loss = 0.43\n",
      "epoch 4, step 34250/55000, batch loss = 0.59\n",
      "epoch 4, step 34500/55000, batch loss = 0.51\n",
      "epoch 4, step 34750/55000, batch loss = 0.42\n",
      "epoch 4, step 35000/55000, batch loss = 0.42\n",
      "Train accuracy = 95.87\n",
      "epoch 4, step 35250/55000, batch loss = 0.43\n",
      "epoch 4, step 35500/55000, batch loss = 0.56\n",
      "epoch 4, step 35750/55000, batch loss = 0.39\n",
      "epoch 4, step 36000/55000, batch loss = 0.40\n",
      "epoch 4, step 36250/55000, batch loss = 0.47\n",
      "epoch 4, step 36500/55000, batch loss = 0.50\n",
      "epoch 4, step 36750/55000, batch loss = 0.42\n",
      "epoch 4, step 37000/55000, batch loss = 0.46\n",
      "epoch 4, step 37250/55000, batch loss = 0.32\n",
      "epoch 4, step 37500/55000, batch loss = 0.40\n",
      "Train accuracy = 95.90\n",
      "epoch 4, step 37750/55000, batch loss = 0.46\n",
      "epoch 4, step 38000/55000, batch loss = 0.44\n",
      "epoch 4, step 38250/55000, batch loss = 0.38\n",
      "epoch 4, step 38500/55000, batch loss = 0.46\n",
      "epoch 4, step 38750/55000, batch loss = 0.36\n",
      "epoch 4, step 39000/55000, batch loss = 0.47\n",
      "epoch 4, step 39250/55000, batch loss = 0.44\n",
      "epoch 4, step 39500/55000, batch loss = 0.52\n",
      "epoch 4, step 39750/55000, batch loss = 0.39\n",
      "epoch 4, step 40000/55000, batch loss = 0.41\n",
      "Train accuracy = 95.91\n",
      "epoch 4, step 40250/55000, batch loss = 0.49\n",
      "epoch 4, step 40500/55000, batch loss = 0.37\n",
      "epoch 4, step 40750/55000, batch loss = 0.41\n",
      "epoch 4, step 41000/55000, batch loss = 0.41\n",
      "epoch 4, step 41250/55000, batch loss = 0.44\n",
      "epoch 4, step 41500/55000, batch loss = 0.38\n",
      "epoch 4, step 41750/55000, batch loss = 0.44\n",
      "epoch 4, step 42000/55000, batch loss = 0.53\n",
      "epoch 4, step 42250/55000, batch loss = 0.55\n",
      "epoch 4, step 42500/55000, batch loss = 0.56\n",
      "Train accuracy = 95.90\n",
      "epoch 4, step 42750/55000, batch loss = 0.57\n",
      "epoch 4, step 43000/55000, batch loss = 0.36\n",
      "epoch 4, step 43250/55000, batch loss = 0.54\n",
      "epoch 4, step 43500/55000, batch loss = 0.42\n",
      "epoch 4, step 43750/55000, batch loss = 0.49\n",
      "epoch 4, step 44000/55000, batch loss = 0.44\n",
      "epoch 4, step 44250/55000, batch loss = 0.52\n",
      "epoch 4, step 44500/55000, batch loss = 0.38\n",
      "epoch 4, step 44750/55000, batch loss = 0.43\n",
      "epoch 4, step 45000/55000, batch loss = 0.46\n",
      "Train accuracy = 95.92\n",
      "epoch 4, step 45250/55000, batch loss = 0.39\n",
      "epoch 4, step 45500/55000, batch loss = 0.50\n",
      "epoch 4, step 45750/55000, batch loss = 0.42\n",
      "epoch 4, step 46000/55000, batch loss = 0.46\n",
      "epoch 4, step 46250/55000, batch loss = 0.39\n",
      "epoch 4, step 46500/55000, batch loss = 0.51\n",
      "epoch 4, step 46750/55000, batch loss = 0.47\n",
      "epoch 4, step 47000/55000, batch loss = 0.43\n",
      "epoch 4, step 47250/55000, batch loss = 0.51\n",
      "epoch 4, step 47500/55000, batch loss = 0.42\n",
      "Train accuracy = 95.90\n",
      "epoch 4, step 47750/55000, batch loss = 0.39\n",
      "epoch 4, step 48000/55000, batch loss = 0.40\n",
      "epoch 4, step 48250/55000, batch loss = 0.35\n",
      "epoch 4, step 48500/55000, batch loss = 0.44\n",
      "epoch 4, step 48750/55000, batch loss = 0.45\n",
      "epoch 4, step 49000/55000, batch loss = 0.50\n",
      "epoch 4, step 49250/55000, batch loss = 0.44\n",
      "epoch 4, step 49500/55000, batch loss = 0.44\n",
      "epoch 4, step 49750/55000, batch loss = 0.35\n",
      "epoch 4, step 50000/55000, batch loss = 0.41\n",
      "Train accuracy = 95.90\n",
      "epoch 4, step 50250/55000, batch loss = 0.49\n",
      "epoch 4, step 50500/55000, batch loss = 0.44\n",
      "epoch 4, step 50750/55000, batch loss = 0.48\n",
      "epoch 4, step 51000/55000, batch loss = 0.43\n",
      "epoch 4, step 51250/55000, batch loss = 0.45\n",
      "epoch 4, step 51500/55000, batch loss = 0.44\n",
      "epoch 4, step 51750/55000, batch loss = 0.40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, step 52000/55000, batch loss = 0.44\n",
      "epoch 4, step 52250/55000, batch loss = 0.49\n",
      "epoch 4, step 52500/55000, batch loss = 0.40\n",
      "Train accuracy = 95.89\n",
      "epoch 4, step 52750/55000, batch loss = 0.38\n",
      "epoch 4, step 53000/55000, batch loss = 0.52\n",
      "epoch 4, step 53250/55000, batch loss = 0.47\n",
      "epoch 4, step 53500/55000, batch loss = 0.38\n",
      "epoch 4, step 53750/55000, batch loss = 0.38\n",
      "epoch 4, step 54000/55000, batch loss = 0.52\n",
      "epoch 4, step 54250/55000, batch loss = 0.46\n",
      "epoch 4, step 54500/55000, batch loss = 0.47\n",
      "epoch 4, step 54750/55000, batch loss = 0.36\n",
      "Train accuracy = 95.88\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 96.38\n",
      "Validation avg loss = 0.42\n",
      "\n",
      "epoch 5, step 0/55000, batch loss = 0.43\n",
      "epoch 5, step 250/55000, batch loss = 0.53\n",
      "epoch 5, step 500/55000, batch loss = 0.47\n",
      "epoch 5, step 750/55000, batch loss = 0.47\n",
      "epoch 5, step 1000/55000, batch loss = 0.42\n",
      "epoch 5, step 1250/55000, batch loss = 0.38\n",
      "epoch 5, step 1500/55000, batch loss = 0.40\n",
      "epoch 5, step 1750/55000, batch loss = 0.47\n",
      "epoch 5, step 2000/55000, batch loss = 0.42\n",
      "epoch 5, step 2250/55000, batch loss = 0.35\n",
      "epoch 5, step 2500/55000, batch loss = 0.34\n",
      "Train accuracy = 96.63\n",
      "epoch 5, step 2750/55000, batch loss = 0.33\n",
      "epoch 5, step 3000/55000, batch loss = 0.43\n",
      "epoch 5, step 3250/55000, batch loss = 0.41\n",
      "epoch 5, step 3500/55000, batch loss = 0.52\n",
      "epoch 5, step 3750/55000, batch loss = 0.45\n",
      "epoch 5, step 4000/55000, batch loss = 0.47\n",
      "epoch 5, step 4250/55000, batch loss = 0.50\n",
      "epoch 5, step 4500/55000, batch loss = 0.45\n",
      "epoch 5, step 4750/55000, batch loss = 0.43\n",
      "epoch 5, step 5000/55000, batch loss = 0.38\n",
      "Train accuracy = 96.40\n",
      "epoch 5, step 5250/55000, batch loss = 0.38\n",
      "epoch 5, step 5500/55000, batch loss = 0.35\n",
      "epoch 5, step 5750/55000, batch loss = 0.33\n",
      "epoch 5, step 6000/55000, batch loss = 0.46\n",
      "epoch 5, step 6250/55000, batch loss = 0.40\n",
      "epoch 5, step 6500/55000, batch loss = 0.40\n",
      "epoch 5, step 6750/55000, batch loss = 0.46\n",
      "epoch 5, step 7000/55000, batch loss = 0.40\n",
      "epoch 5, step 7250/55000, batch loss = 0.51\n",
      "epoch 5, step 7500/55000, batch loss = 0.41\n",
      "Train accuracy = 96.46\n",
      "epoch 5, step 7750/55000, batch loss = 0.44\n",
      "epoch 5, step 8000/55000, batch loss = 0.38\n",
      "epoch 5, step 8250/55000, batch loss = 0.47\n",
      "epoch 5, step 8500/55000, batch loss = 0.44\n",
      "epoch 5, step 8750/55000, batch loss = 0.41\n",
      "epoch 5, step 9000/55000, batch loss = 0.37\n",
      "epoch 5, step 9250/55000, batch loss = 0.39\n",
      "epoch 5, step 9500/55000, batch loss = 0.40\n",
      "epoch 5, step 9750/55000, batch loss = 0.44\n",
      "epoch 5, step 10000/55000, batch loss = 0.52\n",
      "Train accuracy = 96.32\n",
      "epoch 5, step 10250/55000, batch loss = 0.39\n",
      "epoch 5, step 10500/55000, batch loss = 0.47\n",
      "epoch 5, step 10750/55000, batch loss = 0.50\n",
      "epoch 5, step 11000/55000, batch loss = 0.49\n",
      "epoch 5, step 11250/55000, batch loss = 0.43\n",
      "epoch 5, step 11500/55000, batch loss = 0.39\n",
      "epoch 5, step 11750/55000, batch loss = 0.34\n",
      "epoch 5, step 12000/55000, batch loss = 0.41\n",
      "epoch 5, step 12250/55000, batch loss = 0.40\n",
      "epoch 5, step 12500/55000, batch loss = 0.43\n",
      "Train accuracy = 96.27\n",
      "epoch 5, step 12750/55000, batch loss = 0.39\n",
      "epoch 5, step 13000/55000, batch loss = 0.37\n",
      "epoch 5, step 13250/55000, batch loss = 0.37\n",
      "epoch 5, step 13500/55000, batch loss = 0.38\n",
      "epoch 5, step 13750/55000, batch loss = 0.44\n",
      "epoch 5, step 14000/55000, batch loss = 0.41\n",
      "epoch 5, step 14250/55000, batch loss = 0.39\n",
      "epoch 5, step 14500/55000, batch loss = 0.46\n",
      "epoch 5, step 14750/55000, batch loss = 0.40\n",
      "epoch 5, step 15000/55000, batch loss = 0.36\n",
      "Train accuracy = 96.32\n",
      "epoch 5, step 15250/55000, batch loss = 0.53\n",
      "epoch 5, step 15500/55000, batch loss = 0.42\n",
      "epoch 5, step 15750/55000, batch loss = 0.40\n",
      "epoch 5, step 16000/55000, batch loss = 0.35\n",
      "epoch 5, step 16250/55000, batch loss = 0.37\n",
      "epoch 5, step 16500/55000, batch loss = 0.36\n",
      "epoch 5, step 16750/55000, batch loss = 0.40\n",
      "epoch 5, step 17000/55000, batch loss = 0.38\n",
      "epoch 5, step 17250/55000, batch loss = 0.42\n",
      "epoch 5, step 17500/55000, batch loss = 0.48\n",
      "Train accuracy = 96.35\n",
      "epoch 5, step 17750/55000, batch loss = 0.35\n",
      "epoch 5, step 18000/55000, batch loss = 0.63\n",
      "epoch 5, step 18250/55000, batch loss = 0.42\n",
      "epoch 5, step 18500/55000, batch loss = 0.36\n",
      "epoch 5, step 18750/55000, batch loss = 0.33\n",
      "epoch 5, step 19000/55000, batch loss = 0.39\n",
      "epoch 5, step 19250/55000, batch loss = 0.37\n",
      "epoch 5, step 19500/55000, batch loss = 0.56\n",
      "epoch 5, step 19750/55000, batch loss = 0.42\n",
      "epoch 5, step 20000/55000, batch loss = 0.53\n",
      "Train accuracy = 96.27\n",
      "epoch 5, step 20250/55000, batch loss = 0.40\n",
      "epoch 5, step 20500/55000, batch loss = 0.39\n",
      "epoch 5, step 20750/55000, batch loss = 0.36\n",
      "epoch 5, step 21000/55000, batch loss = 0.35\n",
      "epoch 5, step 21250/55000, batch loss = 0.58\n",
      "epoch 5, step 21500/55000, batch loss = 0.52\n",
      "epoch 5, step 21750/55000, batch loss = 0.41\n",
      "epoch 5, step 22000/55000, batch loss = 0.50\n",
      "epoch 5, step 22250/55000, batch loss = 0.46\n",
      "epoch 5, step 22500/55000, batch loss = 0.43\n",
      "Train accuracy = 96.27\n",
      "epoch 5, step 22750/55000, batch loss = 0.42\n",
      "epoch 5, step 23000/55000, batch loss = 0.37\n",
      "epoch 5, step 23250/55000, batch loss = 0.39\n",
      "epoch 5, step 23500/55000, batch loss = 0.39\n",
      "epoch 5, step 23750/55000, batch loss = 0.33\n",
      "epoch 5, step 24000/55000, batch loss = 0.43\n",
      "epoch 5, step 24250/55000, batch loss = 0.36\n",
      "epoch 5, step 24500/55000, batch loss = 0.43\n",
      "epoch 5, step 24750/55000, batch loss = 0.37\n",
      "epoch 5, step 25000/55000, batch loss = 0.50\n",
      "Train accuracy = 96.28\n",
      "epoch 5, step 25250/55000, batch loss = 0.41\n",
      "epoch 5, step 25500/55000, batch loss = 0.46\n",
      "epoch 5, step 25750/55000, batch loss = 0.47\n",
      "epoch 5, step 26000/55000, batch loss = 0.46\n",
      "epoch 5, step 26250/55000, batch loss = 0.37\n",
      "epoch 5, step 26500/55000, batch loss = 0.45\n",
      "epoch 5, step 26750/55000, batch loss = 0.40\n",
      "epoch 5, step 27000/55000, batch loss = 0.47\n",
      "epoch 5, step 27250/55000, batch loss = 0.38\n",
      "epoch 5, step 27500/55000, batch loss = 0.35\n",
      "Train accuracy = 96.32\n",
      "epoch 5, step 27750/55000, batch loss = 0.51\n",
      "epoch 5, step 28000/55000, batch loss = 0.41\n",
      "epoch 5, step 28250/55000, batch loss = 0.44\n",
      "epoch 5, step 28500/55000, batch loss = 0.42\n",
      "epoch 5, step 28750/55000, batch loss = 0.50\n",
      "epoch 5, step 29000/55000, batch loss = 0.40\n",
      "epoch 5, step 29250/55000, batch loss = 0.44\n",
      "epoch 5, step 29500/55000, batch loss = 0.42\n",
      "epoch 5, step 29750/55000, batch loss = 0.38\n",
      "epoch 5, step 30000/55000, batch loss = 0.37\n",
      "Train accuracy = 96.33\n",
      "epoch 5, step 30250/55000, batch loss = 0.45\n",
      "epoch 5, step 30500/55000, batch loss = 0.40\n",
      "epoch 5, step 30750/55000, batch loss = 0.44\n",
      "epoch 5, step 31000/55000, batch loss = 0.46\n",
      "epoch 5, step 31250/55000, batch loss = 0.34\n",
      "epoch 5, step 31500/55000, batch loss = 0.56\n",
      "epoch 5, step 31750/55000, batch loss = 0.44\n",
      "epoch 5, step 32000/55000, batch loss = 0.42\n",
      "epoch 5, step 32250/55000, batch loss = 0.42\n",
      "epoch 5, step 32500/55000, batch loss = 0.48\n",
      "Train accuracy = 96.29\n",
      "epoch 5, step 32750/55000, batch loss = 0.40\n",
      "epoch 5, step 33000/55000, batch loss = 0.40\n",
      "epoch 5, step 33250/55000, batch loss = 0.41\n",
      "epoch 5, step 33500/55000, batch loss = 0.40\n",
      "epoch 5, step 33750/55000, batch loss = 0.38\n",
      "epoch 5, step 34000/55000, batch loss = 0.58\n",
      "epoch 5, step 34250/55000, batch loss = 0.36\n",
      "epoch 5, step 34500/55000, batch loss = 0.39\n",
      "epoch 5, step 34750/55000, batch loss = 0.37\n",
      "epoch 5, step 35000/55000, batch loss = 0.36\n",
      "Train accuracy = 96.31\n",
      "epoch 5, step 35250/55000, batch loss = 0.48\n",
      "epoch 5, step 35500/55000, batch loss = 0.38\n",
      "epoch 5, step 35750/55000, batch loss = 0.41\n",
      "epoch 5, step 36000/55000, batch loss = 0.42\n",
      "epoch 5, step 36250/55000, batch loss = 0.39\n",
      "epoch 5, step 36500/55000, batch loss = 0.44\n",
      "epoch 5, step 36750/55000, batch loss = 0.34\n",
      "epoch 5, step 37000/55000, batch loss = 0.36\n",
      "epoch 5, step 37250/55000, batch loss = 0.40\n",
      "epoch 5, step 37500/55000, batch loss = 0.39\n",
      "Train accuracy = 96.35\n",
      "epoch 5, step 37750/55000, batch loss = 0.34\n",
      "epoch 5, step 38000/55000, batch loss = 0.49\n",
      "epoch 5, step 38250/55000, batch loss = 0.48\n",
      "epoch 5, step 38500/55000, batch loss = 0.45\n",
      "epoch 5, step 38750/55000, batch loss = 0.47\n",
      "epoch 5, step 39000/55000, batch loss = 0.38\n",
      "epoch 5, step 39250/55000, batch loss = 0.38\n",
      "epoch 5, step 39500/55000, batch loss = 0.46\n",
      "epoch 5, step 39750/55000, batch loss = 0.38\n",
      "epoch 5, step 40000/55000, batch loss = 0.37\n",
      "Train accuracy = 96.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, step 40250/55000, batch loss = 0.36\n",
      "epoch 5, step 40500/55000, batch loss = 0.44\n",
      "epoch 5, step 40750/55000, batch loss = 0.37\n",
      "epoch 5, step 41000/55000, batch loss = 0.38\n",
      "epoch 5, step 41250/55000, batch loss = 0.49\n",
      "epoch 5, step 41500/55000, batch loss = 0.46\n",
      "epoch 5, step 41750/55000, batch loss = 0.52\n",
      "epoch 5, step 42000/55000, batch loss = 0.46\n",
      "epoch 5, step 42250/55000, batch loss = 0.48\n",
      "epoch 5, step 42500/55000, batch loss = 0.43\n",
      "Train accuracy = 96.31\n",
      "epoch 5, step 42750/55000, batch loss = 0.45\n",
      "epoch 5, step 43000/55000, batch loss = 0.48\n",
      "epoch 5, step 43250/55000, batch loss = 0.47\n",
      "epoch 5, step 43500/55000, batch loss = 0.44\n",
      "epoch 5, step 43750/55000, batch loss = 0.53\n",
      "epoch 5, step 44000/55000, batch loss = 0.41\n",
      "epoch 5, step 44250/55000, batch loss = 0.54\n",
      "epoch 5, step 44500/55000, batch loss = 0.37\n",
      "epoch 5, step 44750/55000, batch loss = 0.38\n",
      "epoch 5, step 45000/55000, batch loss = 0.48\n",
      "Train accuracy = 96.29\n",
      "epoch 5, step 45250/55000, batch loss = 0.38\n",
      "epoch 5, step 45500/55000, batch loss = 0.40\n",
      "epoch 5, step 45750/55000, batch loss = 0.46\n",
      "epoch 5, step 46000/55000, batch loss = 0.45\n",
      "epoch 5, step 46250/55000, batch loss = 0.36\n",
      "epoch 5, step 46500/55000, batch loss = 0.52\n",
      "epoch 5, step 46750/55000, batch loss = 0.53\n",
      "epoch 5, step 47000/55000, batch loss = 0.37\n",
      "epoch 5, step 47250/55000, batch loss = 0.53\n",
      "epoch 5, step 47500/55000, batch loss = 0.44\n",
      "Train accuracy = 96.32\n",
      "epoch 5, step 47750/55000, batch loss = 0.33\n",
      "epoch 5, step 48000/55000, batch loss = 0.41\n",
      "epoch 5, step 48250/55000, batch loss = 0.47\n",
      "epoch 5, step 48500/55000, batch loss = 0.39\n",
      "epoch 5, step 48750/55000, batch loss = 0.45\n",
      "epoch 5, step 49000/55000, batch loss = 0.38\n",
      "epoch 5, step 49250/55000, batch loss = 0.56\n",
      "epoch 5, step 49500/55000, batch loss = 0.47\n",
      "epoch 5, step 49750/55000, batch loss = 0.37\n",
      "epoch 5, step 50000/55000, batch loss = 0.37\n",
      "Train accuracy = 96.30\n",
      "epoch 5, step 50250/55000, batch loss = 0.32\n",
      "epoch 5, step 50500/55000, batch loss = 0.36\n",
      "epoch 5, step 50750/55000, batch loss = 0.37\n",
      "epoch 5, step 51000/55000, batch loss = 0.37\n",
      "epoch 5, step 51250/55000, batch loss = 0.37\n",
      "epoch 5, step 51500/55000, batch loss = 0.47\n",
      "epoch 5, step 51750/55000, batch loss = 0.39\n",
      "epoch 5, step 52000/55000, batch loss = 0.44\n",
      "epoch 5, step 52250/55000, batch loss = 0.44\n",
      "epoch 5, step 52500/55000, batch loss = 0.36\n",
      "Train accuracy = 96.29\n",
      "epoch 5, step 52750/55000, batch loss = 0.42\n",
      "epoch 5, step 53000/55000, batch loss = 0.52\n",
      "epoch 5, step 53250/55000, batch loss = 0.39\n",
      "epoch 5, step 53500/55000, batch loss = 0.55\n",
      "epoch 5, step 53750/55000, batch loss = 0.43\n",
      "epoch 5, step 54000/55000, batch loss = 0.34\n",
      "epoch 5, step 54250/55000, batch loss = 0.40\n",
      "epoch 5, step 54500/55000, batch loss = 0.46\n",
      "epoch 5, step 54750/55000, batch loss = 0.50\n",
      "Train accuracy = 96.27\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 96.70\n",
      "Validation avg loss = 0.41\n",
      "\n",
      "\n",
      "Running evaluation:  Test\n",
      "Test accuracy = 96.52\n",
      "Test avg loss = 0.41\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LAMBDA: 0.01 \n",
      "\n",
      "epoch 1, step 0/55000, batch loss = 6.70\n",
      "epoch 1, step 250/55000, batch loss = 6.46\n",
      "epoch 1, step 500/55000, batch loss = 5.90\n",
      "epoch 1, step 750/55000, batch loss = 5.85\n",
      "epoch 1, step 1000/55000, batch loss = 5.26\n",
      "epoch 1, step 1250/55000, batch loss = 4.91\n",
      "epoch 1, step 1500/55000, batch loss = 4.72\n",
      "epoch 1, step 1750/55000, batch loss = 4.44\n",
      "epoch 1, step 2000/55000, batch loss = 4.84\n",
      "epoch 1, step 2250/55000, batch loss = 4.26\n",
      "epoch 1, step 2500/55000, batch loss = 4.28\n",
      "Train accuracy = 67.53\n",
      "epoch 1, step 2750/55000, batch loss = 4.66\n",
      "epoch 1, step 3000/55000, batch loss = 4.36\n",
      "epoch 1, step 3250/55000, batch loss = 4.16\n",
      "epoch 1, step 3500/55000, batch loss = 4.36\n",
      "epoch 1, step 3750/55000, batch loss = 4.14\n",
      "epoch 1, step 4000/55000, batch loss = 3.95\n",
      "epoch 1, step 4250/55000, batch loss = 4.01\n",
      "epoch 1, step 4500/55000, batch loss = 3.98\n",
      "epoch 1, step 4750/55000, batch loss = 3.87\n",
      "epoch 1, step 5000/55000, batch loss = 3.88\n",
      "Train accuracy = 78.85\n",
      "epoch 1, step 5250/55000, batch loss = 3.85\n",
      "epoch 1, step 5500/55000, batch loss = 3.68\n",
      "epoch 1, step 5750/55000, batch loss = 3.67\n",
      "epoch 1, step 6000/55000, batch loss = 3.81\n",
      "epoch 1, step 6250/55000, batch loss = 3.78\n",
      "epoch 1, step 6500/55000, batch loss = 3.63\n",
      "epoch 1, step 6750/55000, batch loss = 3.55\n",
      "epoch 1, step 7000/55000, batch loss = 3.56\n",
      "epoch 1, step 7250/55000, batch loss = 3.69\n",
      "epoch 1, step 7500/55000, batch loss = 3.35\n",
      "Train accuracy = 83.54\n",
      "epoch 1, step 7750/55000, batch loss = 3.51\n",
      "epoch 1, step 8000/55000, batch loss = 3.38\n",
      "epoch 1, step 8250/55000, batch loss = 3.38\n",
      "epoch 1, step 8500/55000, batch loss = 3.47\n",
      "epoch 1, step 8750/55000, batch loss = 3.23\n",
      "epoch 1, step 9000/55000, batch loss = 3.39\n",
      "epoch 1, step 9250/55000, batch loss = 3.29\n",
      "epoch 1, step 9500/55000, batch loss = 3.23\n",
      "epoch 1, step 9750/55000, batch loss = 3.19\n",
      "epoch 1, step 10000/55000, batch loss = 3.11\n",
      "Train accuracy = 86.22\n",
      "epoch 1, step 10250/55000, batch loss = 3.05\n",
      "epoch 1, step 10500/55000, batch loss = 3.17\n",
      "epoch 1, step 10750/55000, batch loss = 3.01\n",
      "epoch 1, step 11000/55000, batch loss = 2.99\n",
      "epoch 1, step 11250/55000, batch loss = 3.02\n",
      "epoch 1, step 11500/55000, batch loss = 3.18\n",
      "epoch 1, step 11750/55000, batch loss = 2.87\n",
      "epoch 1, step 12000/55000, batch loss = 2.89\n",
      "epoch 1, step 12250/55000, batch loss = 2.79\n",
      "epoch 1, step 12500/55000, batch loss = 2.83\n",
      "Train accuracy = 88.04\n",
      "epoch 1, step 12750/55000, batch loss = 2.87\n",
      "epoch 1, step 13000/55000, batch loss = 2.80\n",
      "epoch 1, step 13250/55000, batch loss = 2.80\n",
      "epoch 1, step 13500/55000, batch loss = 2.78\n",
      "epoch 1, step 13750/55000, batch loss = 2.71\n",
      "epoch 1, step 14000/55000, batch loss = 2.79\n",
      "epoch 1, step 14250/55000, batch loss = 2.62\n",
      "epoch 1, step 14500/55000, batch loss = 2.56\n",
      "epoch 1, step 14750/55000, batch loss = 2.62\n",
      "epoch 1, step 15000/55000, batch loss = 2.68\n",
      "Train accuracy = 89.22\n",
      "epoch 1, step 15250/55000, batch loss = 2.57\n",
      "epoch 1, step 15500/55000, batch loss = 2.45\n",
      "epoch 1, step 15750/55000, batch loss = 2.65\n",
      "epoch 1, step 16000/55000, batch loss = 2.52\n",
      "epoch 1, step 16250/55000, batch loss = 2.48\n",
      "epoch 1, step 16500/55000, batch loss = 2.52\n",
      "epoch 1, step 16750/55000, batch loss = 2.55\n",
      "epoch 1, step 17000/55000, batch loss = 2.48\n",
      "epoch 1, step 17250/55000, batch loss = 2.35\n",
      "epoch 1, step 17500/55000, batch loss = 2.33\n",
      "Train accuracy = 90.08\n",
      "epoch 1, step 17750/55000, batch loss = 2.27\n",
      "epoch 1, step 18000/55000, batch loss = 2.27\n",
      "epoch 1, step 18250/55000, batch loss = 2.30\n",
      "epoch 1, step 18500/55000, batch loss = 2.24\n",
      "epoch 1, step 18750/55000, batch loss = 2.24\n",
      "epoch 1, step 19000/55000, batch loss = 2.19\n",
      "epoch 1, step 19250/55000, batch loss = 2.14\n",
      "epoch 1, step 19500/55000, batch loss = 2.18\n",
      "epoch 1, step 19750/55000, batch loss = 2.18\n",
      "epoch 1, step 20000/55000, batch loss = 2.19\n",
      "Train accuracy = 90.84\n",
      "epoch 1, step 20250/55000, batch loss = 2.13\n",
      "epoch 1, step 20500/55000, batch loss = 2.04\n",
      "epoch 1, step 20750/55000, batch loss = 2.09\n",
      "epoch 1, step 21000/55000, batch loss = 2.02\n",
      "epoch 1, step 21250/55000, batch loss = 1.97\n",
      "epoch 1, step 21500/55000, batch loss = 2.14\n",
      "epoch 1, step 21750/55000, batch loss = 2.18\n",
      "epoch 1, step 22000/55000, batch loss = 2.00\n",
      "epoch 1, step 22250/55000, batch loss = 1.98\n",
      "epoch 1, step 22500/55000, batch loss = 2.04\n",
      "Train accuracy = 91.43\n",
      "epoch 1, step 22750/55000, batch loss = 1.90\n",
      "epoch 1, step 23000/55000, batch loss = 2.09\n",
      "epoch 1, step 23250/55000, batch loss = 2.23\n",
      "epoch 1, step 23500/55000, batch loss = 1.96\n",
      "epoch 1, step 23750/55000, batch loss = 1.82\n",
      "epoch 1, step 24000/55000, batch loss = 1.78\n",
      "epoch 1, step 24250/55000, batch loss = 1.98\n",
      "epoch 1, step 24500/55000, batch loss = 1.84\n",
      "epoch 1, step 24750/55000, batch loss = 1.90\n",
      "epoch 1, step 25000/55000, batch loss = 1.85\n",
      "Train accuracy = 91.92\n",
      "epoch 1, step 25250/55000, batch loss = 1.77\n",
      "epoch 1, step 25500/55000, batch loss = 1.74\n",
      "epoch 1, step 25750/55000, batch loss = 1.71\n",
      "epoch 1, step 26000/55000, batch loss = 1.77\n",
      "epoch 1, step 26250/55000, batch loss = 1.65\n",
      "epoch 1, step 26500/55000, batch loss = 1.66\n",
      "epoch 1, step 26750/55000, batch loss = 1.62\n",
      "epoch 1, step 27000/55000, batch loss = 1.66\n",
      "epoch 1, step 27250/55000, batch loss = 1.63\n",
      "epoch 1, step 27500/55000, batch loss = 1.65\n",
      "Train accuracy = 92.35\n",
      "epoch 1, step 27750/55000, batch loss = 1.66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, step 28000/55000, batch loss = 1.65\n",
      "epoch 1, step 28250/55000, batch loss = 1.60\n",
      "epoch 1, step 28500/55000, batch loss = 1.69\n",
      "epoch 1, step 28750/55000, batch loss = 1.58\n",
      "epoch 1, step 29000/55000, batch loss = 1.57\n",
      "epoch 1, step 29250/55000, batch loss = 1.84\n",
      "epoch 1, step 29500/55000, batch loss = 1.55\n",
      "epoch 1, step 29750/55000, batch loss = 1.50\n",
      "epoch 1, step 30000/55000, batch loss = 1.42\n",
      "Train accuracy = 92.70\n",
      "epoch 1, step 30250/55000, batch loss = 1.63\n",
      "epoch 1, step 30500/55000, batch loss = 1.50\n",
      "epoch 1, step 30750/55000, batch loss = 1.44\n",
      "epoch 1, step 31000/55000, batch loss = 1.40\n",
      "epoch 1, step 31250/55000, batch loss = 1.44\n",
      "epoch 1, step 31500/55000, batch loss = 1.50\n",
      "epoch 1, step 31750/55000, batch loss = 1.37\n",
      "epoch 1, step 32000/55000, batch loss = 1.32\n",
      "epoch 1, step 32250/55000, batch loss = 1.40\n",
      "epoch 1, step 32500/55000, batch loss = 1.35\n",
      "Train accuracy = 92.99\n",
      "epoch 1, step 32750/55000, batch loss = 1.41\n",
      "epoch 1, step 33000/55000, batch loss = 1.31\n",
      "epoch 1, step 33250/55000, batch loss = 1.28\n",
      "epoch 1, step 33500/55000, batch loss = 1.53\n",
      "epoch 1, step 33750/55000, batch loss = 1.27\n",
      "epoch 1, step 34000/55000, batch loss = 1.33\n",
      "epoch 1, step 34250/55000, batch loss = 1.27\n",
      "epoch 1, step 34500/55000, batch loss = 1.28\n",
      "epoch 1, step 34750/55000, batch loss = 1.21\n",
      "epoch 1, step 35000/55000, batch loss = 1.28\n",
      "Train accuracy = 93.29\n",
      "epoch 1, step 35250/55000, batch loss = 1.18\n",
      "epoch 1, step 35500/55000, batch loss = 1.22\n",
      "epoch 1, step 35750/55000, batch loss = 1.61\n",
      "epoch 1, step 36000/55000, batch loss = 1.26\n",
      "epoch 1, step 36250/55000, batch loss = 1.29\n",
      "epoch 1, step 36500/55000, batch loss = 1.17\n",
      "epoch 1, step 36750/55000, batch loss = 1.20\n",
      "epoch 1, step 37000/55000, batch loss = 1.15\n",
      "epoch 1, step 37250/55000, batch loss = 1.14\n",
      "epoch 1, step 37500/55000, batch loss = 1.11\n",
      "Train accuracy = 93.49\n",
      "epoch 1, step 37750/55000, batch loss = 1.21\n",
      "epoch 1, step 38000/55000, batch loss = 1.16\n",
      "epoch 1, step 38250/55000, batch loss = 1.15\n",
      "epoch 1, step 38500/55000, batch loss = 1.26\n",
      "epoch 1, step 38750/55000, batch loss = 1.12\n",
      "epoch 1, step 39000/55000, batch loss = 1.11\n",
      "epoch 1, step 39250/55000, batch loss = 1.09\n",
      "epoch 1, step 39500/55000, batch loss = 1.04\n",
      "epoch 1, step 39750/55000, batch loss = 1.11\n",
      "epoch 1, step 40000/55000, batch loss = 1.08\n",
      "Train accuracy = 93.70\n",
      "epoch 1, step 40250/55000, batch loss = 1.01\n",
      "epoch 1, step 40500/55000, batch loss = 1.06\n",
      "epoch 1, step 40750/55000, batch loss = 0.97\n",
      "epoch 1, step 41000/55000, batch loss = 0.99\n",
      "epoch 1, step 41250/55000, batch loss = 0.98\n",
      "epoch 1, step 41500/55000, batch loss = 0.99\n",
      "epoch 1, step 41750/55000, batch loss = 1.00\n",
      "epoch 1, step 42000/55000, batch loss = 0.97\n",
      "epoch 1, step 42250/55000, batch loss = 0.94\n",
      "epoch 1, step 42500/55000, batch loss = 0.96\n",
      "Train accuracy = 93.94\n",
      "epoch 1, step 42750/55000, batch loss = 0.93\n",
      "epoch 1, step 43000/55000, batch loss = 1.11\n",
      "epoch 1, step 43250/55000, batch loss = 0.92\n",
      "epoch 1, step 43500/55000, batch loss = 0.93\n",
      "epoch 1, step 43750/55000, batch loss = 0.90\n",
      "epoch 1, step 44000/55000, batch loss = 1.12\n",
      "epoch 1, step 44250/55000, batch loss = 0.91\n",
      "epoch 1, step 44500/55000, batch loss = 0.93\n",
      "epoch 1, step 44750/55000, batch loss = 0.87\n",
      "epoch 1, step 45000/55000, batch loss = 0.98\n",
      "Train accuracy = 94.06\n",
      "epoch 1, step 45250/55000, batch loss = 0.83\n",
      "epoch 1, step 45500/55000, batch loss = 1.12\n",
      "epoch 1, step 45750/55000, batch loss = 0.81\n",
      "epoch 1, step 46000/55000, batch loss = 0.83\n",
      "epoch 1, step 46250/55000, batch loss = 0.98\n",
      "epoch 1, step 46500/55000, batch loss = 0.85\n",
      "epoch 1, step 46750/55000, batch loss = 0.83\n",
      "epoch 1, step 47000/55000, batch loss = 0.81\n",
      "epoch 1, step 47250/55000, batch loss = 0.85\n",
      "epoch 1, step 47500/55000, batch loss = 0.82\n",
      "Train accuracy = 94.24\n",
      "epoch 1, step 47750/55000, batch loss = 0.85\n",
      "epoch 1, step 48000/55000, batch loss = 0.80\n",
      "epoch 1, step 48250/55000, batch loss = 0.79\n",
      "epoch 1, step 48500/55000, batch loss = 0.90\n",
      "epoch 1, step 48750/55000, batch loss = 0.86\n",
      "epoch 1, step 49000/55000, batch loss = 0.85\n",
      "epoch 1, step 49250/55000, batch loss = 0.83\n",
      "epoch 1, step 49500/55000, batch loss = 0.80\n",
      "epoch 1, step 49750/55000, batch loss = 0.80\n",
      "epoch 1, step 50000/55000, batch loss = 0.92\n",
      "Train accuracy = 94.36\n",
      "epoch 1, step 50250/55000, batch loss = 0.75\n",
      "epoch 1, step 50500/55000, batch loss = 0.72\n",
      "epoch 1, step 50750/55000, batch loss = 0.70\n",
      "epoch 1, step 51000/55000, batch loss = 0.70\n",
      "epoch 1, step 51250/55000, batch loss = 0.78\n",
      "epoch 1, step 51500/55000, batch loss = 0.71\n",
      "epoch 1, step 51750/55000, batch loss = 0.87\n",
      "epoch 1, step 52000/55000, batch loss = 0.83\n",
      "epoch 1, step 52250/55000, batch loss = 0.75\n",
      "epoch 1, step 52500/55000, batch loss = 0.67\n",
      "Train accuracy = 94.49\n",
      "epoch 1, step 52750/55000, batch loss = 0.71\n",
      "epoch 1, step 53000/55000, batch loss = 0.68\n",
      "epoch 1, step 53250/55000, batch loss = 0.66\n",
      "epoch 1, step 53500/55000, batch loss = 0.80\n",
      "epoch 1, step 53750/55000, batch loss = 0.64\n",
      "epoch 1, step 54000/55000, batch loss = 0.74\n",
      "epoch 1, step 54250/55000, batch loss = 0.72\n",
      "epoch 1, step 54500/55000, batch loss = 0.67\n",
      "epoch 1, step 54750/55000, batch loss = 0.71\n",
      "Train accuracy = 94.63\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 97.84\n",
      "Validation avg loss = 0.66\n",
      "\n",
      "epoch 2, step 0/55000, batch loss = 0.66\n",
      "epoch 2, step 250/55000, batch loss = 0.68\n",
      "epoch 2, step 500/55000, batch loss = 0.64\n",
      "epoch 2, step 750/55000, batch loss = 0.62\n",
      "epoch 2, step 1000/55000, batch loss = 0.72\n",
      "epoch 2, step 1250/55000, batch loss = 0.61\n",
      "epoch 2, step 1500/55000, batch loss = 0.62\n",
      "epoch 2, step 1750/55000, batch loss = 0.62\n",
      "epoch 2, step 2000/55000, batch loss = 0.66\n",
      "epoch 2, step 2250/55000, batch loss = 0.71\n",
      "epoch 2, step 2500/55000, batch loss = 0.57\n",
      "Train accuracy = 97.18\n",
      "epoch 2, step 2750/55000, batch loss = 0.58\n",
      "epoch 2, step 3000/55000, batch loss = 0.56\n",
      "epoch 2, step 3250/55000, batch loss = 0.59\n",
      "epoch 2, step 3500/55000, batch loss = 0.56\n",
      "epoch 2, step 3750/55000, batch loss = 0.61\n",
      "epoch 2, step 4000/55000, batch loss = 0.52\n",
      "epoch 2, step 4250/55000, batch loss = 0.63\n",
      "epoch 2, step 4500/55000, batch loss = 0.63\n",
      "epoch 2, step 4750/55000, batch loss = 0.71\n",
      "epoch 2, step 5000/55000, batch loss = 0.56\n",
      "Train accuracy = 97.21\n",
      "epoch 2, step 5250/55000, batch loss = 0.57\n",
      "epoch 2, step 5500/55000, batch loss = 0.56\n",
      "epoch 2, step 5750/55000, batch loss = 0.63\n",
      "epoch 2, step 6000/55000, batch loss = 0.62\n",
      "epoch 2, step 6250/55000, batch loss = 0.56\n",
      "epoch 2, step 6500/55000, batch loss = 0.50\n",
      "epoch 2, step 6750/55000, batch loss = 0.59\n",
      "epoch 2, step 7000/55000, batch loss = 0.51\n",
      "epoch 2, step 7250/55000, batch loss = 0.53\n",
      "epoch 2, step 7500/55000, batch loss = 0.69\n",
      "Train accuracy = 97.31\n",
      "epoch 2, step 7750/55000, batch loss = 0.54\n",
      "epoch 2, step 8000/55000, batch loss = 0.50\n",
      "epoch 2, step 8250/55000, batch loss = 0.50\n",
      "epoch 2, step 8500/55000, batch loss = 0.52\n",
      "epoch 2, step 8750/55000, batch loss = 0.58\n",
      "epoch 2, step 9000/55000, batch loss = 0.54\n",
      "epoch 2, step 9250/55000, batch loss = 0.46\n",
      "epoch 2, step 9500/55000, batch loss = 0.46\n",
      "epoch 2, step 9750/55000, batch loss = 0.60\n",
      "epoch 2, step 10000/55000, batch loss = 0.50\n",
      "Train accuracy = 97.30\n",
      "epoch 2, step 10250/55000, batch loss = 0.56\n",
      "epoch 2, step 10500/55000, batch loss = 0.60\n",
      "epoch 2, step 10750/55000, batch loss = 0.48\n",
      "epoch 2, step 11000/55000, batch loss = 0.46\n",
      "epoch 2, step 11250/55000, batch loss = 0.52\n",
      "epoch 2, step 11500/55000, batch loss = 0.57\n",
      "epoch 2, step 11750/55000, batch loss = 0.48\n",
      "epoch 2, step 12000/55000, batch loss = 0.46\n",
      "epoch 2, step 12250/55000, batch loss = 0.47\n",
      "epoch 2, step 12500/55000, batch loss = 0.52\n",
      "Train accuracy = 97.31\n",
      "epoch 2, step 12750/55000, batch loss = 0.48\n",
      "epoch 2, step 13000/55000, batch loss = 0.51\n",
      "epoch 2, step 13250/55000, batch loss = 0.47\n",
      "epoch 2, step 13500/55000, batch loss = 0.59\n",
      "epoch 2, step 13750/55000, batch loss = 0.47\n",
      "epoch 2, step 14000/55000, batch loss = 0.40\n",
      "epoch 2, step 14250/55000, batch loss = 0.42\n",
      "epoch 2, step 14500/55000, batch loss = 0.45\n",
      "epoch 2, step 14750/55000, batch loss = 0.39\n",
      "epoch 2, step 15000/55000, batch loss = 0.42\n",
      "Train accuracy = 97.26\n",
      "epoch 2, step 15250/55000, batch loss = 0.50\n",
      "epoch 2, step 15500/55000, batch loss = 0.64\n",
      "epoch 2, step 15750/55000, batch loss = 0.49\n",
      "epoch 2, step 16000/55000, batch loss = 0.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, step 16250/55000, batch loss = 0.43\n",
      "epoch 2, step 16500/55000, batch loss = 0.39\n",
      "epoch 2, step 16750/55000, batch loss = 0.48\n",
      "epoch 2, step 17000/55000, batch loss = 0.46\n",
      "epoch 2, step 17250/55000, batch loss = 0.44\n",
      "epoch 2, step 17500/55000, batch loss = 0.51\n",
      "Train accuracy = 97.29\n",
      "epoch 2, step 17750/55000, batch loss = 0.46\n",
      "epoch 2, step 18000/55000, batch loss = 0.39\n",
      "epoch 2, step 18250/55000, batch loss = 0.46\n",
      "epoch 2, step 18500/55000, batch loss = 0.44\n",
      "epoch 2, step 18750/55000, batch loss = 0.37\n",
      "epoch 2, step 19000/55000, batch loss = 0.42\n",
      "epoch 2, step 19250/55000, batch loss = 0.36\n",
      "epoch 2, step 19500/55000, batch loss = 0.42\n",
      "epoch 2, step 19750/55000, batch loss = 0.43\n",
      "epoch 2, step 20000/55000, batch loss = 0.48\n",
      "Train accuracy = 97.31\n",
      "epoch 2, step 20250/55000, batch loss = 0.42\n",
      "epoch 2, step 20500/55000, batch loss = 0.46\n",
      "epoch 2, step 20750/55000, batch loss = 0.42\n",
      "epoch 2, step 21000/55000, batch loss = 0.57\n",
      "epoch 2, step 21250/55000, batch loss = 0.37\n",
      "epoch 2, step 21500/55000, batch loss = 0.41\n",
      "epoch 2, step 21750/55000, batch loss = 0.38\n",
      "epoch 2, step 22000/55000, batch loss = 0.55\n",
      "epoch 2, step 22250/55000, batch loss = 0.36\n",
      "epoch 2, step 22500/55000, batch loss = 0.53\n",
      "Train accuracy = 97.30\n",
      "epoch 2, step 22750/55000, batch loss = 0.37\n",
      "epoch 2, step 23000/55000, batch loss = 0.37\n",
      "epoch 2, step 23250/55000, batch loss = 0.35\n",
      "epoch 2, step 23500/55000, batch loss = 0.35\n",
      "epoch 2, step 23750/55000, batch loss = 0.32\n",
      "epoch 2, step 24000/55000, batch loss = 0.33\n",
      "epoch 2, step 24250/55000, batch loss = 0.35\n",
      "epoch 2, step 24500/55000, batch loss = 0.38\n",
      "epoch 2, step 24750/55000, batch loss = 0.48\n",
      "epoch 2, step 25000/55000, batch loss = 0.39\n",
      "Train accuracy = 97.32\n",
      "epoch 2, step 25250/55000, batch loss = 0.45\n",
      "epoch 2, step 25500/55000, batch loss = 0.33\n",
      "epoch 2, step 25750/55000, batch loss = 0.31\n",
      "epoch 2, step 26000/55000, batch loss = 0.39\n",
      "epoch 2, step 26250/55000, batch loss = 0.32\n",
      "epoch 2, step 26500/55000, batch loss = 0.37\n",
      "epoch 2, step 26750/55000, batch loss = 0.41\n",
      "epoch 2, step 27000/55000, batch loss = 0.34\n",
      "epoch 2, step 27250/55000, batch loss = 0.34\n",
      "epoch 2, step 27500/55000, batch loss = 0.40\n",
      "Train accuracy = 97.32\n",
      "epoch 2, step 27750/55000, batch loss = 0.38\n",
      "epoch 2, step 28000/55000, batch loss = 0.27\n",
      "epoch 2, step 28250/55000, batch loss = 0.31\n",
      "epoch 2, step 28500/55000, batch loss = 0.38\n",
      "epoch 2, step 28750/55000, batch loss = 0.33\n",
      "epoch 2, step 29000/55000, batch loss = 0.33\n",
      "epoch 2, step 29250/55000, batch loss = 0.30\n",
      "epoch 2, step 29500/55000, batch loss = 0.27\n",
      "epoch 2, step 29750/55000, batch loss = 0.30\n",
      "epoch 2, step 30000/55000, batch loss = 0.29\n",
      "Train accuracy = 97.33\n",
      "epoch 2, step 30250/55000, batch loss = 0.30\n",
      "epoch 2, step 30500/55000, batch loss = 0.29\n",
      "epoch 2, step 30750/55000, batch loss = 0.39\n",
      "epoch 2, step 31000/55000, batch loss = 0.34\n",
      "epoch 2, step 31250/55000, batch loss = 0.26\n",
      "epoch 2, step 31500/55000, batch loss = 0.27\n",
      "epoch 2, step 31750/55000, batch loss = 0.26\n",
      "epoch 2, step 32000/55000, batch loss = 0.44\n",
      "epoch 2, step 32250/55000, batch loss = 0.27\n",
      "epoch 2, step 32500/55000, batch loss = 0.37\n",
      "Train accuracy = 97.33\n",
      "epoch 2, step 32750/55000, batch loss = 0.34\n",
      "epoch 2, step 33000/55000, batch loss = 0.29\n",
      "epoch 2, step 33250/55000, batch loss = 0.33\n",
      "epoch 2, step 33500/55000, batch loss = 0.32\n",
      "epoch 2, step 33750/55000, batch loss = 0.32\n",
      "epoch 2, step 34000/55000, batch loss = 0.26\n",
      "epoch 2, step 34250/55000, batch loss = 0.27\n",
      "epoch 2, step 34500/55000, batch loss = 0.27\n",
      "epoch 2, step 34750/55000, batch loss = 0.25\n",
      "epoch 2, step 35000/55000, batch loss = 0.33\n",
      "Train accuracy = 97.34\n",
      "epoch 2, step 35250/55000, batch loss = 0.31\n",
      "epoch 2, step 35500/55000, batch loss = 0.32\n",
      "epoch 2, step 35750/55000, batch loss = 0.30\n",
      "epoch 2, step 36000/55000, batch loss = 0.37\n",
      "epoch 2, step 36250/55000, batch loss = 0.26\n",
      "epoch 2, step 36500/55000, batch loss = 0.29\n",
      "epoch 2, step 36750/55000, batch loss = 0.32\n",
      "epoch 2, step 37000/55000, batch loss = 0.27\n",
      "epoch 2, step 37250/55000, batch loss = 0.37\n",
      "epoch 2, step 37500/55000, batch loss = 0.26\n",
      "Train accuracy = 97.36\n",
      "epoch 2, step 37750/55000, batch loss = 0.23\n",
      "epoch 2, step 38000/55000, batch loss = 0.27\n",
      "epoch 2, step 38250/55000, batch loss = 0.33\n",
      "epoch 2, step 38500/55000, batch loss = 0.23\n",
      "epoch 2, step 38750/55000, batch loss = 0.36\n",
      "epoch 2, step 39000/55000, batch loss = 0.33\n",
      "epoch 2, step 39250/55000, batch loss = 0.52\n",
      "epoch 2, step 39500/55000, batch loss = 0.23\n",
      "epoch 2, step 39750/55000, batch loss = 0.25\n",
      "epoch 2, step 40000/55000, batch loss = 0.42\n",
      "Train accuracy = 97.37\n",
      "epoch 2, step 40250/55000, batch loss = 0.27\n",
      "epoch 2, step 40500/55000, batch loss = 0.29\n",
      "epoch 2, step 40750/55000, batch loss = 0.26\n",
      "epoch 2, step 41000/55000, batch loss = 0.26\n",
      "epoch 2, step 41250/55000, batch loss = 0.26\n",
      "epoch 2, step 41500/55000, batch loss = 0.28\n",
      "epoch 2, step 41750/55000, batch loss = 0.23\n",
      "epoch 2, step 42000/55000, batch loss = 0.26\n",
      "epoch 2, step 42250/55000, batch loss = 0.23\n",
      "epoch 2, step 42500/55000, batch loss = 0.36\n",
      "Train accuracy = 97.36\n",
      "epoch 2, step 42750/55000, batch loss = 0.22\n",
      "epoch 2, step 43000/55000, batch loss = 0.39\n",
      "epoch 2, step 43250/55000, batch loss = 0.24\n",
      "epoch 2, step 43500/55000, batch loss = 0.24\n",
      "epoch 2, step 43750/55000, batch loss = 0.25\n",
      "epoch 2, step 44000/55000, batch loss = 0.29\n",
      "epoch 2, step 44250/55000, batch loss = 0.25\n",
      "epoch 2, step 44500/55000, batch loss = 0.21\n",
      "epoch 2, step 44750/55000, batch loss = 0.36\n",
      "epoch 2, step 45000/55000, batch loss = 0.23\n",
      "Train accuracy = 97.35\n",
      "epoch 2, step 45250/55000, batch loss = 0.20\n",
      "epoch 2, step 45500/55000, batch loss = 0.34\n",
      "epoch 2, step 45750/55000, batch loss = 0.28\n",
      "epoch 2, step 46000/55000, batch loss = 0.24\n",
      "epoch 2, step 46250/55000, batch loss = 0.24\n",
      "epoch 2, step 46500/55000, batch loss = 0.24\n",
      "epoch 2, step 46750/55000, batch loss = 0.25\n",
      "epoch 2, step 47000/55000, batch loss = 0.22\n",
      "epoch 2, step 47250/55000, batch loss = 0.19\n",
      "epoch 2, step 47500/55000, batch loss = 0.27\n",
      "Train accuracy = 97.38\n",
      "epoch 2, step 47750/55000, batch loss = 0.27\n",
      "epoch 2, step 48000/55000, batch loss = 0.37\n",
      "epoch 2, step 48250/55000, batch loss = 0.20\n",
      "epoch 2, step 48500/55000, batch loss = 0.25\n",
      "epoch 2, step 48750/55000, batch loss = 0.25\n",
      "epoch 2, step 49000/55000, batch loss = 0.20\n",
      "epoch 2, step 49250/55000, batch loss = 0.28\n",
      "epoch 2, step 49500/55000, batch loss = 0.24\n",
      "epoch 2, step 49750/55000, batch loss = 0.33\n",
      "epoch 2, step 50000/55000, batch loss = 0.20\n",
      "Train accuracy = 97.41\n",
      "epoch 2, step 50250/55000, batch loss = 0.18\n",
      "epoch 2, step 50500/55000, batch loss = 0.28\n",
      "epoch 2, step 50750/55000, batch loss = 0.30\n",
      "epoch 2, step 51000/55000, batch loss = 0.20\n",
      "epoch 2, step 51250/55000, batch loss = 0.21\n",
      "epoch 2, step 51500/55000, batch loss = 0.21\n",
      "epoch 2, step 51750/55000, batch loss = 0.20\n",
      "epoch 2, step 52000/55000, batch loss = 0.19\n",
      "epoch 2, step 52250/55000, batch loss = 0.24\n",
      "epoch 2, step 52500/55000, batch loss = 0.18\n",
      "Train accuracy = 97.40\n",
      "epoch 2, step 52750/55000, batch loss = 0.21\n",
      "epoch 2, step 53000/55000, batch loss = 0.23\n",
      "epoch 2, step 53250/55000, batch loss = 0.22\n",
      "epoch 2, step 53500/55000, batch loss = 0.17\n",
      "epoch 2, step 53750/55000, batch loss = 0.33\n",
      "epoch 2, step 54000/55000, batch loss = 0.33\n",
      "epoch 2, step 54250/55000, batch loss = 0.27\n",
      "epoch 2, step 54500/55000, batch loss = 0.28\n",
      "epoch 2, step 54750/55000, batch loss = 0.22\n",
      "Train accuracy = 97.40\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 97.92\n",
      "Validation avg loss = 0.23\n",
      "\n",
      "epoch 3, step 0/55000, batch loss = 0.23\n",
      "epoch 3, step 250/55000, batch loss = 0.19\n",
      "epoch 3, step 500/55000, batch loss = 0.26\n",
      "epoch 3, step 750/55000, batch loss = 0.25\n",
      "epoch 3, step 1000/55000, batch loss = 0.17\n",
      "epoch 3, step 1250/55000, batch loss = 0.19\n",
      "epoch 3, step 1500/55000, batch loss = 0.19\n",
      "epoch 3, step 1750/55000, batch loss = 0.20\n",
      "epoch 3, step 2000/55000, batch loss = 0.20\n",
      "epoch 3, step 2250/55000, batch loss = 0.24\n",
      "epoch 3, step 2500/55000, batch loss = 0.20\n",
      "Train accuracy = 97.73\n",
      "epoch 3, step 2750/55000, batch loss = 0.16\n",
      "epoch 3, step 3000/55000, batch loss = 0.23\n",
      "epoch 3, step 3250/55000, batch loss = 0.20\n",
      "epoch 3, step 3500/55000, batch loss = 0.22\n",
      "epoch 3, step 3750/55000, batch loss = 0.18\n",
      "epoch 3, step 4000/55000, batch loss = 0.18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, step 4250/55000, batch loss = 0.18\n",
      "epoch 3, step 4500/55000, batch loss = 0.19\n",
      "epoch 3, step 4750/55000, batch loss = 0.31\n",
      "epoch 3, step 5000/55000, batch loss = 0.20\n",
      "Train accuracy = 98.02\n",
      "epoch 3, step 5250/55000, batch loss = 0.28\n",
      "epoch 3, step 5500/55000, batch loss = 0.19\n",
      "epoch 3, step 5750/55000, batch loss = 0.17\n",
      "epoch 3, step 6000/55000, batch loss = 0.17\n",
      "epoch 3, step 6250/55000, batch loss = 0.16\n",
      "epoch 3, step 6500/55000, batch loss = 0.22\n",
      "epoch 3, step 6750/55000, batch loss = 0.18\n",
      "epoch 3, step 7000/55000, batch loss = 0.18\n",
      "epoch 3, step 7250/55000, batch loss = 0.21\n",
      "epoch 3, step 7500/55000, batch loss = 0.19\n",
      "Train accuracy = 98.26\n",
      "epoch 3, step 7750/55000, batch loss = 0.16\n",
      "epoch 3, step 8000/55000, batch loss = 0.23\n",
      "epoch 3, step 8250/55000, batch loss = 0.26\n",
      "epoch 3, step 8500/55000, batch loss = 0.17\n",
      "epoch 3, step 8750/55000, batch loss = 0.19\n",
      "epoch 3, step 9000/55000, batch loss = 0.19\n",
      "epoch 3, step 9250/55000, batch loss = 0.16\n",
      "epoch 3, step 9500/55000, batch loss = 0.17\n",
      "epoch 3, step 9750/55000, batch loss = 0.21\n",
      "epoch 3, step 10000/55000, batch loss = 0.17\n",
      "Train accuracy = 98.36\n",
      "epoch 3, step 10250/55000, batch loss = 0.21\n",
      "epoch 3, step 10500/55000, batch loss = 0.19\n",
      "epoch 3, step 10750/55000, batch loss = 0.22\n",
      "epoch 3, step 11000/55000, batch loss = 0.17\n",
      "epoch 3, step 11250/55000, batch loss = 0.17\n",
      "epoch 3, step 11500/55000, batch loss = 0.20\n",
      "epoch 3, step 11750/55000, batch loss = 0.21\n",
      "epoch 3, step 12000/55000, batch loss = 0.16\n",
      "epoch 3, step 12250/55000, batch loss = 0.21\n",
      "epoch 3, step 12500/55000, batch loss = 0.19\n",
      "Train accuracy = 98.35\n",
      "epoch 3, step 12750/55000, batch loss = 0.24\n",
      "epoch 3, step 13000/55000, batch loss = 0.17\n",
      "epoch 3, step 13250/55000, batch loss = 0.20\n",
      "epoch 3, step 13500/55000, batch loss = 0.22\n",
      "epoch 3, step 13750/55000, batch loss = 0.18\n",
      "epoch 3, step 14000/55000, batch loss = 0.27\n",
      "epoch 3, step 14250/55000, batch loss = 0.24\n",
      "epoch 3, step 14500/55000, batch loss = 0.19\n",
      "epoch 3, step 14750/55000, batch loss = 0.19\n",
      "epoch 3, step 15000/55000, batch loss = 0.19\n",
      "Train accuracy = 98.36\n",
      "epoch 3, step 15250/55000, batch loss = 0.18\n",
      "epoch 3, step 15500/55000, batch loss = 0.16\n",
      "epoch 3, step 15750/55000, batch loss = 0.22\n",
      "epoch 3, step 16000/55000, batch loss = 0.19\n",
      "epoch 3, step 16250/55000, batch loss = 0.19\n",
      "epoch 3, step 16500/55000, batch loss = 0.18\n",
      "epoch 3, step 16750/55000, batch loss = 0.18\n",
      "epoch 3, step 17000/55000, batch loss = 0.16\n",
      "epoch 3, step 17250/55000, batch loss = 0.16\n",
      "epoch 3, step 17500/55000, batch loss = 0.18\n",
      "Train accuracy = 98.44\n",
      "epoch 3, step 17750/55000, batch loss = 0.18\n",
      "epoch 3, step 18000/55000, batch loss = 0.16\n",
      "epoch 3, step 18250/55000, batch loss = 0.16\n",
      "epoch 3, step 18500/55000, batch loss = 0.20\n",
      "epoch 3, step 18750/55000, batch loss = 0.25\n",
      "epoch 3, step 19000/55000, batch loss = 0.24\n",
      "epoch 3, step 19250/55000, batch loss = 0.16\n",
      "epoch 3, step 19500/55000, batch loss = 0.16\n",
      "epoch 3, step 19750/55000, batch loss = 0.22\n",
      "epoch 3, step 20000/55000, batch loss = 0.16\n",
      "Train accuracy = 98.40\n",
      "epoch 3, step 20250/55000, batch loss = 0.39\n",
      "epoch 3, step 20500/55000, batch loss = 0.20\n",
      "epoch 3, step 20750/55000, batch loss = 0.17\n",
      "epoch 3, step 21000/55000, batch loss = 0.25\n",
      "epoch 3, step 21250/55000, batch loss = 0.26\n",
      "epoch 3, step 21500/55000, batch loss = 0.17\n",
      "epoch 3, step 21750/55000, batch loss = 0.23\n",
      "epoch 3, step 22000/55000, batch loss = 0.16\n",
      "epoch 3, step 22250/55000, batch loss = 0.18\n",
      "epoch 3, step 22500/55000, batch loss = 0.16\n",
      "Train accuracy = 98.39\n",
      "epoch 3, step 22750/55000, batch loss = 0.20\n",
      "epoch 3, step 23000/55000, batch loss = 0.21\n",
      "epoch 3, step 23250/55000, batch loss = 0.16\n",
      "epoch 3, step 23500/55000, batch loss = 0.31\n",
      "epoch 3, step 23750/55000, batch loss = 0.18\n",
      "epoch 3, step 24000/55000, batch loss = 0.18\n",
      "epoch 3, step 24250/55000, batch loss = 0.24\n",
      "epoch 3, step 24500/55000, batch loss = 0.19\n",
      "epoch 3, step 24750/55000, batch loss = 0.18\n",
      "epoch 3, step 25000/55000, batch loss = 0.20\n",
      "Train accuracy = 98.41\n",
      "epoch 3, step 25250/55000, batch loss = 0.18\n",
      "epoch 3, step 25500/55000, batch loss = 0.19\n",
      "epoch 3, step 25750/55000, batch loss = 0.25\n",
      "epoch 3, step 26000/55000, batch loss = 0.18\n",
      "epoch 3, step 26250/55000, batch loss = 0.18\n",
      "epoch 3, step 26500/55000, batch loss = 0.25\n",
      "epoch 3, step 26750/55000, batch loss = 0.23\n",
      "epoch 3, step 27000/55000, batch loss = 0.19\n",
      "epoch 3, step 27250/55000, batch loss = 0.17\n",
      "epoch 3, step 27500/55000, batch loss = 0.18\n",
      "Train accuracy = 98.43\n",
      "epoch 3, step 27750/55000, batch loss = 0.18\n",
      "epoch 3, step 28000/55000, batch loss = 0.16\n",
      "epoch 3, step 28250/55000, batch loss = 0.19\n",
      "epoch 3, step 28500/55000, batch loss = 0.22\n",
      "epoch 3, step 28750/55000, batch loss = 0.16\n",
      "epoch 3, step 29000/55000, batch loss = 0.37\n",
      "epoch 3, step 29250/55000, batch loss = 0.36\n",
      "epoch 3, step 29500/55000, batch loss = 0.21\n",
      "epoch 3, step 29750/55000, batch loss = 0.19\n",
      "epoch 3, step 30000/55000, batch loss = 0.18\n",
      "Train accuracy = 98.44\n",
      "epoch 3, step 30250/55000, batch loss = 0.16\n",
      "epoch 3, step 30500/55000, batch loss = 0.17\n",
      "epoch 3, step 30750/55000, batch loss = 0.17\n",
      "epoch 3, step 31000/55000, batch loss = 0.17\n",
      "epoch 3, step 31250/55000, batch loss = 0.23\n",
      "epoch 3, step 31500/55000, batch loss = 0.25\n",
      "epoch 3, step 31750/55000, batch loss = 0.20\n",
      "epoch 3, step 32000/55000, batch loss = 0.21\n",
      "epoch 3, step 32250/55000, batch loss = 0.23\n",
      "epoch 3, step 32500/55000, batch loss = 0.19\n",
      "Train accuracy = 98.45\n",
      "epoch 3, step 32750/55000, batch loss = 0.19\n",
      "epoch 3, step 33000/55000, batch loss = 0.35\n",
      "epoch 3, step 33250/55000, batch loss = 0.15\n",
      "epoch 3, step 33500/55000, batch loss = 0.20\n",
      "epoch 3, step 33750/55000, batch loss = 0.19\n",
      "epoch 3, step 34000/55000, batch loss = 0.17\n",
      "epoch 3, step 34250/55000, batch loss = 0.20\n",
      "epoch 3, step 34500/55000, batch loss = 0.20\n",
      "epoch 3, step 34750/55000, batch loss = 0.17\n",
      "epoch 3, step 35000/55000, batch loss = 0.21\n",
      "Train accuracy = 98.45\n",
      "epoch 3, step 35250/55000, batch loss = 0.21\n",
      "epoch 3, step 35500/55000, batch loss = 0.20\n",
      "epoch 3, step 35750/55000, batch loss = 0.18\n",
      "epoch 3, step 36000/55000, batch loss = 0.20\n",
      "epoch 3, step 36250/55000, batch loss = 0.20\n",
      "epoch 3, step 36500/55000, batch loss = 0.17\n",
      "epoch 3, step 36750/55000, batch loss = 0.18\n",
      "epoch 3, step 37000/55000, batch loss = 0.22\n",
      "epoch 3, step 37250/55000, batch loss = 0.21\n",
      "epoch 3, step 37500/55000, batch loss = 0.17\n",
      "Train accuracy = 98.47\n",
      "epoch 3, step 37750/55000, batch loss = 0.15\n",
      "epoch 3, step 38000/55000, batch loss = 0.24\n",
      "epoch 3, step 38250/55000, batch loss = 0.19\n",
      "epoch 3, step 38500/55000, batch loss = 0.16\n",
      "epoch 3, step 38750/55000, batch loss = 0.20\n",
      "epoch 3, step 39000/55000, batch loss = 0.19\n",
      "epoch 3, step 39250/55000, batch loss = 0.20\n",
      "epoch 3, step 39500/55000, batch loss = 0.24\n",
      "epoch 3, step 39750/55000, batch loss = 0.19\n",
      "epoch 3, step 40000/55000, batch loss = 0.18\n",
      "Train accuracy = 98.47\n",
      "epoch 3, step 40250/55000, batch loss = 0.15\n",
      "epoch 3, step 40500/55000, batch loss = 0.21\n",
      "epoch 3, step 40750/55000, batch loss = 0.15\n",
      "epoch 3, step 41000/55000, batch loss = 0.19\n",
      "epoch 3, step 41250/55000, batch loss = 0.15\n",
      "epoch 3, step 41500/55000, batch loss = 0.15\n",
      "epoch 3, step 41750/55000, batch loss = 0.18\n",
      "epoch 3, step 42000/55000, batch loss = 0.26\n",
      "epoch 3, step 42250/55000, batch loss = 0.15\n",
      "epoch 3, step 42500/55000, batch loss = 0.16\n",
      "Train accuracy = 98.47\n",
      "epoch 3, step 42750/55000, batch loss = 0.18\n",
      "epoch 3, step 43000/55000, batch loss = 0.16\n",
      "epoch 3, step 43250/55000, batch loss = 0.28\n",
      "epoch 3, step 43500/55000, batch loss = 0.23\n",
      "epoch 3, step 43750/55000, batch loss = 0.26\n",
      "epoch 3, step 44000/55000, batch loss = 0.34\n",
      "epoch 3, step 44250/55000, batch loss = 0.15\n",
      "epoch 3, step 44500/55000, batch loss = 0.23\n",
      "epoch 3, step 44750/55000, batch loss = 0.18\n",
      "epoch 3, step 45000/55000, batch loss = 0.18\n",
      "Train accuracy = 98.46\n",
      "epoch 3, step 45250/55000, batch loss = 0.17\n",
      "epoch 3, step 45500/55000, batch loss = 0.15\n",
      "epoch 3, step 45750/55000, batch loss = 0.24\n",
      "epoch 3, step 46000/55000, batch loss = 0.17\n",
      "epoch 3, step 46250/55000, batch loss = 0.19\n",
      "epoch 3, step 46500/55000, batch loss = 0.31\n",
      "epoch 3, step 46750/55000, batch loss = 0.17\n",
      "epoch 3, step 47000/55000, batch loss = 0.16\n",
      "epoch 3, step 47250/55000, batch loss = 0.17\n",
      "epoch 3, step 47500/55000, batch loss = 0.16\n",
      "Train accuracy = 98.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, step 47750/55000, batch loss = 0.18\n",
      "epoch 3, step 48000/55000, batch loss = 0.21\n",
      "epoch 3, step 48250/55000, batch loss = 0.17\n",
      "epoch 3, step 48500/55000, batch loss = 0.19\n",
      "epoch 3, step 48750/55000, batch loss = 0.18\n",
      "epoch 3, step 49000/55000, batch loss = 0.18\n",
      "epoch 3, step 49250/55000, batch loss = 0.18\n",
      "epoch 3, step 49500/55000, batch loss = 0.24\n",
      "epoch 3, step 49750/55000, batch loss = 0.18\n",
      "epoch 3, step 50000/55000, batch loss = 0.17\n",
      "Train accuracy = 98.47\n",
      "epoch 3, step 50250/55000, batch loss = 0.21\n",
      "epoch 3, step 50500/55000, batch loss = 0.20\n",
      "epoch 3, step 50750/55000, batch loss = 0.16\n",
      "epoch 3, step 51000/55000, batch loss = 0.15\n",
      "epoch 3, step 51250/55000, batch loss = 0.24\n",
      "epoch 3, step 51500/55000, batch loss = 0.20\n",
      "epoch 3, step 51750/55000, batch loss = 0.15\n",
      "epoch 3, step 52000/55000, batch loss = 0.18\n",
      "epoch 3, step 52250/55000, batch loss = 0.15\n",
      "epoch 3, step 52500/55000, batch loss = 0.15\n",
      "Train accuracy = 98.47\n",
      "epoch 3, step 52750/55000, batch loss = 0.21\n",
      "epoch 3, step 53000/55000, batch loss = 0.18\n",
      "epoch 3, step 53250/55000, batch loss = 0.23\n",
      "epoch 3, step 53500/55000, batch loss = 0.21\n",
      "epoch 3, step 53750/55000, batch loss = 0.22\n",
      "epoch 3, step 54000/55000, batch loss = 0.22\n",
      "epoch 3, step 54250/55000, batch loss = 0.20\n",
      "epoch 3, step 54500/55000, batch loss = 0.16\n",
      "epoch 3, step 54750/55000, batch loss = 0.21\n",
      "Train accuracy = 98.45\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 98.60\n",
      "Validation avg loss = 0.19\n",
      "\n",
      "epoch 4, step 0/55000, batch loss = 0.17\n",
      "epoch 4, step 250/55000, batch loss = 0.15\n",
      "epoch 4, step 500/55000, batch loss = 0.19\n",
      "epoch 4, step 750/55000, batch loss = 0.15\n",
      "epoch 4, step 1000/55000, batch loss = 0.21\n",
      "epoch 4, step 1250/55000, batch loss = 0.18\n",
      "epoch 4, step 1500/55000, batch loss = 0.16\n",
      "epoch 4, step 1750/55000, batch loss = 0.27\n",
      "epoch 4, step 2000/55000, batch loss = 0.16\n",
      "epoch 4, step 2250/55000, batch loss = 0.16\n",
      "epoch 4, step 2500/55000, batch loss = 0.22\n",
      "Train accuracy = 98.75\n",
      "epoch 4, step 2750/55000, batch loss = 0.15\n",
      "epoch 4, step 3000/55000, batch loss = 0.20\n",
      "epoch 4, step 3250/55000, batch loss = 0.15\n",
      "epoch 4, step 3500/55000, batch loss = 0.16\n",
      "epoch 4, step 3750/55000, batch loss = 0.27\n",
      "epoch 4, step 4000/55000, batch loss = 0.20\n",
      "epoch 4, step 4250/55000, batch loss = 0.21\n",
      "epoch 4, step 4500/55000, batch loss = 0.17\n",
      "epoch 4, step 4750/55000, batch loss = 0.23\n",
      "epoch 4, step 5000/55000, batch loss = 0.20\n",
      "Train accuracy = 98.55\n",
      "epoch 4, step 5250/55000, batch loss = 0.20\n",
      "epoch 4, step 5500/55000, batch loss = 0.18\n",
      "epoch 4, step 5750/55000, batch loss = 0.24\n",
      "epoch 4, step 6000/55000, batch loss = 0.15\n",
      "epoch 4, step 6250/55000, batch loss = 0.15\n",
      "epoch 4, step 6500/55000, batch loss = 0.20\n",
      "epoch 4, step 6750/55000, batch loss = 0.15\n",
      "epoch 4, step 7000/55000, batch loss = 0.16\n",
      "epoch 4, step 7250/55000, batch loss = 0.18\n",
      "epoch 4, step 7500/55000, batch loss = 0.16\n",
      "Train accuracy = 98.64\n",
      "epoch 4, step 7750/55000, batch loss = 0.19\n",
      "epoch 4, step 8000/55000, batch loss = 0.19\n",
      "epoch 4, step 8250/55000, batch loss = 0.22\n",
      "epoch 4, step 8500/55000, batch loss = 0.24\n",
      "epoch 4, step 8750/55000, batch loss = 0.21\n",
      "epoch 4, step 9000/55000, batch loss = 0.20\n",
      "epoch 4, step 9250/55000, batch loss = 0.17\n",
      "epoch 4, step 9500/55000, batch loss = 0.16\n",
      "epoch 4, step 9750/55000, batch loss = 0.14\n",
      "epoch 4, step 10000/55000, batch loss = 0.25\n",
      "Train accuracy = 98.57\n",
      "epoch 4, step 10250/55000, batch loss = 0.15\n",
      "epoch 4, step 10500/55000, batch loss = 0.16\n",
      "epoch 4, step 10750/55000, batch loss = 0.15\n",
      "epoch 4, step 11000/55000, batch loss = 0.20\n",
      "epoch 4, step 11250/55000, batch loss = 0.22\n",
      "epoch 4, step 11500/55000, batch loss = 0.15\n",
      "epoch 4, step 11750/55000, batch loss = 0.20\n",
      "epoch 4, step 12000/55000, batch loss = 0.17\n",
      "epoch 4, step 12250/55000, batch loss = 0.16\n",
      "epoch 4, step 12500/55000, batch loss = 0.20\n",
      "Train accuracy = 98.62\n",
      "epoch 4, step 12750/55000, batch loss = 0.15\n",
      "epoch 4, step 13000/55000, batch loss = 0.18\n",
      "epoch 4, step 13250/55000, batch loss = 0.19\n",
      "epoch 4, step 13500/55000, batch loss = 0.15\n",
      "epoch 4, step 13750/55000, batch loss = 0.17\n",
      "epoch 4, step 14000/55000, batch loss = 0.37\n",
      "epoch 4, step 14250/55000, batch loss = 0.15\n",
      "epoch 4, step 14500/55000, batch loss = 0.17\n",
      "epoch 4, step 14750/55000, batch loss = 0.15\n",
      "epoch 4, step 15000/55000, batch loss = 0.17\n",
      "Train accuracy = 98.61\n",
      "epoch 4, step 15250/55000, batch loss = 0.18\n",
      "epoch 4, step 15500/55000, batch loss = 0.15\n",
      "epoch 4, step 15750/55000, batch loss = 0.19\n",
      "epoch 4, step 16000/55000, batch loss = 0.29\n",
      "epoch 4, step 16250/55000, batch loss = 0.19\n",
      "epoch 4, step 16500/55000, batch loss = 0.17\n",
      "epoch 4, step 16750/55000, batch loss = 0.17\n",
      "epoch 4, step 17000/55000, batch loss = 0.15\n",
      "epoch 4, step 17250/55000, batch loss = 0.19\n",
      "epoch 4, step 17500/55000, batch loss = 0.27\n",
      "Train accuracy = 98.56\n",
      "epoch 4, step 17750/55000, batch loss = 0.17\n",
      "epoch 4, step 18000/55000, batch loss = 0.14\n",
      "epoch 4, step 18250/55000, batch loss = 0.15\n",
      "epoch 4, step 18500/55000, batch loss = 0.15\n",
      "epoch 4, step 18750/55000, batch loss = 0.16\n",
      "epoch 4, step 19000/55000, batch loss = 0.22\n",
      "epoch 4, step 19250/55000, batch loss = 0.16\n",
      "epoch 4, step 19500/55000, batch loss = 0.17\n",
      "epoch 4, step 19750/55000, batch loss = 0.16\n",
      "epoch 4, step 20000/55000, batch loss = 0.17\n",
      "Train accuracy = 98.58\n",
      "epoch 4, step 20250/55000, batch loss = 0.18\n",
      "epoch 4, step 20500/55000, batch loss = 0.15\n",
      "epoch 4, step 20750/55000, batch loss = 0.15\n",
      "epoch 4, step 21000/55000, batch loss = 0.17\n",
      "epoch 4, step 21250/55000, batch loss = 0.26\n",
      "epoch 4, step 21500/55000, batch loss = 0.20\n",
      "epoch 4, step 21750/55000, batch loss = 0.16\n",
      "epoch 4, step 22000/55000, batch loss = 0.22\n",
      "epoch 4, step 22250/55000, batch loss = 0.16\n",
      "epoch 4, step 22500/55000, batch loss = 0.17\n",
      "Train accuracy = 98.60\n",
      "epoch 4, step 22750/55000, batch loss = 0.16\n",
      "epoch 4, step 23000/55000, batch loss = 0.17\n",
      "epoch 4, step 23250/55000, batch loss = 0.15\n",
      "epoch 4, step 23500/55000, batch loss = 0.18\n",
      "epoch 4, step 23750/55000, batch loss = 0.17\n",
      "epoch 4, step 24000/55000, batch loss = 0.15\n",
      "epoch 4, step 24250/55000, batch loss = 0.18\n",
      "epoch 4, step 24500/55000, batch loss = 0.20\n",
      "epoch 4, step 24750/55000, batch loss = 0.17\n",
      "epoch 4, step 25000/55000, batch loss = 0.31\n",
      "Train accuracy = 98.60\n",
      "epoch 4, step 25250/55000, batch loss = 0.18\n",
      "epoch 4, step 25500/55000, batch loss = 0.16\n",
      "epoch 4, step 25750/55000, batch loss = 0.21\n",
      "epoch 4, step 26000/55000, batch loss = 0.18\n",
      "epoch 4, step 26250/55000, batch loss = 0.16\n",
      "epoch 4, step 26500/55000, batch loss = 0.19\n",
      "epoch 4, step 26750/55000, batch loss = 0.17\n",
      "epoch 4, step 27000/55000, batch loss = 0.14\n",
      "epoch 4, step 27250/55000, batch loss = 0.15\n",
      "epoch 4, step 27500/55000, batch loss = 0.16\n",
      "Train accuracy = 98.59\n",
      "epoch 4, step 27750/55000, batch loss = 0.17\n",
      "epoch 4, step 28000/55000, batch loss = 0.21\n",
      "epoch 4, step 28250/55000, batch loss = 0.15\n",
      "epoch 4, step 28500/55000, batch loss = 0.19\n",
      "epoch 4, step 28750/55000, batch loss = 0.16\n",
      "epoch 4, step 29000/55000, batch loss = 0.17\n",
      "epoch 4, step 29250/55000, batch loss = 0.14\n",
      "epoch 4, step 29500/55000, batch loss = 0.18\n",
      "epoch 4, step 29750/55000, batch loss = 0.15\n",
      "epoch 4, step 30000/55000, batch loss = 0.26\n",
      "Train accuracy = 98.59\n",
      "epoch 4, step 30250/55000, batch loss = 0.14\n",
      "epoch 4, step 30500/55000, batch loss = 0.23\n",
      "epoch 4, step 30750/55000, batch loss = 0.17\n",
      "epoch 4, step 31000/55000, batch loss = 0.19\n",
      "epoch 4, step 31250/55000, batch loss = 0.17\n",
      "epoch 4, step 31500/55000, batch loss = 0.20\n",
      "epoch 4, step 31750/55000, batch loss = 0.20\n",
      "epoch 4, step 32000/55000, batch loss = 0.18\n",
      "epoch 4, step 32250/55000, batch loss = 0.13\n",
      "epoch 4, step 32500/55000, batch loss = 0.15\n",
      "Train accuracy = 98.61\n",
      "epoch 4, step 32750/55000, batch loss = 0.22\n",
      "epoch 4, step 33000/55000, batch loss = 0.19\n",
      "epoch 4, step 33250/55000, batch loss = 0.17\n",
      "epoch 4, step 33500/55000, batch loss = 0.15\n",
      "epoch 4, step 33750/55000, batch loss = 0.16\n",
      "epoch 4, step 34000/55000, batch loss = 0.21\n",
      "epoch 4, step 34250/55000, batch loss = 0.16\n",
      "epoch 4, step 34500/55000, batch loss = 0.20\n",
      "epoch 4, step 34750/55000, batch loss = 0.25\n",
      "epoch 4, step 35000/55000, batch loss = 0.20\n",
      "Train accuracy = 98.63\n",
      "epoch 4, step 35250/55000, batch loss = 0.15\n",
      "epoch 4, step 35500/55000, batch loss = 0.20\n",
      "epoch 4, step 35750/55000, batch loss = 0.15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, step 36000/55000, batch loss = 0.16\n",
      "epoch 4, step 36250/55000, batch loss = 0.16\n",
      "epoch 4, step 36500/55000, batch loss = 0.17\n",
      "epoch 4, step 36750/55000, batch loss = 0.17\n",
      "epoch 4, step 37000/55000, batch loss = 0.18\n",
      "epoch 4, step 37250/55000, batch loss = 0.25\n",
      "epoch 4, step 37500/55000, batch loss = 0.16\n",
      "Train accuracy = 98.62\n",
      "epoch 4, step 37750/55000, batch loss = 0.17\n",
      "epoch 4, step 38000/55000, batch loss = 0.22\n",
      "epoch 4, step 38250/55000, batch loss = 0.27\n",
      "epoch 4, step 38500/55000, batch loss = 0.18\n",
      "epoch 4, step 38750/55000, batch loss = 0.23\n",
      "epoch 4, step 39000/55000, batch loss = 0.17\n",
      "epoch 4, step 39250/55000, batch loss = 0.16\n",
      "epoch 4, step 39500/55000, batch loss = 0.17\n",
      "epoch 4, step 39750/55000, batch loss = 0.16\n",
      "epoch 4, step 40000/55000, batch loss = 0.20\n",
      "Train accuracy = 98.63\n",
      "epoch 4, step 40250/55000, batch loss = 0.19\n",
      "epoch 4, step 40500/55000, batch loss = 0.34\n",
      "epoch 4, step 40750/55000, batch loss = 0.16\n",
      "epoch 4, step 41000/55000, batch loss = 0.14\n",
      "epoch 4, step 41250/55000, batch loss = 0.20\n",
      "epoch 4, step 41500/55000, batch loss = 0.16\n",
      "epoch 4, step 41750/55000, batch loss = 0.26\n",
      "epoch 4, step 42000/55000, batch loss = 0.20\n",
      "epoch 4, step 42250/55000, batch loss = 0.17\n",
      "epoch 4, step 42500/55000, batch loss = 0.16\n",
      "Train accuracy = 98.63\n",
      "epoch 4, step 42750/55000, batch loss = 0.15\n",
      "epoch 4, step 43000/55000, batch loss = 0.14\n",
      "epoch 4, step 43250/55000, batch loss = 0.17\n",
      "epoch 4, step 43500/55000, batch loss = 0.17\n",
      "epoch 4, step 43750/55000, batch loss = 0.22\n",
      "epoch 4, step 44000/55000, batch loss = 0.18\n",
      "epoch 4, step 44250/55000, batch loss = 0.21\n",
      "epoch 4, step 44500/55000, batch loss = 0.25\n",
      "epoch 4, step 44750/55000, batch loss = 0.16\n",
      "epoch 4, step 45000/55000, batch loss = 0.15\n",
      "Train accuracy = 98.63\n",
      "epoch 4, step 45250/55000, batch loss = 0.21\n",
      "epoch 4, step 45500/55000, batch loss = 0.17\n",
      "epoch 4, step 45750/55000, batch loss = 0.14\n",
      "epoch 4, step 46000/55000, batch loss = 0.16\n",
      "epoch 4, step 46250/55000, batch loss = 0.17\n",
      "epoch 4, step 46500/55000, batch loss = 0.21\n",
      "epoch 4, step 46750/55000, batch loss = 0.18\n",
      "epoch 4, step 47000/55000, batch loss = 0.15\n",
      "epoch 4, step 47250/55000, batch loss = 0.19\n",
      "epoch 4, step 47500/55000, batch loss = 0.17\n",
      "Train accuracy = 98.60\n",
      "epoch 4, step 47750/55000, batch loss = 0.18\n",
      "epoch 4, step 48000/55000, batch loss = 0.19\n",
      "epoch 4, step 48250/55000, batch loss = 0.16\n",
      "epoch 4, step 48500/55000, batch loss = 0.16\n",
      "epoch 4, step 48750/55000, batch loss = 0.16\n",
      "epoch 4, step 49000/55000, batch loss = 0.21\n",
      "epoch 4, step 49250/55000, batch loss = 0.16\n",
      "epoch 4, step 49500/55000, batch loss = 0.13\n",
      "epoch 4, step 49750/55000, batch loss = 0.19\n",
      "epoch 4, step 50000/55000, batch loss = 0.17\n",
      "Train accuracy = 98.60\n",
      "epoch 4, step 50250/55000, batch loss = 0.19\n",
      "epoch 4, step 50500/55000, batch loss = 0.22\n",
      "epoch 4, step 50750/55000, batch loss = 0.18\n",
      "epoch 4, step 51000/55000, batch loss = 0.17\n",
      "epoch 4, step 51250/55000, batch loss = 0.18\n",
      "epoch 4, step 51500/55000, batch loss = 0.17\n",
      "epoch 4, step 51750/55000, batch loss = 0.15\n",
      "epoch 4, step 52000/55000, batch loss = 0.25\n",
      "epoch 4, step 52250/55000, batch loss = 0.17\n",
      "epoch 4, step 52500/55000, batch loss = 0.17\n",
      "Train accuracy = 98.59\n",
      "epoch 4, step 52750/55000, batch loss = 0.25\n",
      "epoch 4, step 53000/55000, batch loss = 0.15\n",
      "epoch 4, step 53250/55000, batch loss = 0.23\n",
      "epoch 4, step 53500/55000, batch loss = 0.16\n",
      "epoch 4, step 53750/55000, batch loss = 0.17\n",
      "epoch 4, step 54000/55000, batch loss = 0.14\n",
      "epoch 4, step 54250/55000, batch loss = 0.19\n",
      "epoch 4, step 54500/55000, batch loss = 0.16\n",
      "epoch 4, step 54750/55000, batch loss = 0.18\n",
      "Train accuracy = 98.59\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 98.60\n",
      "Validation avg loss = 0.18\n",
      "\n",
      "epoch 5, step 0/55000, batch loss = 0.14\n",
      "epoch 5, step 250/55000, batch loss = 0.18\n",
      "epoch 5, step 500/55000, batch loss = 0.15\n",
      "epoch 5, step 750/55000, batch loss = 0.24\n",
      "epoch 5, step 1000/55000, batch loss = 0.19\n",
      "epoch 5, step 1250/55000, batch loss = 0.15\n",
      "epoch 5, step 1500/55000, batch loss = 0.14\n",
      "epoch 5, step 1750/55000, batch loss = 0.15\n",
      "epoch 5, step 2000/55000, batch loss = 0.22\n",
      "epoch 5, step 2250/55000, batch loss = 0.16\n",
      "epoch 5, step 2500/55000, batch loss = 0.20\n",
      "Train accuracy = 98.71\n",
      "epoch 5, step 2750/55000, batch loss = 0.25\n",
      "epoch 5, step 3000/55000, batch loss = 0.16\n",
      "epoch 5, step 3250/55000, batch loss = 0.16\n",
      "epoch 5, step 3500/55000, batch loss = 0.19\n",
      "epoch 5, step 3750/55000, batch loss = 0.21\n",
      "epoch 5, step 4000/55000, batch loss = 0.14\n",
      "epoch 5, step 4250/55000, batch loss = 0.14\n",
      "epoch 5, step 4500/55000, batch loss = 0.15\n",
      "epoch 5, step 4750/55000, batch loss = 0.14\n",
      "epoch 5, step 5000/55000, batch loss = 0.15\n",
      "Train accuracy = 98.85\n",
      "epoch 5, step 5250/55000, batch loss = 0.19\n",
      "epoch 5, step 5500/55000, batch loss = 0.21\n",
      "epoch 5, step 5750/55000, batch loss = 0.20\n",
      "epoch 5, step 6000/55000, batch loss = 0.16\n",
      "epoch 5, step 6250/55000, batch loss = 0.14\n",
      "epoch 5, step 6500/55000, batch loss = 0.14\n",
      "epoch 5, step 6750/55000, batch loss = 0.15\n",
      "epoch 5, step 7000/55000, batch loss = 0.20\n",
      "epoch 5, step 7250/55000, batch loss = 0.17\n",
      "epoch 5, step 7500/55000, batch loss = 0.17\n",
      "Train accuracy = 98.77\n",
      "epoch 5, step 7750/55000, batch loss = 0.28\n",
      "epoch 5, step 8000/55000, batch loss = 0.20\n",
      "epoch 5, step 8250/55000, batch loss = 0.15\n",
      "epoch 5, step 8500/55000, batch loss = 0.17\n",
      "epoch 5, step 8750/55000, batch loss = 0.23\n",
      "epoch 5, step 9000/55000, batch loss = 0.20\n",
      "epoch 5, step 9250/55000, batch loss = 0.19\n",
      "epoch 5, step 9500/55000, batch loss = 0.20\n",
      "epoch 5, step 9750/55000, batch loss = 0.19\n",
      "epoch 5, step 10000/55000, batch loss = 0.18\n",
      "Train accuracy = 98.56\n",
      "epoch 5, step 10250/55000, batch loss = 0.16\n",
      "epoch 5, step 10500/55000, batch loss = 0.15\n",
      "epoch 5, step 10750/55000, batch loss = 0.15\n",
      "epoch 5, step 11000/55000, batch loss = 0.21\n",
      "epoch 5, step 11250/55000, batch loss = 0.19\n",
      "epoch 5, step 11500/55000, batch loss = 0.25\n",
      "epoch 5, step 11750/55000, batch loss = 0.16\n",
      "epoch 5, step 12000/55000, batch loss = 0.16\n",
      "epoch 5, step 12250/55000, batch loss = 0.15\n",
      "epoch 5, step 12500/55000, batch loss = 0.14\n",
      "Train accuracy = 98.62\n",
      "epoch 5, step 12750/55000, batch loss = 0.16\n",
      "epoch 5, step 13000/55000, batch loss = 0.14\n",
      "epoch 5, step 13250/55000, batch loss = 0.15\n",
      "epoch 5, step 13500/55000, batch loss = 0.19\n",
      "epoch 5, step 13750/55000, batch loss = 0.16\n",
      "epoch 5, step 14000/55000, batch loss = 0.21\n",
      "epoch 5, step 14250/55000, batch loss = 0.16\n",
      "epoch 5, step 14500/55000, batch loss = 0.14\n",
      "epoch 5, step 14750/55000, batch loss = 0.16\n",
      "epoch 5, step 15000/55000, batch loss = 0.16\n",
      "Train accuracy = 98.67\n",
      "epoch 5, step 15250/55000, batch loss = 0.18\n",
      "epoch 5, step 15500/55000, batch loss = 0.15\n",
      "epoch 5, step 15750/55000, batch loss = 0.14\n",
      "epoch 5, step 16000/55000, batch loss = 0.20\n",
      "epoch 5, step 16250/55000, batch loss = 0.23\n",
      "epoch 5, step 16500/55000, batch loss = 0.23\n",
      "epoch 5, step 16750/55000, batch loss = 0.15\n",
      "epoch 5, step 17000/55000, batch loss = 0.14\n",
      "epoch 5, step 17250/55000, batch loss = 0.17\n",
      "epoch 5, step 17500/55000, batch loss = 0.20\n",
      "Train accuracy = 98.65\n",
      "epoch 5, step 17750/55000, batch loss = 0.15\n",
      "epoch 5, step 18000/55000, batch loss = 0.20\n",
      "epoch 5, step 18250/55000, batch loss = 0.18\n",
      "epoch 5, step 18500/55000, batch loss = 0.23\n",
      "epoch 5, step 18750/55000, batch loss = 0.18\n",
      "epoch 5, step 19000/55000, batch loss = 0.16\n",
      "epoch 5, step 19250/55000, batch loss = 0.21\n",
      "epoch 5, step 19500/55000, batch loss = 0.29\n",
      "epoch 5, step 19750/55000, batch loss = 0.15\n",
      "epoch 5, step 20000/55000, batch loss = 0.16\n",
      "Train accuracy = 98.59\n",
      "epoch 5, step 20250/55000, batch loss = 0.21\n",
      "epoch 5, step 20500/55000, batch loss = 0.13\n",
      "epoch 5, step 20750/55000, batch loss = 0.13\n",
      "epoch 5, step 21000/55000, batch loss = 0.19\n",
      "epoch 5, step 21250/55000, batch loss = 0.21\n",
      "epoch 5, step 21500/55000, batch loss = 0.15\n",
      "epoch 5, step 21750/55000, batch loss = 0.16\n",
      "epoch 5, step 22000/55000, batch loss = 0.20\n",
      "epoch 5, step 22250/55000, batch loss = 0.24\n",
      "epoch 5, step 22500/55000, batch loss = 0.25\n",
      "Train accuracy = 98.60\n",
      "epoch 5, step 22750/55000, batch loss = 0.14\n",
      "epoch 5, step 23000/55000, batch loss = 0.15\n",
      "epoch 5, step 23250/55000, batch loss = 0.19\n",
      "epoch 5, step 23500/55000, batch loss = 0.15\n",
      "epoch 5, step 23750/55000, batch loss = 0.16\n",
      "epoch 5, step 24000/55000, batch loss = 0.18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, step 24250/55000, batch loss = 0.14\n",
      "epoch 5, step 24500/55000, batch loss = 0.14\n",
      "epoch 5, step 24750/55000, batch loss = 0.23\n",
      "epoch 5, step 25000/55000, batch loss = 0.16\n",
      "Train accuracy = 98.65\n",
      "epoch 5, step 25250/55000, batch loss = 0.15\n",
      "epoch 5, step 25500/55000, batch loss = 0.14\n",
      "epoch 5, step 25750/55000, batch loss = 0.18\n",
      "epoch 5, step 26000/55000, batch loss = 0.17\n",
      "epoch 5, step 26250/55000, batch loss = 0.14\n",
      "epoch 5, step 26500/55000, batch loss = 0.16\n",
      "epoch 5, step 26750/55000, batch loss = 0.14\n",
      "epoch 5, step 27000/55000, batch loss = 0.18\n",
      "epoch 5, step 27250/55000, batch loss = 0.16\n",
      "epoch 5, step 27500/55000, batch loss = 0.28\n",
      "Train accuracy = 98.68\n",
      "epoch 5, step 27750/55000, batch loss = 0.25\n",
      "epoch 5, step 28000/55000, batch loss = 0.28\n",
      "epoch 5, step 28250/55000, batch loss = 0.21\n",
      "epoch 5, step 28500/55000, batch loss = 0.20\n",
      "epoch 5, step 28750/55000, batch loss = 0.19\n",
      "epoch 5, step 29000/55000, batch loss = 0.17\n",
      "epoch 5, step 29250/55000, batch loss = 0.14\n",
      "epoch 5, step 29500/55000, batch loss = 0.13\n",
      "epoch 5, step 29750/55000, batch loss = 0.19\n",
      "epoch 5, step 30000/55000, batch loss = 0.15\n",
      "Train accuracy = 98.66\n",
      "epoch 5, step 30250/55000, batch loss = 0.17\n",
      "epoch 5, step 30500/55000, batch loss = 0.17\n",
      "epoch 5, step 30750/55000, batch loss = 0.20\n",
      "epoch 5, step 31000/55000, batch loss = 0.19\n",
      "epoch 5, step 31250/55000, batch loss = 0.16\n",
      "epoch 5, step 31500/55000, batch loss = 0.22\n",
      "epoch 5, step 31750/55000, batch loss = 0.19\n",
      "epoch 5, step 32000/55000, batch loss = 0.22\n",
      "epoch 5, step 32250/55000, batch loss = 0.20\n",
      "epoch 5, step 32500/55000, batch loss = 0.15\n",
      "Train accuracy = 98.66\n",
      "epoch 5, step 32750/55000, batch loss = 0.16\n",
      "epoch 5, step 33000/55000, batch loss = 0.18\n",
      "epoch 5, step 33250/55000, batch loss = 0.17\n",
      "epoch 5, step 33500/55000, batch loss = 0.14\n",
      "epoch 5, step 33750/55000, batch loss = 0.16\n",
      "epoch 5, step 34000/55000, batch loss = 0.18\n",
      "epoch 5, step 34250/55000, batch loss = 0.15\n",
      "epoch 5, step 34500/55000, batch loss = 0.15\n",
      "epoch 5, step 34750/55000, batch loss = 0.17\n",
      "epoch 5, step 35000/55000, batch loss = 0.15\n",
      "Train accuracy = 98.67\n",
      "epoch 5, step 35250/55000, batch loss = 0.16\n",
      "epoch 5, step 35500/55000, batch loss = 0.14\n",
      "epoch 5, step 35750/55000, batch loss = 0.19\n",
      "epoch 5, step 36000/55000, batch loss = 0.13\n",
      "epoch 5, step 36250/55000, batch loss = 0.20\n",
      "epoch 5, step 36500/55000, batch loss = 0.14\n",
      "epoch 5, step 36750/55000, batch loss = 0.13\n",
      "epoch 5, step 37000/55000, batch loss = 0.15\n",
      "epoch 5, step 37250/55000, batch loss = 0.13\n",
      "epoch 5, step 37500/55000, batch loss = 0.17\n",
      "Train accuracy = 98.71\n",
      "epoch 5, step 37750/55000, batch loss = 0.14\n",
      "epoch 5, step 38000/55000, batch loss = 0.15\n",
      "epoch 5, step 38250/55000, batch loss = 0.15\n",
      "epoch 5, step 38500/55000, batch loss = 0.28\n",
      "epoch 5, step 38750/55000, batch loss = 0.20\n",
      "epoch 5, step 39000/55000, batch loss = 0.23\n",
      "epoch 5, step 39250/55000, batch loss = 0.18\n",
      "epoch 5, step 39500/55000, batch loss = 0.20\n",
      "epoch 5, step 39750/55000, batch loss = 0.14\n",
      "epoch 5, step 40000/55000, batch loss = 0.16\n",
      "Train accuracy = 98.72\n",
      "epoch 5, step 40250/55000, batch loss = 0.17\n",
      "epoch 5, step 40500/55000, batch loss = 0.17\n",
      "epoch 5, step 40750/55000, batch loss = 0.16\n",
      "epoch 5, step 41000/55000, batch loss = 0.14\n",
      "epoch 5, step 41250/55000, batch loss = 0.21\n",
      "epoch 5, step 41500/55000, batch loss = 0.22\n",
      "epoch 5, step 41750/55000, batch loss = 0.14\n",
      "epoch 5, step 42000/55000, batch loss = 0.18\n",
      "epoch 5, step 42250/55000, batch loss = 0.13\n",
      "epoch 5, step 42500/55000, batch loss = 0.32\n",
      "Train accuracy = 98.71\n",
      "epoch 5, step 42750/55000, batch loss = 0.15\n",
      "epoch 5, step 43000/55000, batch loss = 0.14\n",
      "epoch 5, step 43250/55000, batch loss = 0.14\n",
      "epoch 5, step 43500/55000, batch loss = 0.14\n",
      "epoch 5, step 43750/55000, batch loss = 0.20\n",
      "epoch 5, step 44000/55000, batch loss = 0.19\n",
      "epoch 5, step 44250/55000, batch loss = 0.17\n",
      "epoch 5, step 44500/55000, batch loss = 0.14\n",
      "epoch 5, step 44750/55000, batch loss = 0.15\n",
      "epoch 5, step 45000/55000, batch loss = 0.15\n",
      "Train accuracy = 98.71\n",
      "epoch 5, step 45250/55000, batch loss = 0.18\n",
      "epoch 5, step 45500/55000, batch loss = 0.18\n",
      "epoch 5, step 45750/55000, batch loss = 0.15\n",
      "epoch 5, step 46000/55000, batch loss = 0.15\n",
      "epoch 5, step 46250/55000, batch loss = 0.14\n",
      "epoch 5, step 46500/55000, batch loss = 0.22\n",
      "epoch 5, step 46750/55000, batch loss = 0.19\n",
      "epoch 5, step 47000/55000, batch loss = 0.19\n",
      "epoch 5, step 47250/55000, batch loss = 0.17\n",
      "epoch 5, step 47500/55000, batch loss = 0.23\n",
      "Train accuracy = 98.70\n",
      "epoch 5, step 47750/55000, batch loss = 0.15\n",
      "epoch 5, step 48000/55000, batch loss = 0.15\n",
      "epoch 5, step 48250/55000, batch loss = 0.14\n",
      "epoch 5, step 48500/55000, batch loss = 0.22\n",
      "epoch 5, step 48750/55000, batch loss = 0.19\n",
      "epoch 5, step 49000/55000, batch loss = 0.15\n",
      "epoch 5, step 49250/55000, batch loss = 0.17\n",
      "epoch 5, step 49500/55000, batch loss = 0.25\n",
      "epoch 5, step 49750/55000, batch loss = 0.14\n",
      "epoch 5, step 50000/55000, batch loss = 0.16\n",
      "Train accuracy = 98.69\n",
      "epoch 5, step 50250/55000, batch loss = 0.14\n",
      "epoch 5, step 50500/55000, batch loss = 0.15\n",
      "epoch 5, step 50750/55000, batch loss = 0.14\n",
      "epoch 5, step 51000/55000, batch loss = 0.19\n",
      "epoch 5, step 51250/55000, batch loss = 0.17\n",
      "epoch 5, step 51500/55000, batch loss = 0.16\n",
      "epoch 5, step 51750/55000, batch loss = 0.20\n",
      "epoch 5, step 52000/55000, batch loss = 0.15\n",
      "epoch 5, step 52250/55000, batch loss = 0.17\n",
      "epoch 5, step 52500/55000, batch loss = 0.15\n",
      "Train accuracy = 98.71\n",
      "epoch 5, step 52750/55000, batch loss = 0.14\n",
      "epoch 5, step 53000/55000, batch loss = 0.14\n",
      "epoch 5, step 53250/55000, batch loss = 0.17\n",
      "epoch 5, step 53500/55000, batch loss = 0.14\n",
      "epoch 5, step 53750/55000, batch loss = 0.16\n",
      "epoch 5, step 54000/55000, batch loss = 0.25\n",
      "epoch 5, step 54250/55000, batch loss = 0.17\n",
      "epoch 5, step 54500/55000, batch loss = 0.14\n",
      "epoch 5, step 54750/55000, batch loss = 0.16\n",
      "Train accuracy = 98.73\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 98.74\n",
      "Validation avg loss = 0.17\n",
      "\n",
      "\n",
      "Running evaluation:  Test\n",
      "Test accuracy = 98.75\n",
      "Test avg loss = 0.17\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LAMBDA: 0.001 \n",
      "\n",
      "epoch 1, step 0/55000, batch loss = 2.71\n",
      "epoch 1, step 250/55000, batch loss = 2.25\n",
      "epoch 1, step 500/55000, batch loss = 2.07\n",
      "epoch 1, step 750/55000, batch loss = 2.02\n",
      "epoch 1, step 1000/55000, batch loss = 1.27\n",
      "epoch 1, step 1250/55000, batch loss = 1.24\n",
      "epoch 1, step 1500/55000, batch loss = 0.99\n",
      "epoch 1, step 1750/55000, batch loss = 1.09\n",
      "epoch 1, step 2000/55000, batch loss = 1.06\n",
      "epoch 1, step 2250/55000, batch loss = 0.87\n",
      "epoch 1, step 2500/55000, batch loss = 0.92\n",
      "Train accuracy = 69.41\n",
      "epoch 1, step 2750/55000, batch loss = 0.64\n",
      "epoch 1, step 3000/55000, batch loss = 0.84\n",
      "epoch 1, step 3250/55000, batch loss = 0.58\n",
      "epoch 1, step 3500/55000, batch loss = 0.76\n",
      "epoch 1, step 3750/55000, batch loss = 0.77\n",
      "epoch 1, step 4000/55000, batch loss = 0.73\n",
      "epoch 1, step 4250/55000, batch loss = 0.64\n",
      "epoch 1, step 4500/55000, batch loss = 0.65\n",
      "epoch 1, step 4750/55000, batch loss = 0.75\n",
      "epoch 1, step 5000/55000, batch loss = 0.71\n",
      "Train accuracy = 80.42\n",
      "epoch 1, step 5250/55000, batch loss = 0.67\n",
      "epoch 1, step 5500/55000, batch loss = 0.63\n",
      "epoch 1, step 5750/55000, batch loss = 0.61\n",
      "epoch 1, step 6000/55000, batch loss = 0.63\n",
      "epoch 1, step 6250/55000, batch loss = 0.75\n",
      "epoch 1, step 6500/55000, batch loss = 0.46\n",
      "epoch 1, step 6750/55000, batch loss = 0.66\n",
      "epoch 1, step 7000/55000, batch loss = 0.61\n",
      "epoch 1, step 7250/55000, batch loss = 0.78\n",
      "epoch 1, step 7500/55000, batch loss = 0.48\n",
      "Train accuracy = 84.81\n",
      "epoch 1, step 7750/55000, batch loss = 0.57\n",
      "epoch 1, step 8000/55000, batch loss = 0.53\n",
      "epoch 1, step 8250/55000, batch loss = 0.60\n",
      "epoch 1, step 8500/55000, batch loss = 0.50\n",
      "epoch 1, step 8750/55000, batch loss = 0.62\n",
      "epoch 1, step 9000/55000, batch loss = 0.55\n",
      "epoch 1, step 9250/55000, batch loss = 0.57\n",
      "epoch 1, step 9500/55000, batch loss = 0.52\n",
      "epoch 1, step 9750/55000, batch loss = 0.55\n",
      "epoch 1, step 10000/55000, batch loss = 0.53\n",
      "Train accuracy = 87.41\n",
      "epoch 1, step 10250/55000, batch loss = 0.54\n",
      "epoch 1, step 10500/55000, batch loss = 0.63\n",
      "epoch 1, step 10750/55000, batch loss = 0.51\n",
      "epoch 1, step 11000/55000, batch loss = 0.53\n",
      "epoch 1, step 11250/55000, batch loss = 0.57\n",
      "epoch 1, step 11500/55000, batch loss = 0.57\n",
      "epoch 1, step 11750/55000, batch loss = 0.57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, step 12000/55000, batch loss = 0.49\n",
      "epoch 1, step 12250/55000, batch loss = 0.60\n",
      "epoch 1, step 12500/55000, batch loss = 0.49\n",
      "Train accuracy = 89.08\n",
      "epoch 1, step 12750/55000, batch loss = 0.60\n",
      "epoch 1, step 13000/55000, batch loss = 0.66\n",
      "epoch 1, step 13250/55000, batch loss = 0.47\n",
      "epoch 1, step 13500/55000, batch loss = 0.55\n",
      "epoch 1, step 13750/55000, batch loss = 0.43\n",
      "epoch 1, step 14000/55000, batch loss = 0.62\n",
      "epoch 1, step 14250/55000, batch loss = 0.60\n",
      "epoch 1, step 14500/55000, batch loss = 0.45\n",
      "epoch 1, step 14750/55000, batch loss = 0.50\n",
      "epoch 1, step 15000/55000, batch loss = 0.55\n",
      "Train accuracy = 90.19\n",
      "epoch 1, step 15250/55000, batch loss = 0.50\n",
      "epoch 1, step 15500/55000, batch loss = 0.48\n",
      "epoch 1, step 15750/55000, batch loss = 0.49\n",
      "epoch 1, step 16000/55000, batch loss = 0.59\n",
      "epoch 1, step 16250/55000, batch loss = 0.53\n",
      "epoch 1, step 16500/55000, batch loss = 0.56\n",
      "epoch 1, step 16750/55000, batch loss = 0.45\n",
      "epoch 1, step 17000/55000, batch loss = 0.56\n",
      "epoch 1, step 17250/55000, batch loss = 0.47\n",
      "epoch 1, step 17500/55000, batch loss = 0.46\n",
      "Train accuracy = 90.97\n",
      "epoch 1, step 17750/55000, batch loss = 0.44\n",
      "epoch 1, step 18000/55000, batch loss = 0.54\n",
      "epoch 1, step 18250/55000, batch loss = 0.49\n",
      "epoch 1, step 18500/55000, batch loss = 0.59\n",
      "epoch 1, step 18750/55000, batch loss = 0.47\n",
      "epoch 1, step 19000/55000, batch loss = 0.47\n",
      "epoch 1, step 19250/55000, batch loss = 0.48\n",
      "epoch 1, step 19500/55000, batch loss = 0.48\n",
      "epoch 1, step 19750/55000, batch loss = 0.79\n",
      "epoch 1, step 20000/55000, batch loss = 0.46\n",
      "Train accuracy = 91.66\n",
      "epoch 1, step 20250/55000, batch loss = 0.51\n",
      "epoch 1, step 20500/55000, batch loss = 0.44\n",
      "epoch 1, step 20750/55000, batch loss = 0.57\n",
      "epoch 1, step 21000/55000, batch loss = 0.61\n",
      "epoch 1, step 21250/55000, batch loss = 0.51\n",
      "epoch 1, step 21500/55000, batch loss = 0.51\n",
      "epoch 1, step 21750/55000, batch loss = 0.63\n",
      "epoch 1, step 22000/55000, batch loss = 0.42\n",
      "epoch 1, step 22250/55000, batch loss = 0.59\n",
      "epoch 1, step 22500/55000, batch loss = 0.46\n",
      "Train accuracy = 92.26\n",
      "epoch 1, step 22750/55000, batch loss = 0.45\n",
      "epoch 1, step 23000/55000, batch loss = 0.57\n",
      "epoch 1, step 23250/55000, batch loss = 0.63\n",
      "epoch 1, step 23500/55000, batch loss = 0.58\n",
      "epoch 1, step 23750/55000, batch loss = 0.54\n",
      "epoch 1, step 24000/55000, batch loss = 0.42\n",
      "epoch 1, step 24250/55000, batch loss = 0.47\n",
      "epoch 1, step 24500/55000, batch loss = 0.51\n",
      "epoch 1, step 24750/55000, batch loss = 0.53\n",
      "epoch 1, step 25000/55000, batch loss = 0.53\n",
      "Train accuracy = 92.71\n",
      "epoch 1, step 25250/55000, batch loss = 0.45\n",
      "epoch 1, step 25500/55000, batch loss = 0.42\n",
      "epoch 1, step 25750/55000, batch loss = 0.46\n",
      "epoch 1, step 26000/55000, batch loss = 0.48\n",
      "epoch 1, step 26250/55000, batch loss = 0.65\n",
      "epoch 1, step 26500/55000, batch loss = 0.52\n",
      "epoch 1, step 26750/55000, batch loss = 0.43\n",
      "epoch 1, step 27000/55000, batch loss = 0.44\n",
      "epoch 1, step 27250/55000, batch loss = 0.47\n",
      "epoch 1, step 27500/55000, batch loss = 0.54\n",
      "Train accuracy = 93.06\n",
      "epoch 1, step 27750/55000, batch loss = 0.43\n",
      "epoch 1, step 28000/55000, batch loss = 0.52\n",
      "epoch 1, step 28250/55000, batch loss = 0.42\n",
      "epoch 1, step 28500/55000, batch loss = 0.57\n",
      "epoch 1, step 28750/55000, batch loss = 0.47\n",
      "epoch 1, step 29000/55000, batch loss = 0.44\n",
      "epoch 1, step 29250/55000, batch loss = 0.45\n",
      "epoch 1, step 29500/55000, batch loss = 0.49\n",
      "epoch 1, step 29750/55000, batch loss = 0.51\n",
      "epoch 1, step 30000/55000, batch loss = 0.51\n",
      "Train accuracy = 93.42\n",
      "epoch 1, step 30250/55000, batch loss = 0.51\n",
      "epoch 1, step 30500/55000, batch loss = 0.49\n",
      "epoch 1, step 30750/55000, batch loss = 0.45\n",
      "epoch 1, step 31000/55000, batch loss = 0.53\n",
      "epoch 1, step 31250/55000, batch loss = 0.42\n",
      "epoch 1, step 31500/55000, batch loss = 0.59\n",
      "epoch 1, step 31750/55000, batch loss = 0.45\n",
      "epoch 1, step 32000/55000, batch loss = 0.47\n",
      "epoch 1, step 32250/55000, batch loss = 0.41\n",
      "epoch 1, step 32500/55000, batch loss = 0.46\n",
      "Train accuracy = 93.75\n",
      "epoch 1, step 32750/55000, batch loss = 0.41\n",
      "epoch 1, step 33000/55000, batch loss = 0.41\n",
      "epoch 1, step 33250/55000, batch loss = 0.53\n",
      "epoch 1, step 33500/55000, batch loss = 0.42\n",
      "epoch 1, step 33750/55000, batch loss = 0.51\n",
      "epoch 1, step 34000/55000, batch loss = 0.45\n",
      "epoch 1, step 34250/55000, batch loss = 0.42\n",
      "epoch 1, step 34500/55000, batch loss = 0.46\n",
      "epoch 1, step 34750/55000, batch loss = 0.43\n",
      "epoch 1, step 35000/55000, batch loss = 0.56\n",
      "Train accuracy = 94.04\n",
      "epoch 1, step 35250/55000, batch loss = 0.44\n",
      "epoch 1, step 35500/55000, batch loss = 0.51\n",
      "epoch 1, step 35750/55000, batch loss = 0.44\n",
      "epoch 1, step 36000/55000, batch loss = 0.45\n",
      "epoch 1, step 36250/55000, batch loss = 0.42\n",
      "epoch 1, step 36500/55000, batch loss = 0.46\n",
      "epoch 1, step 36750/55000, batch loss = 0.42\n",
      "epoch 1, step 37000/55000, batch loss = 0.42\n",
      "epoch 1, step 37250/55000, batch loss = 0.45\n",
      "epoch 1, step 37500/55000, batch loss = 0.46\n",
      "Train accuracy = 94.29\n",
      "epoch 1, step 37750/55000, batch loss = 0.55\n",
      "epoch 1, step 38000/55000, batch loss = 0.42\n",
      "epoch 1, step 38250/55000, batch loss = 0.51\n",
      "epoch 1, step 38500/55000, batch loss = 0.50\n",
      "epoch 1, step 38750/55000, batch loss = 0.46\n",
      "epoch 1, step 39000/55000, batch loss = 0.48\n",
      "epoch 1, step 39250/55000, batch loss = 0.64\n",
      "epoch 1, step 39500/55000, batch loss = 0.39\n",
      "epoch 1, step 39750/55000, batch loss = 0.51\n",
      "epoch 1, step 40000/55000, batch loss = 0.43\n",
      "Train accuracy = 94.48\n",
      "epoch 1, step 40250/55000, batch loss = 0.48\n",
      "epoch 1, step 40500/55000, batch loss = 0.42\n",
      "epoch 1, step 40750/55000, batch loss = 0.53\n",
      "epoch 1, step 41000/55000, batch loss = 0.39\n",
      "epoch 1, step 41250/55000, batch loss = 0.42\n",
      "epoch 1, step 41500/55000, batch loss = 0.48\n",
      "epoch 1, step 41750/55000, batch loss = 0.45\n",
      "epoch 1, step 42000/55000, batch loss = 0.41\n",
      "epoch 1, step 42250/55000, batch loss = 0.40\n",
      "epoch 1, step 42500/55000, batch loss = 0.48\n",
      "Train accuracy = 94.67\n",
      "epoch 1, step 42750/55000, batch loss = 0.47\n",
      "epoch 1, step 43000/55000, batch loss = 0.40\n",
      "epoch 1, step 43250/55000, batch loss = 0.39\n",
      "epoch 1, step 43500/55000, batch loss = 0.41\n",
      "epoch 1, step 43750/55000, batch loss = 0.49\n",
      "epoch 1, step 44000/55000, batch loss = 0.43\n",
      "epoch 1, step 44250/55000, batch loss = 0.42\n",
      "epoch 1, step 44500/55000, batch loss = 0.44\n",
      "epoch 1, step 44750/55000, batch loss = 0.40\n",
      "epoch 1, step 45000/55000, batch loss = 0.39\n",
      "Train accuracy = 94.87\n",
      "epoch 1, step 45250/55000, batch loss = 0.39\n",
      "epoch 1, step 45500/55000, batch loss = 0.60\n",
      "epoch 1, step 45750/55000, batch loss = 0.49\n",
      "epoch 1, step 46000/55000, batch loss = 0.42\n",
      "epoch 1, step 46250/55000, batch loss = 0.42\n",
      "epoch 1, step 46500/55000, batch loss = 0.48\n",
      "epoch 1, step 46750/55000, batch loss = 0.39\n",
      "epoch 1, step 47000/55000, batch loss = 0.48\n",
      "epoch 1, step 47250/55000, batch loss = 0.59\n",
      "epoch 1, step 47500/55000, batch loss = 0.51\n",
      "Train accuracy = 95.02\n",
      "epoch 1, step 47750/55000, batch loss = 0.43\n",
      "epoch 1, step 48000/55000, batch loss = 0.39\n",
      "epoch 1, step 48250/55000, batch loss = 0.38\n",
      "epoch 1, step 48500/55000, batch loss = 0.49\n",
      "epoch 1, step 48750/55000, batch loss = 0.64\n",
      "epoch 1, step 49000/55000, batch loss = 0.51\n",
      "epoch 1, step 49250/55000, batch loss = 0.41\n",
      "epoch 1, step 49500/55000, batch loss = 0.47\n",
      "epoch 1, step 49750/55000, batch loss = 0.40\n",
      "epoch 1, step 50000/55000, batch loss = 0.56\n",
      "Train accuracy = 95.14\n",
      "epoch 1, step 50250/55000, batch loss = 0.39\n",
      "epoch 1, step 50500/55000, batch loss = 0.55\n",
      "epoch 1, step 50750/55000, batch loss = 0.44\n",
      "epoch 1, step 51000/55000, batch loss = 0.55\n",
      "epoch 1, step 51250/55000, batch loss = 0.58\n",
      "epoch 1, step 51500/55000, batch loss = 0.40\n",
      "epoch 1, step 51750/55000, batch loss = 0.54\n",
      "epoch 1, step 52000/55000, batch loss = 0.55\n",
      "epoch 1, step 52250/55000, batch loss = 0.52\n",
      "epoch 1, step 52500/55000, batch loss = 0.39\n",
      "Train accuracy = 95.26\n",
      "epoch 1, step 52750/55000, batch loss = 0.40\n",
      "epoch 1, step 53000/55000, batch loss = 0.53\n",
      "epoch 1, step 53250/55000, batch loss = 0.47\n",
      "epoch 1, step 53500/55000, batch loss = 0.44\n",
      "epoch 1, step 53750/55000, batch loss = 0.44\n",
      "epoch 1, step 54000/55000, batch loss = 0.44\n",
      "epoch 1, step 54250/55000, batch loss = 0.42\n",
      "epoch 1, step 54500/55000, batch loss = 0.38\n",
      "epoch 1, step 54750/55000, batch loss = 0.46\n",
      "Train accuracy = 95.39\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 98.34\n",
      "Validation avg loss = 0.42\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, step 0/55000, batch loss = 0.38\n",
      "epoch 2, step 250/55000, batch loss = 0.43\n",
      "epoch 2, step 500/55000, batch loss = 0.50\n",
      "epoch 2, step 750/55000, batch loss = 0.41\n",
      "epoch 2, step 1000/55000, batch loss = 0.43\n",
      "epoch 2, step 1250/55000, batch loss = 0.37\n",
      "epoch 2, step 1500/55000, batch loss = 0.37\n",
      "epoch 2, step 1750/55000, batch loss = 0.53\n",
      "epoch 2, step 2000/55000, batch loss = 0.37\n",
      "epoch 2, step 2250/55000, batch loss = 0.37\n",
      "epoch 2, step 2500/55000, batch loss = 0.38\n",
      "Train accuracy = 98.08\n",
      "epoch 2, step 2750/55000, batch loss = 0.39\n",
      "epoch 2, step 3000/55000, batch loss = 0.44\n",
      "epoch 2, step 3250/55000, batch loss = 0.39\n",
      "epoch 2, step 3500/55000, batch loss = 0.38\n",
      "epoch 2, step 3750/55000, batch loss = 0.36\n",
      "epoch 2, step 4000/55000, batch loss = 0.36\n",
      "epoch 2, step 4250/55000, batch loss = 0.38\n",
      "epoch 2, step 4500/55000, batch loss = 0.40\n",
      "epoch 2, step 4750/55000, batch loss = 0.39\n",
      "epoch 2, step 5000/55000, batch loss = 0.41\n",
      "Train accuracy = 98.22\n",
      "epoch 2, step 5250/55000, batch loss = 0.38\n",
      "epoch 2, step 5500/55000, batch loss = 0.37\n",
      "epoch 2, step 5750/55000, batch loss = 0.37\n",
      "epoch 2, step 6000/55000, batch loss = 0.44\n",
      "epoch 2, step 6250/55000, batch loss = 0.37\n",
      "epoch 2, step 6500/55000, batch loss = 0.36\n",
      "epoch 2, step 6750/55000, batch loss = 0.36\n",
      "epoch 2, step 7000/55000, batch loss = 0.42\n",
      "epoch 2, step 7250/55000, batch loss = 0.38\n",
      "epoch 2, step 7500/55000, batch loss = 0.43\n",
      "Train accuracy = 98.23\n",
      "epoch 2, step 7750/55000, batch loss = 0.36\n",
      "epoch 2, step 8000/55000, batch loss = 0.37\n",
      "epoch 2, step 8250/55000, batch loss = 0.53\n",
      "epoch 2, step 8500/55000, batch loss = 0.48\n",
      "epoch 2, step 8750/55000, batch loss = 0.38\n",
      "epoch 2, step 9000/55000, batch loss = 0.36\n",
      "epoch 2, step 9250/55000, batch loss = 0.36\n",
      "epoch 2, step 9500/55000, batch loss = 0.41\n",
      "epoch 2, step 9750/55000, batch loss = 0.40\n",
      "epoch 2, step 10000/55000, batch loss = 0.38\n",
      "Train accuracy = 98.09\n",
      "epoch 2, step 10250/55000, batch loss = 0.37\n",
      "epoch 2, step 10500/55000, batch loss = 0.39\n",
      "epoch 2, step 10750/55000, batch loss = 0.48\n",
      "epoch 2, step 11000/55000, batch loss = 0.35\n",
      "epoch 2, step 11250/55000, batch loss = 0.37\n",
      "epoch 2, step 11500/55000, batch loss = 0.41\n",
      "epoch 2, step 11750/55000, batch loss = 0.39\n",
      "epoch 2, step 12000/55000, batch loss = 0.39\n",
      "epoch 2, step 12250/55000, batch loss = 0.37\n",
      "epoch 2, step 12500/55000, batch loss = 0.37\n",
      "Train accuracy = 98.17\n",
      "epoch 2, step 12750/55000, batch loss = 0.36\n",
      "epoch 2, step 13000/55000, batch loss = 0.36\n",
      "epoch 2, step 13250/55000, batch loss = 0.37\n",
      "epoch 2, step 13500/55000, batch loss = 0.37\n",
      "epoch 2, step 13750/55000, batch loss = 0.37\n",
      "epoch 2, step 14000/55000, batch loss = 0.50\n",
      "epoch 2, step 14250/55000, batch loss = 0.41\n",
      "epoch 2, step 14500/55000, batch loss = 0.44\n",
      "epoch 2, step 14750/55000, batch loss = 0.44\n",
      "epoch 2, step 15000/55000, batch loss = 0.35\n",
      "Train accuracy = 98.21\n",
      "epoch 2, step 15250/55000, batch loss = 0.37\n",
      "epoch 2, step 15500/55000, batch loss = 0.35\n",
      "epoch 2, step 15750/55000, batch loss = 0.38\n",
      "epoch 2, step 16000/55000, batch loss = 0.52\n",
      "epoch 2, step 16250/55000, batch loss = 0.38\n",
      "epoch 2, step 16500/55000, batch loss = 0.37\n",
      "epoch 2, step 16750/55000, batch loss = 0.43\n",
      "epoch 2, step 17000/55000, batch loss = 0.36\n",
      "epoch 2, step 17250/55000, batch loss = 0.42\n",
      "epoch 2, step 17500/55000, batch loss = 0.35\n",
      "Train accuracy = 98.25\n",
      "epoch 2, step 17750/55000, batch loss = 0.38\n",
      "epoch 2, step 18000/55000, batch loss = 0.38\n",
      "epoch 2, step 18250/55000, batch loss = 0.35\n",
      "epoch 2, step 18500/55000, batch loss = 0.35\n",
      "epoch 2, step 18750/55000, batch loss = 0.37\n",
      "epoch 2, step 19000/55000, batch loss = 0.37\n",
      "epoch 2, step 19250/55000, batch loss = 0.35\n",
      "epoch 2, step 19500/55000, batch loss = 0.35\n",
      "epoch 2, step 19750/55000, batch loss = 0.41\n",
      "epoch 2, step 20000/55000, batch loss = 0.36\n",
      "Train accuracy = 98.29\n",
      "epoch 2, step 20250/55000, batch loss = 0.38\n",
      "epoch 2, step 20500/55000, batch loss = 0.38\n",
      "epoch 2, step 20750/55000, batch loss = 0.38\n",
      "epoch 2, step 21000/55000, batch loss = 0.39\n",
      "epoch 2, step 21250/55000, batch loss = 0.36\n",
      "epoch 2, step 21500/55000, batch loss = 0.37\n",
      "epoch 2, step 21750/55000, batch loss = 0.35\n",
      "epoch 2, step 22000/55000, batch loss = 0.38\n",
      "epoch 2, step 22250/55000, batch loss = 0.36\n",
      "epoch 2, step 22500/55000, batch loss = 0.38\n",
      "Train accuracy = 98.32\n",
      "epoch 2, step 22750/55000, batch loss = 0.35\n",
      "epoch 2, step 23000/55000, batch loss = 0.35\n",
      "epoch 2, step 23250/55000, batch loss = 0.48\n",
      "epoch 2, step 23500/55000, batch loss = 0.37\n",
      "epoch 2, step 23750/55000, batch loss = 0.53\n",
      "epoch 2, step 24000/55000, batch loss = 0.56\n",
      "epoch 2, step 24250/55000, batch loss = 0.35\n",
      "epoch 2, step 24500/55000, batch loss = 0.40\n",
      "epoch 2, step 24750/55000, batch loss = 0.37\n",
      "epoch 2, step 25000/55000, batch loss = 0.34\n",
      "Train accuracy = 98.37\n",
      "epoch 2, step 25250/55000, batch loss = 0.43\n",
      "epoch 2, step 25500/55000, batch loss = 0.40\n",
      "epoch 2, step 25750/55000, batch loss = 0.38\n",
      "epoch 2, step 26000/55000, batch loss = 0.35\n",
      "epoch 2, step 26250/55000, batch loss = 0.34\n",
      "epoch 2, step 26500/55000, batch loss = 0.35\n",
      "epoch 2, step 26750/55000, batch loss = 0.36\n",
      "epoch 2, step 27000/55000, batch loss = 0.36\n",
      "epoch 2, step 27250/55000, batch loss = 0.40\n",
      "epoch 2, step 27500/55000, batch loss = 0.37\n",
      "Train accuracy = 98.41\n",
      "epoch 2, step 27750/55000, batch loss = 0.45\n",
      "epoch 2, step 28000/55000, batch loss = 0.52\n",
      "epoch 2, step 28250/55000, batch loss = 0.38\n",
      "epoch 2, step 28500/55000, batch loss = 0.35\n",
      "epoch 2, step 28750/55000, batch loss = 0.38\n",
      "epoch 2, step 29000/55000, batch loss = 0.35\n",
      "epoch 2, step 29250/55000, batch loss = 0.39\n",
      "epoch 2, step 29500/55000, batch loss = 0.43\n",
      "epoch 2, step 29750/55000, batch loss = 0.42\n",
      "epoch 2, step 30000/55000, batch loss = 0.44\n",
      "Train accuracy = 98.39\n",
      "epoch 2, step 30250/55000, batch loss = 0.36\n",
      "epoch 2, step 30500/55000, batch loss = 0.35\n",
      "epoch 2, step 30750/55000, batch loss = 0.37\n",
      "epoch 2, step 31000/55000, batch loss = 0.42\n",
      "epoch 2, step 31250/55000, batch loss = 0.35\n",
      "epoch 2, step 31500/55000, batch loss = 0.34\n",
      "epoch 2, step 31750/55000, batch loss = 0.34\n",
      "epoch 2, step 32000/55000, batch loss = 0.34\n",
      "epoch 2, step 32250/55000, batch loss = 0.35\n",
      "epoch 2, step 32500/55000, batch loss = 0.35\n",
      "Train accuracy = 98.41\n",
      "epoch 2, step 32750/55000, batch loss = 0.42\n",
      "epoch 2, step 33000/55000, batch loss = 0.35\n",
      "epoch 2, step 33250/55000, batch loss = 0.39\n",
      "epoch 2, step 33500/55000, batch loss = 0.49\n",
      "epoch 2, step 33750/55000, batch loss = 0.33\n",
      "epoch 2, step 34000/55000, batch loss = 0.33\n",
      "epoch 2, step 34250/55000, batch loss = 0.35\n",
      "epoch 2, step 34500/55000, batch loss = 0.37\n",
      "epoch 2, step 34750/55000, batch loss = 0.42\n",
      "epoch 2, step 35000/55000, batch loss = 0.34\n",
      "Train accuracy = 98.39\n",
      "epoch 2, step 35250/55000, batch loss = 0.33\n",
      "epoch 2, step 35500/55000, batch loss = 0.41\n",
      "epoch 2, step 35750/55000, batch loss = 0.33\n",
      "epoch 2, step 36000/55000, batch loss = 0.36\n",
      "epoch 2, step 36250/55000, batch loss = 0.33\n",
      "epoch 2, step 36500/55000, batch loss = 0.34\n",
      "epoch 2, step 36750/55000, batch loss = 0.37\n",
      "epoch 2, step 37000/55000, batch loss = 0.39\n",
      "epoch 2, step 37250/55000, batch loss = 0.34\n",
      "epoch 2, step 37500/55000, batch loss = 0.35\n",
      "Train accuracy = 98.40\n",
      "epoch 2, step 37750/55000, batch loss = 0.42\n",
      "epoch 2, step 38000/55000, batch loss = 0.37\n",
      "epoch 2, step 38250/55000, batch loss = 0.33\n",
      "epoch 2, step 38500/55000, batch loss = 0.38\n",
      "epoch 2, step 38750/55000, batch loss = 0.32\n",
      "epoch 2, step 39000/55000, batch loss = 0.32\n",
      "epoch 2, step 39250/55000, batch loss = 0.33\n",
      "epoch 2, step 39500/55000, batch loss = 0.35\n",
      "epoch 2, step 39750/55000, batch loss = 0.35\n",
      "epoch 2, step 40000/55000, batch loss = 0.48\n",
      "Train accuracy = 98.42\n",
      "epoch 2, step 40250/55000, batch loss = 0.33\n",
      "epoch 2, step 40500/55000, batch loss = 0.32\n",
      "epoch 2, step 40750/55000, batch loss = 0.47\n",
      "epoch 2, step 41000/55000, batch loss = 0.34\n",
      "epoch 2, step 41250/55000, batch loss = 0.36\n",
      "epoch 2, step 41500/55000, batch loss = 0.36\n",
      "epoch 2, step 41750/55000, batch loss = 0.35\n",
      "epoch 2, step 42000/55000, batch loss = 0.37\n",
      "epoch 2, step 42250/55000, batch loss = 0.32\n",
      "epoch 2, step 42500/55000, batch loss = 0.37\n",
      "Train accuracy = 98.43\n",
      "epoch 2, step 42750/55000, batch loss = 0.41\n",
      "epoch 2, step 43000/55000, batch loss = 0.32\n",
      "epoch 2, step 43250/55000, batch loss = 0.36\n",
      "epoch 2, step 43500/55000, batch loss = 0.32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, step 43750/55000, batch loss = 0.35\n",
      "epoch 2, step 44000/55000, batch loss = 0.34\n",
      "epoch 2, step 44250/55000, batch loss = 0.32\n",
      "epoch 2, step 44500/55000, batch loss = 0.38\n",
      "epoch 2, step 44750/55000, batch loss = 0.33\n",
      "epoch 2, step 45000/55000, batch loss = 0.43\n",
      "Train accuracy = 98.45\n",
      "epoch 2, step 45250/55000, batch loss = 0.34\n",
      "epoch 2, step 45500/55000, batch loss = 0.32\n",
      "epoch 2, step 45750/55000, batch loss = 0.32\n",
      "epoch 2, step 46000/55000, batch loss = 0.33\n",
      "epoch 2, step 46250/55000, batch loss = 0.42\n",
      "epoch 2, step 46500/55000, batch loss = 0.32\n",
      "epoch 2, step 46750/55000, batch loss = 0.31\n",
      "epoch 2, step 47000/55000, batch loss = 0.39\n",
      "epoch 2, step 47250/55000, batch loss = 0.40\n",
      "epoch 2, step 47500/55000, batch loss = 0.33\n",
      "Train accuracy = 98.46\n",
      "epoch 2, step 47750/55000, batch loss = 0.40\n",
      "epoch 2, step 48000/55000, batch loss = 0.31\n",
      "epoch 2, step 48250/55000, batch loss = 0.32\n",
      "epoch 2, step 48500/55000, batch loss = 0.43\n",
      "epoch 2, step 48750/55000, batch loss = 0.33\n",
      "epoch 2, step 49000/55000, batch loss = 0.34\n",
      "epoch 2, step 49250/55000, batch loss = 0.37\n",
      "epoch 2, step 49500/55000, batch loss = 0.33\n",
      "epoch 2, step 49750/55000, batch loss = 0.39\n",
      "epoch 2, step 50000/55000, batch loss = 0.39\n",
      "Train accuracy = 98.47\n",
      "epoch 2, step 50250/55000, batch loss = 0.32\n",
      "epoch 2, step 50500/55000, batch loss = 0.34\n",
      "epoch 2, step 50750/55000, batch loss = 0.35\n",
      "epoch 2, step 51000/55000, batch loss = 0.39\n",
      "epoch 2, step 51250/55000, batch loss = 0.33\n",
      "epoch 2, step 51500/55000, batch loss = 0.35\n",
      "epoch 2, step 51750/55000, batch loss = 0.32\n",
      "epoch 2, step 52000/55000, batch loss = 0.39\n",
      "epoch 2, step 52250/55000, batch loss = 0.32\n",
      "epoch 2, step 52500/55000, batch loss = 0.32\n",
      "Train accuracy = 98.46\n",
      "epoch 2, step 52750/55000, batch loss = 0.34\n",
      "epoch 2, step 53000/55000, batch loss = 0.38\n",
      "epoch 2, step 53250/55000, batch loss = 0.31\n",
      "epoch 2, step 53500/55000, batch loss = 0.35\n",
      "epoch 2, step 53750/55000, batch loss = 0.33\n",
      "epoch 2, step 54000/55000, batch loss = 0.33\n",
      "epoch 2, step 54250/55000, batch loss = 0.31\n",
      "epoch 2, step 54500/55000, batch loss = 0.34\n",
      "epoch 2, step 54750/55000, batch loss = 0.44\n",
      "Train accuracy = 98.47\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 98.56\n",
      "Validation avg loss = 0.35\n",
      "\n",
      "epoch 3, step 0/55000, batch loss = 0.38\n",
      "epoch 3, step 250/55000, batch loss = 0.31\n",
      "epoch 3, step 500/55000, batch loss = 0.37\n",
      "epoch 3, step 750/55000, batch loss = 0.33\n",
      "epoch 3, step 1000/55000, batch loss = 0.31\n",
      "epoch 3, step 1250/55000, batch loss = 0.30\n",
      "epoch 3, step 1500/55000, batch loss = 0.30\n",
      "epoch 3, step 1750/55000, batch loss = 0.30\n",
      "epoch 3, step 2000/55000, batch loss = 0.35\n",
      "epoch 3, step 2250/55000, batch loss = 0.33\n",
      "epoch 3, step 2500/55000, batch loss = 0.31\n",
      "Train accuracy = 99.10\n",
      "epoch 3, step 2750/55000, batch loss = 0.30\n",
      "epoch 3, step 3000/55000, batch loss = 0.30\n",
      "epoch 3, step 3250/55000, batch loss = 0.31\n",
      "epoch 3, step 3500/55000, batch loss = 0.33\n",
      "epoch 3, step 3750/55000, batch loss = 0.33\n",
      "epoch 3, step 4000/55000, batch loss = 0.31\n",
      "epoch 3, step 4250/55000, batch loss = 0.33\n",
      "epoch 3, step 4500/55000, batch loss = 0.31\n",
      "epoch 3, step 4750/55000, batch loss = 0.30\n",
      "epoch 3, step 5000/55000, batch loss = 0.31\n",
      "Train accuracy = 99.23\n",
      "epoch 3, step 5250/55000, batch loss = 0.42\n",
      "epoch 3, step 5500/55000, batch loss = 0.32\n",
      "epoch 3, step 5750/55000, batch loss = 0.32\n",
      "epoch 3, step 6000/55000, batch loss = 0.43\n",
      "epoch 3, step 6250/55000, batch loss = 0.30\n",
      "epoch 3, step 6500/55000, batch loss = 0.30\n",
      "epoch 3, step 6750/55000, batch loss = 0.31\n",
      "epoch 3, step 7000/55000, batch loss = 0.31\n",
      "epoch 3, step 7250/55000, batch loss = 0.33\n",
      "epoch 3, step 7500/55000, batch loss = 0.31\n",
      "Train accuracy = 99.14\n",
      "epoch 3, step 7750/55000, batch loss = 0.30\n",
      "epoch 3, step 8000/55000, batch loss = 0.31\n",
      "epoch 3, step 8250/55000, batch loss = 0.31\n",
      "epoch 3, step 8500/55000, batch loss = 0.33\n",
      "epoch 3, step 8750/55000, batch loss = 0.32\n",
      "epoch 3, step 9000/55000, batch loss = 0.31\n",
      "epoch 3, step 9250/55000, batch loss = 0.35\n",
      "epoch 3, step 9500/55000, batch loss = 0.41\n",
      "epoch 3, step 9750/55000, batch loss = 0.31\n",
      "epoch 3, step 10000/55000, batch loss = 0.32\n",
      "Train accuracy = 99.21\n",
      "epoch 3, step 10250/55000, batch loss = 0.40\n",
      "epoch 3, step 10500/55000, batch loss = 0.32\n",
      "epoch 3, step 10750/55000, batch loss = 0.32\n",
      "epoch 3, step 11000/55000, batch loss = 0.33\n",
      "epoch 3, step 11250/55000, batch loss = 0.31\n",
      "epoch 3, step 11500/55000, batch loss = 0.32\n",
      "epoch 3, step 11750/55000, batch loss = 0.30\n",
      "epoch 3, step 12000/55000, batch loss = 0.32\n",
      "epoch 3, step 12250/55000, batch loss = 0.40\n",
      "epoch 3, step 12500/55000, batch loss = 0.50\n",
      "Train accuracy = 99.24\n",
      "epoch 3, step 12750/55000, batch loss = 0.30\n",
      "epoch 3, step 13000/55000, batch loss = 0.30\n",
      "epoch 3, step 13250/55000, batch loss = 0.30\n",
      "epoch 3, step 13500/55000, batch loss = 0.31\n",
      "epoch 3, step 13750/55000, batch loss = 0.30\n",
      "epoch 3, step 14000/55000, batch loss = 0.36\n",
      "epoch 3, step 14250/55000, batch loss = 0.32\n",
      "epoch 3, step 14500/55000, batch loss = 0.31\n",
      "epoch 3, step 14750/55000, batch loss = 0.30\n",
      "epoch 3, step 15000/55000, batch loss = 0.31\n",
      "Train accuracy = 99.28\n",
      "epoch 3, step 15250/55000, batch loss = 0.32\n",
      "epoch 3, step 15500/55000, batch loss = 0.31\n",
      "epoch 3, step 15750/55000, batch loss = 0.36\n",
      "epoch 3, step 16000/55000, batch loss = 0.31\n",
      "epoch 3, step 16250/55000, batch loss = 0.30\n",
      "epoch 3, step 16500/55000, batch loss = 0.38\n",
      "epoch 3, step 16750/55000, batch loss = 0.32\n",
      "epoch 3, step 17000/55000, batch loss = 0.31\n",
      "epoch 3, step 17250/55000, batch loss = 0.33\n",
      "epoch 3, step 17500/55000, batch loss = 0.34\n",
      "Train accuracy = 99.26\n",
      "epoch 3, step 17750/55000, batch loss = 0.30\n",
      "epoch 3, step 18000/55000, batch loss = 0.32\n",
      "epoch 3, step 18250/55000, batch loss = 0.30\n",
      "epoch 3, step 18500/55000, batch loss = 0.37\n",
      "epoch 3, step 18750/55000, batch loss = 0.30\n",
      "epoch 3, step 19000/55000, batch loss = 0.31\n",
      "epoch 3, step 19250/55000, batch loss = 0.32\n",
      "epoch 3, step 19500/55000, batch loss = 0.31\n",
      "epoch 3, step 19750/55000, batch loss = 0.31\n",
      "epoch 3, step 20000/55000, batch loss = 0.32\n",
      "Train accuracy = 99.28\n",
      "epoch 3, step 20250/55000, batch loss = 0.31\n",
      "epoch 3, step 20500/55000, batch loss = 0.33\n",
      "epoch 3, step 20750/55000, batch loss = 0.32\n",
      "epoch 3, step 21000/55000, batch loss = 0.31\n",
      "epoch 3, step 21250/55000, batch loss = 0.35\n",
      "epoch 3, step 21500/55000, batch loss = 0.30\n",
      "epoch 3, step 21750/55000, batch loss = 0.34\n",
      "epoch 3, step 22000/55000, batch loss = 0.30\n",
      "epoch 3, step 22250/55000, batch loss = 0.38\n",
      "epoch 3, step 22500/55000, batch loss = 0.31\n",
      "Train accuracy = 99.30\n",
      "epoch 3, step 22750/55000, batch loss = 0.31\n",
      "epoch 3, step 23000/55000, batch loss = 0.30\n",
      "epoch 3, step 23250/55000, batch loss = 0.30\n",
      "epoch 3, step 23500/55000, batch loss = 0.32\n",
      "epoch 3, step 23750/55000, batch loss = 0.35\n",
      "epoch 3, step 24000/55000, batch loss = 0.31\n",
      "epoch 3, step 24250/55000, batch loss = 0.35\n",
      "epoch 3, step 24500/55000, batch loss = 0.30\n",
      "epoch 3, step 24750/55000, batch loss = 0.30\n",
      "epoch 3, step 25000/55000, batch loss = 0.36\n",
      "Train accuracy = 99.26\n",
      "epoch 3, step 25250/55000, batch loss = 0.32\n",
      "epoch 3, step 25500/55000, batch loss = 0.33\n",
      "epoch 3, step 25750/55000, batch loss = 0.30\n",
      "epoch 3, step 26000/55000, batch loss = 0.31\n",
      "epoch 3, step 26250/55000, batch loss = 0.44\n",
      "epoch 3, step 26500/55000, batch loss = 0.31\n",
      "epoch 3, step 26750/55000, batch loss = 0.31\n",
      "epoch 3, step 27000/55000, batch loss = 0.30\n",
      "epoch 3, step 27250/55000, batch loss = 0.30\n",
      "epoch 3, step 27500/55000, batch loss = 0.30\n",
      "Train accuracy = 99.25\n",
      "epoch 3, step 27750/55000, batch loss = 0.31\n",
      "epoch 3, step 28000/55000, batch loss = 0.32\n",
      "epoch 3, step 28250/55000, batch loss = 0.35\n",
      "epoch 3, step 28500/55000, batch loss = 0.33\n",
      "epoch 3, step 28750/55000, batch loss = 0.32\n",
      "epoch 3, step 29000/55000, batch loss = 0.30\n",
      "epoch 3, step 29250/55000, batch loss = 0.30\n",
      "epoch 3, step 29500/55000, batch loss = 0.34\n",
      "epoch 3, step 29750/55000, batch loss = 0.30\n",
      "epoch 3, step 30000/55000, batch loss = 0.30\n",
      "Train accuracy = 99.24\n",
      "epoch 3, step 30250/55000, batch loss = 0.34\n",
      "epoch 3, step 30500/55000, batch loss = 0.32\n",
      "epoch 3, step 30750/55000, batch loss = 0.33\n",
      "epoch 3, step 31000/55000, batch loss = 0.30\n",
      "epoch 3, step 31250/55000, batch loss = 0.30\n",
      "epoch 3, step 31500/55000, batch loss = 0.33\n",
      "epoch 3, step 31750/55000, batch loss = 0.30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, step 32000/55000, batch loss = 0.31\n",
      "epoch 3, step 32250/55000, batch loss = 0.30\n",
      "epoch 3, step 32500/55000, batch loss = 0.32\n",
      "Train accuracy = 99.24\n",
      "epoch 3, step 32750/55000, batch loss = 0.33\n",
      "epoch 3, step 33000/55000, batch loss = 0.31\n",
      "epoch 3, step 33250/55000, batch loss = 0.31\n",
      "epoch 3, step 33500/55000, batch loss = 0.31\n",
      "epoch 3, step 33750/55000, batch loss = 0.31\n",
      "epoch 3, step 34000/55000, batch loss = 0.30\n",
      "epoch 3, step 34250/55000, batch loss = 0.37\n",
      "epoch 3, step 34500/55000, batch loss = 0.32\n",
      "epoch 3, step 34750/55000, batch loss = 0.30\n",
      "epoch 3, step 35000/55000, batch loss = 0.32\n",
      "Train accuracy = 99.24\n",
      "epoch 3, step 35250/55000, batch loss = 0.32\n",
      "epoch 3, step 35500/55000, batch loss = 0.30\n",
      "epoch 3, step 35750/55000, batch loss = 0.30\n",
      "epoch 3, step 36000/55000, batch loss = 0.33\n",
      "epoch 3, step 36250/55000, batch loss = 0.32\n",
      "epoch 3, step 36500/55000, batch loss = 0.35\n",
      "epoch 3, step 36750/55000, batch loss = 0.30\n",
      "epoch 3, step 37000/55000, batch loss = 0.33\n",
      "epoch 3, step 37250/55000, batch loss = 0.40\n",
      "epoch 3, step 37500/55000, batch loss = 0.31\n",
      "Train accuracy = 99.23\n",
      "epoch 3, step 37750/55000, batch loss = 0.33\n",
      "epoch 3, step 38000/55000, batch loss = 0.30\n",
      "epoch 3, step 38250/55000, batch loss = 0.38\n",
      "epoch 3, step 38500/55000, batch loss = 0.30\n",
      "epoch 3, step 38750/55000, batch loss = 0.32\n",
      "epoch 3, step 39000/55000, batch loss = 0.31\n",
      "epoch 3, step 39250/55000, batch loss = 0.37\n",
      "epoch 3, step 39500/55000, batch loss = 0.30\n",
      "epoch 3, step 39750/55000, batch loss = 0.33\n",
      "epoch 3, step 40000/55000, batch loss = 0.35\n",
      "Train accuracy = 99.22\n",
      "epoch 3, step 40250/55000, batch loss = 0.30\n",
      "epoch 3, step 40500/55000, batch loss = 0.33\n",
      "epoch 3, step 40750/55000, batch loss = 0.30\n",
      "epoch 3, step 41000/55000, batch loss = 0.30\n",
      "epoch 3, step 41250/55000, batch loss = 0.30\n",
      "epoch 3, step 41500/55000, batch loss = 0.30\n",
      "epoch 3, step 41750/55000, batch loss = 0.30\n",
      "epoch 3, step 42000/55000, batch loss = 0.31\n",
      "epoch 3, step 42250/55000, batch loss = 0.38\n",
      "epoch 3, step 42500/55000, batch loss = 0.35\n",
      "Train accuracy = 99.22\n",
      "epoch 3, step 42750/55000, batch loss = 0.30\n",
      "epoch 3, step 43000/55000, batch loss = 0.32\n",
      "epoch 3, step 43250/55000, batch loss = 0.31\n",
      "epoch 3, step 43500/55000, batch loss = 0.31\n",
      "epoch 3, step 43750/55000, batch loss = 0.32\n",
      "epoch 3, step 44000/55000, batch loss = 0.30\n",
      "epoch 3, step 44250/55000, batch loss = 0.32\n",
      "epoch 3, step 44500/55000, batch loss = 0.30\n",
      "epoch 3, step 44750/55000, batch loss = 0.32\n",
      "epoch 3, step 45000/55000, batch loss = 0.31\n",
      "Train accuracy = 99.24\n",
      "epoch 3, step 45250/55000, batch loss = 0.30\n",
      "epoch 3, step 45500/55000, batch loss = 0.35\n",
      "epoch 3, step 45750/55000, batch loss = 0.30\n",
      "epoch 3, step 46000/55000, batch loss = 0.30\n",
      "epoch 3, step 46250/55000, batch loss = 0.31\n",
      "epoch 3, step 46500/55000, batch loss = 0.31\n",
      "epoch 3, step 46750/55000, batch loss = 0.30\n",
      "epoch 3, step 47000/55000, batch loss = 0.30\n",
      "epoch 3, step 47250/55000, batch loss = 0.33\n",
      "epoch 3, step 47500/55000, batch loss = 0.30\n",
      "Train accuracy = 99.25\n",
      "epoch 3, step 47750/55000, batch loss = 0.32\n",
      "epoch 3, step 48000/55000, batch loss = 0.49\n",
      "epoch 3, step 48250/55000, batch loss = 0.33\n",
      "epoch 3, step 48500/55000, batch loss = 0.30\n",
      "epoch 3, step 48750/55000, batch loss = 0.32\n",
      "epoch 3, step 49000/55000, batch loss = 0.30\n",
      "epoch 3, step 49250/55000, batch loss = 0.30\n",
      "epoch 3, step 49500/55000, batch loss = 0.36\n",
      "epoch 3, step 49750/55000, batch loss = 0.33\n",
      "epoch 3, step 50000/55000, batch loss = 0.30\n",
      "Train accuracy = 99.26\n",
      "epoch 3, step 50250/55000, batch loss = 0.44\n",
      "epoch 3, step 50500/55000, batch loss = 0.31\n",
      "epoch 3, step 50750/55000, batch loss = 0.30\n",
      "epoch 3, step 51000/55000, batch loss = 0.31\n",
      "epoch 3, step 51250/55000, batch loss = 0.30\n",
      "epoch 3, step 51500/55000, batch loss = 0.30\n",
      "epoch 3, step 51750/55000, batch loss = 0.30\n",
      "epoch 3, step 52000/55000, batch loss = 0.41\n",
      "epoch 3, step 52250/55000, batch loss = 0.30\n",
      "epoch 3, step 52500/55000, batch loss = 0.30\n",
      "Train accuracy = 99.26\n",
      "epoch 3, step 52750/55000, batch loss = 0.46\n",
      "epoch 3, step 53000/55000, batch loss = 0.30\n",
      "epoch 3, step 53250/55000, batch loss = 0.31\n",
      "epoch 3, step 53500/55000, batch loss = 0.30\n",
      "epoch 3, step 53750/55000, batch loss = 0.34\n",
      "epoch 3, step 54000/55000, batch loss = 0.35\n",
      "epoch 3, step 54250/55000, batch loss = 0.32\n",
      "epoch 3, step 54500/55000, batch loss = 0.31\n",
      "epoch 3, step 54750/55000, batch loss = 0.31\n",
      "Train accuracy = 99.26\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 99.08\n",
      "Validation avg loss = 0.33\n",
      "\n",
      "epoch 4, step 0/55000, batch loss = 0.34\n",
      "epoch 4, step 250/55000, batch loss = 0.32\n",
      "epoch 4, step 500/55000, batch loss = 0.32\n",
      "epoch 4, step 750/55000, batch loss = 0.29\n",
      "epoch 4, step 1000/55000, batch loss = 0.30\n",
      "epoch 4, step 1250/55000, batch loss = 0.34\n",
      "epoch 4, step 1500/55000, batch loss = 0.30\n",
      "epoch 4, step 1750/55000, batch loss = 0.30\n",
      "epoch 4, step 2000/55000, batch loss = 0.29\n",
      "epoch 4, step 2250/55000, batch loss = 0.30\n",
      "epoch 4, step 2500/55000, batch loss = 0.32\n",
      "Train accuracy = 99.73\n",
      "epoch 4, step 2750/55000, batch loss = 0.29\n",
      "epoch 4, step 3000/55000, batch loss = 0.30\n",
      "epoch 4, step 3250/55000, batch loss = 0.32\n",
      "epoch 4, step 3500/55000, batch loss = 0.31\n",
      "epoch 4, step 3750/55000, batch loss = 0.30\n",
      "epoch 4, step 4000/55000, batch loss = 0.30\n",
      "epoch 4, step 4250/55000, batch loss = 0.30\n",
      "epoch 4, step 4500/55000, batch loss = 0.31\n",
      "epoch 4, step 4750/55000, batch loss = 0.30\n",
      "epoch 4, step 5000/55000, batch loss = 0.41\n",
      "Train accuracy = 99.47\n",
      "epoch 4, step 5250/55000, batch loss = 0.32\n",
      "epoch 4, step 5500/55000, batch loss = 0.31\n",
      "epoch 4, step 5750/55000, batch loss = 0.31\n",
      "epoch 4, step 6000/55000, batch loss = 0.30\n",
      "epoch 4, step 6250/55000, batch loss = 0.41\n",
      "epoch 4, step 6500/55000, batch loss = 0.30\n",
      "epoch 4, step 6750/55000, batch loss = 0.29\n",
      "epoch 4, step 7000/55000, batch loss = 0.30\n",
      "epoch 4, step 7250/55000, batch loss = 0.29\n",
      "epoch 4, step 7500/55000, batch loss = 0.29\n",
      "Train accuracy = 99.40\n",
      "epoch 4, step 7750/55000, batch loss = 0.33\n",
      "epoch 4, step 8000/55000, batch loss = 0.33\n",
      "epoch 4, step 8250/55000, batch loss = 0.30\n",
      "epoch 4, step 8500/55000, batch loss = 0.33\n",
      "epoch 4, step 8750/55000, batch loss = 0.30\n",
      "epoch 4, step 9000/55000, batch loss = 0.30\n",
      "epoch 4, step 9250/55000, batch loss = 0.29\n",
      "epoch 4, step 9500/55000, batch loss = 0.30\n",
      "epoch 4, step 9750/55000, batch loss = 0.31\n",
      "epoch 4, step 10000/55000, batch loss = 0.30\n",
      "Train accuracy = 99.42\n",
      "epoch 4, step 10250/55000, batch loss = 0.29\n",
      "epoch 4, step 10500/55000, batch loss = 0.30\n",
      "epoch 4, step 10750/55000, batch loss = 0.31\n",
      "epoch 4, step 11000/55000, batch loss = 0.30\n",
      "epoch 4, step 11250/55000, batch loss = 0.30\n",
      "epoch 4, step 11500/55000, batch loss = 0.33\n",
      "epoch 4, step 11750/55000, batch loss = 0.31\n",
      "epoch 4, step 12000/55000, batch loss = 0.30\n",
      "epoch 4, step 12250/55000, batch loss = 0.29\n",
      "epoch 4, step 12500/55000, batch loss = 0.29\n",
      "Train accuracy = 99.45\n",
      "epoch 4, step 12750/55000, batch loss = 0.30\n",
      "epoch 4, step 13000/55000, batch loss = 0.32\n",
      "epoch 4, step 13250/55000, batch loss = 0.32\n",
      "epoch 4, step 13500/55000, batch loss = 0.30\n",
      "epoch 4, step 13750/55000, batch loss = 0.31\n",
      "epoch 4, step 14000/55000, batch loss = 0.32\n",
      "epoch 4, step 14250/55000, batch loss = 0.32\n",
      "epoch 4, step 14500/55000, batch loss = 0.32\n",
      "epoch 4, step 14750/55000, batch loss = 0.31\n",
      "epoch 4, step 15000/55000, batch loss = 0.30\n",
      "Train accuracy = 99.44\n",
      "epoch 4, step 15250/55000, batch loss = 0.30\n",
      "epoch 4, step 15500/55000, batch loss = 0.35\n",
      "epoch 4, step 15750/55000, batch loss = 0.31\n",
      "epoch 4, step 16000/55000, batch loss = 0.29\n",
      "epoch 4, step 16250/55000, batch loss = 0.33\n",
      "epoch 4, step 16500/55000, batch loss = 0.29\n",
      "epoch 4, step 16750/55000, batch loss = 0.32\n",
      "epoch 4, step 17000/55000, batch loss = 0.39\n",
      "epoch 4, step 17250/55000, batch loss = 0.31\n",
      "epoch 4, step 17500/55000, batch loss = 0.30\n",
      "Train accuracy = 99.38\n",
      "epoch 4, step 17750/55000, batch loss = 0.30\n",
      "epoch 4, step 18000/55000, batch loss = 0.30\n",
      "epoch 4, step 18250/55000, batch loss = 0.33\n",
      "epoch 4, step 18500/55000, batch loss = 0.31\n",
      "epoch 4, step 18750/55000, batch loss = 0.30\n",
      "epoch 4, step 19000/55000, batch loss = 0.30\n",
      "epoch 4, step 19250/55000, batch loss = 0.31\n",
      "epoch 4, step 19500/55000, batch loss = 0.29\n",
      "epoch 4, step 19750/55000, batch loss = 0.31\n",
      "epoch 4, step 20000/55000, batch loss = 0.33\n",
      "Train accuracy = 99.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, step 20250/55000, batch loss = 0.29\n",
      "epoch 4, step 20500/55000, batch loss = 0.33\n",
      "epoch 4, step 20750/55000, batch loss = 0.30\n",
      "epoch 4, step 21000/55000, batch loss = 0.30\n",
      "epoch 4, step 21250/55000, batch loss = 0.30\n",
      "epoch 4, step 21500/55000, batch loss = 0.31\n",
      "epoch 4, step 21750/55000, batch loss = 0.31\n",
      "epoch 4, step 22000/55000, batch loss = 0.30\n",
      "epoch 4, step 22250/55000, batch loss = 0.34\n",
      "epoch 4, step 22500/55000, batch loss = 0.29\n",
      "Train accuracy = 99.33\n",
      "epoch 4, step 22750/55000, batch loss = 0.30\n",
      "epoch 4, step 23000/55000, batch loss = 0.31\n",
      "epoch 4, step 23250/55000, batch loss = 0.33\n",
      "epoch 4, step 23500/55000, batch loss = 0.31\n",
      "epoch 4, step 23750/55000, batch loss = 0.31\n",
      "epoch 4, step 24000/55000, batch loss = 0.29\n",
      "epoch 4, step 24250/55000, batch loss = 0.30\n",
      "epoch 4, step 24500/55000, batch loss = 0.30\n",
      "epoch 4, step 24750/55000, batch loss = 0.30\n",
      "epoch 4, step 25000/55000, batch loss = 0.30\n",
      "Train accuracy = 99.35\n",
      "epoch 4, step 25250/55000, batch loss = 0.29\n",
      "epoch 4, step 25500/55000, batch loss = 0.32\n",
      "epoch 4, step 25750/55000, batch loss = 0.29\n",
      "epoch 4, step 26000/55000, batch loss = 0.37\n",
      "epoch 4, step 26250/55000, batch loss = 0.30\n",
      "epoch 4, step 26500/55000, batch loss = 0.30\n",
      "epoch 4, step 26750/55000, batch loss = 0.31\n",
      "epoch 4, step 27000/55000, batch loss = 0.30\n",
      "epoch 4, step 27250/55000, batch loss = 0.30\n",
      "epoch 4, step 27500/55000, batch loss = 0.29\n",
      "Train accuracy = 99.36\n",
      "epoch 4, step 27750/55000, batch loss = 0.30\n",
      "epoch 4, step 28000/55000, batch loss = 0.31\n",
      "epoch 4, step 28250/55000, batch loss = 0.30\n",
      "epoch 4, step 28500/55000, batch loss = 0.34\n",
      "epoch 4, step 28750/55000, batch loss = 0.32\n",
      "epoch 4, step 29000/55000, batch loss = 0.31\n",
      "epoch 4, step 29250/55000, batch loss = 0.34\n",
      "epoch 4, step 29500/55000, batch loss = 0.29\n",
      "epoch 4, step 29750/55000, batch loss = 0.30\n",
      "epoch 4, step 30000/55000, batch loss = 0.38\n",
      "Train accuracy = 99.35\n",
      "epoch 4, step 30250/55000, batch loss = 0.29\n",
      "epoch 4, step 30500/55000, batch loss = 0.30\n",
      "epoch 4, step 30750/55000, batch loss = 0.30\n",
      "epoch 4, step 31000/55000, batch loss = 0.40\n",
      "epoch 4, step 31250/55000, batch loss = 0.29\n",
      "epoch 4, step 31500/55000, batch loss = 0.32\n",
      "epoch 4, step 31750/55000, batch loss = 0.31\n",
      "epoch 4, step 32000/55000, batch loss = 0.30\n",
      "epoch 4, step 32250/55000, batch loss = 0.41\n",
      "epoch 4, step 32500/55000, batch loss = 0.30\n",
      "Train accuracy = 99.36\n",
      "epoch 4, step 32750/55000, batch loss = 0.29\n",
      "epoch 4, step 33000/55000, batch loss = 0.29\n",
      "epoch 4, step 33250/55000, batch loss = 0.30\n",
      "epoch 4, step 33500/55000, batch loss = 0.29\n",
      "epoch 4, step 33750/55000, batch loss = 0.37\n",
      "epoch 4, step 34000/55000, batch loss = 0.30\n",
      "epoch 4, step 34250/55000, batch loss = 0.29\n",
      "epoch 4, step 34500/55000, batch loss = 0.30\n",
      "epoch 4, step 34750/55000, batch loss = 0.32\n",
      "epoch 4, step 35000/55000, batch loss = 0.29\n",
      "Train accuracy = 99.37\n",
      "epoch 4, step 35250/55000, batch loss = 0.32\n",
      "epoch 4, step 35500/55000, batch loss = 0.30\n",
      "epoch 4, step 35750/55000, batch loss = 0.30\n",
      "epoch 4, step 36000/55000, batch loss = 0.35\n",
      "epoch 4, step 36250/55000, batch loss = 0.31\n",
      "epoch 4, step 36500/55000, batch loss = 0.30\n",
      "epoch 4, step 36750/55000, batch loss = 0.31\n",
      "epoch 4, step 37000/55000, batch loss = 0.30\n",
      "epoch 4, step 37250/55000, batch loss = 0.29\n",
      "epoch 4, step 37500/55000, batch loss = 0.29\n",
      "Train accuracy = 99.38\n",
      "epoch 4, step 37750/55000, batch loss = 0.40\n",
      "epoch 4, step 38000/55000, batch loss = 0.34\n",
      "epoch 4, step 38250/55000, batch loss = 0.29\n",
      "epoch 4, step 38500/55000, batch loss = 0.34\n",
      "epoch 4, step 38750/55000, batch loss = 0.30\n",
      "epoch 4, step 39000/55000, batch loss = 0.32\n",
      "epoch 4, step 39250/55000, batch loss = 0.30\n",
      "epoch 4, step 39500/55000, batch loss = 0.32\n",
      "epoch 4, step 39750/55000, batch loss = 0.30\n",
      "epoch 4, step 40000/55000, batch loss = 0.30\n",
      "Train accuracy = 99.37\n",
      "epoch 4, step 40250/55000, batch loss = 0.32\n",
      "epoch 4, step 40500/55000, batch loss = 0.31\n",
      "epoch 4, step 40750/55000, batch loss = 0.29\n",
      "epoch 4, step 41000/55000, batch loss = 0.30\n",
      "epoch 4, step 41250/55000, batch loss = 0.30\n",
      "epoch 4, step 41500/55000, batch loss = 0.30\n",
      "epoch 4, step 41750/55000, batch loss = 0.29\n",
      "epoch 4, step 42000/55000, batch loss = 0.31\n",
      "epoch 4, step 42250/55000, batch loss = 0.31\n",
      "epoch 4, step 42500/55000, batch loss = 0.33\n",
      "Train accuracy = 99.37\n",
      "epoch 4, step 42750/55000, batch loss = 0.29\n",
      "epoch 4, step 43000/55000, batch loss = 0.40\n",
      "epoch 4, step 43250/55000, batch loss = 0.30\n",
      "epoch 4, step 43500/55000, batch loss = 0.30\n",
      "epoch 4, step 43750/55000, batch loss = 0.30\n",
      "epoch 4, step 44000/55000, batch loss = 0.30\n",
      "epoch 4, step 44250/55000, batch loss = 0.30\n",
      "epoch 4, step 44500/55000, batch loss = 0.29\n",
      "epoch 4, step 44750/55000, batch loss = 0.29\n",
      "epoch 4, step 45000/55000, batch loss = 0.30\n",
      "Train accuracy = 99.38\n",
      "epoch 4, step 45250/55000, batch loss = 0.29\n",
      "epoch 4, step 45500/55000, batch loss = 0.29\n",
      "epoch 4, step 45750/55000, batch loss = 0.30\n",
      "epoch 4, step 46000/55000, batch loss = 0.29\n",
      "epoch 4, step 46250/55000, batch loss = 0.29\n",
      "epoch 4, step 46500/55000, batch loss = 0.29\n",
      "epoch 4, step 46750/55000, batch loss = 0.30\n",
      "epoch 4, step 47000/55000, batch loss = 0.33\n",
      "epoch 4, step 47250/55000, batch loss = 0.29\n",
      "epoch 4, step 47500/55000, batch loss = 0.29\n",
      "Train accuracy = 99.38\n",
      "epoch 4, step 47750/55000, batch loss = 0.30\n",
      "epoch 4, step 48000/55000, batch loss = 0.30\n",
      "epoch 4, step 48250/55000, batch loss = 0.29\n",
      "epoch 4, step 48500/55000, batch loss = 0.34\n",
      "epoch 4, step 48750/55000, batch loss = 0.29\n",
      "epoch 4, step 49000/55000, batch loss = 0.30\n",
      "epoch 4, step 49250/55000, batch loss = 0.30\n",
      "epoch 4, step 49500/55000, batch loss = 0.29\n",
      "epoch 4, step 49750/55000, batch loss = 0.31\n",
      "epoch 4, step 50000/55000, batch loss = 0.30\n",
      "Train accuracy = 99.38\n",
      "epoch 4, step 50250/55000, batch loss = 0.31\n",
      "epoch 4, step 50500/55000, batch loss = 0.29\n",
      "epoch 4, step 50750/55000, batch loss = 0.29\n",
      "epoch 4, step 51000/55000, batch loss = 0.29\n",
      "epoch 4, step 51250/55000, batch loss = 0.31\n",
      "epoch 4, step 51500/55000, batch loss = 0.30\n",
      "epoch 4, step 51750/55000, batch loss = 0.29\n",
      "epoch 4, step 52000/55000, batch loss = 0.31\n",
      "epoch 4, step 52250/55000, batch loss = 0.29\n",
      "epoch 4, step 52500/55000, batch loss = 0.29\n",
      "Train accuracy = 99.39\n",
      "epoch 4, step 52750/55000, batch loss = 0.34\n",
      "epoch 4, step 53000/55000, batch loss = 0.30\n",
      "epoch 4, step 53250/55000, batch loss = 0.32\n",
      "epoch 4, step 53500/55000, batch loss = 0.30\n",
      "epoch 4, step 53750/55000, batch loss = 0.29\n",
      "epoch 4, step 54000/55000, batch loss = 0.29\n",
      "epoch 4, step 54250/55000, batch loss = 0.31\n",
      "epoch 4, step 54500/55000, batch loss = 0.29\n",
      "epoch 4, step 54750/55000, batch loss = 0.29\n",
      "Train accuracy = 99.38\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 99.04\n",
      "Validation avg loss = 0.32\n",
      "\n",
      "epoch 5, step 0/55000, batch loss = 0.33\n",
      "epoch 5, step 250/55000, batch loss = 0.29\n",
      "epoch 5, step 500/55000, batch loss = 0.31\n",
      "epoch 5, step 750/55000, batch loss = 0.33\n",
      "epoch 5, step 1000/55000, batch loss = 0.47\n",
      "epoch 5, step 1250/55000, batch loss = 0.30\n",
      "epoch 5, step 1500/55000, batch loss = 0.39\n",
      "epoch 5, step 1750/55000, batch loss = 0.30\n",
      "epoch 5, step 2000/55000, batch loss = 0.30\n",
      "epoch 5, step 2250/55000, batch loss = 0.29\n",
      "epoch 5, step 2500/55000, batch loss = 0.29\n",
      "Train accuracy = 99.41\n",
      "epoch 5, step 2750/55000, batch loss = 0.33\n",
      "epoch 5, step 3000/55000, batch loss = 0.31\n",
      "epoch 5, step 3250/55000, batch loss = 0.36\n",
      "epoch 5, step 3500/55000, batch loss = 0.36\n",
      "epoch 5, step 3750/55000, batch loss = 0.30\n",
      "epoch 5, step 4000/55000, batch loss = 0.30\n",
      "epoch 5, step 4250/55000, batch loss = 0.29\n",
      "epoch 5, step 4500/55000, batch loss = 0.29\n",
      "epoch 5, step 4750/55000, batch loss = 0.39\n",
      "epoch 5, step 5000/55000, batch loss = 0.29\n",
      "Train accuracy = 99.49\n",
      "epoch 5, step 5250/55000, batch loss = 0.29\n",
      "epoch 5, step 5500/55000, batch loss = 0.30\n",
      "epoch 5, step 5750/55000, batch loss = 0.29\n",
      "epoch 5, step 6000/55000, batch loss = 0.36\n",
      "epoch 5, step 6250/55000, batch loss = 0.30\n",
      "epoch 5, step 6500/55000, batch loss = 0.36\n",
      "epoch 5, step 6750/55000, batch loss = 0.30\n",
      "epoch 5, step 7000/55000, batch loss = 0.29\n",
      "epoch 5, step 7250/55000, batch loss = 0.29\n",
      "epoch 5, step 7500/55000, batch loss = 0.30\n",
      "Train accuracy = 99.46\n",
      "epoch 5, step 7750/55000, batch loss = 0.29\n",
      "epoch 5, step 8000/55000, batch loss = 0.29\n",
      "epoch 5, step 8250/55000, batch loss = 0.29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, step 8500/55000, batch loss = 0.37\n",
      "epoch 5, step 8750/55000, batch loss = 0.29\n",
      "epoch 5, step 9000/55000, batch loss = 0.29\n",
      "epoch 5, step 9250/55000, batch loss = 0.29\n",
      "epoch 5, step 9500/55000, batch loss = 0.30\n",
      "epoch 5, step 9750/55000, batch loss = 0.30\n",
      "epoch 5, step 10000/55000, batch loss = 0.42\n",
      "Train accuracy = 99.46\n",
      "epoch 5, step 10250/55000, batch loss = 0.33\n",
      "epoch 5, step 10500/55000, batch loss = 0.30\n",
      "epoch 5, step 10750/55000, batch loss = 0.30\n",
      "epoch 5, step 11000/55000, batch loss = 0.32\n",
      "epoch 5, step 11250/55000, batch loss = 0.29\n",
      "epoch 5, step 11500/55000, batch loss = 0.39\n",
      "epoch 5, step 11750/55000, batch loss = 0.29\n",
      "epoch 5, step 12000/55000, batch loss = 0.29\n",
      "epoch 5, step 12250/55000, batch loss = 0.29\n",
      "epoch 5, step 12500/55000, batch loss = 0.30\n",
      "Train accuracy = 99.47\n",
      "epoch 5, step 12750/55000, batch loss = 0.31\n",
      "epoch 5, step 13000/55000, batch loss = 0.60\n",
      "epoch 5, step 13250/55000, batch loss = 0.29\n",
      "epoch 5, step 13500/55000, batch loss = 0.29\n",
      "epoch 5, step 13750/55000, batch loss = 0.32\n",
      "epoch 5, step 14000/55000, batch loss = 0.30\n",
      "epoch 5, step 14250/55000, batch loss = 0.31\n",
      "epoch 5, step 14500/55000, batch loss = 0.29\n",
      "epoch 5, step 14750/55000, batch loss = 0.31\n",
      "epoch 5, step 15000/55000, batch loss = 0.30\n",
      "Train accuracy = 99.46\n",
      "epoch 5, step 15250/55000, batch loss = 0.31\n",
      "epoch 5, step 15500/55000, batch loss = 0.29\n",
      "epoch 5, step 15750/55000, batch loss = 0.30\n",
      "epoch 5, step 16000/55000, batch loss = 0.30\n",
      "epoch 5, step 16250/55000, batch loss = 0.31\n",
      "epoch 5, step 16500/55000, batch loss = 0.29\n",
      "epoch 5, step 16750/55000, batch loss = 0.29\n",
      "epoch 5, step 17000/55000, batch loss = 0.30\n",
      "epoch 5, step 17250/55000, batch loss = 0.29\n",
      "epoch 5, step 17500/55000, batch loss = 0.29\n",
      "Train accuracy = 99.46\n",
      "epoch 5, step 17750/55000, batch loss = 0.29\n",
      "epoch 5, step 18000/55000, batch loss = 0.30\n",
      "epoch 5, step 18250/55000, batch loss = 0.30\n",
      "epoch 5, step 18500/55000, batch loss = 0.31\n",
      "epoch 5, step 18750/55000, batch loss = 0.29\n",
      "epoch 5, step 19000/55000, batch loss = 0.30\n",
      "epoch 5, step 19250/55000, batch loss = 0.31\n",
      "epoch 5, step 19500/55000, batch loss = 0.30\n",
      "epoch 5, step 19750/55000, batch loss = 0.29\n",
      "epoch 5, step 20000/55000, batch loss = 0.33\n",
      "Train accuracy = 99.46\n",
      "epoch 5, step 20250/55000, batch loss = 0.31\n",
      "epoch 5, step 20500/55000, batch loss = 0.32\n",
      "epoch 5, step 20750/55000, batch loss = 0.29\n",
      "epoch 5, step 21000/55000, batch loss = 0.30\n",
      "epoch 5, step 21250/55000, batch loss = 0.30\n",
      "epoch 5, step 21500/55000, batch loss = 0.30\n",
      "epoch 5, step 21750/55000, batch loss = 0.31\n",
      "epoch 5, step 22000/55000, batch loss = 0.32\n",
      "epoch 5, step 22250/55000, batch loss = 0.30\n",
      "epoch 5, step 22500/55000, batch loss = 0.29\n",
      "Train accuracy = 99.49\n",
      "epoch 5, step 22750/55000, batch loss = 0.32\n",
      "epoch 5, step 23000/55000, batch loss = 0.30\n",
      "epoch 5, step 23250/55000, batch loss = 0.29\n",
      "epoch 5, step 23500/55000, batch loss = 0.34\n",
      "epoch 5, step 23750/55000, batch loss = 0.32\n",
      "epoch 5, step 24000/55000, batch loss = 0.34\n",
      "epoch 5, step 24250/55000, batch loss = 0.29\n",
      "epoch 5, step 24500/55000, batch loss = 0.30\n",
      "epoch 5, step 24750/55000, batch loss = 0.29\n",
      "epoch 5, step 25000/55000, batch loss = 0.36\n",
      "Train accuracy = 99.45\n",
      "epoch 5, step 25250/55000, batch loss = 0.29\n",
      "epoch 5, step 25500/55000, batch loss = 0.33\n",
      "epoch 5, step 25750/55000, batch loss = 0.30\n",
      "epoch 5, step 26000/55000, batch loss = 0.29\n",
      "epoch 5, step 26250/55000, batch loss = 0.31\n",
      "epoch 5, step 26500/55000, batch loss = 0.31\n",
      "epoch 5, step 26750/55000, batch loss = 0.33\n",
      "epoch 5, step 27000/55000, batch loss = 0.29\n",
      "epoch 5, step 27250/55000, batch loss = 0.29\n",
      "epoch 5, step 27500/55000, batch loss = 0.29\n",
      "Train accuracy = 99.44\n",
      "epoch 5, step 27750/55000, batch loss = 0.29\n",
      "epoch 5, step 28000/55000, batch loss = 0.30\n",
      "epoch 5, step 28250/55000, batch loss = 0.29\n",
      "epoch 5, step 28500/55000, batch loss = 0.34\n",
      "epoch 5, step 28750/55000, batch loss = 0.30\n",
      "epoch 5, step 29000/55000, batch loss = 0.34\n",
      "epoch 5, step 29250/55000, batch loss = 0.31\n",
      "epoch 5, step 29500/55000, batch loss = 0.32\n",
      "epoch 5, step 29750/55000, batch loss = 0.44\n",
      "epoch 5, step 30000/55000, batch loss = 0.30\n",
      "Train accuracy = 99.45\n",
      "epoch 5, step 30250/55000, batch loss = 0.29\n",
      "epoch 5, step 30500/55000, batch loss = 0.31\n",
      "epoch 5, step 30750/55000, batch loss = 0.29\n",
      "epoch 5, step 31000/55000, batch loss = 0.31\n",
      "epoch 5, step 31250/55000, batch loss = 0.30\n",
      "epoch 5, step 31500/55000, batch loss = 0.36\n",
      "epoch 5, step 31750/55000, batch loss = 0.30\n",
      "epoch 5, step 32000/55000, batch loss = 0.31\n",
      "epoch 5, step 32250/55000, batch loss = 0.31\n",
      "epoch 5, step 32500/55000, batch loss = 0.29\n",
      "Train accuracy = 99.44\n",
      "epoch 5, step 32750/55000, batch loss = 0.29\n",
      "epoch 5, step 33000/55000, batch loss = 0.30\n",
      "epoch 5, step 33250/55000, batch loss = 0.34\n",
      "epoch 5, step 33500/55000, batch loss = 0.29\n",
      "epoch 5, step 33750/55000, batch loss = 0.29\n",
      "epoch 5, step 34000/55000, batch loss = 0.31\n",
      "epoch 5, step 34250/55000, batch loss = 0.35\n",
      "epoch 5, step 34500/55000, batch loss = 0.31\n",
      "epoch 5, step 34750/55000, batch loss = 0.29\n",
      "epoch 5, step 35000/55000, batch loss = 0.35\n",
      "Train accuracy = 99.44\n",
      "epoch 5, step 35250/55000, batch loss = 0.29\n",
      "epoch 5, step 35500/55000, batch loss = 0.29\n",
      "epoch 5, step 35750/55000, batch loss = 0.29\n",
      "epoch 5, step 36000/55000, batch loss = 0.35\n",
      "epoch 5, step 36250/55000, batch loss = 0.32\n",
      "epoch 5, step 36500/55000, batch loss = 0.30\n",
      "epoch 5, step 36750/55000, batch loss = 0.30\n",
      "epoch 5, step 37000/55000, batch loss = 0.29\n",
      "epoch 5, step 37250/55000, batch loss = 0.30\n",
      "epoch 5, step 37500/55000, batch loss = 0.29\n",
      "Train accuracy = 99.44\n",
      "epoch 5, step 37750/55000, batch loss = 0.29\n",
      "epoch 5, step 38000/55000, batch loss = 0.31\n",
      "epoch 5, step 38250/55000, batch loss = 0.29\n",
      "epoch 5, step 38500/55000, batch loss = 0.30\n",
      "epoch 5, step 38750/55000, batch loss = 0.29\n",
      "epoch 5, step 39000/55000, batch loss = 0.29\n",
      "epoch 5, step 39250/55000, batch loss = 0.30\n",
      "epoch 5, step 39500/55000, batch loss = 0.29\n",
      "epoch 5, step 39750/55000, batch loss = 0.34\n",
      "epoch 5, step 40000/55000, batch loss = 0.29\n",
      "Train accuracy = 99.44\n",
      "epoch 5, step 40250/55000, batch loss = 0.29\n",
      "epoch 5, step 40500/55000, batch loss = 0.30\n",
      "epoch 5, step 40750/55000, batch loss = 0.30\n",
      "epoch 5, step 41000/55000, batch loss = 0.29\n",
      "epoch 5, step 41250/55000, batch loss = 0.30\n",
      "epoch 5, step 41500/55000, batch loss = 0.29\n",
      "epoch 5, step 41750/55000, batch loss = 0.31\n",
      "epoch 5, step 42000/55000, batch loss = 0.30\n",
      "epoch 5, step 42250/55000, batch loss = 0.29\n",
      "epoch 5, step 42500/55000, batch loss = 0.31\n",
      "Train accuracy = 99.44\n",
      "epoch 5, step 42750/55000, batch loss = 0.31\n",
      "epoch 5, step 43000/55000, batch loss = 0.29\n",
      "epoch 5, step 43250/55000, batch loss = 0.30\n",
      "epoch 5, step 43500/55000, batch loss = 0.30\n",
      "epoch 5, step 43750/55000, batch loss = 0.36\n",
      "epoch 5, step 44000/55000, batch loss = 0.29\n",
      "epoch 5, step 44250/55000, batch loss = 0.37\n",
      "epoch 5, step 44500/55000, batch loss = 0.32\n",
      "epoch 5, step 44750/55000, batch loss = 0.29\n",
      "epoch 5, step 45000/55000, batch loss = 0.29\n",
      "Train accuracy = 99.43\n",
      "epoch 5, step 45250/55000, batch loss = 0.32\n",
      "epoch 5, step 45500/55000, batch loss = 0.31\n",
      "epoch 5, step 45750/55000, batch loss = 0.33\n",
      "epoch 5, step 46000/55000, batch loss = 0.30\n",
      "epoch 5, step 46250/55000, batch loss = 0.31\n",
      "epoch 5, step 46500/55000, batch loss = 0.31\n",
      "epoch 5, step 46750/55000, batch loss = 0.32\n",
      "epoch 5, step 47000/55000, batch loss = 0.31\n",
      "epoch 5, step 47250/55000, batch loss = 0.29\n",
      "epoch 5, step 47500/55000, batch loss = 0.29\n",
      "Train accuracy = 99.44\n",
      "epoch 5, step 47750/55000, batch loss = 0.29\n",
      "epoch 5, step 48000/55000, batch loss = 0.29\n",
      "epoch 5, step 48250/55000, batch loss = 0.30\n",
      "epoch 5, step 48500/55000, batch loss = 0.29\n",
      "epoch 5, step 48750/55000, batch loss = 0.30\n",
      "epoch 5, step 49000/55000, batch loss = 0.42\n",
      "epoch 5, step 49250/55000, batch loss = 0.29\n",
      "epoch 5, step 49500/55000, batch loss = 0.29\n",
      "epoch 5, step 49750/55000, batch loss = 0.32\n",
      "epoch 5, step 50000/55000, batch loss = 0.30\n",
      "Train accuracy = 99.45\n",
      "epoch 5, step 50250/55000, batch loss = 0.29\n",
      "epoch 5, step 50500/55000, batch loss = 0.31\n",
      "epoch 5, step 50750/55000, batch loss = 0.29\n",
      "epoch 5, step 51000/55000, batch loss = 0.33\n",
      "epoch 5, step 51250/55000, batch loss = 0.30\n",
      "epoch 5, step 51500/55000, batch loss = 0.30\n",
      "epoch 5, step 51750/55000, batch loss = 0.31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, step 52000/55000, batch loss = 0.30\n",
      "epoch 5, step 52250/55000, batch loss = 0.29\n",
      "epoch 5, step 52500/55000, batch loss = 0.32\n",
      "Train accuracy = 99.43\n",
      "epoch 5, step 52750/55000, batch loss = 0.30\n",
      "epoch 5, step 53000/55000, batch loss = 0.29\n",
      "epoch 5, step 53250/55000, batch loss = 0.30\n",
      "epoch 5, step 53500/55000, batch loss = 0.29\n",
      "epoch 5, step 53750/55000, batch loss = 0.36\n",
      "epoch 5, step 54000/55000, batch loss = 0.29\n",
      "epoch 5, step 54250/55000, batch loss = 0.35\n",
      "epoch 5, step 54500/55000, batch loss = 0.31\n",
      "epoch 5, step 54750/55000, batch loss = 0.31\n",
      "Train accuracy = 99.43\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 99.16\n",
      "Validation avg loss = 0.32\n",
      "\n",
      "\n",
      "Running evaluation:  Test\n",
      "Test accuracy = 99.09\n",
      "Test avg loss = 0.31\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SAVE_DIR = \"/Users/goran/Documents/III_Semestar/DU/labosi/2_lab/zad2_images/\"\n",
    "\n",
    "\n",
    "config = {}\n",
    "config['max_epochs'] = 5\n",
    "config['batch_size'] = 50\n",
    "config['save_dir'] = SAVE_DIR\n",
    "config['lr_policy'] = {1:{'lr':1e-1}, 3:{'lr':1e-2}, 5:{'lr':1e-3}, 7:{'lr':1e-4}}\n",
    "\n",
    "i = 0\n",
    "names = [\"0_1\", \"0_01\", \"0_001\"]\n",
    "for lmbd in [1e-1, 1e-2, 1e-3]:\n",
    "    print(\"LAMBDA:\", lmbd, \"\\n\")\n",
    "    SAVE_DIR = \"/Users/goran/Documents/III_Semestar/DU/labosi/2_lab/zad2_images/\" + names[i] + \"/\"\n",
    "    i += 1\n",
    "    config['save_dir'] = SAVE_DIR\n",
    "    np.random.seed(int(time.time() * 1e6) % 2**31)\n",
    "\n",
    "    train_x = mnist.train.images\n",
    "    train_x = train_x.reshape([-1, 1, 28, 28])\n",
    "    train_y = mnist.train.labels\n",
    "\n",
    "    valid_x = mnist.validation.images\n",
    "    valid_x = valid_x.reshape([-1, 1, 28, 28])\n",
    "    valid_y = mnist.validation.labels\n",
    "\n",
    "    test_x = mnist.test.images\n",
    "    test_x = test_x.reshape([-1, 1, 28, 28])\n",
    "    test_y = mnist.test.labels\n",
    "\n",
    "    train_mean = train_x.mean()\n",
    "    train_x -= train_mean\n",
    "    valid_x -= train_mean\n",
    "    test_x -= train_mean\n",
    "\n",
    "    weight_decay = lmbd\n",
    "    net = []\n",
    "\n",
    "    regularizers = []\n",
    "    inputs = np.random.randn(config['batch_size'], 1, 28, 28)\n",
    "    net += [Convolution(inputs, 16, 5, \"conv1\")]\n",
    "    regularizers += [L2Regularizer(net[-1].weights, weight_decay, 'conv1_l2reg')]\n",
    "    net += [MaxPooling(net[-1], \"pool1\")]\n",
    "    net += [ReLU(net[-1], \"relu1\")]\n",
    "    net += [Convolution(net[-1], 32, 5, \"conv2\")]\n",
    "    regularizers += [L2Regularizer(net[-1].weights, weight_decay, 'conv2_l2reg')]\n",
    "    net += [MaxPooling(net[-1], \"pool2\")]\n",
    "    net += [ReLU(net[-1], \"relu2\")]\n",
    "    ## 7x7\n",
    "    net += [Flatten(net[-1], \"flatten3\")]\n",
    "    net += [FC(net[-1], 512, \"fc3\")]\n",
    "    regularizers += [L2Regularizer(net[-1].weights, weight_decay, 'fc3_l2reg')]\n",
    "    net += [ReLU(net[-1], \"relu3\")]\n",
    "    net += [FC(net[-1], 10, \"logits\")]\n",
    "\n",
    "    data_loss = SoftmaxCrossEntropyWithLogits()\n",
    "    loss = RegularizedLoss(data_loss, regularizers)\n",
    "\n",
    "    train(train_x, train_y, valid_x, valid_y, net, loss, config)\n",
    "    evaluate(\"Test\", test_x, test_y, net, loss, config)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ZADATAK - usporedba s Tensorflowom\n",
    "\n",
    "U Tensorflowu definirajte i nauƒçite model koji je ekvivalentan regulariziranom modelu iz 2. zadatka. Korisite identiƒçnu arhitekturu i parametre uƒçenja da biste reproducirali rezultate. Tijekom uƒçenja vizualizirajte filtre u prvom sloju kao u prethodnoj vje≈æbi. Kako biste u graf dodali operaciju konvolucije koristite tf.nn.conv2d ili tf.contrib.layers.convolution2d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mnist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-30ea1aff88eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1e6\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m31\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mtrain_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mtrain_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mnist' is not defined"
     ]
    }
   ],
   "source": [
    "SAVE_DIR = \"C:\\\\Users\\\\Korisnik\\\\Desktop\\\\du_lab2\\\\Deep-Learning\\\\2_lab\\\\zad3_images\\\\\"\n",
    "\n",
    "config = {}\n",
    "config['max_epochs'] = 8\n",
    "config['batch_size'] = 50\n",
    "config['save_dir'] = SAVE_DIR\n",
    "config['weight_decay'] = 1e-4\n",
    "config['lr_policy'] = {1:{'lr':1e-1}, 3:{'lr':1e-2}, 5:{'lr':1e-3}, 7:{'lr':1e-4}}\n",
    "\n",
    "np.random.seed(int(time.time() * 1e6) % 2**31)\n",
    "train_x = mnist.train.images\n",
    "train_x = train_x.reshape([-1, 28, 28, 1])\n",
    "train_y = mnist.train.labels\n",
    "\n",
    "valid_x = mnist.validation.images\n",
    "valid_x = valid_x.reshape([-1, 28, 28, 1])\n",
    "valid_y = mnist.validation.labels\n",
    "\n",
    "test_x = mnist.test.images\n",
    "test_x = test_x.reshape([-1, 28, 28, 1])\n",
    "test_y = mnist.test.labels\n",
    "\n",
    "train_mean = train_x.mean()\n",
    "train_x -= train_mean\n",
    "valid_x -= train_mean\n",
    "test_x -= train_mean\n",
    "\n",
    "weight_decay = config['weight_decay']\n",
    "\n",
    "n_input = 768\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l2_loss(weights):\n",
    "    regularization = 0;\n",
    "    for w in weights:\n",
    "        regularization += tf.nn.l2_loss(w)\n",
    "    return regularization\n",
    "        \n",
    "def conv2d(x, W, b, activation=tf.nn.relu, strides=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return activation(x)\n",
    "\n",
    "def maxpool2d(x, k=2, stride=2):\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, stride, stride, 1], padding='SAME')\n",
    "\n",
    "def fc(x, W, b, activation=None):\n",
    "    x = tf.reshape(x, [-1, W.get_shape().as_list()[0]])\n",
    "    if activation :\n",
    "        return activation(tf.matmul(x, W) +  b)    \n",
    "    return tf.matmul(x, W) +  b\n",
    "\n",
    "def init_var(shape, fin):\n",
    "    sigma = np.sqrt(2/fin)\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural net definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_classes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-93569f0b99f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;34m'fc3'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'w_fc3'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m512\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxavier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;34m'fc4'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'w_fc4'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxavier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m }\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'n_classes' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib.layers import xavier_initializer_conv2d as xavier_conv2d\n",
    "from tensorflow.contrib.layers import xavier_initializer as xavier\n",
    "tf.reset_default_graph()\n",
    "\n",
    "weights = {\n",
    "    'conv1': tf.get_variable('w_conv1', [5, 5, 1, 16], initializer=xavier_conv2d()),\n",
    "    'conv2': tf.get_variable('w_conv2', [5, 5, 16, 32], initializer=xavier_conv2d()),\n",
    "    \n",
    "    'fc3': tf.get_variable('w_fc3', [7*7*32, 512], initializer=xavier()),\n",
    "    'fc4': tf.get_variable('w_fc4', [512, n_classes], initializer=xavier())\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'conv1': tf.Variable(tf.zeros([16]), name='b_conv1'),\n",
    "    'conv2': tf.Variable(tf.zeros([32]), name='b_conv2'),\n",
    "    'fc3': tf.Variable(tf.zeros([512]), name='b_fc3'),\n",
    "    'fc4': tf.Variable(tf.zeros([n_classes]), name='b_fc4')\n",
    "}\n",
    "\n",
    "\n",
    "def convnet(x, weights, biases):\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "    net = conv2d(x, weights['conv1'], biases['conv1'], tf.nn.relu)\n",
    "    net = maxpool2d(net, k=2)\n",
    "    \n",
    "    net = conv2d(net, weights['conv2'], biases['conv2'], tf.nn.relu)\n",
    "    net = maxpool2d(net, k=2)\n",
    "    \n",
    "    net = fc(net, weights['fc3'],  biases['fc3'], tf.nn.relu)\n",
    "    net = fc(net, weights['fc4'],  biases['fc4'])\n",
    "    return net\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "Yoh_ = tf.placeholder(tf.float32, [None, n_classes])\n",
    "logits = convnet(X, weights, biases)\n",
    "\n",
    "# loss\n",
    "regularization = l2_loss([weights['conv1'], weights['conv2'], weights['fc3']])\n",
    "data_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Yoh_))\n",
    "loss = data_loss + weight_decay * regularization\n",
    "\n",
    "lr = tf.placeholder(tf.float32)\n",
    "train_step =  tf.train.GradientDescentOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_tf(session, train_x, train_y, valid_x, valid_y, config):\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    lr_policy = config['lr_policy']\n",
    "    batch_size = config['batch_size']\n",
    "    max_epochs = config['max_epochs']\n",
    "    save_dir = config['save_dir']\n",
    "    \n",
    "    num_examples = train_x.shape[0]\n",
    "    assert num_examples % batch_size == 0\n",
    "    num_batches = num_examples // batch_size\n",
    "    \n",
    "    for epoch in range(1, max_epochs+1):\n",
    "        if epoch in lr_policy:\n",
    "            solver_config = lr_policy[epoch]\n",
    "            \n",
    "        cnt_correct = 0\n",
    "\n",
    "        permutation_idx = np.random.permutation(num_examples)\n",
    "        train_x = train_x[permutation_idx]\n",
    "        train_y = train_y[permutation_idx]\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            # store mini-batch to ndarray\n",
    "            batch_x = train_x[i*batch_size:(i+1)*batch_size, :]\n",
    "            batch_y = train_y[i*batch_size:(i+1)*batch_size, :]\n",
    "               \n",
    "            data_dict = {X: batch_x, Yoh_: batch_y, lr:solver_config['lr']}\n",
    "            logits_val, loss_val, _ = session.run([logits, loss, train_step] ,feed_dict=data_dict)\n",
    "            \n",
    "            # compute classification accuracy\n",
    "            yp = np.argmax(logits_val, axis=1)\n",
    "            yt = np.argmax(batch_y, axis=1)\n",
    "            cnt_correct += (yp == yt).sum()\n",
    "            \n",
    "            if i % 5 == 0:\n",
    "                print(\"epoch %d, step %d/%d, batch loss = %.2f\" % (epoch, i*batch_size, num_examples, loss_val))\n",
    "            if i % 100 == 0:\n",
    "                w = session.run(weights['conv1'])\n",
    "                draw_conv_filters(epoch, i*batch_size, \"conv1\", w, save_dir)\n",
    "            if i > 0 and i % 50 == 0:\n",
    "                print(\"Train accuracy = %.2f\" % (cnt_correct / ((i+1)*batch_size) * 100))\n",
    "        \n",
    "        print(\"Train accuracy = %.2f\" % (cnt_correct / num_examples * 100))\n",
    "        evaluate_tf(session, \"Validation\", valid_x, valid_y, config)\n",
    "\n",
    "\n",
    "def evaluate_tf(session, name, x, y, config):\n",
    "    print(\"\\nRunning evaluation: \", name)\n",
    "    batch_size = config['batch_size']\n",
    "    num_examples = x.shape[0]\n",
    "    assert num_examples % batch_size == 0\n",
    "    num_batches = num_examples // batch_size\n",
    "    cnt_correct = 0\n",
    "    loss_avg = 0\n",
    "    for i in range(num_batches):\n",
    "        batch_x = x[i*batch_size:(i+1)*batch_size, :]\n",
    "        batch_y = y[i*batch_size:(i+1)*batch_size, :]\n",
    "        \n",
    "        data_dict = {X: batch_x, Yoh_: batch_y}\n",
    "        logits_val, loss_val = session.run([logits, loss] ,feed_dict=data_dict)\n",
    "    \n",
    "        yp = np.argmax(logits_val, axis=1)\n",
    "        yt = np.argmax(batch_y, axis=1)\n",
    "        cnt_correct += (yp == yt).sum()\n",
    "        \n",
    "        loss_avg += loss_val\n",
    "        \n",
    "    valid_acc = cnt_correct / num_examples * 100\n",
    "    loss_avg /= num_batches\n",
    "    print(name + \" accuracy = %.2f\" % valid_acc)\n",
    "    print(name + \" avg loss = %.2f\\n\" % loss_avg)\n",
    "    \n",
    "def draw_conv_filters(epoch, step, name, weights, save_dir):\n",
    "    # kxkxCxn_filters\n",
    "    k, k, C, num_filters = weights.shape\n",
    "\n",
    "    w = weights.copy().swapaxes(0, 3).swapaxes(1,2)\n",
    "    w = w.reshape(num_filters, C, k, k)\n",
    "    w -= w.min()\n",
    "    w /= w.max()\n",
    "\n",
    "    border = 1\n",
    "    cols = 8\n",
    "    rows = math.ceil(num_filters / cols)\n",
    "    width = cols * k + (cols-1) * border\n",
    "    height = rows * k + (rows-1) * border\n",
    "\n",
    "    for i in range(1):\n",
    "        img = np.zeros([height, width])\n",
    "        for j in range(num_filters):\n",
    "            r = int(j / cols) * (k + border)\n",
    "            c = int(j % cols) * (k + border)\n",
    "            img[r:r+k,c:c+k] = w[j,i]\n",
    "        filename = '%s_epoch_%02d_step_%06d_input_%03d.png' % (name, epoch, step, i)\n",
    "        ski.io.imsave(os.path.join(save_dir, filename), img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, step 0/55000, batch loss = 2.37\n",
      "epoch 1, step 250/55000, batch loss = 2.22\n",
      "epoch 1, step 500/55000, batch loss = 2.05\n",
      "epoch 1, step 750/55000, batch loss = 1.95\n",
      "epoch 1, step 1000/55000, batch loss = 1.52\n",
      "epoch 1, step 1250/55000, batch loss = 0.90\n",
      "epoch 1, step 1500/55000, batch loss = 1.32\n",
      "epoch 1, step 1750/55000, batch loss = 0.63\n",
      "epoch 1, step 2000/55000, batch loss = 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Korisnik\\Anaconda3\\lib\\site-packages\\skimage\\util\\dtype.py:122: UserWarning: Possible precision loss when converting from float64 to uint16\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, step 2250/55000, batch loss = 0.56\n",
      "epoch 1, step 2500/55000, batch loss = 0.47\n",
      "Train accuracy = 57.18\n",
      "epoch 1, step 2750/55000, batch loss = 0.77\n",
      "epoch 1, step 3000/55000, batch loss = 0.60\n",
      "epoch 1, step 3250/55000, batch loss = 0.38\n",
      "epoch 1, step 3500/55000, batch loss = 0.47\n",
      "epoch 1, step 3750/55000, batch loss = 0.58\n",
      "epoch 1, step 4000/55000, batch loss = 0.27\n",
      "epoch 1, step 4250/55000, batch loss = 0.49\n",
      "epoch 1, step 4500/55000, batch loss = 0.30\n",
      "epoch 1, step 4750/55000, batch loss = 0.36\n",
      "epoch 1, step 5000/55000, batch loss = 0.32\n",
      "Train accuracy = 72.34\n",
      "epoch 1, step 5250/55000, batch loss = 0.32\n",
      "epoch 1, step 5500/55000, batch loss = 0.50\n",
      "epoch 1, step 5750/55000, batch loss = 0.24\n",
      "epoch 1, step 6000/55000, batch loss = 0.34\n",
      "epoch 1, step 6250/55000, batch loss = 0.38\n",
      "epoch 1, step 6500/55000, batch loss = 0.24\n",
      "epoch 1, step 6750/55000, batch loss = 0.13\n",
      "epoch 1, step 7000/55000, batch loss = 0.24\n",
      "epoch 1, step 7250/55000, batch loss = 0.31\n",
      "epoch 1, step 7500/55000, batch loss = 0.37\n",
      "Train accuracy = 78.64\n",
      "epoch 1, step 7750/55000, batch loss = 0.41\n",
      "epoch 1, step 8000/55000, batch loss = 0.16\n",
      "epoch 1, step 8250/55000, batch loss = 0.22\n",
      "epoch 1, step 8500/55000, batch loss = 0.16\n",
      "epoch 1, step 8750/55000, batch loss = 0.24\n",
      "epoch 1, step 9000/55000, batch loss = 0.18\n",
      "epoch 1, step 9250/55000, batch loss = 0.14\n",
      "epoch 1, step 9500/55000, batch loss = 0.36\n",
      "epoch 1, step 9750/55000, batch loss = 0.22\n",
      "epoch 1, step 10000/55000, batch loss = 0.13\n",
      "Train accuracy = 82.45\n",
      "epoch 1, step 10250/55000, batch loss = 0.23\n",
      "epoch 1, step 10500/55000, batch loss = 0.36\n",
      "epoch 1, step 10750/55000, batch loss = 0.17\n",
      "epoch 1, step 11000/55000, batch loss = 0.16\n",
      "epoch 1, step 11250/55000, batch loss = 0.24\n",
      "epoch 1, step 11500/55000, batch loss = 0.18\n",
      "epoch 1, step 11750/55000, batch loss = 0.11\n",
      "epoch 1, step 12000/55000, batch loss = 0.25\n",
      "epoch 1, step 12250/55000, batch loss = 0.20\n",
      "epoch 1, step 12500/55000, batch loss = 0.22\n",
      "Train accuracy = 85.05\n",
      "epoch 1, step 12750/55000, batch loss = 0.29\n",
      "epoch 1, step 13000/55000, batch loss = 0.12\n",
      "epoch 1, step 13250/55000, batch loss = 0.17\n",
      "epoch 1, step 13500/55000, batch loss = 0.13\n",
      "epoch 1, step 13750/55000, batch loss = 0.10\n",
      "epoch 1, step 14000/55000, batch loss = 0.17\n",
      "epoch 1, step 14250/55000, batch loss = 0.09\n",
      "epoch 1, step 14500/55000, batch loss = 0.29\n",
      "epoch 1, step 14750/55000, batch loss = 0.15\n",
      "epoch 1, step 15000/55000, batch loss = 0.26\n",
      "Train accuracy = 86.71\n",
      "epoch 1, step 15250/55000, batch loss = 0.07\n",
      "epoch 1, step 15500/55000, batch loss = 0.25\n",
      "epoch 1, step 15750/55000, batch loss = 0.15\n",
      "epoch 1, step 16000/55000, batch loss = 0.14\n",
      "epoch 1, step 16250/55000, batch loss = 0.27\n",
      "epoch 1, step 16500/55000, batch loss = 0.13\n",
      "epoch 1, step 16750/55000, batch loss = 0.07\n",
      "epoch 1, step 17000/55000, batch loss = 0.09\n",
      "epoch 1, step 17250/55000, batch loss = 0.08\n",
      "epoch 1, step 17500/55000, batch loss = 0.24\n",
      "Train accuracy = 88.07\n",
      "epoch 1, step 17750/55000, batch loss = 0.27\n",
      "epoch 1, step 18000/55000, batch loss = 0.07\n",
      "epoch 1, step 18250/55000, batch loss = 0.17\n",
      "epoch 1, step 18500/55000, batch loss = 0.22\n",
      "epoch 1, step 18750/55000, batch loss = 0.11\n",
      "epoch 1, step 19000/55000, batch loss = 0.25\n",
      "epoch 1, step 19250/55000, batch loss = 0.06\n",
      "epoch 1, step 19500/55000, batch loss = 0.13\n",
      "epoch 1, step 19750/55000, batch loss = 0.26\n",
      "epoch 1, step 20000/55000, batch loss = 0.11\n",
      "Train accuracy = 89.05\n",
      "epoch 1, step 20250/55000, batch loss = 0.20\n",
      "epoch 1, step 20500/55000, batch loss = 0.22\n",
      "epoch 1, step 20750/55000, batch loss = 0.16\n",
      "epoch 1, step 21000/55000, batch loss = 0.22\n",
      "epoch 1, step 21250/55000, batch loss = 0.10\n",
      "epoch 1, step 21500/55000, batch loss = 0.09\n",
      "epoch 1, step 21750/55000, batch loss = 0.13\n",
      "epoch 1, step 22000/55000, batch loss = 0.14\n",
      "epoch 1, step 22250/55000, batch loss = 0.10\n",
      "epoch 1, step 22500/55000, batch loss = 0.16\n",
      "Train accuracy = 89.84\n",
      "epoch 1, step 22750/55000, batch loss = 0.17\n",
      "epoch 1, step 23000/55000, batch loss = 0.16\n",
      "epoch 1, step 23250/55000, batch loss = 0.29\n",
      "epoch 1, step 23500/55000, batch loss = 0.22\n",
      "epoch 1, step 23750/55000, batch loss = 0.09\n",
      "epoch 1, step 24000/55000, batch loss = 0.14\n",
      "epoch 1, step 24250/55000, batch loss = 0.12\n",
      "epoch 1, step 24500/55000, batch loss = 0.12\n",
      "epoch 1, step 24750/55000, batch loss = 0.09\n",
      "epoch 1, step 25000/55000, batch loss = 0.17\n",
      "Train accuracy = 90.51\n",
      "epoch 1, step 25250/55000, batch loss = 0.15\n",
      "epoch 1, step 25500/55000, batch loss = 0.06\n",
      "epoch 1, step 25750/55000, batch loss = 0.20\n",
      "epoch 1, step 26000/55000, batch loss = 0.23\n",
      "epoch 1, step 26250/55000, batch loss = 0.15\n",
      "epoch 1, step 26500/55000, batch loss = 0.40\n",
      "epoch 1, step 26750/55000, batch loss = 0.21\n",
      "epoch 1, step 27000/55000, batch loss = 0.05\n",
      "epoch 1, step 27250/55000, batch loss = 0.07\n",
      "epoch 1, step 27500/55000, batch loss = 0.13\n",
      "Train accuracy = 91.10\n",
      "epoch 1, step 27750/55000, batch loss = 0.30\n",
      "epoch 1, step 28000/55000, batch loss = 0.08\n",
      "epoch 1, step 28250/55000, batch loss = 0.22\n",
      "epoch 1, step 28500/55000, batch loss = 0.17\n",
      "epoch 1, step 28750/55000, batch loss = 0.16\n",
      "epoch 1, step 29000/55000, batch loss = 0.07\n",
      "epoch 1, step 29250/55000, batch loss = 0.06\n",
      "epoch 1, step 29500/55000, batch loss = 0.15\n",
      "epoch 1, step 29750/55000, batch loss = 0.17\n",
      "epoch 1, step 30000/55000, batch loss = 0.09\n",
      "Train accuracy = 91.59\n",
      "epoch 1, step 30250/55000, batch loss = 0.08\n",
      "epoch 1, step 30500/55000, batch loss = 0.12\n",
      "epoch 1, step 30750/55000, batch loss = 0.09\n",
      "epoch 1, step 31000/55000, batch loss = 0.26\n",
      "epoch 1, step 31250/55000, batch loss = 0.23\n",
      "epoch 1, step 31500/55000, batch loss = 0.14\n",
      "epoch 1, step 31750/55000, batch loss = 0.08\n",
      "epoch 1, step 32000/55000, batch loss = 0.13\n",
      "epoch 1, step 32250/55000, batch loss = 0.07\n",
      "epoch 1, step 32500/55000, batch loss = 0.22\n",
      "Train accuracy = 92.10\n",
      "epoch 1, step 32750/55000, batch loss = 0.07\n",
      "epoch 1, step 33000/55000, batch loss = 0.10\n",
      "epoch 1, step 33250/55000, batch loss = 0.13\n",
      "epoch 1, step 33500/55000, batch loss = 0.20\n",
      "epoch 1, step 33750/55000, batch loss = 0.12\n",
      "epoch 1, step 34000/55000, batch loss = 0.15\n",
      "epoch 1, step 34250/55000, batch loss = 0.12\n",
      "epoch 1, step 34500/55000, batch loss = 0.10\n",
      "epoch 1, step 34750/55000, batch loss = 0.15\n",
      "epoch 1, step 35000/55000, batch loss = 0.12\n",
      "Train accuracy = 92.45\n",
      "epoch 1, step 35250/55000, batch loss = 0.06\n",
      "epoch 1, step 35500/55000, batch loss = 0.06\n",
      "epoch 1, step 35750/55000, batch loss = 0.28\n",
      "epoch 1, step 36000/55000, batch loss = 0.09\n",
      "epoch 1, step 36250/55000, batch loss = 0.06\n",
      "epoch 1, step 36500/55000, batch loss = 0.08\n",
      "epoch 1, step 36750/55000, batch loss = 0.13\n",
      "epoch 1, step 37000/55000, batch loss = 0.09\n",
      "epoch 1, step 37250/55000, batch loss = 0.14\n",
      "epoch 1, step 37500/55000, batch loss = 0.07\n",
      "Train accuracy = 92.79\n",
      "epoch 1, step 37750/55000, batch loss = 0.07\n",
      "epoch 1, step 38000/55000, batch loss = 0.05\n",
      "epoch 1, step 38250/55000, batch loss = 0.08\n",
      "epoch 1, step 38500/55000, batch loss = 0.05\n",
      "epoch 1, step 38750/55000, batch loss = 0.24\n",
      "epoch 1, step 39000/55000, batch loss = 0.09\n",
      "epoch 1, step 39250/55000, batch loss = 0.06\n",
      "epoch 1, step 39500/55000, batch loss = 0.11\n",
      "epoch 1, step 39750/55000, batch loss = 0.07\n",
      "epoch 1, step 40000/55000, batch loss = 0.19\n",
      "Train accuracy = 93.08\n",
      "epoch 1, step 40250/55000, batch loss = 0.19\n",
      "epoch 1, step 40500/55000, batch loss = 0.13\n",
      "epoch 1, step 40750/55000, batch loss = 0.10\n",
      "epoch 1, step 41000/55000, batch loss = 0.12\n",
      "epoch 1, step 41250/55000, batch loss = 0.22\n",
      "epoch 1, step 41500/55000, batch loss = 0.24\n",
      "epoch 1, step 41750/55000, batch loss = 0.11\n",
      "epoch 1, step 42000/55000, batch loss = 0.08\n",
      "epoch 1, step 42250/55000, batch loss = 0.08\n",
      "epoch 1, step 42500/55000, batch loss = 0.05\n",
      "Train accuracy = 93.30\n",
      "epoch 1, step 42750/55000, batch loss = 0.12\n",
      "epoch 1, step 43000/55000, batch loss = 0.05\n",
      "epoch 1, step 43250/55000, batch loss = 0.10\n",
      "epoch 1, step 43500/55000, batch loss = 0.07\n",
      "epoch 1, step 43750/55000, batch loss = 0.07\n",
      "epoch 1, step 44000/55000, batch loss = 0.13\n",
      "epoch 1, step 44250/55000, batch loss = 0.07\n",
      "epoch 1, step 44500/55000, batch loss = 0.09\n",
      "epoch 1, step 44750/55000, batch loss = 0.05\n",
      "epoch 1, step 45000/55000, batch loss = 0.23\n",
      "Train accuracy = 93.54\n",
      "epoch 1, step 45250/55000, batch loss = 0.05\n",
      "epoch 1, step 45500/55000, batch loss = 0.09\n",
      "epoch 1, step 45750/55000, batch loss = 0.15\n",
      "epoch 1, step 46000/55000, batch loss = 0.15\n",
      "epoch 1, step 46250/55000, batch loss = 0.06\n",
      "epoch 1, step 46500/55000, batch loss = 0.15\n",
      "epoch 1, step 46750/55000, batch loss = 0.26\n",
      "epoch 1, step 47000/55000, batch loss = 0.20\n",
      "epoch 1, step 47250/55000, batch loss = 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, step 47500/55000, batch loss = 0.14\n",
      "Train accuracy = 93.73\n",
      "epoch 1, step 47750/55000, batch loss = 0.07\n",
      "epoch 1, step 48000/55000, batch loss = 0.10\n",
      "epoch 1, step 48250/55000, batch loss = 0.11\n",
      "epoch 1, step 48500/55000, batch loss = 0.14\n",
      "epoch 1, step 48750/55000, batch loss = 0.17\n",
      "epoch 1, step 49000/55000, batch loss = 0.06\n",
      "epoch 1, step 49250/55000, batch loss = 0.12\n",
      "epoch 1, step 49500/55000, batch loss = 0.15\n",
      "epoch 1, step 49750/55000, batch loss = 0.07\n",
      "epoch 1, step 50000/55000, batch loss = 0.29\n",
      "Train accuracy = 93.91\n",
      "epoch 1, step 50250/55000, batch loss = 0.13\n",
      "epoch 1, step 50500/55000, batch loss = 0.09\n",
      "epoch 1, step 50750/55000, batch loss = 0.16\n",
      "epoch 1, step 51000/55000, batch loss = 0.07\n",
      "epoch 1, step 51250/55000, batch loss = 0.07\n",
      "epoch 1, step 51500/55000, batch loss = 0.10\n",
      "epoch 1, step 51750/55000, batch loss = 0.09\n",
      "epoch 1, step 52000/55000, batch loss = 0.07\n",
      "epoch 1, step 52250/55000, batch loss = 0.15\n",
      "epoch 1, step 52500/55000, batch loss = 0.08\n",
      "Train accuracy = 94.11\n",
      "epoch 1, step 52750/55000, batch loss = 0.10\n",
      "epoch 1, step 53000/55000, batch loss = 0.18\n",
      "epoch 1, step 53250/55000, batch loss = 0.07\n",
      "epoch 1, step 53500/55000, batch loss = 0.05\n",
      "epoch 1, step 53750/55000, batch loss = 0.28\n",
      "epoch 1, step 54000/55000, batch loss = 0.11\n",
      "epoch 1, step 54250/55000, batch loss = 0.14\n",
      "epoch 1, step 54500/55000, batch loss = 0.05\n",
      "epoch 1, step 54750/55000, batch loss = 0.07\n",
      "Train accuracy = 94.25\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 98.40\n",
      "Validation avg loss = 0.10\n",
      "\n",
      "epoch 2, step 0/55000, batch loss = 0.04\n",
      "epoch 2, step 250/55000, batch loss = 0.08\n",
      "epoch 2, step 500/55000, batch loss = 0.10\n",
      "epoch 2, step 750/55000, batch loss = 0.06\n",
      "epoch 2, step 1000/55000, batch loss = 0.11\n",
      "epoch 2, step 1250/55000, batch loss = 0.18\n",
      "epoch 2, step 1500/55000, batch loss = 0.08\n",
      "epoch 2, step 1750/55000, batch loss = 0.06\n",
      "epoch 2, step 2000/55000, batch loss = 0.14\n",
      "epoch 2, step 2250/55000, batch loss = 0.09\n",
      "epoch 2, step 2500/55000, batch loss = 0.06\n",
      "Train accuracy = 98.20\n",
      "epoch 2, step 2750/55000, batch loss = 0.25\n",
      "epoch 2, step 3000/55000, batch loss = 0.07\n",
      "epoch 2, step 3250/55000, batch loss = 0.10\n",
      "epoch 2, step 3500/55000, batch loss = 0.05\n",
      "epoch 2, step 3750/55000, batch loss = 0.05\n",
      "epoch 2, step 4000/55000, batch loss = 0.13\n",
      "epoch 2, step 4250/55000, batch loss = 0.05\n",
      "epoch 2, step 4500/55000, batch loss = 0.10\n",
      "epoch 2, step 4750/55000, batch loss = 0.23\n",
      "epoch 2, step 5000/55000, batch loss = 0.11\n",
      "Train accuracy = 98.16\n",
      "epoch 2, step 5250/55000, batch loss = 0.14\n",
      "epoch 2, step 5500/55000, batch loss = 0.05\n",
      "epoch 2, step 5750/55000, batch loss = 0.05\n",
      "epoch 2, step 6000/55000, batch loss = 0.22\n",
      "epoch 2, step 6250/55000, batch loss = 0.16\n",
      "epoch 2, step 6500/55000, batch loss = 0.05\n",
      "epoch 2, step 6750/55000, batch loss = 0.05\n",
      "epoch 2, step 7000/55000, batch loss = 0.19\n",
      "epoch 2, step 7250/55000, batch loss = 0.05\n",
      "epoch 2, step 7500/55000, batch loss = 0.06\n",
      "Train accuracy = 98.19\n",
      "epoch 2, step 7750/55000, batch loss = 0.19\n",
      "epoch 2, step 8000/55000, batch loss = 0.05\n",
      "epoch 2, step 8250/55000, batch loss = 0.16\n",
      "epoch 2, step 8500/55000, batch loss = 0.09\n",
      "epoch 2, step 8750/55000, batch loss = 0.07\n",
      "epoch 2, step 9000/55000, batch loss = 0.06\n",
      "epoch 2, step 9250/55000, batch loss = 0.07\n",
      "epoch 2, step 9500/55000, batch loss = 0.06\n",
      "epoch 2, step 9750/55000, batch loss = 0.08\n",
      "epoch 2, step 10000/55000, batch loss = 0.10\n",
      "Train accuracy = 98.29\n",
      "epoch 2, step 10250/55000, batch loss = 0.06\n",
      "epoch 2, step 10500/55000, batch loss = 0.06\n",
      "epoch 2, step 10750/55000, batch loss = 0.14\n",
      "epoch 2, step 11000/55000, batch loss = 0.13\n",
      "epoch 2, step 11250/55000, batch loss = 0.14\n",
      "epoch 2, step 11500/55000, batch loss = 0.10\n",
      "epoch 2, step 11750/55000, batch loss = 0.09\n",
      "epoch 2, step 12000/55000, batch loss = 0.05\n",
      "epoch 2, step 12250/55000, batch loss = 0.06\n",
      "epoch 2, step 12500/55000, batch loss = 0.09\n",
      "Train accuracy = 98.30\n",
      "epoch 2, step 12750/55000, batch loss = 0.05\n",
      "epoch 2, step 13000/55000, batch loss = 0.21\n",
      "epoch 2, step 13250/55000, batch loss = 0.07\n",
      "epoch 2, step 13500/55000, batch loss = 0.17\n",
      "epoch 2, step 13750/55000, batch loss = 0.16\n",
      "epoch 2, step 14000/55000, batch loss = 0.06\n",
      "epoch 2, step 14250/55000, batch loss = 0.10\n",
      "epoch 2, step 14500/55000, batch loss = 0.05\n",
      "epoch 2, step 14750/55000, batch loss = 0.08\n",
      "epoch 2, step 15000/55000, batch loss = 0.05\n",
      "Train accuracy = 98.22\n",
      "epoch 2, step 15250/55000, batch loss = 0.05\n",
      "epoch 2, step 15500/55000, batch loss = 0.24\n",
      "epoch 2, step 15750/55000, batch loss = 0.14\n",
      "epoch 2, step 16000/55000, batch loss = 0.07\n",
      "epoch 2, step 16250/55000, batch loss = 0.06\n",
      "epoch 2, step 16500/55000, batch loss = 0.07\n",
      "epoch 2, step 16750/55000, batch loss = 0.17\n",
      "epoch 2, step 17000/55000, batch loss = 0.15\n",
      "epoch 2, step 17250/55000, batch loss = 0.12\n",
      "epoch 2, step 17500/55000, batch loss = 0.05\n",
      "Train accuracy = 98.16\n",
      "epoch 2, step 17750/55000, batch loss = 0.07\n",
      "epoch 2, step 18000/55000, batch loss = 0.06\n",
      "epoch 2, step 18250/55000, batch loss = 0.05\n",
      "epoch 2, step 18500/55000, batch loss = 0.08\n",
      "epoch 2, step 18750/55000, batch loss = 0.05\n",
      "epoch 2, step 19000/55000, batch loss = 0.06\n",
      "epoch 2, step 19250/55000, batch loss = 0.05\n",
      "epoch 2, step 19500/55000, batch loss = 0.07\n",
      "epoch 2, step 19750/55000, batch loss = 0.08\n",
      "epoch 2, step 20000/55000, batch loss = 0.06\n",
      "Train accuracy = 98.25\n",
      "epoch 2, step 20250/55000, batch loss = 0.14\n",
      "epoch 2, step 20500/55000, batch loss = 0.14\n",
      "epoch 2, step 20750/55000, batch loss = 0.08\n",
      "epoch 2, step 21000/55000, batch loss = 0.08\n",
      "epoch 2, step 21250/55000, batch loss = 0.13\n",
      "epoch 2, step 21500/55000, batch loss = 0.07\n",
      "epoch 2, step 21750/55000, batch loss = 0.07\n",
      "epoch 2, step 22000/55000, batch loss = 0.06\n",
      "epoch 2, step 22250/55000, batch loss = 0.05\n",
      "epoch 2, step 22500/55000, batch loss = 0.19\n",
      "Train accuracy = 98.29\n",
      "epoch 2, step 22750/55000, batch loss = 0.11\n",
      "epoch 2, step 23000/55000, batch loss = 0.05\n",
      "epoch 2, step 23250/55000, batch loss = 0.07\n",
      "epoch 2, step 23500/55000, batch loss = 0.16\n",
      "epoch 2, step 23750/55000, batch loss = 0.07\n",
      "epoch 2, step 24000/55000, batch loss = 0.14\n",
      "epoch 2, step 24250/55000, batch loss = 0.07\n",
      "epoch 2, step 24500/55000, batch loss = 0.16\n",
      "epoch 2, step 24750/55000, batch loss = 0.08\n",
      "epoch 2, step 25000/55000, batch loss = 0.19\n",
      "Train accuracy = 98.30\n",
      "epoch 2, step 25250/55000, batch loss = 0.05\n",
      "epoch 2, step 25500/55000, batch loss = 0.06\n",
      "epoch 2, step 25750/55000, batch loss = 0.07\n",
      "epoch 2, step 26000/55000, batch loss = 0.14\n",
      "epoch 2, step 26250/55000, batch loss = 0.10\n",
      "epoch 2, step 26500/55000, batch loss = 0.10\n",
      "epoch 2, step 26750/55000, batch loss = 0.13\n",
      "epoch 2, step 27000/55000, batch loss = 0.07\n",
      "epoch 2, step 27250/55000, batch loss = 0.07\n",
      "epoch 2, step 27500/55000, batch loss = 0.16\n",
      "Train accuracy = 98.26\n",
      "epoch 2, step 27750/55000, batch loss = 0.12\n",
      "epoch 2, step 28000/55000, batch loss = 0.09\n",
      "epoch 2, step 28250/55000, batch loss = 0.06\n",
      "epoch 2, step 28500/55000, batch loss = 0.13\n",
      "epoch 2, step 28750/55000, batch loss = 0.08\n",
      "epoch 2, step 29000/55000, batch loss = 0.17\n",
      "epoch 2, step 29250/55000, batch loss = 0.07\n",
      "epoch 2, step 29500/55000, batch loss = 0.09\n",
      "epoch 2, step 29750/55000, batch loss = 0.08\n",
      "epoch 2, step 30000/55000, batch loss = 0.10\n",
      "Train accuracy = 98.28\n",
      "epoch 2, step 30250/55000, batch loss = 0.27\n",
      "epoch 2, step 30500/55000, batch loss = 0.17\n",
      "epoch 2, step 30750/55000, batch loss = 0.17\n",
      "epoch 2, step 31000/55000, batch loss = 0.08\n",
      "epoch 2, step 31250/55000, batch loss = 0.21\n",
      "epoch 2, step 31500/55000, batch loss = 0.07\n",
      "epoch 2, step 31750/55000, batch loss = 0.09\n",
      "epoch 2, step 32000/55000, batch loss = 0.12\n",
      "epoch 2, step 32250/55000, batch loss = 0.13\n",
      "epoch 2, step 32500/55000, batch loss = 0.24\n",
      "Train accuracy = 98.27\n",
      "epoch 2, step 32750/55000, batch loss = 0.09\n",
      "epoch 2, step 33000/55000, batch loss = 0.08\n",
      "epoch 2, step 33250/55000, batch loss = 0.13\n",
      "epoch 2, step 33500/55000, batch loss = 0.07\n",
      "epoch 2, step 33750/55000, batch loss = 0.06\n",
      "epoch 2, step 34000/55000, batch loss = 0.08\n",
      "epoch 2, step 34250/55000, batch loss = 0.08\n",
      "epoch 2, step 34500/55000, batch loss = 0.06\n",
      "epoch 2, step 34750/55000, batch loss = 0.10\n",
      "epoch 2, step 35000/55000, batch loss = 0.05\n",
      "Train accuracy = 98.31\n",
      "epoch 2, step 35250/55000, batch loss = 0.07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, step 35500/55000, batch loss = 0.07\n",
      "epoch 2, step 35750/55000, batch loss = 0.16\n",
      "epoch 2, step 36000/55000, batch loss = 0.09\n",
      "epoch 2, step 36250/55000, batch loss = 0.06\n",
      "epoch 2, step 36500/55000, batch loss = 0.14\n",
      "epoch 2, step 36750/55000, batch loss = 0.14\n",
      "epoch 2, step 37000/55000, batch loss = 0.05\n",
      "epoch 2, step 37250/55000, batch loss = 0.05\n",
      "epoch 2, step 37500/55000, batch loss = 0.05\n",
      "Train accuracy = 98.31\n",
      "epoch 2, step 37750/55000, batch loss = 0.07\n",
      "epoch 2, step 38000/55000, batch loss = 0.14\n",
      "epoch 2, step 38250/55000, batch loss = 0.25\n",
      "epoch 2, step 38500/55000, batch loss = 0.10\n",
      "epoch 2, step 38750/55000, batch loss = 0.07\n",
      "epoch 2, step 39000/55000, batch loss = 0.07\n",
      "epoch 2, step 39250/55000, batch loss = 0.09\n",
      "epoch 2, step 39500/55000, batch loss = 0.22\n",
      "epoch 2, step 39750/55000, batch loss = 0.05\n",
      "epoch 2, step 40000/55000, batch loss = 0.08\n",
      "Train accuracy = 98.31\n",
      "epoch 2, step 40250/55000, batch loss = 0.06\n",
      "epoch 2, step 40500/55000, batch loss = 0.13\n",
      "epoch 2, step 40750/55000, batch loss = 0.05\n",
      "epoch 2, step 41000/55000, batch loss = 0.09\n",
      "epoch 2, step 41250/55000, batch loss = 0.09\n",
      "epoch 2, step 41500/55000, batch loss = 0.13\n",
      "epoch 2, step 41750/55000, batch loss = 0.05\n",
      "epoch 2, step 42000/55000, batch loss = 0.14\n",
      "epoch 2, step 42250/55000, batch loss = 0.08\n",
      "epoch 2, step 42500/55000, batch loss = 0.07\n",
      "Train accuracy = 98.33\n",
      "epoch 2, step 42750/55000, batch loss = 0.07\n",
      "epoch 2, step 43000/55000, batch loss = 0.11\n",
      "epoch 2, step 43250/55000, batch loss = 0.16\n",
      "epoch 2, step 43500/55000, batch loss = 0.07\n",
      "epoch 2, step 43750/55000, batch loss = 0.05\n",
      "epoch 2, step 44000/55000, batch loss = 0.05\n",
      "epoch 2, step 44250/55000, batch loss = 0.07\n",
      "epoch 2, step 44500/55000, batch loss = 0.08\n",
      "epoch 2, step 44750/55000, batch loss = 0.07\n",
      "epoch 2, step 45000/55000, batch loss = 0.07\n",
      "Train accuracy = 98.34\n",
      "epoch 2, step 45250/55000, batch loss = 0.08\n",
      "epoch 2, step 45500/55000, batch loss = 0.11\n",
      "epoch 2, step 45750/55000, batch loss = 0.09\n",
      "epoch 2, step 46000/55000, batch loss = 0.04\n",
      "epoch 2, step 46250/55000, batch loss = 0.07\n",
      "epoch 2, step 46500/55000, batch loss = 0.06\n",
      "epoch 2, step 46750/55000, batch loss = 0.14\n",
      "epoch 2, step 47000/55000, batch loss = 0.06\n",
      "epoch 2, step 47250/55000, batch loss = 0.21\n",
      "epoch 2, step 47500/55000, batch loss = 0.04\n",
      "Train accuracy = 98.32\n",
      "epoch 2, step 47750/55000, batch loss = 0.12\n",
      "epoch 2, step 48000/55000, batch loss = 0.04\n",
      "epoch 2, step 48250/55000, batch loss = 0.05\n",
      "epoch 2, step 48500/55000, batch loss = 0.06\n",
      "epoch 2, step 48750/55000, batch loss = 0.08\n",
      "epoch 2, step 49000/55000, batch loss = 0.07\n",
      "epoch 2, step 49250/55000, batch loss = 0.05\n",
      "epoch 2, step 49500/55000, batch loss = 0.07\n",
      "epoch 2, step 49750/55000, batch loss = 0.11\n",
      "epoch 2, step 50000/55000, batch loss = 0.06\n",
      "Train accuracy = 98.34\n",
      "epoch 2, step 50250/55000, batch loss = 0.11\n",
      "epoch 2, step 50500/55000, batch loss = 0.09\n",
      "epoch 2, step 50750/55000, batch loss = 0.13\n",
      "epoch 2, step 51000/55000, batch loss = 0.09\n",
      "epoch 2, step 51250/55000, batch loss = 0.05\n",
      "epoch 2, step 51500/55000, batch loss = 0.06\n",
      "epoch 2, step 51750/55000, batch loss = 0.05\n",
      "epoch 2, step 52000/55000, batch loss = 0.05\n",
      "epoch 2, step 52250/55000, batch loss = 0.06\n",
      "epoch 2, step 52500/55000, batch loss = 0.23\n",
      "Train accuracy = 98.35\n",
      "epoch 2, step 52750/55000, batch loss = 0.11\n",
      "epoch 2, step 53000/55000, batch loss = 0.06\n",
      "epoch 2, step 53250/55000, batch loss = 0.07\n",
      "epoch 2, step 53500/55000, batch loss = 0.06\n",
      "epoch 2, step 53750/55000, batch loss = 0.16\n",
      "epoch 2, step 54000/55000, batch loss = 0.11\n",
      "epoch 2, step 54250/55000, batch loss = 0.08\n",
      "epoch 2, step 54500/55000, batch loss = 0.04\n",
      "epoch 2, step 54750/55000, batch loss = 0.18\n",
      "Train accuracy = 98.36\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 98.60\n",
      "Validation avg loss = 0.09\n",
      "\n",
      "epoch 3, step 0/55000, batch loss = 0.06\n",
      "epoch 3, step 250/55000, batch loss = 0.05\n",
      "epoch 3, step 500/55000, batch loss = 0.34\n",
      "epoch 3, step 750/55000, batch loss = 0.04\n",
      "epoch 3, step 1000/55000, batch loss = 0.04\n",
      "epoch 3, step 1250/55000, batch loss = 0.07\n",
      "epoch 3, step 1500/55000, batch loss = 0.04\n",
      "epoch 3, step 1750/55000, batch loss = 0.05\n",
      "epoch 3, step 2000/55000, batch loss = 0.05\n",
      "epoch 3, step 2250/55000, batch loss = 0.04\n",
      "epoch 3, step 2500/55000, batch loss = 0.05\n",
      "Train accuracy = 99.10\n",
      "epoch 3, step 2750/55000, batch loss = 0.06\n",
      "epoch 3, step 3000/55000, batch loss = 0.05\n",
      "epoch 3, step 3250/55000, batch loss = 0.06\n",
      "epoch 3, step 3500/55000, batch loss = 0.07\n",
      "epoch 3, step 3750/55000, batch loss = 0.06\n",
      "epoch 3, step 4000/55000, batch loss = 0.04\n",
      "epoch 3, step 4250/55000, batch loss = 0.10\n",
      "epoch 3, step 4500/55000, batch loss = 0.06\n",
      "epoch 3, step 4750/55000, batch loss = 0.06\n",
      "epoch 3, step 5000/55000, batch loss = 0.06\n",
      "Train accuracy = 99.07\n",
      "epoch 3, step 5250/55000, batch loss = 0.07\n",
      "epoch 3, step 5500/55000, batch loss = 0.10\n",
      "epoch 3, step 5750/55000, batch loss = 0.05\n",
      "epoch 3, step 6000/55000, batch loss = 0.04\n",
      "epoch 3, step 6250/55000, batch loss = 0.10\n",
      "epoch 3, step 6500/55000, batch loss = 0.07\n",
      "epoch 3, step 6750/55000, batch loss = 0.04\n",
      "epoch 3, step 7000/55000, batch loss = 0.09\n",
      "epoch 3, step 7250/55000, batch loss = 0.05\n",
      "epoch 3, step 7500/55000, batch loss = 0.05\n",
      "Train accuracy = 99.15\n",
      "epoch 3, step 7750/55000, batch loss = 0.05\n",
      "epoch 3, step 8000/55000, batch loss = 0.05\n",
      "epoch 3, step 8250/55000, batch loss = 0.05\n",
      "epoch 3, step 8500/55000, batch loss = 0.05\n",
      "epoch 3, step 8750/55000, batch loss = 0.09\n",
      "epoch 3, step 9000/55000, batch loss = 0.19\n",
      "epoch 3, step 9250/55000, batch loss = 0.06\n",
      "epoch 3, step 9500/55000, batch loss = 0.10\n",
      "epoch 3, step 9750/55000, batch loss = 0.06\n",
      "epoch 3, step 10000/55000, batch loss = 0.05\n",
      "Train accuracy = 99.19\n",
      "epoch 3, step 10250/55000, batch loss = 0.04\n",
      "epoch 3, step 10500/55000, batch loss = 0.08\n",
      "epoch 3, step 10750/55000, batch loss = 0.08\n",
      "epoch 3, step 11000/55000, batch loss = 0.05\n",
      "epoch 3, step 11250/55000, batch loss = 0.13\n",
      "epoch 3, step 11500/55000, batch loss = 0.12\n",
      "epoch 3, step 11750/55000, batch loss = 0.05\n",
      "epoch 3, step 12000/55000, batch loss = 0.04\n",
      "epoch 3, step 12250/55000, batch loss = 0.07\n",
      "epoch 3, step 12500/55000, batch loss = 0.11\n",
      "Train accuracy = 99.16\n",
      "epoch 3, step 12750/55000, batch loss = 0.05\n",
      "epoch 3, step 13000/55000, batch loss = 0.04\n",
      "epoch 3, step 13250/55000, batch loss = 0.04\n",
      "epoch 3, step 13500/55000, batch loss = 0.04\n",
      "epoch 3, step 13750/55000, batch loss = 0.10\n",
      "epoch 3, step 14000/55000, batch loss = 0.05\n",
      "epoch 3, step 14250/55000, batch loss = 0.05\n",
      "epoch 3, step 14500/55000, batch loss = 0.07\n",
      "epoch 3, step 14750/55000, batch loss = 0.05\n",
      "epoch 3, step 15000/55000, batch loss = 0.06\n",
      "Train accuracy = 99.17\n",
      "epoch 3, step 15250/55000, batch loss = 0.04\n",
      "epoch 3, step 15500/55000, batch loss = 0.13\n",
      "epoch 3, step 15750/55000, batch loss = 0.05\n",
      "epoch 3, step 16000/55000, batch loss = 0.05\n",
      "epoch 3, step 16250/55000, batch loss = 0.05\n",
      "epoch 3, step 16500/55000, batch loss = 0.07\n",
      "epoch 3, step 16750/55000, batch loss = 0.05\n",
      "epoch 3, step 17000/55000, batch loss = 0.05\n",
      "epoch 3, step 17250/55000, batch loss = 0.12\n",
      "epoch 3, step 17500/55000, batch loss = 0.11\n",
      "Train accuracy = 99.18\n",
      "epoch 3, step 17750/55000, batch loss = 0.04\n",
      "epoch 3, step 18000/55000, batch loss = 0.04\n",
      "epoch 3, step 18250/55000, batch loss = 0.05\n",
      "epoch 3, step 18500/55000, batch loss = 0.06\n",
      "epoch 3, step 18750/55000, batch loss = 0.10\n",
      "epoch 3, step 19000/55000, batch loss = 0.04\n",
      "epoch 3, step 19250/55000, batch loss = 0.05\n",
      "epoch 3, step 19500/55000, batch loss = 0.04\n",
      "epoch 3, step 19750/55000, batch loss = 0.07\n",
      "epoch 3, step 20000/55000, batch loss = 0.08\n",
      "Train accuracy = 99.20\n",
      "epoch 3, step 20250/55000, batch loss = 0.12\n",
      "epoch 3, step 20500/55000, batch loss = 0.08\n",
      "epoch 3, step 20750/55000, batch loss = 0.05\n",
      "epoch 3, step 21000/55000, batch loss = 0.05\n",
      "epoch 3, step 21250/55000, batch loss = 0.06\n",
      "epoch 3, step 21500/55000, batch loss = 0.08\n",
      "epoch 3, step 21750/55000, batch loss = 0.04\n",
      "epoch 3, step 22000/55000, batch loss = 0.05\n",
      "epoch 3, step 22250/55000, batch loss = 0.05\n",
      "epoch 3, step 22500/55000, batch loss = 0.05\n",
      "Train accuracy = 99.22\n",
      "epoch 3, step 22750/55000, batch loss = 0.08\n",
      "epoch 3, step 23000/55000, batch loss = 0.05\n",
      "epoch 3, step 23250/55000, batch loss = 0.06\n",
      "epoch 3, step 23500/55000, batch loss = 0.05\n",
      "epoch 3, step 23750/55000, batch loss = 0.07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, step 24000/55000, batch loss = 0.05\n",
      "epoch 3, step 24250/55000, batch loss = 0.05\n",
      "epoch 3, step 24500/55000, batch loss = 0.07\n",
      "epoch 3, step 24750/55000, batch loss = 0.06\n",
      "epoch 3, step 25000/55000, batch loss = 0.05\n",
      "Train accuracy = 99.23\n",
      "epoch 3, step 25250/55000, batch loss = 0.05\n",
      "epoch 3, step 25500/55000, batch loss = 0.06\n",
      "epoch 3, step 25750/55000, batch loss = 0.05\n",
      "epoch 3, step 26000/55000, batch loss = 0.05\n",
      "epoch 3, step 26250/55000, batch loss = 0.06\n",
      "epoch 3, step 26500/55000, batch loss = 0.11\n",
      "epoch 3, step 26750/55000, batch loss = 0.06\n",
      "epoch 3, step 27000/55000, batch loss = 0.06\n",
      "epoch 3, step 27250/55000, batch loss = 0.05\n",
      "epoch 3, step 27500/55000, batch loss = 0.05\n",
      "Train accuracy = 99.22\n",
      "epoch 3, step 27750/55000, batch loss = 0.05\n",
      "epoch 3, step 28000/55000, batch loss = 0.04\n",
      "epoch 3, step 28250/55000, batch loss = 0.07\n",
      "epoch 3, step 28500/55000, batch loss = 0.13\n",
      "epoch 3, step 28750/55000, batch loss = 0.04\n",
      "epoch 3, step 29000/55000, batch loss = 0.10\n",
      "epoch 3, step 29250/55000, batch loss = 0.10\n",
      "epoch 3, step 29500/55000, batch loss = 0.09\n",
      "epoch 3, step 29750/55000, batch loss = 0.05\n",
      "epoch 3, step 30000/55000, batch loss = 0.05\n",
      "Train accuracy = 99.21\n",
      "epoch 3, step 30250/55000, batch loss = 0.11\n",
      "epoch 3, step 30500/55000, batch loss = 0.09\n",
      "epoch 3, step 30750/55000, batch loss = 0.05\n",
      "epoch 3, step 31000/55000, batch loss = 0.04\n",
      "epoch 3, step 31250/55000, batch loss = 0.07\n",
      "epoch 3, step 31500/55000, batch loss = 0.16\n",
      "epoch 3, step 31750/55000, batch loss = 0.05\n",
      "epoch 3, step 32000/55000, batch loss = 0.04\n",
      "epoch 3, step 32250/55000, batch loss = 0.17\n",
      "epoch 3, step 32500/55000, batch loss = 0.04\n",
      "Train accuracy = 99.21\n",
      "epoch 3, step 32750/55000, batch loss = 0.06\n",
      "epoch 3, step 33000/55000, batch loss = 0.05\n",
      "epoch 3, step 33250/55000, batch loss = 0.04\n",
      "epoch 3, step 33500/55000, batch loss = 0.04\n",
      "epoch 3, step 33750/55000, batch loss = 0.05\n",
      "epoch 3, step 34000/55000, batch loss = 0.08\n",
      "epoch 3, step 34250/55000, batch loss = 0.07\n",
      "epoch 3, step 34500/55000, batch loss = 0.04\n",
      "epoch 3, step 34750/55000, batch loss = 0.05\n",
      "epoch 3, step 35000/55000, batch loss = 0.04\n",
      "Train accuracy = 99.22\n",
      "epoch 3, step 35250/55000, batch loss = 0.05\n",
      "epoch 3, step 35500/55000, batch loss = 0.08\n",
      "epoch 3, step 35750/55000, batch loss = 0.05\n",
      "epoch 3, step 36000/55000, batch loss = 0.07\n",
      "epoch 3, step 36250/55000, batch loss = 0.05\n",
      "epoch 3, step 36500/55000, batch loss = 0.06\n",
      "epoch 3, step 36750/55000, batch loss = 0.05\n",
      "epoch 3, step 37000/55000, batch loss = 0.10\n",
      "epoch 3, step 37250/55000, batch loss = 0.05\n",
      "epoch 3, step 37500/55000, batch loss = 0.05\n",
      "Train accuracy = 99.23\n",
      "epoch 3, step 37750/55000, batch loss = 0.05\n",
      "epoch 3, step 38000/55000, batch loss = 0.06\n",
      "epoch 3, step 38250/55000, batch loss = 0.16\n",
      "epoch 3, step 38500/55000, batch loss = 0.10\n",
      "epoch 3, step 38750/55000, batch loss = 0.04\n",
      "epoch 3, step 39000/55000, batch loss = 0.07\n",
      "epoch 3, step 39250/55000, batch loss = 0.05\n",
      "epoch 3, step 39500/55000, batch loss = 0.04\n",
      "epoch 3, step 39750/55000, batch loss = 0.09\n",
      "epoch 3, step 40000/55000, batch loss = 0.04\n",
      "Train accuracy = 99.23\n",
      "epoch 3, step 40250/55000, batch loss = 0.05\n",
      "epoch 3, step 40500/55000, batch loss = 0.05\n",
      "epoch 3, step 40750/55000, batch loss = 0.05\n",
      "epoch 3, step 41000/55000, batch loss = 0.05\n",
      "epoch 3, step 41250/55000, batch loss = 0.06\n",
      "epoch 3, step 41500/55000, batch loss = 0.06\n",
      "epoch 3, step 41750/55000, batch loss = 0.05\n",
      "epoch 3, step 42000/55000, batch loss = 0.06\n",
      "epoch 3, step 42250/55000, batch loss = 0.11\n",
      "epoch 3, step 42500/55000, batch loss = 0.05\n",
      "Train accuracy = 99.23\n",
      "epoch 3, step 42750/55000, batch loss = 0.04\n",
      "epoch 3, step 43000/55000, batch loss = 0.05\n",
      "epoch 3, step 43250/55000, batch loss = 0.05\n",
      "epoch 3, step 43500/55000, batch loss = 0.06\n",
      "epoch 3, step 43750/55000, batch loss = 0.07\n",
      "epoch 3, step 44000/55000, batch loss = 0.07\n",
      "epoch 3, step 44250/55000, batch loss = 0.04\n",
      "epoch 3, step 44500/55000, batch loss = 0.05\n",
      "epoch 3, step 44750/55000, batch loss = 0.05\n",
      "epoch 3, step 45000/55000, batch loss = 0.11\n",
      "Train accuracy = 99.23\n",
      "epoch 3, step 45250/55000, batch loss = 0.05\n",
      "epoch 3, step 45500/55000, batch loss = 0.20\n",
      "epoch 3, step 45750/55000, batch loss = 0.04\n",
      "epoch 3, step 46000/55000, batch loss = 0.06\n",
      "epoch 3, step 46250/55000, batch loss = 0.05\n",
      "epoch 3, step 46500/55000, batch loss = 0.09\n",
      "epoch 3, step 46750/55000, batch loss = 0.05\n",
      "epoch 3, step 47000/55000, batch loss = 0.06\n",
      "epoch 3, step 47250/55000, batch loss = 0.08\n",
      "epoch 3, step 47500/55000, batch loss = 0.06\n",
      "Train accuracy = 99.23\n",
      "epoch 3, step 47750/55000, batch loss = 0.05\n",
      "epoch 3, step 48000/55000, batch loss = 0.08\n",
      "epoch 3, step 48250/55000, batch loss = 0.05\n",
      "epoch 3, step 48500/55000, batch loss = 0.04\n",
      "epoch 3, step 48750/55000, batch loss = 0.05\n",
      "epoch 3, step 49000/55000, batch loss = 0.05\n",
      "epoch 3, step 49250/55000, batch loss = 0.05\n",
      "epoch 3, step 49500/55000, batch loss = 0.04\n",
      "epoch 3, step 49750/55000, batch loss = 0.05\n",
      "epoch 3, step 50000/55000, batch loss = 0.05\n",
      "Train accuracy = 99.24\n",
      "epoch 3, step 50250/55000, batch loss = 0.05\n",
      "epoch 3, step 50500/55000, batch loss = 0.06\n",
      "epoch 3, step 50750/55000, batch loss = 0.05\n",
      "epoch 3, step 51000/55000, batch loss = 0.04\n",
      "epoch 3, step 51250/55000, batch loss = 0.04\n",
      "epoch 3, step 51500/55000, batch loss = 0.12\n",
      "epoch 3, step 51750/55000, batch loss = 0.04\n",
      "epoch 3, step 52000/55000, batch loss = 0.09\n",
      "epoch 3, step 52250/55000, batch loss = 0.05\n",
      "epoch 3, step 52500/55000, batch loss = 0.05\n",
      "Train accuracy = 99.23\n",
      "epoch 3, step 52750/55000, batch loss = 0.04\n",
      "epoch 3, step 53000/55000, batch loss = 0.07\n",
      "epoch 3, step 53250/55000, batch loss = 0.04\n",
      "epoch 3, step 53500/55000, batch loss = 0.04\n",
      "epoch 3, step 53750/55000, batch loss = 0.05\n",
      "epoch 3, step 54000/55000, batch loss = 0.05\n",
      "epoch 3, step 54250/55000, batch loss = 0.05\n",
      "epoch 3, step 54500/55000, batch loss = 0.08\n",
      "epoch 3, step 54750/55000, batch loss = 0.05\n",
      "Train accuracy = 99.23\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 99.04\n",
      "Validation avg loss = 0.07\n",
      "\n",
      "epoch 4, step 0/55000, batch loss = 0.11\n",
      "epoch 4, step 250/55000, batch loss = 0.06\n",
      "epoch 4, step 500/55000, batch loss = 0.06\n",
      "epoch 4, step 750/55000, batch loss = 0.08\n",
      "epoch 4, step 1000/55000, batch loss = 0.30\n",
      "epoch 4, step 1250/55000, batch loss = 0.05\n",
      "epoch 4, step 1500/55000, batch loss = 0.04\n",
      "epoch 4, step 1750/55000, batch loss = 0.06\n",
      "epoch 4, step 2000/55000, batch loss = 0.04\n",
      "epoch 4, step 2250/55000, batch loss = 0.13\n",
      "epoch 4, step 2500/55000, batch loss = 0.05\n",
      "Train accuracy = 99.37\n",
      "epoch 4, step 2750/55000, batch loss = 0.08\n",
      "epoch 4, step 3000/55000, batch loss = 0.07\n",
      "epoch 4, step 3250/55000, batch loss = 0.08\n",
      "epoch 4, step 3500/55000, batch loss = 0.05\n",
      "epoch 4, step 3750/55000, batch loss = 0.05\n",
      "epoch 4, step 4000/55000, batch loss = 0.17\n",
      "epoch 4, step 4250/55000, batch loss = 0.07\n",
      "epoch 4, step 4500/55000, batch loss = 0.08\n",
      "epoch 4, step 4750/55000, batch loss = 0.05\n",
      "epoch 4, step 5000/55000, batch loss = 0.05\n",
      "Train accuracy = 99.31\n",
      "epoch 4, step 5250/55000, batch loss = 0.05\n",
      "epoch 4, step 5500/55000, batch loss = 0.05\n",
      "epoch 4, step 5750/55000, batch loss = 0.05\n",
      "epoch 4, step 6000/55000, batch loss = 0.09\n",
      "epoch 4, step 6250/55000, batch loss = 0.07\n",
      "epoch 4, step 6500/55000, batch loss = 0.05\n",
      "epoch 4, step 6750/55000, batch loss = 0.05\n",
      "epoch 4, step 7000/55000, batch loss = 0.04\n",
      "epoch 4, step 7250/55000, batch loss = 0.05\n",
      "epoch 4, step 7500/55000, batch loss = 0.06\n",
      "Train accuracy = 99.38\n",
      "epoch 4, step 7750/55000, batch loss = 0.05\n",
      "epoch 4, step 8000/55000, batch loss = 0.04\n",
      "epoch 4, step 8250/55000, batch loss = 0.04\n",
      "epoch 4, step 8500/55000, batch loss = 0.06\n",
      "epoch 4, step 8750/55000, batch loss = 0.05\n",
      "epoch 4, step 9000/55000, batch loss = 0.04\n",
      "epoch 4, step 9250/55000, batch loss = 0.06\n",
      "epoch 4, step 9500/55000, batch loss = 0.08\n",
      "epoch 4, step 9750/55000, batch loss = 0.04\n",
      "epoch 4, step 10000/55000, batch loss = 0.05\n",
      "Train accuracy = 99.36\n",
      "epoch 4, step 10250/55000, batch loss = 0.08\n",
      "epoch 4, step 10500/55000, batch loss = 0.04\n",
      "epoch 4, step 10750/55000, batch loss = 0.04\n",
      "epoch 4, step 11000/55000, batch loss = 0.07\n",
      "epoch 4, step 11250/55000, batch loss = 0.09\n",
      "epoch 4, step 11500/55000, batch loss = 0.06\n",
      "epoch 4, step 11750/55000, batch loss = 0.05\n",
      "epoch 4, step 12000/55000, batch loss = 0.05\n",
      "epoch 4, step 12250/55000, batch loss = 0.10\n",
      "epoch 4, step 12500/55000, batch loss = 0.05\n",
      "Train accuracy = 99.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, step 12750/55000, batch loss = 0.07\n",
      "epoch 4, step 13000/55000, batch loss = 0.05\n",
      "epoch 4, step 13250/55000, batch loss = 0.04\n",
      "epoch 4, step 13500/55000, batch loss = 0.04\n",
      "epoch 4, step 13750/55000, batch loss = 0.05\n",
      "epoch 4, step 14000/55000, batch loss = 0.05\n",
      "epoch 4, step 14250/55000, batch loss = 0.05\n",
      "epoch 4, step 14500/55000, batch loss = 0.09\n",
      "epoch 4, step 14750/55000, batch loss = 0.06\n",
      "epoch 4, step 15000/55000, batch loss = 0.06\n",
      "Train accuracy = 99.34\n",
      "epoch 4, step 15250/55000, batch loss = 0.05\n",
      "epoch 4, step 15500/55000, batch loss = 0.04\n",
      "epoch 4, step 15750/55000, batch loss = 0.08\n",
      "epoch 4, step 16000/55000, batch loss = 0.14\n",
      "epoch 4, step 16250/55000, batch loss = 0.07\n",
      "epoch 4, step 16500/55000, batch loss = 0.04\n",
      "epoch 4, step 16750/55000, batch loss = 0.04\n",
      "epoch 4, step 17000/55000, batch loss = 0.04\n",
      "epoch 4, step 17250/55000, batch loss = 0.07\n",
      "epoch 4, step 17500/55000, batch loss = 0.06\n",
      "Train accuracy = 99.33\n",
      "epoch 4, step 17750/55000, batch loss = 0.04\n",
      "epoch 4, step 18000/55000, batch loss = 0.04\n",
      "epoch 4, step 18250/55000, batch loss = 0.05\n",
      "epoch 4, step 18500/55000, batch loss = 0.04\n",
      "epoch 4, step 18750/55000, batch loss = 0.06\n",
      "epoch 4, step 19000/55000, batch loss = 0.07\n",
      "epoch 4, step 19250/55000, batch loss = 0.04\n",
      "epoch 4, step 19500/55000, batch loss = 0.05\n",
      "epoch 4, step 19750/55000, batch loss = 0.11\n",
      "epoch 4, step 20000/55000, batch loss = 0.06\n",
      "Train accuracy = 99.30\n",
      "epoch 4, step 20250/55000, batch loss = 0.05\n",
      "epoch 4, step 20500/55000, batch loss = 0.07\n",
      "epoch 4, step 20750/55000, batch loss = 0.05\n",
      "epoch 4, step 21000/55000, batch loss = 0.05\n",
      "epoch 4, step 21250/55000, batch loss = 0.08\n",
      "epoch 4, step 21500/55000, batch loss = 0.05\n",
      "epoch 4, step 21750/55000, batch loss = 0.05\n",
      "epoch 4, step 22000/55000, batch loss = 0.04\n",
      "epoch 4, step 22250/55000, batch loss = 0.05\n",
      "epoch 4, step 22500/55000, batch loss = 0.04\n",
      "Train accuracy = 99.34\n",
      "epoch 4, step 22750/55000, batch loss = 0.17\n",
      "epoch 4, step 23000/55000, batch loss = 0.05\n",
      "epoch 4, step 23250/55000, batch loss = 0.04\n",
      "epoch 4, step 23500/55000, batch loss = 0.06\n",
      "epoch 4, step 23750/55000, batch loss = 0.08\n",
      "epoch 4, step 24000/55000, batch loss = 0.06\n",
      "epoch 4, step 24250/55000, batch loss = 0.06\n",
      "epoch 4, step 24500/55000, batch loss = 0.05\n",
      "epoch 4, step 24750/55000, batch loss = 0.04\n",
      "epoch 4, step 25000/55000, batch loss = 0.06\n",
      "Train accuracy = 99.34\n",
      "epoch 4, step 25250/55000, batch loss = 0.07\n",
      "epoch 4, step 25500/55000, batch loss = 0.05\n",
      "epoch 4, step 25750/55000, batch loss = 0.04\n",
      "epoch 4, step 26000/55000, batch loss = 0.16\n",
      "epoch 4, step 26250/55000, batch loss = 0.05\n",
      "epoch 4, step 26500/55000, batch loss = 0.04\n",
      "epoch 4, step 26750/55000, batch loss = 0.06\n",
      "epoch 4, step 27000/55000, batch loss = 0.07\n",
      "epoch 4, step 27250/55000, batch loss = 0.05\n",
      "epoch 4, step 27500/55000, batch loss = 0.04\n",
      "Train accuracy = 99.35\n",
      "epoch 4, step 27750/55000, batch loss = 0.06\n",
      "epoch 4, step 28000/55000, batch loss = 0.05\n",
      "epoch 4, step 28250/55000, batch loss = 0.04\n",
      "epoch 4, step 28500/55000, batch loss = 0.05\n",
      "epoch 4, step 28750/55000, batch loss = 0.06\n",
      "epoch 4, step 29000/55000, batch loss = 0.05\n",
      "epoch 4, step 29250/55000, batch loss = 0.05\n",
      "epoch 4, step 29500/55000, batch loss = 0.05\n",
      "epoch 4, step 29750/55000, batch loss = 0.05\n",
      "epoch 4, step 30000/55000, batch loss = 0.04\n",
      "Train accuracy = 99.36\n",
      "epoch 4, step 30250/55000, batch loss = 0.04\n",
      "epoch 4, step 30500/55000, batch loss = 0.06\n",
      "epoch 4, step 30750/55000, batch loss = 0.06\n",
      "epoch 4, step 31000/55000, batch loss = 0.05\n",
      "epoch 4, step 31250/55000, batch loss = 0.05\n",
      "epoch 4, step 31500/55000, batch loss = 0.05\n",
      "epoch 4, step 31750/55000, batch loss = 0.05\n",
      "epoch 4, step 32000/55000, batch loss = 0.05\n",
      "epoch 4, step 32250/55000, batch loss = 0.04\n",
      "epoch 4, step 32500/55000, batch loss = 0.06\n",
      "Train accuracy = 99.36\n",
      "epoch 4, step 32750/55000, batch loss = 0.04\n",
      "epoch 4, step 33000/55000, batch loss = 0.04\n",
      "epoch 4, step 33250/55000, batch loss = 0.08\n",
      "epoch 4, step 33500/55000, batch loss = 0.06\n",
      "epoch 4, step 33750/55000, batch loss = 0.05\n",
      "epoch 4, step 34000/55000, batch loss = 0.05\n",
      "epoch 4, step 34250/55000, batch loss = 0.06\n",
      "epoch 4, step 34500/55000, batch loss = 0.04\n",
      "epoch 4, step 34750/55000, batch loss = 0.05\n",
      "epoch 4, step 35000/55000, batch loss = 0.04\n",
      "Train accuracy = 99.36\n",
      "epoch 4, step 35250/55000, batch loss = 0.05\n",
      "epoch 4, step 35500/55000, batch loss = 0.09\n",
      "epoch 4, step 35750/55000, batch loss = 0.08\n",
      "epoch 4, step 36000/55000, batch loss = 0.07\n",
      "epoch 4, step 36250/55000, batch loss = 0.04\n",
      "epoch 4, step 36500/55000, batch loss = 0.05\n",
      "epoch 4, step 36750/55000, batch loss = 0.04\n",
      "epoch 4, step 37000/55000, batch loss = 0.05\n",
      "epoch 4, step 37250/55000, batch loss = 0.04\n",
      "epoch 4, step 37500/55000, batch loss = 0.07\n",
      "Train accuracy = 99.36\n",
      "epoch 4, step 37750/55000, batch loss = 0.05\n",
      "epoch 4, step 38000/55000, batch loss = 0.04\n",
      "epoch 4, step 38250/55000, batch loss = 0.04\n",
      "epoch 4, step 38500/55000, batch loss = 0.05\n",
      "epoch 4, step 38750/55000, batch loss = 0.08\n",
      "epoch 4, step 39000/55000, batch loss = 0.07\n",
      "epoch 4, step 39250/55000, batch loss = 0.05\n",
      "epoch 4, step 39500/55000, batch loss = 0.04\n",
      "epoch 4, step 39750/55000, batch loss = 0.05\n",
      "epoch 4, step 40000/55000, batch loss = 0.05\n",
      "Train accuracy = 99.36\n",
      "epoch 4, step 40250/55000, batch loss = 0.05\n",
      "epoch 4, step 40500/55000, batch loss = 0.04\n",
      "epoch 4, step 40750/55000, batch loss = 0.07\n",
      "epoch 4, step 41000/55000, batch loss = 0.05\n",
      "epoch 4, step 41250/55000, batch loss = 0.04\n",
      "epoch 4, step 41500/55000, batch loss = 0.04\n",
      "epoch 4, step 41750/55000, batch loss = 0.08\n",
      "epoch 4, step 42000/55000, batch loss = 0.05\n",
      "epoch 4, step 42250/55000, batch loss = 0.04\n",
      "epoch 4, step 42500/55000, batch loss = 0.07\n",
      "Train accuracy = 99.36\n",
      "epoch 4, step 42750/55000, batch loss = 0.04\n",
      "epoch 4, step 43000/55000, batch loss = 0.05\n",
      "epoch 4, step 43250/55000, batch loss = 0.07\n",
      "epoch 4, step 43500/55000, batch loss = 0.07\n",
      "epoch 4, step 43750/55000, batch loss = 0.07\n",
      "epoch 4, step 44000/55000, batch loss = 0.05\n",
      "epoch 4, step 44250/55000, batch loss = 0.06\n",
      "epoch 4, step 44500/55000, batch loss = 0.06\n",
      "epoch 4, step 44750/55000, batch loss = 0.04\n",
      "epoch 4, step 45000/55000, batch loss = 0.05\n",
      "Train accuracy = 99.35\n",
      "epoch 4, step 45250/55000, batch loss = 0.06\n",
      "epoch 4, step 45500/55000, batch loss = 0.04\n",
      "epoch 4, step 45750/55000, batch loss = 0.05\n",
      "epoch 4, step 46000/55000, batch loss = 0.04\n",
      "epoch 4, step 46250/55000, batch loss = 0.05\n",
      "epoch 4, step 46500/55000, batch loss = 0.04\n",
      "epoch 4, step 46750/55000, batch loss = 0.06\n",
      "epoch 4, step 47000/55000, batch loss = 0.08\n",
      "epoch 4, step 47250/55000, batch loss = 0.04\n",
      "epoch 4, step 47500/55000, batch loss = 0.08\n",
      "Train accuracy = 99.35\n",
      "epoch 4, step 47750/55000, batch loss = 0.05\n",
      "epoch 4, step 48000/55000, batch loss = 0.10\n",
      "epoch 4, step 48250/55000, batch loss = 0.09\n",
      "epoch 4, step 48500/55000, batch loss = 0.04\n",
      "epoch 4, step 48750/55000, batch loss = 0.04\n",
      "epoch 4, step 49000/55000, batch loss = 0.04\n",
      "epoch 4, step 49250/55000, batch loss = 0.11\n",
      "epoch 4, step 49500/55000, batch loss = 0.05\n",
      "epoch 4, step 49750/55000, batch loss = 0.05\n",
      "epoch 4, step 50000/55000, batch loss = 0.04\n",
      "Train accuracy = 99.35\n",
      "epoch 4, step 50250/55000, batch loss = 0.06\n",
      "epoch 4, step 50500/55000, batch loss = 0.05\n",
      "epoch 4, step 50750/55000, batch loss = 0.05\n",
      "epoch 4, step 51000/55000, batch loss = 0.05\n",
      "epoch 4, step 51250/55000, batch loss = 0.05\n",
      "epoch 4, step 51500/55000, batch loss = 0.05\n",
      "epoch 4, step 51750/55000, batch loss = 0.05\n",
      "epoch 4, step 52000/55000, batch loss = 0.04\n",
      "epoch 4, step 52250/55000, batch loss = 0.06\n",
      "epoch 4, step 52500/55000, batch loss = 0.04\n",
      "Train accuracy = 99.35\n",
      "epoch 4, step 52750/55000, batch loss = 0.11\n",
      "epoch 4, step 53000/55000, batch loss = 0.08\n",
      "epoch 4, step 53250/55000, batch loss = 0.05\n",
      "epoch 4, step 53500/55000, batch loss = 0.07\n",
      "epoch 4, step 53750/55000, batch loss = 0.07\n",
      "epoch 4, step 54000/55000, batch loss = 0.05\n",
      "epoch 4, step 54250/55000, batch loss = 0.10\n",
      "epoch 4, step 54500/55000, batch loss = 0.04\n",
      "epoch 4, step 54750/55000, batch loss = 0.06\n",
      "Train accuracy = 99.35\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 99.12\n",
      "Validation avg loss = 0.07\n",
      "\n",
      "epoch 5, step 0/55000, batch loss = 0.06\n",
      "epoch 5, step 250/55000, batch loss = 0.09\n",
      "epoch 5, step 500/55000, batch loss = 0.04\n",
      "epoch 5, step 750/55000, batch loss = 0.08\n",
      "epoch 5, step 1000/55000, batch loss = 0.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, step 1250/55000, batch loss = 0.07\n",
      "epoch 5, step 1500/55000, batch loss = 0.05\n",
      "epoch 5, step 1750/55000, batch loss = 0.04\n",
      "epoch 5, step 2000/55000, batch loss = 0.05\n",
      "epoch 5, step 2250/55000, batch loss = 0.04\n",
      "epoch 5, step 2500/55000, batch loss = 0.04\n",
      "Train accuracy = 99.37\n",
      "epoch 5, step 2750/55000, batch loss = 0.05\n",
      "epoch 5, step 3000/55000, batch loss = 0.07\n",
      "epoch 5, step 3250/55000, batch loss = 0.05\n",
      "epoch 5, step 3500/55000, batch loss = 0.05\n",
      "epoch 5, step 3750/55000, batch loss = 0.04\n",
      "epoch 5, step 4000/55000, batch loss = 0.05\n",
      "epoch 5, step 4250/55000, batch loss = 0.07\n",
      "epoch 5, step 4500/55000, batch loss = 0.15\n",
      "epoch 5, step 4750/55000, batch loss = 0.04\n",
      "epoch 5, step 5000/55000, batch loss = 0.05\n",
      "Train accuracy = 99.33\n",
      "epoch 5, step 5250/55000, batch loss = 0.07\n",
      "epoch 5, step 5500/55000, batch loss = 0.04\n",
      "epoch 5, step 5750/55000, batch loss = 0.05\n",
      "epoch 5, step 6000/55000, batch loss = 0.04\n",
      "epoch 5, step 6250/55000, batch loss = 0.07\n",
      "epoch 5, step 6500/55000, batch loss = 0.05\n",
      "epoch 5, step 6750/55000, batch loss = 0.04\n",
      "epoch 5, step 7000/55000, batch loss = 0.04\n",
      "epoch 5, step 7250/55000, batch loss = 0.04\n",
      "epoch 5, step 7500/55000, batch loss = 0.04\n",
      "Train accuracy = 99.39\n",
      "epoch 5, step 7750/55000, batch loss = 0.06\n",
      "epoch 5, step 8000/55000, batch loss = 0.04\n",
      "epoch 5, step 8250/55000, batch loss = 0.05\n",
      "epoch 5, step 8500/55000, batch loss = 0.10\n",
      "epoch 5, step 8750/55000, batch loss = 0.05\n",
      "epoch 5, step 9000/55000, batch loss = 0.06\n",
      "epoch 5, step 9250/55000, batch loss = 0.04\n",
      "epoch 5, step 9500/55000, batch loss = 0.05\n",
      "epoch 5, step 9750/55000, batch loss = 0.09\n",
      "epoch 5, step 10000/55000, batch loss = 0.09\n",
      "Train accuracy = 99.35\n",
      "epoch 5, step 10250/55000, batch loss = 0.05\n",
      "epoch 5, step 10500/55000, batch loss = 0.06\n",
      "epoch 5, step 10750/55000, batch loss = 0.04\n",
      "epoch 5, step 11000/55000, batch loss = 0.11\n",
      "epoch 5, step 11250/55000, batch loss = 0.05\n",
      "epoch 5, step 11500/55000, batch loss = 0.04\n",
      "epoch 5, step 11750/55000, batch loss = 0.16\n",
      "epoch 5, step 12000/55000, batch loss = 0.05\n",
      "epoch 5, step 12250/55000, batch loss = 0.04\n",
      "epoch 5, step 12500/55000, batch loss = 0.05\n",
      "Train accuracy = 99.31\n",
      "epoch 5, step 12750/55000, batch loss = 0.07\n",
      "epoch 5, step 13000/55000, batch loss = 0.10\n",
      "epoch 5, step 13250/55000, batch loss = 0.08\n",
      "epoch 5, step 13500/55000, batch loss = 0.04\n",
      "epoch 5, step 13750/55000, batch loss = 0.06\n",
      "epoch 5, step 14000/55000, batch loss = 0.05\n",
      "epoch 5, step 14250/55000, batch loss = 0.08\n",
      "epoch 5, step 14500/55000, batch loss = 0.04\n",
      "epoch 5, step 14750/55000, batch loss = 0.06\n",
      "epoch 5, step 15000/55000, batch loss = 0.10\n",
      "Train accuracy = 99.34\n",
      "epoch 5, step 15250/55000, batch loss = 0.05\n",
      "epoch 5, step 15500/55000, batch loss = 0.05\n",
      "epoch 5, step 15750/55000, batch loss = 0.05\n",
      "epoch 5, step 16000/55000, batch loss = 0.05\n",
      "epoch 5, step 16250/55000, batch loss = 0.05\n",
      "epoch 5, step 16500/55000, batch loss = 0.05\n",
      "epoch 5, step 16750/55000, batch loss = 0.04\n",
      "epoch 5, step 17000/55000, batch loss = 0.05\n",
      "epoch 5, step 17250/55000, batch loss = 0.09\n",
      "epoch 5, step 17500/55000, batch loss = 0.04\n",
      "Train accuracy = 99.36\n",
      "epoch 5, step 17750/55000, batch loss = 0.05\n",
      "epoch 5, step 18000/55000, batch loss = 0.05\n",
      "epoch 5, step 18250/55000, batch loss = 0.06\n",
      "epoch 5, step 18500/55000, batch loss = 0.07\n",
      "epoch 5, step 18750/55000, batch loss = 0.04\n",
      "epoch 5, step 19000/55000, batch loss = 0.06\n",
      "epoch 5, step 19250/55000, batch loss = 0.04\n",
      "epoch 5, step 19500/55000, batch loss = 0.05\n",
      "epoch 5, step 19750/55000, batch loss = 0.05\n",
      "epoch 5, step 20000/55000, batch loss = 0.04\n",
      "Train accuracy = 99.37\n",
      "epoch 5, step 20250/55000, batch loss = 0.04\n",
      "epoch 5, step 20500/55000, batch loss = 0.07\n",
      "epoch 5, step 20750/55000, batch loss = 0.05\n",
      "epoch 5, step 21000/55000, batch loss = 0.05\n",
      "epoch 5, step 21250/55000, batch loss = 0.04\n",
      "epoch 5, step 21500/55000, batch loss = 0.04\n",
      "epoch 5, step 21750/55000, batch loss = 0.07\n",
      "epoch 5, step 22000/55000, batch loss = 0.05\n",
      "epoch 5, step 22250/55000, batch loss = 0.10\n",
      "epoch 5, step 22500/55000, batch loss = 0.04\n",
      "Train accuracy = 99.38\n",
      "epoch 5, step 22750/55000, batch loss = 0.04\n",
      "epoch 5, step 23000/55000, batch loss = 0.14\n",
      "epoch 5, step 23250/55000, batch loss = 0.07\n",
      "epoch 5, step 23500/55000, batch loss = 0.04\n",
      "epoch 5, step 23750/55000, batch loss = 0.04\n",
      "epoch 5, step 24000/55000, batch loss = 0.05\n",
      "epoch 5, step 24250/55000, batch loss = 0.10\n",
      "epoch 5, step 24500/55000, batch loss = 0.07\n",
      "epoch 5, step 24750/55000, batch loss = 0.07\n",
      "epoch 5, step 25000/55000, batch loss = 0.05\n",
      "Train accuracy = 99.38\n",
      "epoch 5, step 25250/55000, batch loss = 0.06\n",
      "epoch 5, step 25500/55000, batch loss = 0.05\n",
      "epoch 5, step 25750/55000, batch loss = 0.04\n",
      "epoch 5, step 26000/55000, batch loss = 0.04\n",
      "epoch 5, step 26250/55000, batch loss = 0.04\n",
      "epoch 5, step 26500/55000, batch loss = 0.08\n",
      "epoch 5, step 26750/55000, batch loss = 0.04\n",
      "epoch 5, step 27000/55000, batch loss = 0.08\n",
      "epoch 5, step 27250/55000, batch loss = 0.04\n",
      "epoch 5, step 27500/55000, batch loss = 0.04\n",
      "Train accuracy = 99.38\n",
      "epoch 5, step 27750/55000, batch loss = 0.04\n",
      "epoch 5, step 28000/55000, batch loss = 0.07\n",
      "epoch 5, step 28250/55000, batch loss = 0.10\n",
      "epoch 5, step 28500/55000, batch loss = 0.04\n",
      "epoch 5, step 28750/55000, batch loss = 0.04\n",
      "epoch 5, step 29000/55000, batch loss = 0.04\n",
      "epoch 5, step 29250/55000, batch loss = 0.06\n",
      "epoch 5, step 29500/55000, batch loss = 0.05\n",
      "epoch 5, step 29750/55000, batch loss = 0.05\n",
      "epoch 5, step 30000/55000, batch loss = 0.08\n",
      "Train accuracy = 99.37\n",
      "epoch 5, step 30250/55000, batch loss = 0.04\n",
      "epoch 5, step 30500/55000, batch loss = 0.04\n",
      "epoch 5, step 30750/55000, batch loss = 0.05\n",
      "epoch 5, step 31000/55000, batch loss = 0.04\n",
      "epoch 5, step 31250/55000, batch loss = 0.08\n",
      "epoch 5, step 31500/55000, batch loss = 0.07\n",
      "epoch 5, step 31750/55000, batch loss = 0.07\n",
      "epoch 5, step 32000/55000, batch loss = 0.04\n",
      "epoch 5, step 32250/55000, batch loss = 0.05\n",
      "epoch 5, step 32500/55000, batch loss = 0.04\n",
      "Train accuracy = 99.39\n",
      "epoch 5, step 32750/55000, batch loss = 0.05\n",
      "epoch 5, step 33000/55000, batch loss = 0.06\n",
      "epoch 5, step 33250/55000, batch loss = 0.05\n",
      "epoch 5, step 33500/55000, batch loss = 0.05\n",
      "epoch 5, step 33750/55000, batch loss = 0.05\n",
      "epoch 5, step 34000/55000, batch loss = 0.06\n",
      "epoch 5, step 34250/55000, batch loss = 0.06\n",
      "epoch 5, step 34500/55000, batch loss = 0.05\n",
      "epoch 5, step 34750/55000, batch loss = 0.05\n",
      "epoch 5, step 35000/55000, batch loss = 0.06\n",
      "Train accuracy = 99.40\n",
      "epoch 5, step 35250/55000, batch loss = 0.04\n",
      "epoch 5, step 35500/55000, batch loss = 0.05\n",
      "epoch 5, step 35750/55000, batch loss = 0.12\n",
      "epoch 5, step 36000/55000, batch loss = 0.04\n",
      "epoch 5, step 36250/55000, batch loss = 0.17\n",
      "epoch 5, step 36500/55000, batch loss = 0.06\n",
      "epoch 5, step 36750/55000, batch loss = 0.06\n",
      "epoch 5, step 37000/55000, batch loss = 0.09\n",
      "epoch 5, step 37250/55000, batch loss = 0.04\n",
      "epoch 5, step 37500/55000, batch loss = 0.10\n",
      "Train accuracy = 99.40\n",
      "epoch 5, step 37750/55000, batch loss = 0.10\n",
      "epoch 5, step 38000/55000, batch loss = 0.06\n",
      "epoch 5, step 38250/55000, batch loss = 0.06\n",
      "epoch 5, step 38500/55000, batch loss = 0.06\n",
      "epoch 5, step 38750/55000, batch loss = 0.13\n",
      "epoch 5, step 39000/55000, batch loss = 0.05\n",
      "epoch 5, step 39250/55000, batch loss = 0.08\n",
      "epoch 5, step 39500/55000, batch loss = 0.05\n",
      "epoch 5, step 39750/55000, batch loss = 0.05\n",
      "epoch 5, step 40000/55000, batch loss = 0.04\n",
      "Train accuracy = 99.42\n",
      "epoch 5, step 40250/55000, batch loss = 0.05\n",
      "epoch 5, step 40500/55000, batch loss = 0.04\n",
      "epoch 5, step 40750/55000, batch loss = 0.04\n",
      "epoch 5, step 41000/55000, batch loss = 0.09\n",
      "epoch 5, step 41250/55000, batch loss = 0.04\n",
      "epoch 5, step 41500/55000, batch loss = 0.05\n",
      "epoch 5, step 41750/55000, batch loss = 0.04\n",
      "epoch 5, step 42000/55000, batch loss = 0.09\n",
      "epoch 5, step 42250/55000, batch loss = 0.04\n",
      "epoch 5, step 42500/55000, batch loss = 0.05\n",
      "Train accuracy = 99.41\n",
      "epoch 5, step 42750/55000, batch loss = 0.05\n",
      "epoch 5, step 43000/55000, batch loss = 0.04\n",
      "epoch 5, step 43250/55000, batch loss = 0.06\n",
      "epoch 5, step 43500/55000, batch loss = 0.04\n",
      "epoch 5, step 43750/55000, batch loss = 0.05\n",
      "epoch 5, step 44000/55000, batch loss = 0.05\n",
      "epoch 5, step 44250/55000, batch loss = 0.04\n",
      "epoch 5, step 44500/55000, batch loss = 0.11\n",
      "epoch 5, step 44750/55000, batch loss = 0.04\n",
      "epoch 5, step 45000/55000, batch loss = 0.06\n",
      "Train accuracy = 99.41\n",
      "epoch 5, step 45250/55000, batch loss = 0.10\n",
      "epoch 5, step 45500/55000, batch loss = 0.09\n",
      "epoch 5, step 45750/55000, batch loss = 0.05\n",
      "epoch 5, step 46000/55000, batch loss = 0.06\n",
      "epoch 5, step 46250/55000, batch loss = 0.04\n",
      "epoch 5, step 46500/55000, batch loss = 0.05\n",
      "epoch 5, step 46750/55000, batch loss = 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, step 47000/55000, batch loss = 0.04\n",
      "epoch 5, step 47250/55000, batch loss = 0.05\n",
      "epoch 5, step 47500/55000, batch loss = 0.04\n",
      "Train accuracy = 99.41\n",
      "epoch 5, step 47750/55000, batch loss = 0.05\n",
      "epoch 5, step 48000/55000, batch loss = 0.07\n",
      "epoch 5, step 48250/55000, batch loss = 0.05\n",
      "epoch 5, step 48500/55000, batch loss = 0.08\n",
      "epoch 5, step 48750/55000, batch loss = 0.05\n",
      "epoch 5, step 49000/55000, batch loss = 0.04\n",
      "epoch 5, step 49250/55000, batch loss = 0.04\n",
      "epoch 5, step 49500/55000, batch loss = 0.09\n",
      "epoch 5, step 49750/55000, batch loss = 0.04\n",
      "epoch 5, step 50000/55000, batch loss = 0.06\n",
      "Train accuracy = 99.42\n",
      "epoch 5, step 50250/55000, batch loss = 0.04\n",
      "epoch 5, step 50500/55000, batch loss = 0.05\n",
      "epoch 5, step 50750/55000, batch loss = 0.05\n",
      "epoch 5, step 51000/55000, batch loss = 0.14\n",
      "epoch 5, step 51250/55000, batch loss = 0.06\n",
      "epoch 5, step 51500/55000, batch loss = 0.04\n",
      "epoch 5, step 51750/55000, batch loss = 0.04\n",
      "epoch 5, step 52000/55000, batch loss = 0.04\n",
      "epoch 5, step 52250/55000, batch loss = 0.04\n",
      "epoch 5, step 52500/55000, batch loss = 0.07\n",
      "Train accuracy = 99.44\n",
      "epoch 5, step 52750/55000, batch loss = 0.05\n",
      "epoch 5, step 53000/55000, batch loss = 0.06\n",
      "epoch 5, step 53250/55000, batch loss = 0.05\n",
      "epoch 5, step 53500/55000, batch loss = 0.05\n",
      "epoch 5, step 53750/55000, batch loss = 0.08\n",
      "epoch 5, step 54000/55000, batch loss = 0.04\n",
      "epoch 5, step 54250/55000, batch loss = 0.05\n",
      "epoch 5, step 54500/55000, batch loss = 0.05\n",
      "epoch 5, step 54750/55000, batch loss = 0.06\n",
      "Train accuracy = 99.44\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 99.08\n",
      "Validation avg loss = 0.07\n",
      "\n",
      "epoch 6, step 0/55000, batch loss = 0.09\n",
      "epoch 6, step 250/55000, batch loss = 0.04\n",
      "epoch 6, step 500/55000, batch loss = 0.14\n",
      "epoch 6, step 750/55000, batch loss = 0.07\n",
      "epoch 6, step 1000/55000, batch loss = 0.06\n",
      "epoch 6, step 1250/55000, batch loss = 0.05\n",
      "epoch 6, step 1500/55000, batch loss = 0.05\n",
      "epoch 6, step 1750/55000, batch loss = 0.06\n",
      "epoch 6, step 2000/55000, batch loss = 0.05\n",
      "epoch 6, step 2250/55000, batch loss = 0.05\n",
      "epoch 6, step 2500/55000, batch loss = 0.04\n",
      "Train accuracy = 99.61\n",
      "epoch 6, step 2750/55000, batch loss = 0.07\n",
      "epoch 6, step 3000/55000, batch loss = 0.06\n",
      "epoch 6, step 3250/55000, batch loss = 0.05\n",
      "epoch 6, step 3500/55000, batch loss = 0.04\n",
      "epoch 6, step 3750/55000, batch loss = 0.15\n",
      "epoch 6, step 4000/55000, batch loss = 0.08\n",
      "epoch 6, step 4250/55000, batch loss = 0.06\n",
      "epoch 6, step 4500/55000, batch loss = 0.07\n",
      "epoch 6, step 4750/55000, batch loss = 0.10\n",
      "epoch 6, step 5000/55000, batch loss = 0.07\n",
      "Train accuracy = 99.47\n",
      "epoch 6, step 5250/55000, batch loss = 0.04\n",
      "epoch 6, step 5500/55000, batch loss = 0.08\n",
      "epoch 6, step 5750/55000, batch loss = 0.04\n",
      "epoch 6, step 6000/55000, batch loss = 0.07\n",
      "epoch 6, step 6250/55000, batch loss = 0.10\n",
      "epoch 6, step 6500/55000, batch loss = 0.05\n",
      "epoch 6, step 6750/55000, batch loss = 0.04\n",
      "epoch 6, step 7000/55000, batch loss = 0.04\n",
      "epoch 6, step 7250/55000, batch loss = 0.05\n",
      "epoch 6, step 7500/55000, batch loss = 0.05\n",
      "Train accuracy = 99.54\n",
      "epoch 6, step 7750/55000, batch loss = 0.05\n",
      "epoch 6, step 8000/55000, batch loss = 0.06\n",
      "epoch 6, step 8250/55000, batch loss = 0.12\n",
      "epoch 6, step 8500/55000, batch loss = 0.05\n",
      "epoch 6, step 8750/55000, batch loss = 0.05\n",
      "epoch 6, step 9000/55000, batch loss = 0.05\n",
      "epoch 6, step 9250/55000, batch loss = 0.06\n",
      "epoch 6, step 9500/55000, batch loss = 0.10\n",
      "epoch 6, step 9750/55000, batch loss = 0.06\n",
      "epoch 6, step 10000/55000, batch loss = 0.05\n",
      "Train accuracy = 99.53\n",
      "epoch 6, step 10250/55000, batch loss = 0.04\n",
      "epoch 6, step 10500/55000, batch loss = 0.04\n",
      "epoch 6, step 10750/55000, batch loss = 0.04\n",
      "epoch 6, step 11000/55000, batch loss = 0.34\n",
      "epoch 6, step 11250/55000, batch loss = 0.05\n",
      "epoch 6, step 11500/55000, batch loss = 0.04\n",
      "epoch 6, step 11750/55000, batch loss = 0.05\n",
      "epoch 6, step 12000/55000, batch loss = 0.04\n",
      "epoch 6, step 12250/55000, batch loss = 0.04\n",
      "epoch 6, step 12500/55000, batch loss = 0.05\n",
      "Train accuracy = 99.55\n",
      "epoch 6, step 12750/55000, batch loss = 0.04\n",
      "epoch 6, step 13000/55000, batch loss = 0.04\n",
      "epoch 6, step 13250/55000, batch loss = 0.05\n",
      "epoch 6, step 13500/55000, batch loss = 0.05\n",
      "epoch 6, step 13750/55000, batch loss = 0.05\n",
      "epoch 6, step 14000/55000, batch loss = 0.07\n",
      "epoch 6, step 14250/55000, batch loss = 0.06\n",
      "epoch 6, step 14500/55000, batch loss = 0.06\n",
      "epoch 6, step 14750/55000, batch loss = 0.05\n",
      "epoch 6, step 15000/55000, batch loss = 0.04\n",
      "Train accuracy = 99.57\n",
      "epoch 6, step 15250/55000, batch loss = 0.06\n",
      "epoch 6, step 15500/55000, batch loss = 0.04\n",
      "epoch 6, step 15750/55000, batch loss = 0.04\n",
      "epoch 6, step 16000/55000, batch loss = 0.06\n",
      "epoch 6, step 16250/55000, batch loss = 0.06\n",
      "epoch 6, step 16500/55000, batch loss = 0.05\n",
      "epoch 6, step 16750/55000, batch loss = 0.09\n",
      "epoch 6, step 17000/55000, batch loss = 0.04\n",
      "epoch 6, step 17250/55000, batch loss = 0.04\n",
      "epoch 6, step 17500/55000, batch loss = 0.04\n",
      "Train accuracy = 99.54\n",
      "epoch 6, step 17750/55000, batch loss = 0.05\n",
      "epoch 6, step 18000/55000, batch loss = 0.05\n",
      "epoch 6, step 18250/55000, batch loss = 0.04\n",
      "epoch 6, step 18500/55000, batch loss = 0.04\n",
      "epoch 6, step 18750/55000, batch loss = 0.04\n",
      "epoch 6, step 19000/55000, batch loss = 0.08\n",
      "epoch 6, step 19250/55000, batch loss = 0.06\n",
      "epoch 6, step 19500/55000, batch loss = 0.05\n",
      "epoch 6, step 19750/55000, batch loss = 0.04\n",
      "epoch 6, step 20000/55000, batch loss = 0.09\n",
      "Train accuracy = 99.53\n",
      "epoch 6, step 20250/55000, batch loss = 0.06\n",
      "epoch 6, step 20500/55000, batch loss = 0.05\n",
      "epoch 6, step 20750/55000, batch loss = 0.06\n",
      "epoch 6, step 21000/55000, batch loss = 0.05\n",
      "epoch 6, step 21250/55000, batch loss = 0.13\n",
      "epoch 6, step 21500/55000, batch loss = 0.04\n",
      "epoch 6, step 21750/55000, batch loss = 0.05\n",
      "epoch 6, step 22000/55000, batch loss = 0.05\n",
      "epoch 6, step 22250/55000, batch loss = 0.04\n",
      "epoch 6, step 22500/55000, batch loss = 0.05\n",
      "Train accuracy = 99.50\n",
      "epoch 6, step 22750/55000, batch loss = 0.12\n",
      "epoch 6, step 23000/55000, batch loss = 0.04\n",
      "epoch 6, step 23250/55000, batch loss = 0.08\n",
      "epoch 6, step 23500/55000, batch loss = 0.05\n",
      "epoch 6, step 23750/55000, batch loss = 0.04\n",
      "epoch 6, step 24000/55000, batch loss = 0.09\n",
      "epoch 6, step 24250/55000, batch loss = 0.06\n",
      "epoch 6, step 24500/55000, batch loss = 0.06\n",
      "epoch 6, step 24750/55000, batch loss = 0.07\n",
      "epoch 6, step 25000/55000, batch loss = 0.05\n",
      "Train accuracy = 99.47\n",
      "epoch 6, step 25250/55000, batch loss = 0.04\n",
      "epoch 6, step 25500/55000, batch loss = 0.10\n",
      "epoch 6, step 25750/55000, batch loss = 0.04\n",
      "epoch 6, step 26000/55000, batch loss = 0.04\n",
      "epoch 6, step 26250/55000, batch loss = 0.04\n",
      "epoch 6, step 26500/55000, batch loss = 0.06\n",
      "epoch 6, step 26750/55000, batch loss = 0.05\n",
      "epoch 6, step 27000/55000, batch loss = 0.05\n",
      "epoch 6, step 27250/55000, batch loss = 0.06\n",
      "epoch 6, step 27500/55000, batch loss = 0.05\n",
      "Train accuracy = 99.48\n",
      "epoch 6, step 27750/55000, batch loss = 0.05\n",
      "epoch 6, step 28000/55000, batch loss = 0.06\n",
      "epoch 6, step 28250/55000, batch loss = 0.06\n",
      "epoch 6, step 28500/55000, batch loss = 0.04\n",
      "epoch 6, step 28750/55000, batch loss = 0.05\n",
      "epoch 6, step 29000/55000, batch loss = 0.05\n",
      "epoch 6, step 29250/55000, batch loss = 0.05\n",
      "epoch 6, step 29500/55000, batch loss = 0.04\n",
      "epoch 6, step 29750/55000, batch loss = 0.09\n",
      "epoch 6, step 30000/55000, batch loss = 0.06\n",
      "Train accuracy = 99.49\n",
      "epoch 6, step 30250/55000, batch loss = 0.04\n",
      "epoch 6, step 30500/55000, batch loss = 0.04\n",
      "epoch 6, step 30750/55000, batch loss = 0.04\n",
      "epoch 6, step 31000/55000, batch loss = 0.23\n",
      "epoch 6, step 31250/55000, batch loss = 0.09\n",
      "epoch 6, step 31500/55000, batch loss = 0.04\n",
      "epoch 6, step 31750/55000, batch loss = 0.05\n",
      "epoch 6, step 32000/55000, batch loss = 0.06\n",
      "epoch 6, step 32250/55000, batch loss = 0.04\n",
      "epoch 6, step 32500/55000, batch loss = 0.05\n",
      "Train accuracy = 99.47\n",
      "epoch 6, step 32750/55000, batch loss = 0.04\n",
      "epoch 6, step 33000/55000, batch loss = 0.04\n",
      "epoch 6, step 33250/55000, batch loss = 0.07\n",
      "epoch 6, step 33500/55000, batch loss = 0.06\n",
      "epoch 6, step 33750/55000, batch loss = 0.07\n",
      "epoch 6, step 34000/55000, batch loss = 0.07\n",
      "epoch 6, step 34250/55000, batch loss = 0.06\n",
      "epoch 6, step 34500/55000, batch loss = 0.04\n",
      "epoch 6, step 34750/55000, batch loss = 0.04\n",
      "epoch 6, step 35000/55000, batch loss = 0.04\n",
      "Train accuracy = 99.46\n",
      "epoch 6, step 35250/55000, batch loss = 0.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6, step 35500/55000, batch loss = 0.04\n",
      "epoch 6, step 35750/55000, batch loss = 0.05\n",
      "epoch 6, step 36000/55000, batch loss = 0.07\n",
      "epoch 6, step 36250/55000, batch loss = 0.08\n",
      "epoch 6, step 36500/55000, batch loss = 0.07\n",
      "epoch 6, step 36750/55000, batch loss = 0.06\n",
      "epoch 6, step 37000/55000, batch loss = 0.05\n",
      "epoch 6, step 37250/55000, batch loss = 0.04\n",
      "epoch 6, step 37500/55000, batch loss = 0.05\n",
      "Train accuracy = 99.47\n",
      "epoch 6, step 37750/55000, batch loss = 0.06\n",
      "epoch 6, step 38000/55000, batch loss = 0.05\n",
      "epoch 6, step 38250/55000, batch loss = 0.06\n",
      "epoch 6, step 38500/55000, batch loss = 0.14\n",
      "epoch 6, step 38750/55000, batch loss = 0.04\n",
      "epoch 6, step 39000/55000, batch loss = 0.04\n",
      "epoch 6, step 39250/55000, batch loss = 0.15\n",
      "epoch 6, step 39500/55000, batch loss = 0.04\n",
      "epoch 6, step 39750/55000, batch loss = 0.05\n",
      "epoch 6, step 40000/55000, batch loss = 0.04\n",
      "Train accuracy = 99.47\n",
      "epoch 6, step 40250/55000, batch loss = 0.04\n",
      "epoch 6, step 40500/55000, batch loss = 0.04\n",
      "epoch 6, step 40750/55000, batch loss = 0.06\n",
      "epoch 6, step 41000/55000, batch loss = 0.04\n",
      "epoch 6, step 41250/55000, batch loss = 0.10\n",
      "epoch 6, step 41500/55000, batch loss = 0.05\n",
      "epoch 6, step 41750/55000, batch loss = 0.05\n",
      "epoch 6, step 42000/55000, batch loss = 0.04\n",
      "epoch 6, step 42250/55000, batch loss = 0.05\n",
      "epoch 6, step 42500/55000, batch loss = 0.06\n",
      "Train accuracy = 99.46\n",
      "epoch 6, step 42750/55000, batch loss = 0.07\n",
      "epoch 6, step 43000/55000, batch loss = 0.04\n",
      "epoch 6, step 43250/55000, batch loss = 0.04\n",
      "epoch 6, step 43500/55000, batch loss = 0.04\n",
      "epoch 6, step 43750/55000, batch loss = 0.05\n",
      "epoch 6, step 44000/55000, batch loss = 0.04\n",
      "epoch 6, step 44250/55000, batch loss = 0.05\n",
      "epoch 6, step 44500/55000, batch loss = 0.07\n",
      "epoch 6, step 44750/55000, batch loss = 0.05\n",
      "epoch 6, step 45000/55000, batch loss = 0.05\n",
      "Train accuracy = 99.46\n",
      "epoch 6, step 45250/55000, batch loss = 0.08\n",
      "epoch 6, step 45500/55000, batch loss = 0.05\n",
      "epoch 6, step 45750/55000, batch loss = 0.06\n",
      "epoch 6, step 46000/55000, batch loss = 0.04\n",
      "epoch 6, step 46250/55000, batch loss = 0.27\n",
      "epoch 6, step 46500/55000, batch loss = 0.04\n",
      "epoch 6, step 46750/55000, batch loss = 0.07\n",
      "epoch 6, step 47000/55000, batch loss = 0.07\n",
      "epoch 6, step 47250/55000, batch loss = 0.09\n",
      "epoch 6, step 47500/55000, batch loss = 0.10\n",
      "Train accuracy = 99.46\n",
      "epoch 6, step 47750/55000, batch loss = 0.05\n",
      "epoch 6, step 48000/55000, batch loss = 0.04\n",
      "epoch 6, step 48250/55000, batch loss = 0.07\n",
      "epoch 6, step 48500/55000, batch loss = 0.04\n",
      "epoch 6, step 48750/55000, batch loss = 0.06\n",
      "epoch 6, step 49000/55000, batch loss = 0.11\n",
      "epoch 6, step 49250/55000, batch loss = 0.04\n",
      "epoch 6, step 49500/55000, batch loss = 0.04\n",
      "epoch 6, step 49750/55000, batch loss = 0.05\n",
      "epoch 6, step 50000/55000, batch loss = 0.08\n",
      "Train accuracy = 99.46\n",
      "epoch 6, step 50250/55000, batch loss = 0.15\n",
      "epoch 6, step 50500/55000, batch loss = 0.09\n",
      "epoch 6, step 50750/55000, batch loss = 0.12\n",
      "epoch 6, step 51000/55000, batch loss = 0.05\n",
      "epoch 6, step 51250/55000, batch loss = 0.04\n",
      "epoch 6, step 51500/55000, batch loss = 0.05\n",
      "epoch 6, step 51750/55000, batch loss = 0.06\n",
      "epoch 6, step 52000/55000, batch loss = 0.04\n",
      "epoch 6, step 52250/55000, batch loss = 0.14\n",
      "epoch 6, step 52500/55000, batch loss = 0.05\n",
      "Train accuracy = 99.46\n",
      "epoch 6, step 52750/55000, batch loss = 0.04\n",
      "epoch 6, step 53000/55000, batch loss = 0.05\n",
      "epoch 6, step 53250/55000, batch loss = 0.04\n",
      "epoch 6, step 53500/55000, batch loss = 0.06\n",
      "epoch 6, step 53750/55000, batch loss = 0.05\n",
      "epoch 6, step 54000/55000, batch loss = 0.08\n",
      "epoch 6, step 54250/55000, batch loss = 0.05\n",
      "epoch 6, step 54500/55000, batch loss = 0.05\n",
      "epoch 6, step 54750/55000, batch loss = 0.05\n",
      "Train accuracy = 99.45\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 99.08\n",
      "Validation avg loss = 0.07\n",
      "\n",
      "epoch 7, step 0/55000, batch loss = 0.10\n",
      "epoch 7, step 250/55000, batch loss = 0.06\n",
      "epoch 7, step 500/55000, batch loss = 0.05\n",
      "epoch 7, step 750/55000, batch loss = 0.12\n",
      "epoch 7, step 1000/55000, batch loss = 0.04\n",
      "epoch 7, step 1250/55000, batch loss = 0.05\n",
      "epoch 7, step 1500/55000, batch loss = 0.07\n",
      "epoch 7, step 1750/55000, batch loss = 0.06\n",
      "epoch 7, step 2000/55000, batch loss = 0.06\n",
      "epoch 7, step 2250/55000, batch loss = 0.09\n",
      "epoch 7, step 2500/55000, batch loss = 0.05\n",
      "Train accuracy = 99.53\n",
      "epoch 7, step 2750/55000, batch loss = 0.07\n",
      "epoch 7, step 3000/55000, batch loss = 0.06\n",
      "epoch 7, step 3250/55000, batch loss = 0.05\n",
      "epoch 7, step 3500/55000, batch loss = 0.08\n",
      "epoch 7, step 3750/55000, batch loss = 0.08\n",
      "epoch 7, step 4000/55000, batch loss = 0.05\n",
      "epoch 7, step 4250/55000, batch loss = 0.04\n",
      "epoch 7, step 4500/55000, batch loss = 0.04\n",
      "epoch 7, step 4750/55000, batch loss = 0.13\n",
      "epoch 7, step 5000/55000, batch loss = 0.04\n",
      "Train accuracy = 99.39\n",
      "epoch 7, step 5250/55000, batch loss = 0.05\n",
      "epoch 7, step 5500/55000, batch loss = 0.08\n",
      "epoch 7, step 5750/55000, batch loss = 0.04\n",
      "epoch 7, step 6000/55000, batch loss = 0.04\n",
      "epoch 7, step 6250/55000, batch loss = 0.05\n",
      "epoch 7, step 6500/55000, batch loss = 0.07\n",
      "epoch 7, step 6750/55000, batch loss = 0.05\n",
      "epoch 7, step 7000/55000, batch loss = 0.04\n",
      "epoch 7, step 7250/55000, batch loss = 0.04\n",
      "epoch 7, step 7500/55000, batch loss = 0.05\n",
      "Train accuracy = 99.47\n",
      "epoch 7, step 7750/55000, batch loss = 0.04\n",
      "epoch 7, step 8000/55000, batch loss = 0.06\n",
      "epoch 7, step 8250/55000, batch loss = 0.04\n",
      "epoch 7, step 8500/55000, batch loss = 0.04\n",
      "epoch 7, step 8750/55000, batch loss = 0.04\n",
      "epoch 7, step 9000/55000, batch loss = 0.05\n",
      "epoch 7, step 9250/55000, batch loss = 0.05\n",
      "epoch 7, step 9500/55000, batch loss = 0.05\n",
      "epoch 7, step 9750/55000, batch loss = 0.04\n",
      "epoch 7, step 10000/55000, batch loss = 0.08\n",
      "Train accuracy = 99.46\n",
      "epoch 7, step 10250/55000, batch loss = 0.05\n",
      "epoch 7, step 10500/55000, batch loss = 0.05\n",
      "epoch 7, step 10750/55000, batch loss = 0.05\n",
      "epoch 7, step 11000/55000, batch loss = 0.09\n",
      "epoch 7, step 11250/55000, batch loss = 0.05\n",
      "epoch 7, step 11500/55000, batch loss = 0.08\n",
      "epoch 7, step 11750/55000, batch loss = 0.04\n",
      "epoch 7, step 12000/55000, batch loss = 0.04\n",
      "epoch 7, step 12250/55000, batch loss = 0.05\n",
      "epoch 7, step 12500/55000, batch loss = 0.04\n",
      "Train accuracy = 99.47\n",
      "epoch 7, step 12750/55000, batch loss = 0.05\n",
      "epoch 7, step 13000/55000, batch loss = 0.05\n",
      "epoch 7, step 13250/55000, batch loss = 0.12\n",
      "epoch 7, step 13500/55000, batch loss = 0.04\n",
      "epoch 7, step 13750/55000, batch loss = 0.04\n",
      "epoch 7, step 14000/55000, batch loss = 0.05\n",
      "epoch 7, step 14250/55000, batch loss = 0.06\n",
      "epoch 7, step 14500/55000, batch loss = 0.05\n",
      "epoch 7, step 14750/55000, batch loss = 0.05\n",
      "epoch 7, step 15000/55000, batch loss = 0.08\n",
      "Train accuracy = 99.49\n",
      "epoch 7, step 15250/55000, batch loss = 0.05\n",
      "epoch 7, step 15500/55000, batch loss = 0.06\n",
      "epoch 7, step 15750/55000, batch loss = 0.06\n",
      "epoch 7, step 16000/55000, batch loss = 0.04\n",
      "epoch 7, step 16250/55000, batch loss = 0.04\n",
      "epoch 7, step 16500/55000, batch loss = 0.05\n",
      "epoch 7, step 16750/55000, batch loss = 0.04\n",
      "epoch 7, step 17000/55000, batch loss = 0.05\n",
      "epoch 7, step 17250/55000, batch loss = 0.04\n",
      "epoch 7, step 17500/55000, batch loss = 0.05\n",
      "Train accuracy = 99.48\n",
      "epoch 7, step 17750/55000, batch loss = 0.06\n",
      "epoch 7, step 18000/55000, batch loss = 0.07\n",
      "epoch 7, step 18250/55000, batch loss = 0.07\n",
      "epoch 7, step 18500/55000, batch loss = 0.05\n",
      "epoch 7, step 18750/55000, batch loss = 0.05\n",
      "epoch 7, step 19000/55000, batch loss = 0.06\n",
      "epoch 7, step 19250/55000, batch loss = 0.06\n",
      "epoch 7, step 19500/55000, batch loss = 0.05\n",
      "epoch 7, step 19750/55000, batch loss = 0.04\n",
      "epoch 7, step 20000/55000, batch loss = 0.05\n",
      "Train accuracy = 99.49\n",
      "epoch 7, step 20250/55000, batch loss = 0.06\n",
      "epoch 7, step 20500/55000, batch loss = 0.04\n",
      "epoch 7, step 20750/55000, batch loss = 0.05\n",
      "epoch 7, step 21000/55000, batch loss = 0.04\n",
      "epoch 7, step 21250/55000, batch loss = 0.04\n",
      "epoch 7, step 21500/55000, batch loss = 0.04\n",
      "epoch 7, step 21750/55000, batch loss = 0.04\n",
      "epoch 7, step 22000/55000, batch loss = 0.06\n",
      "epoch 7, step 22250/55000, batch loss = 0.13\n",
      "epoch 7, step 22500/55000, batch loss = 0.04\n",
      "Train accuracy = 99.50\n",
      "epoch 7, step 22750/55000, batch loss = 0.04\n",
      "epoch 7, step 23000/55000, batch loss = 0.05\n",
      "epoch 7, step 23250/55000, batch loss = 0.05\n",
      "epoch 7, step 23500/55000, batch loss = 0.06\n",
      "epoch 7, step 23750/55000, batch loss = 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7, step 24000/55000, batch loss = 0.04\n",
      "epoch 7, step 24250/55000, batch loss = 0.06\n",
      "epoch 7, step 24500/55000, batch loss = 0.15\n",
      "epoch 7, step 24750/55000, batch loss = 0.04\n",
      "epoch 7, step 25000/55000, batch loss = 0.05\n",
      "Train accuracy = 99.49\n",
      "epoch 7, step 25250/55000, batch loss = 0.07\n",
      "epoch 7, step 25500/55000, batch loss = 0.05\n",
      "epoch 7, step 25750/55000, batch loss = 0.04\n",
      "epoch 7, step 26000/55000, batch loss = 0.05\n",
      "epoch 7, step 26250/55000, batch loss = 0.06\n",
      "epoch 7, step 26500/55000, batch loss = 0.05\n",
      "epoch 7, step 26750/55000, batch loss = 0.05\n",
      "epoch 7, step 27000/55000, batch loss = 0.08\n",
      "epoch 7, step 27250/55000, batch loss = 0.04\n",
      "epoch 7, step 27500/55000, batch loss = 0.05\n",
      "Train accuracy = 99.48\n",
      "epoch 7, step 27750/55000, batch loss = 0.09\n",
      "epoch 7, step 28000/55000, batch loss = 0.06\n",
      "epoch 7, step 28250/55000, batch loss = 0.05\n",
      "epoch 7, step 28500/55000, batch loss = 0.05\n",
      "epoch 7, step 28750/55000, batch loss = 0.05\n",
      "epoch 7, step 29000/55000, batch loss = 0.04\n",
      "epoch 7, step 29250/55000, batch loss = 0.05\n",
      "epoch 7, step 29500/55000, batch loss = 0.11\n",
      "epoch 7, step 29750/55000, batch loss = 0.04\n",
      "epoch 7, step 30000/55000, batch loss = 0.04\n",
      "Train accuracy = 99.47\n",
      "epoch 7, step 30250/55000, batch loss = 0.04\n",
      "epoch 7, step 30500/55000, batch loss = 0.06\n",
      "epoch 7, step 30750/55000, batch loss = 0.04\n",
      "epoch 7, step 31000/55000, batch loss = 0.05\n",
      "epoch 7, step 31250/55000, batch loss = 0.04\n",
      "epoch 7, step 31500/55000, batch loss = 0.07\n",
      "epoch 7, step 31750/55000, batch loss = 0.13\n",
      "epoch 7, step 32000/55000, batch loss = 0.04\n",
      "epoch 7, step 32250/55000, batch loss = 0.05\n",
      "epoch 7, step 32500/55000, batch loss = 0.06\n",
      "Train accuracy = 99.47\n",
      "epoch 7, step 32750/55000, batch loss = 0.05\n",
      "epoch 7, step 33000/55000, batch loss = 0.04\n",
      "epoch 7, step 33250/55000, batch loss = 0.06\n",
      "epoch 7, step 33500/55000, batch loss = 0.04\n",
      "epoch 7, step 33750/55000, batch loss = 0.04\n",
      "epoch 7, step 34000/55000, batch loss = 0.05\n",
      "epoch 7, step 34250/55000, batch loss = 0.10\n",
      "epoch 7, step 34500/55000, batch loss = 0.06\n",
      "epoch 7, step 34750/55000, batch loss = 0.06\n",
      "epoch 7, step 35000/55000, batch loss = 0.07\n",
      "Train accuracy = 99.46\n",
      "epoch 7, step 35250/55000, batch loss = 0.04\n",
      "epoch 7, step 35500/55000, batch loss = 0.05\n",
      "epoch 7, step 35750/55000, batch loss = 0.04\n",
      "epoch 7, step 36000/55000, batch loss = 0.11\n",
      "epoch 7, step 36250/55000, batch loss = 0.10\n",
      "epoch 7, step 36500/55000, batch loss = 0.06\n",
      "epoch 7, step 36750/55000, batch loss = 0.04\n",
      "epoch 7, step 37000/55000, batch loss = 0.04\n",
      "epoch 7, step 37250/55000, batch loss = 0.05\n",
      "epoch 7, step 37500/55000, batch loss = 0.16\n",
      "Train accuracy = 99.46\n",
      "epoch 7, step 37750/55000, batch loss = 0.09\n",
      "epoch 7, step 38000/55000, batch loss = 0.05\n",
      "epoch 7, step 38250/55000, batch loss = 0.05\n",
      "epoch 7, step 38500/55000, batch loss = 0.08\n",
      "epoch 7, step 38750/55000, batch loss = 0.05\n",
      "epoch 7, step 39000/55000, batch loss = 0.05\n",
      "epoch 7, step 39250/55000, batch loss = 0.04\n",
      "epoch 7, step 39500/55000, batch loss = 0.05\n",
      "epoch 7, step 39750/55000, batch loss = 0.08\n",
      "epoch 7, step 40000/55000, batch loss = 0.05\n",
      "Train accuracy = 99.47\n",
      "epoch 7, step 40250/55000, batch loss = 0.05\n",
      "epoch 7, step 40500/55000, batch loss = 0.05\n",
      "epoch 7, step 40750/55000, batch loss = 0.05\n",
      "epoch 7, step 41000/55000, batch loss = 0.05\n",
      "epoch 7, step 41250/55000, batch loss = 0.07\n",
      "epoch 7, step 41500/55000, batch loss = 0.04\n",
      "epoch 7, step 41750/55000, batch loss = 0.07\n",
      "epoch 7, step 42000/55000, batch loss = 0.04\n",
      "epoch 7, step 42250/55000, batch loss = 0.04\n",
      "epoch 7, step 42500/55000, batch loss = 0.05\n",
      "Train accuracy = 99.47\n",
      "epoch 7, step 42750/55000, batch loss = 0.08\n",
      "epoch 7, step 43000/55000, batch loss = 0.04\n",
      "epoch 7, step 43250/55000, batch loss = 0.09\n",
      "epoch 7, step 43500/55000, batch loss = 0.05\n",
      "epoch 7, step 43750/55000, batch loss = 0.16\n",
      "epoch 7, step 44000/55000, batch loss = 0.05\n",
      "epoch 7, step 44250/55000, batch loss = 0.06\n",
      "epoch 7, step 44500/55000, batch loss = 0.04\n",
      "epoch 7, step 44750/55000, batch loss = 0.04\n",
      "epoch 7, step 45000/55000, batch loss = 0.04\n",
      "Train accuracy = 99.47\n",
      "epoch 7, step 45250/55000, batch loss = 0.06\n",
      "epoch 7, step 45500/55000, batch loss = 0.06\n",
      "epoch 7, step 45750/55000, batch loss = 0.05\n",
      "epoch 7, step 46000/55000, batch loss = 0.05\n",
      "epoch 7, step 46250/55000, batch loss = 0.05\n",
      "epoch 7, step 46500/55000, batch loss = 0.08\n",
      "epoch 7, step 46750/55000, batch loss = 0.04\n",
      "epoch 7, step 47000/55000, batch loss = 0.04\n",
      "epoch 7, step 47250/55000, batch loss = 0.07\n",
      "epoch 7, step 47500/55000, batch loss = 0.05\n",
      "Train accuracy = 99.47\n",
      "epoch 7, step 47750/55000, batch loss = 0.04\n",
      "epoch 7, step 48000/55000, batch loss = 0.04\n",
      "epoch 7, step 48250/55000, batch loss = 0.06\n",
      "epoch 7, step 48500/55000, batch loss = 0.05\n",
      "epoch 7, step 48750/55000, batch loss = 0.05\n",
      "epoch 7, step 49000/55000, batch loss = 0.08\n",
      "epoch 7, step 49250/55000, batch loss = 0.05\n",
      "epoch 7, step 49500/55000, batch loss = 0.04\n",
      "epoch 7, step 49750/55000, batch loss = 0.11\n",
      "epoch 7, step 50000/55000, batch loss = 0.07\n",
      "Train accuracy = 99.48\n",
      "epoch 7, step 50250/55000, batch loss = 0.04\n",
      "epoch 7, step 50500/55000, batch loss = 0.08\n",
      "epoch 7, step 50750/55000, batch loss = 0.04\n",
      "epoch 7, step 51000/55000, batch loss = 0.05\n",
      "epoch 7, step 51250/55000, batch loss = 0.04\n",
      "epoch 7, step 51500/55000, batch loss = 0.07\n",
      "epoch 7, step 51750/55000, batch loss = 0.04\n",
      "epoch 7, step 52000/55000, batch loss = 0.07\n",
      "epoch 7, step 52250/55000, batch loss = 0.06\n",
      "epoch 7, step 52500/55000, batch loss = 0.05\n",
      "Train accuracy = 99.46\n",
      "epoch 7, step 52750/55000, batch loss = 0.05\n",
      "epoch 7, step 53000/55000, batch loss = 0.07\n",
      "epoch 7, step 53250/55000, batch loss = 0.04\n",
      "epoch 7, step 53500/55000, batch loss = 0.04\n",
      "epoch 7, step 53750/55000, batch loss = 0.06\n",
      "epoch 7, step 54000/55000, batch loss = 0.05\n",
      "epoch 7, step 54250/55000, batch loss = 0.05\n",
      "epoch 7, step 54500/55000, batch loss = 0.09\n",
      "epoch 7, step 54750/55000, batch loss = 0.05\n",
      "Train accuracy = 99.46\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 99.08\n",
      "Validation avg loss = 0.07\n",
      "\n",
      "epoch 8, step 0/55000, batch loss = 0.05\n",
      "epoch 8, step 250/55000, batch loss = 0.16\n",
      "epoch 8, step 500/55000, batch loss = 0.06\n",
      "epoch 8, step 750/55000, batch loss = 0.04\n",
      "epoch 8, step 1000/55000, batch loss = 0.06\n",
      "epoch 8, step 1250/55000, batch loss = 0.07\n",
      "epoch 8, step 1500/55000, batch loss = 0.04\n",
      "epoch 8, step 1750/55000, batch loss = 0.06\n",
      "epoch 8, step 2000/55000, batch loss = 0.04\n",
      "epoch 8, step 2250/55000, batch loss = 0.05\n",
      "epoch 8, step 2500/55000, batch loss = 0.06\n",
      "Train accuracy = 99.29\n",
      "epoch 8, step 2750/55000, batch loss = 0.04\n",
      "epoch 8, step 3000/55000, batch loss = 0.04\n",
      "epoch 8, step 3250/55000, batch loss = 0.05\n",
      "epoch 8, step 3500/55000, batch loss = 0.13\n",
      "epoch 8, step 3750/55000, batch loss = 0.09\n",
      "epoch 8, step 4000/55000, batch loss = 0.06\n",
      "epoch 8, step 4250/55000, batch loss = 0.08\n",
      "epoch 8, step 4500/55000, batch loss = 0.05\n",
      "epoch 8, step 4750/55000, batch loss = 0.04\n",
      "epoch 8, step 5000/55000, batch loss = 0.05\n",
      "Train accuracy = 99.29\n",
      "epoch 8, step 5250/55000, batch loss = 0.08\n",
      "epoch 8, step 5500/55000, batch loss = 0.07\n",
      "epoch 8, step 5750/55000, batch loss = 0.12\n",
      "epoch 8, step 6000/55000, batch loss = 0.07\n",
      "epoch 8, step 6250/55000, batch loss = 0.04\n",
      "epoch 8, step 6500/55000, batch loss = 0.04\n",
      "epoch 8, step 6750/55000, batch loss = 0.05\n",
      "epoch 8, step 7000/55000, batch loss = 0.06\n",
      "epoch 8, step 7250/55000, batch loss = 0.04\n",
      "epoch 8, step 7500/55000, batch loss = 0.04\n",
      "Train accuracy = 99.39\n",
      "epoch 8, step 7750/55000, batch loss = 0.05\n",
      "epoch 8, step 8000/55000, batch loss = 0.04\n",
      "epoch 8, step 8250/55000, batch loss = 0.05\n",
      "epoch 8, step 8500/55000, batch loss = 0.05\n",
      "epoch 8, step 8750/55000, batch loss = 0.06\n",
      "epoch 8, step 9000/55000, batch loss = 0.08\n",
      "epoch 8, step 9250/55000, batch loss = 0.05\n",
      "epoch 8, step 9500/55000, batch loss = 0.05\n",
      "epoch 8, step 9750/55000, batch loss = 0.05\n",
      "epoch 8, step 10000/55000, batch loss = 0.05\n",
      "Train accuracy = 99.38\n",
      "epoch 8, step 10250/55000, batch loss = 0.04\n",
      "epoch 8, step 10500/55000, batch loss = 0.08\n",
      "epoch 8, step 10750/55000, batch loss = 0.08\n",
      "epoch 8, step 11000/55000, batch loss = 0.04\n",
      "epoch 8, step 11250/55000, batch loss = 0.04\n",
      "epoch 8, step 11500/55000, batch loss = 0.05\n",
      "epoch 8, step 11750/55000, batch loss = 0.05\n",
      "epoch 8, step 12000/55000, batch loss = 0.04\n",
      "epoch 8, step 12250/55000, batch loss = 0.05\n",
      "epoch 8, step 12500/55000, batch loss = 0.04\n",
      "Train accuracy = 99.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8, step 12750/55000, batch loss = 0.04\n",
      "epoch 8, step 13000/55000, batch loss = 0.06\n",
      "epoch 8, step 13250/55000, batch loss = 0.04\n",
      "epoch 8, step 13500/55000, batch loss = 0.05\n",
      "epoch 8, step 13750/55000, batch loss = 0.06\n",
      "epoch 8, step 14000/55000, batch loss = 0.06\n",
      "epoch 8, step 14250/55000, batch loss = 0.05\n",
      "epoch 8, step 14500/55000, batch loss = 0.06\n",
      "epoch 8, step 14750/55000, batch loss = 0.05\n",
      "epoch 8, step 15000/55000, batch loss = 0.15\n",
      "Train accuracy = 99.40\n",
      "epoch 8, step 15250/55000, batch loss = 0.04\n",
      "epoch 8, step 15500/55000, batch loss = 0.19\n",
      "epoch 8, step 15750/55000, batch loss = 0.04\n",
      "epoch 8, step 16000/55000, batch loss = 0.04\n",
      "epoch 8, step 16250/55000, batch loss = 0.06\n",
      "epoch 8, step 16500/55000, batch loss = 0.06\n",
      "epoch 8, step 16750/55000, batch loss = 0.07\n",
      "epoch 8, step 17000/55000, batch loss = 0.06\n",
      "epoch 8, step 17250/55000, batch loss = 0.09\n",
      "epoch 8, step 17500/55000, batch loss = 0.08\n",
      "Train accuracy = 99.42\n",
      "epoch 8, step 17750/55000, batch loss = 0.08\n",
      "epoch 8, step 18000/55000, batch loss = 0.05\n",
      "epoch 8, step 18250/55000, batch loss = 0.05\n",
      "epoch 8, step 18500/55000, batch loss = 0.06\n",
      "epoch 8, step 18750/55000, batch loss = 0.04\n",
      "epoch 8, step 19000/55000, batch loss = 0.04\n",
      "epoch 8, step 19250/55000, batch loss = 0.06\n",
      "epoch 8, step 19500/55000, batch loss = 0.04\n",
      "epoch 8, step 19750/55000, batch loss = 0.06\n",
      "epoch 8, step 20000/55000, batch loss = 0.05\n",
      "Train accuracy = 99.43\n",
      "epoch 8, step 20250/55000, batch loss = 0.05\n",
      "epoch 8, step 20500/55000, batch loss = 0.04\n",
      "epoch 8, step 20750/55000, batch loss = 0.05\n",
      "epoch 8, step 21000/55000, batch loss = 0.04\n",
      "epoch 8, step 21250/55000, batch loss = 0.06\n",
      "epoch 8, step 21500/55000, batch loss = 0.05\n",
      "epoch 8, step 21750/55000, batch loss = 0.04\n",
      "epoch 8, step 22000/55000, batch loss = 0.04\n",
      "epoch 8, step 22250/55000, batch loss = 0.05\n",
      "epoch 8, step 22500/55000, batch loss = 0.05\n",
      "Train accuracy = 99.43\n",
      "epoch 8, step 22750/55000, batch loss = 0.04\n",
      "epoch 8, step 23000/55000, batch loss = 0.05\n",
      "epoch 8, step 23250/55000, batch loss = 0.05\n",
      "epoch 8, step 23500/55000, batch loss = 0.14\n",
      "epoch 8, step 23750/55000, batch loss = 0.04\n",
      "epoch 8, step 24000/55000, batch loss = 0.04\n",
      "epoch 8, step 24250/55000, batch loss = 0.04\n",
      "epoch 8, step 24500/55000, batch loss = 0.04\n",
      "epoch 8, step 24750/55000, batch loss = 0.04\n",
      "epoch 8, step 25000/55000, batch loss = 0.09\n",
      "Train accuracy = 99.45\n",
      "epoch 8, step 25250/55000, batch loss = 0.04\n",
      "epoch 8, step 25500/55000, batch loss = 0.05\n",
      "epoch 8, step 25750/55000, batch loss = 0.07\n",
      "epoch 8, step 26000/55000, batch loss = 0.07\n",
      "epoch 8, step 26250/55000, batch loss = 0.05\n",
      "epoch 8, step 26500/55000, batch loss = 0.04\n",
      "epoch 8, step 26750/55000, batch loss = 0.04\n",
      "epoch 8, step 27000/55000, batch loss = 0.05\n",
      "epoch 8, step 27250/55000, batch loss = 0.04\n",
      "epoch 8, step 27500/55000, batch loss = 0.04\n",
      "Train accuracy = 99.44\n",
      "epoch 8, step 27750/55000, batch loss = 0.05\n",
      "epoch 8, step 28000/55000, batch loss = 0.06\n",
      "epoch 8, step 28250/55000, batch loss = 0.06\n",
      "epoch 8, step 28500/55000, batch loss = 0.04\n",
      "epoch 8, step 28750/55000, batch loss = 0.05\n",
      "epoch 8, step 29000/55000, batch loss = 0.09\n",
      "epoch 8, step 29250/55000, batch loss = 0.07\n",
      "epoch 8, step 29500/55000, batch loss = 0.04\n",
      "epoch 8, step 29750/55000, batch loss = 0.05\n",
      "epoch 8, step 30000/55000, batch loss = 0.04\n",
      "Train accuracy = 99.46\n",
      "epoch 8, step 30250/55000, batch loss = 0.08\n",
      "epoch 8, step 30500/55000, batch loss = 0.05\n",
      "epoch 8, step 30750/55000, batch loss = 0.10\n",
      "epoch 8, step 31000/55000, batch loss = 0.05\n",
      "epoch 8, step 31250/55000, batch loss = 0.04\n",
      "epoch 8, step 31500/55000, batch loss = 0.05\n",
      "epoch 8, step 31750/55000, batch loss = 0.07\n",
      "epoch 8, step 32000/55000, batch loss = 0.05\n",
      "epoch 8, step 32250/55000, batch loss = 0.06\n",
      "epoch 8, step 32500/55000, batch loss = 0.06\n",
      "Train accuracy = 99.47\n",
      "epoch 8, step 32750/55000, batch loss = 0.04\n",
      "epoch 8, step 33000/55000, batch loss = 0.04\n",
      "epoch 8, step 33250/55000, batch loss = 0.06\n",
      "epoch 8, step 33500/55000, batch loss = 0.06\n",
      "epoch 8, step 33750/55000, batch loss = 0.05\n",
      "epoch 8, step 34000/55000, batch loss = 0.04\n",
      "epoch 8, step 34250/55000, batch loss = 0.05\n",
      "epoch 8, step 34500/55000, batch loss = 0.07\n",
      "epoch 8, step 34750/55000, batch loss = 0.05\n",
      "epoch 8, step 35000/55000, batch loss = 0.04\n",
      "Train accuracy = 99.47\n",
      "epoch 8, step 35250/55000, batch loss = 0.06\n",
      "epoch 8, step 35500/55000, batch loss = 0.05\n",
      "epoch 8, step 35750/55000, batch loss = 0.05\n",
      "epoch 8, step 36000/55000, batch loss = 0.05\n",
      "epoch 8, step 36250/55000, batch loss = 0.04\n",
      "epoch 8, step 36500/55000, batch loss = 0.05\n",
      "epoch 8, step 36750/55000, batch loss = 0.08\n",
      "epoch 8, step 37000/55000, batch loss = 0.06\n",
      "epoch 8, step 37250/55000, batch loss = 0.05\n",
      "epoch 8, step 37500/55000, batch loss = 0.04\n",
      "Train accuracy = 99.46\n",
      "epoch 8, step 37750/55000, batch loss = 0.05\n",
      "epoch 8, step 38000/55000, batch loss = 0.04\n",
      "epoch 8, step 38250/55000, batch loss = 0.06\n",
      "epoch 8, step 38500/55000, batch loss = 0.05\n",
      "epoch 8, step 38750/55000, batch loss = 0.05\n",
      "epoch 8, step 39000/55000, batch loss = 0.10\n",
      "epoch 8, step 39250/55000, batch loss = 0.04\n",
      "epoch 8, step 39500/55000, batch loss = 0.07\n",
      "epoch 8, step 39750/55000, batch loss = 0.14\n",
      "epoch 8, step 40000/55000, batch loss = 0.04\n",
      "Train accuracy = 99.49\n",
      "epoch 8, step 40250/55000, batch loss = 0.13\n",
      "epoch 8, step 40500/55000, batch loss = 0.05\n",
      "epoch 8, step 40750/55000, batch loss = 0.05\n",
      "epoch 8, step 41000/55000, batch loss = 0.04\n",
      "epoch 8, step 41250/55000, batch loss = 0.04\n",
      "epoch 8, step 41500/55000, batch loss = 0.04\n",
      "epoch 8, step 41750/55000, batch loss = 0.05\n",
      "epoch 8, step 42000/55000, batch loss = 0.04\n",
      "epoch 8, step 42250/55000, batch loss = 0.04\n",
      "epoch 8, step 42500/55000, batch loss = 0.08\n",
      "Train accuracy = 99.48\n",
      "epoch 8, step 42750/55000, batch loss = 0.04\n",
      "epoch 8, step 43000/55000, batch loss = 0.11\n",
      "epoch 8, step 43250/55000, batch loss = 0.06\n",
      "epoch 8, step 43500/55000, batch loss = 0.16\n",
      "epoch 8, step 43750/55000, batch loss = 0.06\n",
      "epoch 8, step 44000/55000, batch loss = 0.05\n",
      "epoch 8, step 44250/55000, batch loss = 0.04\n",
      "epoch 8, step 44500/55000, batch loss = 0.05\n",
      "epoch 8, step 44750/55000, batch loss = 0.04\n",
      "epoch 8, step 45000/55000, batch loss = 0.05\n",
      "Train accuracy = 99.48\n",
      "epoch 8, step 45250/55000, batch loss = 0.07\n",
      "epoch 8, step 45500/55000, batch loss = 0.05\n",
      "epoch 8, step 45750/55000, batch loss = 0.05\n",
      "epoch 8, step 46000/55000, batch loss = 0.05\n",
      "epoch 8, step 46250/55000, batch loss = 0.06\n",
      "epoch 8, step 46500/55000, batch loss = 0.04\n",
      "epoch 8, step 46750/55000, batch loss = 0.06\n",
      "epoch 8, step 47000/55000, batch loss = 0.04\n",
      "epoch 8, step 47250/55000, batch loss = 0.04\n",
      "epoch 8, step 47500/55000, batch loss = 0.05\n",
      "Train accuracy = 99.48\n",
      "epoch 8, step 47750/55000, batch loss = 0.05\n",
      "epoch 8, step 48000/55000, batch loss = 0.04\n",
      "epoch 8, step 48250/55000, batch loss = 0.04\n",
      "epoch 8, step 48500/55000, batch loss = 0.05\n",
      "epoch 8, step 48750/55000, batch loss = 0.08\n",
      "epoch 8, step 49000/55000, batch loss = 0.04\n",
      "epoch 8, step 49250/55000, batch loss = 0.10\n",
      "epoch 8, step 49500/55000, batch loss = 0.07\n",
      "epoch 8, step 49750/55000, batch loss = 0.12\n",
      "epoch 8, step 50000/55000, batch loss = 0.07\n",
      "Train accuracy = 99.47\n",
      "epoch 8, step 50250/55000, batch loss = 0.04\n",
      "epoch 8, step 50500/55000, batch loss = 0.06\n",
      "epoch 8, step 50750/55000, batch loss = 0.21\n",
      "epoch 8, step 51000/55000, batch loss = 0.04\n",
      "epoch 8, step 51250/55000, batch loss = 0.04\n",
      "epoch 8, step 51500/55000, batch loss = 0.05\n",
      "epoch 8, step 51750/55000, batch loss = 0.05\n",
      "epoch 8, step 52000/55000, batch loss = 0.04\n",
      "epoch 8, step 52250/55000, batch loss = 0.06\n",
      "epoch 8, step 52500/55000, batch loss = 0.04\n",
      "Train accuracy = 99.46\n",
      "epoch 8, step 52750/55000, batch loss = 0.05\n",
      "epoch 8, step 53000/55000, batch loss = 0.05\n",
      "epoch 8, step 53250/55000, batch loss = 0.06\n",
      "epoch 8, step 53500/55000, batch loss = 0.05\n",
      "epoch 8, step 53750/55000, batch loss = 0.04\n",
      "epoch 8, step 54000/55000, batch loss = 0.09\n",
      "epoch 8, step 54250/55000, batch loss = 0.06\n",
      "epoch 8, step 54500/55000, batch loss = 0.06\n",
      "epoch 8, step 54750/55000, batch loss = 0.06\n",
      "Train accuracy = 99.46\n",
      "\n",
      "Running evaluation:  Validation\n",
      "Validation accuracy = 99.08\n",
      "Validation avg loss = 0.07\n",
      "\n",
      "\n",
      "Running evaluation:  Test\n",
      "Test accuracy = 99.09\n",
      "Test avg loss = 0.07\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session = tf.Session()\n",
    "train_tf(session, train_x, train_y, valid_x, valid_y, config)\n",
    "evaluate_tf(session, \"Test\", test_x, test_y, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. zadatak - Klasifikacija na CIFAR-10 skupu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_image(img, path, mean, std):\n",
    "    img = img.copy()\n",
    "    img *= std\n",
    "    img += mean\n",
    "    img = img.astype(np.uint8)\n",
    "    ski.io.imsave(path, img)\n",
    "\n",
    "def show_image(img, mean, std):\n",
    "    img = img.copy()\n",
    "    img *= std\n",
    "    img += mean\n",
    "    img = img.astype(np.uint8)\n",
    "    ski.io.imshow(img)\n",
    "    ski.io.show()\n",
    "\n",
    "def shuffle_data(data_x, data_y):\n",
    "    indices = np.arange(data_x.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    shuffled_data_x = np.ascontiguousarray(data_x[indices])\n",
    "    shuffled_data_y = np.ascontiguousarray(data_y[indices])\n",
    "    return shuffled_data_x, shuffled_data_y\n",
    "\n",
    "def unpickle(file):\n",
    "    print(file)\n",
    "    fo = open(file, 'rb')\n",
    "    dict = pickle.load(fo, encoding='latin1')\n",
    "    fo.close()\n",
    "    return dict\n",
    "\n",
    "def get_label_names(blob):\n",
    "    label_dict = unpickle(blob)['label_names']\n",
    "    return np.array(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = 'C:\\\\Users\\\\Korisnik\\\\Desktop\\\\cifar-10-batches-py\\\\'\n",
    "SAVE_DIR = 'C:\\\\Users\\\\Korisnik\\\\Desktop\\\\du_lab2\\\\Deep-Learning\\\\2_lab\\\\zad4_images\\\\'\n",
    "\n",
    "config = {}\n",
    "config['max_epochs'] = 40\n",
    "config['batch_size'] = 50\n",
    "config['save_dir'] = SAVE_DIR\n",
    "config['weight_decay'] = 1e-4\n",
    "lr_initial = 0.01\n",
    "config['lr_policy'] = {e:{'lr':(0.9**e)*lr_initial} for e in range(1, config['max_epochs']+1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Korisnik\\Desktop\\cifar-10-batches-py\\data_batch_1\n",
      "C:\\Users\\Korisnik\\Desktop\\cifar-10-batches-py\\data_batch_2\n",
      "C:\\Users\\Korisnik\\Desktop\\cifar-10-batches-py\\data_batch_3\n",
      "C:\\Users\\Korisnik\\Desktop\\cifar-10-batches-py\\data_batch_4\n",
      "C:\\Users\\Korisnik\\Desktop\\cifar-10-batches-py\\data_batch_5\n",
      "C:\\Users\\Korisnik\\Desktop\\cifar-10-batches-py\\test_batch\n",
      "(45000, 32, 32, 3)\n",
      "C:\\Users\\Korisnik\\Desktop\\cifar-10-batches-py\\batches.meta\n",
      "['airplane' 'automobile' 'bird' 'cat' 'deer' 'dog' 'frog' 'horse' 'ship'\n",
      " 'truck']\n"
     ]
    }
   ],
   "source": [
    "img_height, img_width, num_channels = 32, 32, 3\n",
    "train_x = np.ndarray((0, img_height * img_width * num_channels), dtype=np.float32)\n",
    "train_y = []\n",
    "for i in range(1, 6):\n",
    "    subset = unpickle(os.path.join(DATA_DIR, 'data_batch_%d' % i))\n",
    "    train_x = np.vstack((train_x, subset['data']))\n",
    "    train_y += subset['labels']\n",
    "    \n",
    "train_x = train_x.reshape((-1, num_channels, img_height, img_width)).transpose(0,2,3,1)\n",
    "train_y = np.array(train_y, dtype=np.int32)\n",
    "\n",
    "subset = unpickle(os.path.join(DATA_DIR, 'test_batch'))\n",
    "test_x = subset['data'].reshape((-1, num_channels, img_height, img_width)).transpose(0,2,3,1).astype(np.float32)\n",
    "test_y = np.array(subset['labels'], dtype=np.int32)\n",
    "\n",
    "valid_size = 5000\n",
    "train_x, train_y = shuffle_data(train_x, train_y)\n",
    "valid_x = train_x[:valid_size, ...]\n",
    "valid_y = train_y[:valid_size, ...]\n",
    "train_x = train_x[valid_size:, ...]\n",
    "train_y = train_y[valid_size:, ...]\n",
    "data_mean = train_x.mean((0,1,2))\n",
    "data_std = train_x.std((0,1,2))\n",
    "\n",
    "train_x = (train_x - data_mean) / data_std\n",
    "valid_x = (valid_x - data_mean) / data_std\n",
    "test_x = (test_x - data_mean) / data_std\n",
    "print(train_x.shape)\n",
    "\n",
    "weight_decay = config['weight_decay']\n",
    "n_classes = 10\n",
    "\n",
    "class_names = get_label_names(os.path.join(DATA_DIR, 'batches.meta'))\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l2_loss(weights):\n",
    "    regularization = 0;\n",
    "    for w in weights:\n",
    "        regularization += tf.nn.l2_loss(w)\n",
    "    return regularization\n",
    "        \n",
    "def conv2d(x, W, b, activation=tf.nn.relu, strides=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return activation(x)\n",
    "\n",
    "def maxpool2d(x, k=2, stride=2):\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, stride, stride, 1], padding='SAME')\n",
    "\n",
    "def fc(x, W, b, activation=None):\n",
    "    x = tf.reshape(x, [-1, W.get_shape().as_list()[0]])\n",
    "    if activation :\n",
    "        return activation(tf.matmul(x, W) +  b)    \n",
    "    return tf.matmul(x, W) +  b\n",
    "\n",
    "def init_var(shape, fin):\n",
    "    sigma = np.sqrt(2/fin)\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural net definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "weights = {\n",
    "    'conv1': tf.get_variable('w_conv1', [5, 5, 3, 16], initializer=xavier_conv2d()),\n",
    "    'conv2': tf.get_variable('w_conv2', [5, 5, 16, 32], initializer=xavier_conv2d()),\n",
    "    \n",
    "    'fc3': tf.get_variable('w_fc3', [8*8*32, 256], initializer=xavier()),\n",
    "    'fc4': tf.get_variable('w_fc4', [256, 128], initializer=xavier()),\n",
    "    'fc5': tf.get_variable('w_fc5', [128, n_classes], initializer=xavier())\n",
    "    \n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'conv1': tf.Variable(tf.zeros([16]), name='b_conv1'),\n",
    "    'conv2': tf.Variable(tf.zeros([32]), name='b_conv2'),\n",
    "    \n",
    "    'fc3': tf.Variable(tf.zeros([256]), name='b_fc3'),\n",
    "    'fc4': tf.Variable(tf.zeros([128]), name='b_fc4'),\n",
    "    'fc5': tf.Variable(tf.zeros([n_classes]), name='b_fc5')\n",
    "}\n",
    "\n",
    "#conv(16,5) -> pool(3,2) -> conv(32,5) -> pool(3,2) -> fc(256) -> fc(128) -> fc(10)\n",
    "def convnet(x, weights, biases):\n",
    "    x = tf.reshape(x, shape=[-1, img_height, img_width, num_channels])\n",
    "    net = conv2d(x, weights['conv1'], biases['conv1'], tf.nn.relu)\n",
    "    net = maxpool2d(net, k=3, stride=2)\n",
    "    net = tf.nn.lrn(net, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n",
    "                    name='norm1')\n",
    "    \n",
    "    \n",
    "    net = conv2d(net, weights['conv2'], biases['conv2'], tf.nn.relu)\n",
    "    net = tf.nn.lrn(net, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n",
    "                    name='norm2')\n",
    "    net = maxpool2d(net, k=3, stride=2) \n",
    "    \n",
    "    net = fc(net, weights['fc3'],  biases['fc3'], tf.nn.relu)\n",
    "    net = fc(net, weights['fc4'],  biases['fc4'], tf.nn.relu)\n",
    "    net = fc(net, weights['fc5'],  biases['fc5'])\n",
    "    return net\n",
    "\n",
    "\n",
    "# Graph\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "Y_ = tf.placeholder(tf.int32, [None,])\n",
    "logits = convnet(X, weights, biases)\n",
    "\n",
    "# loss\n",
    "regularizers = l2_loss(weights.values())\n",
    "loss_per_sample = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=Y_)\n",
    "data_loss = tf.reduce_mean(loss_per_sample)\n",
    "loss = data_loss + weight_decay*regularizers\n",
    "\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(0.01, global_step, 900, 0.9, staircase=True)\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_cifar(session, train_x, train_y, valid_x, valid_y, config, plot_data={}):\n",
    "    plot_data['train_loss'] = []\n",
    "    plot_data['valid_loss'] = []\n",
    "    plot_data['train_acc'] = []\n",
    "    plot_data['valid_acc'] = []\n",
    "    plot_data['lr'] = []\n",
    "    \n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    batch_size = config['batch_size']\n",
    "    max_epochs = config['max_epochs']\n",
    "    save_dir = config['save_dir']\n",
    "    num_examples = train_x.shape[0]\n",
    "    assert num_examples % batch_size == 0\n",
    "    num_batches = num_examples // batch_size\n",
    "    \n",
    "    for epoch_num in range(1, max_epochs + 1):\n",
    "        epoch_start = time.time()\n",
    "        train_x, train_y = shuffle_data(train_x, train_y)\n",
    "    \n",
    "        for step in range(num_batches):\n",
    "            offset = step * batch_size \n",
    "\n",
    "            batch_x = train_x[offset:(offset + batch_size), ...]\n",
    "            batch_y = train_y[offset:(offset + batch_size), ...]\n",
    "\n",
    "            feed_dict = {X: batch_x, Y_: batch_y}\n",
    "            start_time = time.time()\n",
    "            ret_val = session.run([train_step, loss, logits], feed_dict=feed_dict)\n",
    "            _, loss_val, logits_val = ret_val\n",
    "            duration = time.time() - start_time\n",
    "\n",
    "            if (step+1) % 50 == 0:\n",
    "                sec_per_batch = float(duration)\n",
    "                format_str = 'epoch %d, step %d / %d, loss = %.2f (%.3f sec/batch)'\n",
    "                print(format_str % (epoch_num, step+1, num_batches, loss_val, sec_per_batch))\n",
    "\n",
    "            if (step+1) % 100 == 0:\n",
    "                w = session.run(weights['conv1'])\n",
    "                draw_conv_filters(epoch_num, step+1, w, save_dir)\n",
    "\n",
    "        print('Train error:')\n",
    "        train_loss, train_acc = evaluate_cifar(session, train_x, train_y, config)\n",
    "        print('Validation error:')\n",
    "        valid_loss, valid_acc = evaluate_cifar(session, valid_x, valid_y, config)\n",
    "        print('Epoch time:', time.time() - epoch_start)\n",
    "        plot_data['train_loss'] += [train_loss]\n",
    "        plot_data['valid_loss'] += [valid_loss]\n",
    "        plot_data['train_acc'] += [train_acc]\n",
    "        plot_data['valid_acc'] += [valid_acc]\n",
    "        plot_data['lr'] += [session.run(learning_rate)]\n",
    "        plot_training_progress(plot_data, SAVE_DIR)\n",
    "        \n",
    "    return plot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_cifar(session, x, y, config):\n",
    "    batch_size = config['batch_size']\n",
    "    num_examples = x.shape[0]\n",
    "    assert num_examples % batch_size == 0\n",
    "    num_batches = num_examples // batch_size\n",
    "    cnt_correct = 0\n",
    "    loss_avg = 0\n",
    "\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        batch_x = x[i*batch_size:(i+1)*batch_size, ...]\n",
    "        batch_y = y[i*batch_size:(i+1)*batch_size, ...]\n",
    "\n",
    "        data_dict = {X: batch_x, Y_: batch_y}\n",
    "        logits_val, loss_val = session.run([logits, loss] ,feed_dict=data_dict)\n",
    "\n",
    "        yp = np.argmax(logits_val, 1)\n",
    "        yt = batch_y\n",
    "        cnt_correct += (yp == yt).sum()\n",
    "\n",
    "        loss_avg += loss_val\n",
    "        \n",
    "    valid_acc = cnt_correct / num_examples * 100\n",
    "    loss_avg /= num_batches\n",
    "    \n",
    "    print(\" accuracy = %.2f\" % valid_acc)\n",
    "    print(\" avg loss = %.2f\\n\" % loss_avg)\n",
    "    return loss_avg, valid_acc\n",
    "\n",
    "def worst_samples(session, x, y, config):\n",
    "    batch_size = config['batch_size']\n",
    "    num_examples = x.shape[0]\n",
    "    num_batches = num_examples // batch_size\n",
    "\n",
    "    worst_samples = []\n",
    "    for i in range(num_batches):\n",
    "        batch_x = x[i*batch_size:(i+1)*batch_size, ...]\n",
    "        batch_y = y[i*batch_size:(i+1)*batch_size, ...]\n",
    "\n",
    "        data_dict = {X: batch_x, Y_: batch_y}\n",
    "        loss_vals, logits_val = session.run([loss_per_sample, logits] ,feed_dict=data_dict)\n",
    "        prediction = np.argmax(logits_val, 1)\n",
    "        loss_pairs = [(i*batch_size+id, loss, p) for id, (loss, p) in enumerate(zip(loss_vals, prediction))]\n",
    "        worst_samples = sorted(loss_pairs + worst_samples, key=lambda x: -x[1])[:20]\n",
    "    return worst_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_conv_filters(epoch, step, weights, save_dir):\n",
    "    w = weights.copy()\n",
    "    num_filters = w.shape[3]\n",
    "    num_channels = w.shape[2]\n",
    "    k = w.shape[0]\n",
    "    assert w.shape[0] == w.shape[1]\n",
    "    w = w.reshape(k, k, num_channels, num_filters)\n",
    "    w -= w.min()\n",
    "    w /= w.max()\n",
    "    border = 1\n",
    "    cols = 8\n",
    "    rows = math.ceil(num_filters / cols)\n",
    "    width = cols * k + (cols-1) * border\n",
    "    height = rows * k + (rows-1) * border\n",
    "    img = np.zeros([height, width, num_channels])\n",
    "    for i in range(num_filters):\n",
    "        r = int(i / cols) * (k + border)\n",
    "        c = int(i % cols) * (k + border)\n",
    "        img[r:r+k,c:c+k,:] = w[:,:,:,i]\n",
    "    filename = 'epoch_%02d_step_%06d.png' % (epoch, step)\n",
    "    ski.io.imsave(os.path.join(save_dir, filename), img)\n",
    "\n",
    "def plot_training_progress(data, save_dir=None):\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16,8))\n",
    "    linewidth = 2\n",
    "    legend_size = 10\n",
    "    train_color = 'm'\n",
    "    val_color = 'c'\n",
    "\n",
    "    num_points = len(data['train_loss'])\n",
    "    x_data = np.linspace(1, num_points, num_points)\n",
    "    ax1.set_title('Cross-entropy loss')\n",
    "    ax1.plot(x_data, data['train_loss'], marker='o', color=train_color,\n",
    "           linewidth=linewidth, linestyle='-', label='train')\n",
    "    ax1.plot(x_data, data['valid_loss'], marker='o', color=val_color,\n",
    "           linewidth=linewidth, linestyle='-', label='validation')\n",
    "    ax1.legend(loc='upper right', fontsize=legend_size)\n",
    "    ax2.set_title('Average class accuracy')\n",
    "    ax2.plot(x_data, data['train_acc'], marker='o', color=train_color,\n",
    "           linewidth=linewidth, linestyle='-', label='train')\n",
    "    ax2.plot(x_data, data['valid_acc'], marker='o', color=val_color,\n",
    "           linewidth=linewidth, linestyle='-', label='validation')\n",
    "    ax2.legend(loc='upper left', fontsize=legend_size)\n",
    "    ax3.set_title('Learning rate')\n",
    "    ax3.plot(x_data, data['lr'], marker='o', color=train_color,\n",
    "           linewidth=linewidth, linestyle='-', label='learning_rate')\n",
    "    ax3.legend(loc='upper left', fontsize=legend_size)\n",
    "    if save_dir is not None:\n",
    "        save_path = os.path.join(save_dir, 'training_plot.pdf')\n",
    "        print('Plotting in: ', save_path)\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, step 50 / 900, loss = 2.11 (0.007 sec/batch)\n",
      "epoch 1, step 100 / 900, loss = 2.13 (0.007 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Korisnik\\Anaconda3\\lib\\site-packages\\skimage\\util\\dtype.py:122: UserWarning: Possible precision loss when converting from float64 to uint8\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, step 150 / 900, loss = 2.19 (0.007 sec/batch)\n",
      "epoch 1, step 200 / 900, loss = 1.79 (0.007 sec/batch)\n",
      "epoch 1, step 250 / 900, loss = 1.71 (0.007 sec/batch)\n",
      "epoch 1, step 300 / 900, loss = 1.78 (0.008 sec/batch)\n",
      "epoch 1, step 350 / 900, loss = 1.84 (0.008 sec/batch)\n",
      "epoch 1, step 400 / 900, loss = 1.63 (0.007 sec/batch)\n",
      "epoch 1, step 450 / 900, loss = 1.59 (0.007 sec/batch)\n",
      "epoch 1, step 500 / 900, loss = 1.55 (0.008 sec/batch)\n",
      "epoch 1, step 550 / 900, loss = 1.50 (0.008 sec/batch)\n",
      "epoch 1, step 600 / 900, loss = 1.79 (0.007 sec/batch)\n",
      "epoch 1, step 650 / 900, loss = 1.35 (0.007 sec/batch)\n",
      "epoch 1, step 700 / 900, loss = 1.45 (0.007 sec/batch)\n",
      "epoch 1, step 750 / 900, loss = 1.71 (0.007 sec/batch)\n",
      "epoch 1, step 800 / 900, loss = 1.41 (0.007 sec/batch)\n",
      "epoch 1, step 850 / 900, loss = 1.28 (0.008 sec/batch)\n",
      "epoch 1, step 900 / 900, loss = 1.17 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 51.38\n",
      " avg loss = 1.39\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.68\n",
      " avg loss = 1.44\n",
      "\n",
      "Epoch time: 9.394724130630493\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 2, step 50 / 900, loss = 1.47 (0.008 sec/batch)\n",
      "epoch 2, step 100 / 900, loss = 1.47 (0.007 sec/batch)\n",
      "epoch 2, step 150 / 900, loss = 1.27 (0.007 sec/batch)\n",
      "epoch 2, step 200 / 900, loss = 1.34 (0.007 sec/batch)\n",
      "epoch 2, step 250 / 900, loss = 1.56 (0.007 sec/batch)\n",
      "epoch 2, step 300 / 900, loss = 1.41 (0.008 sec/batch)\n",
      "epoch 2, step 350 / 900, loss = 1.60 (0.007 sec/batch)\n",
      "epoch 2, step 400 / 900, loss = 1.23 (0.007 sec/batch)\n",
      "epoch 2, step 450 / 900, loss = 1.62 (0.007 sec/batch)\n",
      "epoch 2, step 500 / 900, loss = 1.16 (0.007 sec/batch)\n",
      "epoch 2, step 550 / 900, loss = 1.32 (0.007 sec/batch)\n",
      "epoch 2, step 600 / 900, loss = 1.36 (0.007 sec/batch)\n",
      "epoch 2, step 650 / 900, loss = 1.50 (0.007 sec/batch)\n",
      "epoch 2, step 700 / 900, loss = 1.12 (0.007 sec/batch)\n",
      "epoch 2, step 750 / 900, loss = 1.14 (0.007 sec/batch)\n",
      "epoch 2, step 800 / 900, loss = 1.12 (0.007 sec/batch)\n",
      "epoch 2, step 850 / 900, loss = 1.54 (0.007 sec/batch)\n",
      "epoch 2, step 900 / 900, loss = 1.17 (0.008 sec/batch)\n",
      "Train error:\n",
      " accuracy = 59.46\n",
      " avg loss = 1.19\n",
      "\n",
      "Validation error:\n",
      " accuracy = 57.56\n",
      " avg loss = 1.26\n",
      "\n",
      "Epoch time: 9.287004947662354\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 3, step 50 / 900, loss = 1.04 (0.007 sec/batch)\n",
      "epoch 3, step 100 / 900, loss = 0.90 (0.007 sec/batch)\n",
      "epoch 3, step 150 / 900, loss = 1.11 (0.007 sec/batch)\n",
      "epoch 3, step 200 / 900, loss = 0.99 (0.007 sec/batch)\n",
      "epoch 3, step 250 / 900, loss = 1.16 (0.007 sec/batch)\n",
      "epoch 3, step 300 / 900, loss = 1.05 (0.007 sec/batch)\n",
      "epoch 3, step 350 / 900, loss = 1.18 (0.007 sec/batch)\n",
      "epoch 3, step 400 / 900, loss = 1.13 (0.007 sec/batch)\n",
      "epoch 3, step 450 / 900, loss = 1.22 (0.007 sec/batch)\n",
      "epoch 3, step 500 / 900, loss = 1.19 (0.007 sec/batch)\n",
      "epoch 3, step 550 / 900, loss = 0.91 (0.007 sec/batch)\n",
      "epoch 3, step 600 / 900, loss = 1.10 (0.007 sec/batch)\n",
      "epoch 3, step 650 / 900, loss = 1.19 (0.007 sec/batch)\n",
      "epoch 3, step 700 / 900, loss = 0.85 (0.007 sec/batch)\n",
      "epoch 3, step 750 / 900, loss = 1.05 (0.007 sec/batch)\n",
      "epoch 3, step 800 / 900, loss = 1.17 (0.007 sec/batch)\n",
      "epoch 3, step 850 / 900, loss = 1.14 (0.007 sec/batch)\n",
      "epoch 3, step 900 / 900, loss = 1.03 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 64.99\n",
      " avg loss = 1.04\n",
      "\n",
      "Validation error:\n",
      " accuracy = 62.00\n",
      " avg loss = 1.13\n",
      "\n",
      "Epoch time: 9.283029556274414\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 4, step 50 / 900, loss = 1.07 (0.007 sec/batch)\n",
      "epoch 4, step 100 / 900, loss = 0.82 (0.007 sec/batch)\n",
      "epoch 4, step 150 / 900, loss = 1.25 (0.007 sec/batch)\n",
      "epoch 4, step 200 / 900, loss = 0.99 (0.007 sec/batch)\n",
      "epoch 4, step 250 / 900, loss = 0.99 (0.008 sec/batch)\n",
      "epoch 4, step 300 / 900, loss = 1.11 (0.007 sec/batch)\n",
      "epoch 4, step 350 / 900, loss = 1.14 (0.007 sec/batch)\n",
      "epoch 4, step 400 / 900, loss = 1.11 (0.007 sec/batch)\n",
      "epoch 4, step 450 / 900, loss = 0.98 (0.007 sec/batch)\n",
      "epoch 4, step 500 / 900, loss = 0.97 (0.007 sec/batch)\n",
      "epoch 4, step 550 / 900, loss = 1.11 (0.007 sec/batch)\n",
      "epoch 4, step 600 / 900, loss = 1.02 (0.007 sec/batch)\n",
      "epoch 4, step 650 / 900, loss = 0.81 (0.008 sec/batch)\n",
      "epoch 4, step 700 / 900, loss = 0.86 (0.007 sec/batch)\n",
      "epoch 4, step 750 / 900, loss = 1.00 (0.007 sec/batch)\n",
      "epoch 4, step 800 / 900, loss = 0.88 (0.008 sec/batch)\n",
      "epoch 4, step 850 / 900, loss = 1.33 (0.007 sec/batch)\n",
      "epoch 4, step 900 / 900, loss = 1.17 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 66.16\n",
      " avg loss = 1.01\n",
      "\n",
      "Validation error:\n",
      " accuracy = 63.52\n",
      " avg loss = 1.12\n",
      "\n",
      "Epoch time: 9.367656469345093\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 5, step 50 / 900, loss = 0.94 (0.007 sec/batch)\n",
      "epoch 5, step 100 / 900, loss = 1.13 (0.007 sec/batch)\n",
      "epoch 5, step 150 / 900, loss = 0.94 (0.007 sec/batch)\n",
      "epoch 5, step 200 / 900, loss = 0.95 (0.008 sec/batch)\n",
      "epoch 5, step 250 / 900, loss = 1.29 (0.007 sec/batch)\n",
      "epoch 5, step 300 / 900, loss = 0.75 (0.008 sec/batch)\n",
      "epoch 5, step 350 / 900, loss = 0.77 (0.007 sec/batch)\n",
      "epoch 5, step 400 / 900, loss = 0.85 (0.007 sec/batch)\n",
      "epoch 5, step 450 / 900, loss = 0.97 (0.008 sec/batch)\n",
      "epoch 5, step 500 / 900, loss = 0.93 (0.007 sec/batch)\n",
      "epoch 5, step 550 / 900, loss = 0.98 (0.007 sec/batch)\n",
      "epoch 5, step 600 / 900, loss = 1.30 (0.007 sec/batch)\n",
      "epoch 5, step 650 / 900, loss = 0.92 (0.008 sec/batch)\n",
      "epoch 5, step 700 / 900, loss = 1.36 (0.008 sec/batch)\n",
      "epoch 5, step 750 / 900, loss = 0.83 (0.007 sec/batch)\n",
      "epoch 5, step 800 / 900, loss = 0.86 (0.007 sec/batch)\n",
      "epoch 5, step 850 / 900, loss = 1.14 (0.007 sec/batch)\n",
      "epoch 5, step 900 / 900, loss = 0.84 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 69.57\n",
      " avg loss = 0.91\n",
      "\n",
      "Validation error:\n",
      " accuracy = 65.62\n",
      " avg loss = 1.05\n",
      "\n",
      "Epoch time: 9.523624897003174\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 6, step 50 / 900, loss = 0.96 (0.007 sec/batch)\n",
      "epoch 6, step 100 / 900, loss = 0.75 (0.008 sec/batch)\n",
      "epoch 6, step 150 / 900, loss = 0.80 (0.007 sec/batch)\n",
      "epoch 6, step 200 / 900, loss = 1.03 (0.007 sec/batch)\n",
      "epoch 6, step 250 / 900, loss = 1.15 (0.007 sec/batch)\n",
      "epoch 6, step 300 / 900, loss = 1.12 (0.007 sec/batch)\n",
      "epoch 6, step 350 / 900, loss = 0.99 (0.007 sec/batch)\n",
      "epoch 6, step 400 / 900, loss = 1.04 (0.008 sec/batch)\n",
      "epoch 6, step 450 / 900, loss = 0.93 (0.008 sec/batch)\n",
      "epoch 6, step 500 / 900, loss = 0.70 (0.007 sec/batch)\n",
      "epoch 6, step 550 / 900, loss = 0.93 (0.007 sec/batch)\n",
      "epoch 6, step 600 / 900, loss = 1.14 (0.007 sec/batch)\n",
      "epoch 6, step 650 / 900, loss = 1.18 (0.007 sec/batch)\n",
      "epoch 6, step 700 / 900, loss = 1.08 (0.007 sec/batch)\n",
      "epoch 6, step 750 / 900, loss = 0.93 (0.007 sec/batch)\n",
      "epoch 6, step 800 / 900, loss = 0.76 (0.007 sec/batch)\n",
      "epoch 6, step 850 / 900, loss = 0.72 (0.007 sec/batch)\n",
      "epoch 6, step 900 / 900, loss = 0.96 (0.008 sec/batch)\n",
      "Train error:\n",
      " accuracy = 71.88\n",
      " avg loss = 0.84\n",
      "\n",
      "Validation error:\n",
      " accuracy = 67.20\n",
      " avg loss = 1.00\n",
      "\n",
      "Epoch time: 9.321552276611328\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 7, step 50 / 900, loss = 0.69 (0.007 sec/batch)\n",
      "epoch 7, step 100 / 900, loss = 0.64 (0.007 sec/batch)\n",
      "epoch 7, step 150 / 900, loss = 0.80 (0.008 sec/batch)\n",
      "epoch 7, step 200 / 900, loss = 0.91 (0.008 sec/batch)\n",
      "epoch 7, step 250 / 900, loss = 0.78 (0.008 sec/batch)\n",
      "epoch 7, step 300 / 900, loss = 0.80 (0.007 sec/batch)\n",
      "epoch 7, step 350 / 900, loss = 0.78 (0.007 sec/batch)\n",
      "epoch 7, step 400 / 900, loss = 1.00 (0.008 sec/batch)\n",
      "epoch 7, step 450 / 900, loss = 1.04 (0.007 sec/batch)\n",
      "epoch 7, step 500 / 900, loss = 0.82 (0.007 sec/batch)\n",
      "epoch 7, step 550 / 900, loss = 0.80 (0.007 sec/batch)\n",
      "epoch 7, step 600 / 900, loss = 0.97 (0.008 sec/batch)\n",
      "epoch 7, step 650 / 900, loss = 0.71 (0.008 sec/batch)\n",
      "epoch 7, step 700 / 900, loss = 0.72 (0.007 sec/batch)\n",
      "epoch 7, step 750 / 900, loss = 1.29 (0.008 sec/batch)\n",
      "epoch 7, step 800 / 900, loss = 0.98 (0.007 sec/batch)\n",
      "epoch 7, step 850 / 900, loss = 0.84 (0.008 sec/batch)\n",
      "epoch 7, step 900 / 900, loss = 1.14 (0.007 sec/batch)\n",
      "Train error:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy = 74.13\n",
      " avg loss = 0.79\n",
      "\n",
      "Validation error:\n",
      " accuracy = 68.84\n",
      " avg loss = 0.96\n",
      "\n",
      "Epoch time: 9.492079019546509\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 8, step 50 / 900, loss = 1.05 (0.007 sec/batch)\n",
      "epoch 8, step 100 / 900, loss = 0.64 (0.007 sec/batch)\n",
      "epoch 8, step 150 / 900, loss = 1.03 (0.008 sec/batch)\n",
      "epoch 8, step 200 / 900, loss = 0.96 (0.007 sec/batch)\n",
      "epoch 8, step 250 / 900, loss = 0.84 (0.008 sec/batch)\n",
      "epoch 8, step 300 / 900, loss = 0.77 (0.008 sec/batch)\n",
      "epoch 8, step 350 / 900, loss = 0.69 (0.007 sec/batch)\n",
      "epoch 8, step 400 / 900, loss = 0.76 (0.007 sec/batch)\n",
      "epoch 8, step 450 / 900, loss = 0.73 (0.007 sec/batch)\n",
      "epoch 8, step 500 / 900, loss = 0.71 (0.008 sec/batch)\n",
      "epoch 8, step 550 / 900, loss = 0.79 (0.007 sec/batch)\n",
      "epoch 8, step 600 / 900, loss = 0.82 (0.007 sec/batch)\n",
      "epoch 8, step 650 / 900, loss = 0.61 (0.007 sec/batch)\n",
      "epoch 8, step 700 / 900, loss = 0.67 (0.007 sec/batch)\n",
      "epoch 8, step 750 / 900, loss = 0.75 (0.007 sec/batch)\n",
      "epoch 8, step 800 / 900, loss = 0.97 (0.007 sec/batch)\n",
      "epoch 8, step 850 / 900, loss = 0.73 (0.007 sec/batch)\n",
      "epoch 8, step 900 / 900, loss = 0.86 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 74.75\n",
      " avg loss = 0.76\n",
      "\n",
      "Validation error:\n",
      " accuracy = 68.48\n",
      " avg loss = 0.97\n",
      "\n",
      "Epoch time: 9.376022577285767\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 9, step 50 / 900, loss = 1.03 (0.007 sec/batch)\n",
      "epoch 9, step 100 / 900, loss = 0.85 (0.007 sec/batch)\n",
      "epoch 9, step 150 / 900, loss = 0.74 (0.007 sec/batch)\n",
      "epoch 9, step 200 / 900, loss = 0.72 (0.007 sec/batch)\n",
      "epoch 9, step 250 / 900, loss = 0.94 (0.007 sec/batch)\n",
      "epoch 9, step 300 / 900, loss = 1.02 (0.007 sec/batch)\n",
      "epoch 9, step 350 / 900, loss = 0.54 (0.007 sec/batch)\n",
      "epoch 9, step 400 / 900, loss = 0.87 (0.008 sec/batch)\n",
      "epoch 9, step 450 / 900, loss = 0.72 (0.007 sec/batch)\n",
      "epoch 9, step 500 / 900, loss = 1.03 (0.007 sec/batch)\n",
      "epoch 9, step 550 / 900, loss = 0.85 (0.008 sec/batch)\n",
      "epoch 9, step 600 / 900, loss = 0.54 (0.007 sec/batch)\n",
      "epoch 9, step 650 / 900, loss = 0.71 (0.007 sec/batch)\n",
      "epoch 9, step 700 / 900, loss = 0.75 (0.007 sec/batch)\n",
      "epoch 9, step 750 / 900, loss = 0.83 (0.007 sec/batch)\n",
      "epoch 9, step 800 / 900, loss = 0.79 (0.007 sec/batch)\n",
      "epoch 9, step 850 / 900, loss = 0.81 (0.007 sec/batch)\n",
      "epoch 9, step 900 / 900, loss = 0.62 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 75.28\n",
      " avg loss = 0.75\n",
      "\n",
      "Validation error:\n",
      " accuracy = 69.60\n",
      " avg loss = 0.97\n",
      "\n",
      "Epoch time: 9.281993627548218\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 10, step 50 / 900, loss = 0.73 (0.007 sec/batch)\n",
      "epoch 10, step 100 / 900, loss = 0.58 (0.007 sec/batch)\n",
      "epoch 10, step 150 / 900, loss = 0.60 (0.007 sec/batch)\n",
      "epoch 10, step 200 / 900, loss = 0.74 (0.007 sec/batch)\n",
      "epoch 10, step 250 / 900, loss = 0.83 (0.007 sec/batch)\n",
      "epoch 10, step 300 / 900, loss = 0.67 (0.007 sec/batch)\n",
      "epoch 10, step 350 / 900, loss = 0.63 (0.008 sec/batch)\n",
      "epoch 10, step 400 / 900, loss = 0.52 (0.007 sec/batch)\n",
      "epoch 10, step 450 / 900, loss = 0.71 (0.007 sec/batch)\n",
      "epoch 10, step 500 / 900, loss = 1.01 (0.007 sec/batch)\n",
      "epoch 10, step 550 / 900, loss = 0.77 (0.007 sec/batch)\n",
      "epoch 10, step 600 / 900, loss = 0.70 (0.007 sec/batch)\n",
      "epoch 10, step 650 / 900, loss = 0.84 (0.007 sec/batch)\n",
      "epoch 10, step 700 / 900, loss = 0.96 (0.009 sec/batch)\n",
      "epoch 10, step 750 / 900, loss = 0.73 (0.007 sec/batch)\n",
      "epoch 10, step 800 / 900, loss = 0.64 (0.007 sec/batch)\n",
      "epoch 10, step 850 / 900, loss = 0.91 (0.007 sec/batch)\n",
      "epoch 10, step 900 / 900, loss = 0.77 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 76.90\n",
      " avg loss = 0.71\n",
      "\n",
      "Validation error:\n",
      " accuracy = 69.82\n",
      " avg loss = 0.95\n",
      "\n",
      "Epoch time: 9.339023113250732\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 11, step 50 / 900, loss = 0.80 (0.007 sec/batch)\n",
      "epoch 11, step 100 / 900, loss = 0.87 (0.007 sec/batch)\n",
      "epoch 11, step 150 / 900, loss = 0.73 (0.007 sec/batch)\n",
      "epoch 11, step 200 / 900, loss = 0.66 (0.008 sec/batch)\n",
      "epoch 11, step 250 / 900, loss = 0.47 (0.007 sec/batch)\n",
      "epoch 11, step 300 / 900, loss = 0.74 (0.007 sec/batch)\n",
      "epoch 11, step 350 / 900, loss = 0.56 (0.007 sec/batch)\n",
      "epoch 11, step 400 / 900, loss = 0.83 (0.008 sec/batch)\n",
      "epoch 11, step 450 / 900, loss = 0.75 (0.007 sec/batch)\n",
      "epoch 11, step 500 / 900, loss = 0.75 (0.007 sec/batch)\n",
      "epoch 11, step 550 / 900, loss = 0.65 (0.007 sec/batch)\n",
      "epoch 11, step 600 / 900, loss = 1.02 (0.007 sec/batch)\n",
      "epoch 11, step 650 / 900, loss = 0.78 (0.007 sec/batch)\n",
      "epoch 11, step 700 / 900, loss = 0.57 (0.008 sec/batch)\n",
      "epoch 11, step 750 / 900, loss = 0.60 (0.008 sec/batch)\n",
      "epoch 11, step 800 / 900, loss = 0.66 (0.008 sec/batch)\n",
      "epoch 11, step 850 / 900, loss = 0.77 (0.007 sec/batch)\n",
      "epoch 11, step 900 / 900, loss = 0.87 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 78.54\n",
      " avg loss = 0.66\n",
      "\n",
      "Validation error:\n",
      " accuracy = 70.40\n",
      " avg loss = 0.92\n",
      "\n",
      "Epoch time: 9.331868171691895\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 12, step 50 / 900, loss = 0.49 (0.007 sec/batch)\n",
      "epoch 12, step 100 / 900, loss = 0.84 (0.007 sec/batch)\n",
      "epoch 12, step 150 / 900, loss = 0.84 (0.006 sec/batch)\n",
      "epoch 12, step 200 / 900, loss = 0.76 (0.007 sec/batch)\n",
      "epoch 12, step 250 / 900, loss = 0.74 (0.007 sec/batch)\n",
      "epoch 12, step 300 / 900, loss = 0.48 (0.008 sec/batch)\n",
      "epoch 12, step 350 / 900, loss = 0.47 (0.008 sec/batch)\n",
      "epoch 12, step 400 / 900, loss = 0.66 (0.007 sec/batch)\n",
      "epoch 12, step 450 / 900, loss = 0.76 (0.008 sec/batch)\n",
      "epoch 12, step 500 / 900, loss = 0.71 (0.008 sec/batch)\n",
      "epoch 12, step 550 / 900, loss = 0.91 (0.007 sec/batch)\n",
      "epoch 12, step 600 / 900, loss = 0.58 (0.008 sec/batch)\n",
      "epoch 12, step 650 / 900, loss = 0.52 (0.007 sec/batch)\n",
      "epoch 12, step 700 / 900, loss = 0.58 (0.007 sec/batch)\n",
      "epoch 12, step 750 / 900, loss = 0.77 (0.007 sec/batch)\n",
      "epoch 12, step 800 / 900, loss = 0.75 (0.008 sec/batch)\n",
      "epoch 12, step 850 / 900, loss = 0.60 (0.007 sec/batch)\n",
      "epoch 12, step 900 / 900, loss = 0.77 (0.008 sec/batch)\n",
      "Train error:\n",
      " accuracy = 78.64\n",
      " avg loss = 0.66\n",
      "\n",
      "Validation error:\n",
      " accuracy = 70.08\n",
      " avg loss = 0.94\n",
      "\n",
      "Epoch time: 9.401463031768799\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 13, step 50 / 900, loss = 0.71 (0.007 sec/batch)\n",
      "epoch 13, step 100 / 900, loss = 0.79 (0.007 sec/batch)\n",
      "epoch 13, step 150 / 900, loss = 0.48 (0.007 sec/batch)\n",
      "epoch 13, step 200 / 900, loss = 0.57 (0.007 sec/batch)\n",
      "epoch 13, step 250 / 900, loss = 0.62 (0.008 sec/batch)\n",
      "epoch 13, step 300 / 900, loss = 0.65 (0.007 sec/batch)\n",
      "epoch 13, step 350 / 900, loss = 0.52 (0.008 sec/batch)\n",
      "epoch 13, step 400 / 900, loss = 0.72 (0.008 sec/batch)\n",
      "epoch 13, step 450 / 900, loss = 0.46 (0.008 sec/batch)\n",
      "epoch 13, step 500 / 900, loss = 0.47 (0.008 sec/batch)\n",
      "epoch 13, step 550 / 900, loss = 0.50 (0.007 sec/batch)\n",
      "epoch 13, step 600 / 900, loss = 0.63 (0.008 sec/batch)\n",
      "epoch 13, step 650 / 900, loss = 0.50 (0.009 sec/batch)\n",
      "epoch 13, step 700 / 900, loss = 0.61 (0.008 sec/batch)\n",
      "epoch 13, step 750 / 900, loss = 0.44 (0.009 sec/batch)\n",
      "epoch 13, step 800 / 900, loss = 0.73 (0.008 sec/batch)\n",
      "epoch 13, step 850 / 900, loss = 0.39 (0.008 sec/batch)\n",
      "epoch 13, step 900 / 900, loss = 0.85 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 79.82\n",
      " avg loss = 0.63\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.18\n",
      " avg loss = 0.93\n",
      "\n",
      "Epoch time: 9.833303451538086\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 14, step 50 / 900, loss = 0.60 (0.008 sec/batch)\n",
      "epoch 14, step 100 / 900, loss = 0.51 (0.007 sec/batch)\n",
      "epoch 14, step 150 / 900, loss = 0.62 (0.007 sec/batch)\n",
      "epoch 14, step 200 / 900, loss = 0.51 (0.008 sec/batch)\n",
      "epoch 14, step 250 / 900, loss = 0.64 (0.007 sec/batch)\n",
      "epoch 14, step 300 / 900, loss = 0.63 (0.007 sec/batch)\n",
      "epoch 14, step 350 / 900, loss = 0.65 (0.007 sec/batch)\n",
      "epoch 14, step 400 / 900, loss = 0.60 (0.007 sec/batch)\n",
      "epoch 14, step 450 / 900, loss = 0.68 (0.007 sec/batch)\n",
      "epoch 14, step 500 / 900, loss = 0.85 (0.007 sec/batch)\n",
      "epoch 14, step 550 / 900, loss = 0.56 (0.008 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14, step 600 / 900, loss = 0.67 (0.008 sec/batch)\n",
      "epoch 14, step 650 / 900, loss = 0.68 (0.008 sec/batch)\n",
      "epoch 14, step 700 / 900, loss = 0.79 (0.008 sec/batch)\n",
      "epoch 14, step 750 / 900, loss = 0.85 (0.008 sec/batch)\n",
      "epoch 14, step 800 / 900, loss = 0.64 (0.008 sec/batch)\n",
      "epoch 14, step 850 / 900, loss = 0.74 (0.007 sec/batch)\n",
      "epoch 14, step 900 / 900, loss = 0.54 (0.008 sec/batch)\n",
      "Train error:\n",
      " accuracy = 80.71\n",
      " avg loss = 0.60\n",
      "\n",
      "Validation error:\n",
      " accuracy = 70.84\n",
      " avg loss = 0.93\n",
      "\n",
      "Epoch time: 9.513582944869995\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 15, step 50 / 900, loss = 0.45 (0.008 sec/batch)\n",
      "epoch 15, step 100 / 900, loss = 0.49 (0.007 sec/batch)\n",
      "epoch 15, step 150 / 900, loss = 0.60 (0.008 sec/batch)\n",
      "epoch 15, step 200 / 900, loss = 0.82 (0.007 sec/batch)\n",
      "epoch 15, step 250 / 900, loss = 0.61 (0.008 sec/batch)\n",
      "epoch 15, step 300 / 900, loss = 0.75 (0.007 sec/batch)\n",
      "epoch 15, step 350 / 900, loss = 0.36 (0.007 sec/batch)\n",
      "epoch 15, step 400 / 900, loss = 0.44 (0.008 sec/batch)\n",
      "epoch 15, step 450 / 900, loss = 0.67 (0.008 sec/batch)\n",
      "epoch 15, step 500 / 900, loss = 0.67 (0.007 sec/batch)\n",
      "epoch 15, step 550 / 900, loss = 0.45 (0.008 sec/batch)\n",
      "epoch 15, step 600 / 900, loss = 0.71 (0.007 sec/batch)\n",
      "epoch 15, step 650 / 900, loss = 0.83 (0.008 sec/batch)\n",
      "epoch 15, step 700 / 900, loss = 0.51 (0.008 sec/batch)\n",
      "epoch 15, step 750 / 900, loss = 0.38 (0.008 sec/batch)\n",
      "epoch 15, step 800 / 900, loss = 0.59 (0.008 sec/batch)\n",
      "epoch 15, step 850 / 900, loss = 0.77 (0.007 sec/batch)\n",
      "epoch 15, step 900 / 900, loss = 0.55 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 81.05\n",
      " avg loss = 0.59\n",
      "\n",
      "Validation error:\n",
      " accuracy = 70.72\n",
      " avg loss = 0.94\n",
      "\n",
      "Epoch time: 9.412766933441162\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 16, step 50 / 900, loss = 0.70 (0.008 sec/batch)\n",
      "epoch 16, step 100 / 900, loss = 0.52 (0.007 sec/batch)\n",
      "epoch 16, step 150 / 900, loss = 0.72 (0.007 sec/batch)\n",
      "epoch 16, step 200 / 900, loss = 0.61 (0.008 sec/batch)\n",
      "epoch 16, step 250 / 900, loss = 0.57 (0.008 sec/batch)\n",
      "epoch 16, step 300 / 900, loss = 0.49 (0.007 sec/batch)\n",
      "epoch 16, step 350 / 900, loss = 0.62 (0.008 sec/batch)\n",
      "epoch 16, step 400 / 900, loss = 0.73 (0.008 sec/batch)\n",
      "epoch 16, step 450 / 900, loss = 0.50 (0.008 sec/batch)\n",
      "epoch 16, step 500 / 900, loss = 0.63 (0.007 sec/batch)\n",
      "epoch 16, step 550 / 900, loss = 0.36 (0.008 sec/batch)\n",
      "epoch 16, step 600 / 900, loss = 0.68 (0.008 sec/batch)\n",
      "epoch 16, step 650 / 900, loss = 0.46 (0.007 sec/batch)\n",
      "epoch 16, step 700 / 900, loss = 0.47 (0.007 sec/batch)\n",
      "epoch 16, step 750 / 900, loss = 0.57 (0.008 sec/batch)\n",
      "epoch 16, step 800 / 900, loss = 0.89 (0.007 sec/batch)\n",
      "epoch 16, step 850 / 900, loss = 0.57 (0.007 sec/batch)\n",
      "epoch 16, step 900 / 900, loss = 0.64 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 82.37\n",
      " avg loss = 0.56\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.28\n",
      " avg loss = 0.92\n",
      "\n",
      "Epoch time: 9.552573919296265\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 17, step 50 / 900, loss = 0.68 (0.008 sec/batch)\n",
      "epoch 17, step 100 / 900, loss = 0.78 (0.008 sec/batch)\n",
      "epoch 17, step 150 / 900, loss = 0.59 (0.009 sec/batch)\n",
      "epoch 17, step 200 / 900, loss = 0.83 (0.008 sec/batch)\n",
      "epoch 17, step 250 / 900, loss = 0.39 (0.007 sec/batch)\n",
      "epoch 17, step 300 / 900, loss = 0.58 (0.008 sec/batch)\n",
      "epoch 17, step 350 / 900, loss = 0.55 (0.007 sec/batch)\n",
      "epoch 17, step 400 / 900, loss = 0.49 (0.008 sec/batch)\n",
      "epoch 17, step 450 / 900, loss = 0.64 (0.008 sec/batch)\n",
      "epoch 17, step 500 / 900, loss = 0.55 (0.007 sec/batch)\n",
      "epoch 17, step 550 / 900, loss = 0.59 (0.007 sec/batch)\n",
      "epoch 17, step 600 / 900, loss = 0.36 (0.007 sec/batch)\n",
      "epoch 17, step 650 / 900, loss = 0.50 (0.008 sec/batch)\n",
      "epoch 17, step 700 / 900, loss = 0.65 (0.007 sec/batch)\n",
      "epoch 17, step 750 / 900, loss = 0.62 (0.007 sec/batch)\n",
      "epoch 17, step 800 / 900, loss = 0.60 (0.008 sec/batch)\n",
      "epoch 17, step 850 / 900, loss = 0.62 (0.008 sec/batch)\n",
      "epoch 17, step 900 / 900, loss = 0.47 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 82.80\n",
      " avg loss = 0.54\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.22\n",
      " avg loss = 0.93\n",
      "\n",
      "Epoch time: 9.578385829925537\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 18, step 50 / 900, loss = 0.41 (0.008 sec/batch)\n",
      "epoch 18, step 100 / 900, loss = 0.55 (0.008 sec/batch)\n",
      "epoch 18, step 150 / 900, loss = 0.78 (0.008 sec/batch)\n",
      "epoch 18, step 200 / 900, loss = 0.58 (0.008 sec/batch)\n",
      "epoch 18, step 250 / 900, loss = 0.62 (0.007 sec/batch)\n",
      "epoch 18, step 300 / 900, loss = 0.62 (0.007 sec/batch)\n",
      "epoch 18, step 350 / 900, loss = 0.38 (0.007 sec/batch)\n",
      "epoch 18, step 400 / 900, loss = 0.40 (0.008 sec/batch)\n",
      "epoch 18, step 450 / 900, loss = 0.48 (0.007 sec/batch)\n",
      "epoch 18, step 500 / 900, loss = 0.52 (0.008 sec/batch)\n",
      "epoch 18, step 550 / 900, loss = 0.68 (0.008 sec/batch)\n",
      "epoch 18, step 600 / 900, loss = 0.67 (0.008 sec/batch)\n",
      "epoch 18, step 650 / 900, loss = 0.63 (0.007 sec/batch)\n",
      "epoch 18, step 700 / 900, loss = 0.58 (0.008 sec/batch)\n",
      "epoch 18, step 750 / 900, loss = 0.96 (0.008 sec/batch)\n",
      "epoch 18, step 800 / 900, loss = 0.43 (0.008 sec/batch)\n",
      "epoch 18, step 850 / 900, loss = 0.55 (0.007 sec/batch)\n",
      "epoch 18, step 900 / 900, loss = 0.69 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 82.97\n",
      " avg loss = 0.54\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.66\n",
      " avg loss = 0.93\n",
      "\n",
      "Epoch time: 9.540223836898804\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 19, step 50 / 900, loss = 0.52 (0.007 sec/batch)\n",
      "epoch 19, step 100 / 900, loss = 0.61 (0.007 sec/batch)\n",
      "epoch 19, step 150 / 900, loss = 0.49 (0.007 sec/batch)\n",
      "epoch 19, step 200 / 900, loss = 0.71 (0.008 sec/batch)\n",
      "epoch 19, step 250 / 900, loss = 0.56 (0.007 sec/batch)\n",
      "epoch 19, step 300 / 900, loss = 0.41 (0.007 sec/batch)\n",
      "epoch 19, step 350 / 900, loss = 0.73 (0.007 sec/batch)\n",
      "epoch 19, step 400 / 900, loss = 0.58 (0.007 sec/batch)\n",
      "epoch 19, step 450 / 900, loss = 0.50 (0.007 sec/batch)\n",
      "epoch 19, step 500 / 900, loss = 0.47 (0.007 sec/batch)\n",
      "epoch 19, step 550 / 900, loss = 0.35 (0.008 sec/batch)\n",
      "epoch 19, step 600 / 900, loss = 0.43 (0.008 sec/batch)\n",
      "epoch 19, step 650 / 900, loss = 0.54 (0.008 sec/batch)\n",
      "epoch 19, step 700 / 900, loss = 0.50 (0.007 sec/batch)\n",
      "epoch 19, step 750 / 900, loss = 0.72 (0.008 sec/batch)\n",
      "epoch 19, step 800 / 900, loss = 0.44 (0.007 sec/batch)\n",
      "epoch 19, step 850 / 900, loss = 0.58 (0.007 sec/batch)\n",
      "epoch 19, step 900 / 900, loss = 0.50 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 83.99\n",
      " avg loss = 0.51\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.74\n",
      " avg loss = 0.92\n",
      "\n",
      "Epoch time: 9.31870150566101\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 20, step 50 / 900, loss = 0.68 (0.008 sec/batch)\n",
      "epoch 20, step 100 / 900, loss = 0.50 (0.007 sec/batch)\n",
      "epoch 20, step 150 / 900, loss = 0.36 (0.007 sec/batch)\n",
      "epoch 20, step 200 / 900, loss = 0.71 (0.008 sec/batch)\n",
      "epoch 20, step 250 / 900, loss = 0.43 (0.008 sec/batch)\n",
      "epoch 20, step 300 / 900, loss = 0.46 (0.007 sec/batch)\n",
      "epoch 20, step 350 / 900, loss = 0.35 (0.007 sec/batch)\n",
      "epoch 20, step 400 / 900, loss = 0.52 (0.007 sec/batch)\n",
      "epoch 20, step 450 / 900, loss = 0.56 (0.007 sec/batch)\n",
      "epoch 20, step 500 / 900, loss = 0.48 (0.007 sec/batch)\n",
      "epoch 20, step 550 / 900, loss = 0.58 (0.008 sec/batch)\n",
      "epoch 20, step 600 / 900, loss = 0.32 (0.008 sec/batch)\n",
      "epoch 20, step 650 / 900, loss = 0.54 (0.007 sec/batch)\n",
      "epoch 20, step 700 / 900, loss = 0.48 (0.007 sec/batch)\n",
      "epoch 20, step 750 / 900, loss = 0.63 (0.007 sec/batch)\n",
      "epoch 20, step 800 / 900, loss = 0.70 (0.008 sec/batch)\n",
      "epoch 20, step 850 / 900, loss = 0.36 (0.007 sec/batch)\n",
      "epoch 20, step 900 / 900, loss = 0.72 (0.008 sec/batch)\n",
      "Train error:\n",
      " accuracy = 84.26\n",
      " avg loss = 0.51\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.52\n",
      " avg loss = 0.92\n",
      "\n",
      "Epoch time: 9.489323139190674\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 21, step 50 / 900, loss = 0.45 (0.007 sec/batch)\n",
      "epoch 21, step 100 / 900, loss = 0.38 (0.007 sec/batch)\n",
      "epoch 21, step 150 / 900, loss = 0.54 (0.007 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21, step 200 / 900, loss = 0.54 (0.007 sec/batch)\n",
      "epoch 21, step 250 / 900, loss = 0.37 (0.008 sec/batch)\n",
      "epoch 21, step 300 / 900, loss = 0.52 (0.007 sec/batch)\n",
      "epoch 21, step 350 / 900, loss = 0.59 (0.008 sec/batch)\n",
      "epoch 21, step 400 / 900, loss = 0.52 (0.008 sec/batch)\n",
      "epoch 21, step 450 / 900, loss = 0.54 (0.007 sec/batch)\n",
      "epoch 21, step 500 / 900, loss = 0.59 (0.006 sec/batch)\n",
      "epoch 21, step 550 / 900, loss = 0.48 (0.007 sec/batch)\n",
      "epoch 21, step 600 / 900, loss = 0.52 (0.007 sec/batch)\n",
      "epoch 21, step 650 / 900, loss = 0.40 (0.007 sec/batch)\n",
      "epoch 21, step 700 / 900, loss = 0.49 (0.007 sec/batch)\n",
      "epoch 21, step 750 / 900, loss = 0.49 (0.007 sec/batch)\n",
      "epoch 21, step 800 / 900, loss = 0.37 (0.007 sec/batch)\n",
      "epoch 21, step 850 / 900, loss = 0.30 (0.007 sec/batch)\n",
      "epoch 21, step 900 / 900, loss = 0.69 (0.008 sec/batch)\n",
      "Train error:\n",
      " accuracy = 84.80\n",
      " avg loss = 0.49\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.38\n",
      " avg loss = 0.93\n",
      "\n",
      "Epoch time: 9.367961406707764\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 22, step 50 / 900, loss = 0.45 (0.007 sec/batch)\n",
      "epoch 22, step 100 / 900, loss = 0.39 (0.007 sec/batch)\n",
      "epoch 22, step 150 / 900, loss = 0.79 (0.008 sec/batch)\n",
      "epoch 22, step 200 / 900, loss = 0.39 (0.007 sec/batch)\n",
      "epoch 22, step 250 / 900, loss = 0.48 (0.007 sec/batch)\n",
      "epoch 22, step 300 / 900, loss = 0.38 (0.008 sec/batch)\n",
      "epoch 22, step 350 / 900, loss = 0.50 (0.007 sec/batch)\n",
      "epoch 22, step 400 / 900, loss = 0.37 (0.009 sec/batch)\n",
      "epoch 22, step 450 / 900, loss = 0.31 (0.007 sec/batch)\n",
      "epoch 22, step 500 / 900, loss = 0.49 (0.007 sec/batch)\n",
      "epoch 22, step 550 / 900, loss = 0.47 (0.008 sec/batch)\n",
      "epoch 22, step 600 / 900, loss = 0.39 (0.008 sec/batch)\n",
      "epoch 22, step 650 / 900, loss = 0.70 (0.008 sec/batch)\n",
      "epoch 22, step 700 / 900, loss = 0.51 (0.007 sec/batch)\n",
      "epoch 22, step 750 / 900, loss = 0.58 (0.007 sec/batch)\n",
      "epoch 22, step 800 / 900, loss = 0.35 (0.007 sec/batch)\n",
      "epoch 22, step 850 / 900, loss = 0.42 (0.007 sec/batch)\n",
      "epoch 22, step 900 / 900, loss = 0.67 (0.008 sec/batch)\n",
      "Train error:\n",
      " accuracy = 85.47\n",
      " avg loss = 0.48\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.86\n",
      " avg loss = 0.93\n",
      "\n",
      "Epoch time: 9.43444275856018\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 23, step 50 / 900, loss = 0.45 (0.007 sec/batch)\n",
      "epoch 23, step 100 / 900, loss = 0.72 (0.007 sec/batch)\n",
      "epoch 23, step 150 / 900, loss = 0.37 (0.008 sec/batch)\n",
      "epoch 23, step 200 / 900, loss = 0.40 (0.007 sec/batch)\n",
      "epoch 23, step 250 / 900, loss = 0.40 (0.007 sec/batch)\n",
      "epoch 23, step 300 / 900, loss = 0.66 (0.008 sec/batch)\n",
      "epoch 23, step 350 / 900, loss = 0.54 (0.008 sec/batch)\n",
      "epoch 23, step 400 / 900, loss = 0.67 (0.007 sec/batch)\n",
      "epoch 23, step 450 / 900, loss = 0.60 (0.007 sec/batch)\n",
      "epoch 23, step 500 / 900, loss = 0.64 (0.007 sec/batch)\n",
      "epoch 23, step 550 / 900, loss = 0.44 (0.007 sec/batch)\n",
      "epoch 23, step 600 / 900, loss = 0.52 (0.007 sec/batch)\n",
      "epoch 23, step 650 / 900, loss = 0.35 (0.008 sec/batch)\n",
      "epoch 23, step 700 / 900, loss = 0.51 (0.008 sec/batch)\n",
      "epoch 23, step 750 / 900, loss = 0.45 (0.007 sec/batch)\n",
      "epoch 23, step 800 / 900, loss = 0.36 (0.008 sec/batch)\n",
      "epoch 23, step 850 / 900, loss = 0.65 (0.008 sec/batch)\n",
      "epoch 23, step 900 / 900, loss = 0.62 (0.008 sec/batch)\n",
      "Train error:\n",
      " accuracy = 85.51\n",
      " avg loss = 0.47\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.78\n",
      " avg loss = 0.93\n",
      "\n",
      "Epoch time: 9.53591251373291\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 24, step 50 / 900, loss = 0.54 (0.007 sec/batch)\n",
      "epoch 24, step 100 / 900, loss = 0.47 (0.008 sec/batch)\n",
      "epoch 24, step 150 / 900, loss = 0.47 (0.007 sec/batch)\n",
      "epoch 24, step 200 / 900, loss = 0.30 (0.008 sec/batch)\n",
      "epoch 24, step 250 / 900, loss = 0.61 (0.008 sec/batch)\n",
      "epoch 24, step 300 / 900, loss = 0.51 (0.007 sec/batch)\n",
      "epoch 24, step 350 / 900, loss = 0.62 (0.007 sec/batch)\n",
      "epoch 24, step 400 / 900, loss = 0.66 (0.007 sec/batch)\n",
      "epoch 24, step 450 / 900, loss = 0.42 (0.007 sec/batch)\n",
      "epoch 24, step 500 / 900, loss = 0.43 (0.007 sec/batch)\n",
      "epoch 24, step 550 / 900, loss = 0.65 (0.008 sec/batch)\n",
      "epoch 24, step 600 / 900, loss = 0.48 (0.008 sec/batch)\n",
      "epoch 24, step 650 / 900, loss = 0.33 (0.008 sec/batch)\n",
      "epoch 24, step 700 / 900, loss = 0.52 (0.007 sec/batch)\n",
      "epoch 24, step 750 / 900, loss = 0.56 (0.007 sec/batch)\n",
      "epoch 24, step 800 / 900, loss = 0.31 (0.007 sec/batch)\n",
      "epoch 24, step 850 / 900, loss = 0.63 (0.007 sec/batch)\n",
      "epoch 24, step 900 / 900, loss = 0.36 (0.008 sec/batch)\n",
      "Train error:\n",
      " accuracy = 86.00\n",
      " avg loss = 0.46\n",
      "\n",
      "Validation error:\n",
      " accuracy = 72.12\n",
      " avg loss = 0.93\n",
      "\n",
      "Epoch time: 9.397541761398315\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 25, step 50 / 900, loss = 0.49 (0.007 sec/batch)\n",
      "epoch 25, step 100 / 900, loss = 0.30 (0.007 sec/batch)\n",
      "epoch 25, step 150 / 900, loss = 0.35 (0.007 sec/batch)\n",
      "epoch 25, step 200 / 900, loss = 0.53 (0.007 sec/batch)\n",
      "epoch 25, step 250 / 900, loss = 0.76 (0.007 sec/batch)\n",
      "epoch 25, step 300 / 900, loss = 0.49 (0.007 sec/batch)\n",
      "epoch 25, step 350 / 900, loss = 0.41 (0.008 sec/batch)\n",
      "epoch 25, step 400 / 900, loss = 0.46 (0.007 sec/batch)\n",
      "epoch 25, step 450 / 900, loss = 0.71 (0.008 sec/batch)\n",
      "epoch 25, step 500 / 900, loss = 0.50 (0.008 sec/batch)\n",
      "epoch 25, step 550 / 900, loss = 0.47 (0.007 sec/batch)\n",
      "epoch 25, step 600 / 900, loss = 0.46 (0.007 sec/batch)\n",
      "epoch 25, step 650 / 900, loss = 0.38 (0.008 sec/batch)\n",
      "epoch 25, step 700 / 900, loss = 0.46 (0.008 sec/batch)\n",
      "epoch 25, step 750 / 900, loss = 0.59 (0.008 sec/batch)\n",
      "epoch 25, step 800 / 900, loss = 0.58 (0.007 sec/batch)\n",
      "epoch 25, step 850 / 900, loss = 0.45 (0.007 sec/batch)\n",
      "epoch 25, step 900 / 900, loss = 0.53 (0.008 sec/batch)\n",
      "Train error:\n",
      " accuracy = 86.32\n",
      " avg loss = 0.45\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.84\n",
      " avg loss = 0.94\n",
      "\n",
      "Epoch time: 9.367055654525757\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 26, step 50 / 900, loss = 0.30 (0.008 sec/batch)\n",
      "epoch 26, step 100 / 900, loss = 0.58 (0.007 sec/batch)\n",
      "epoch 26, step 150 / 900, loss = 0.46 (0.008 sec/batch)\n",
      "epoch 26, step 200 / 900, loss = 0.62 (0.007 sec/batch)\n",
      "epoch 26, step 250 / 900, loss = 0.29 (0.007 sec/batch)\n",
      "epoch 26, step 300 / 900, loss = 0.37 (0.007 sec/batch)\n",
      "epoch 26, step 350 / 900, loss = 0.45 (0.007 sec/batch)\n",
      "epoch 26, step 400 / 900, loss = 0.41 (0.007 sec/batch)\n",
      "epoch 26, step 450 / 900, loss = 0.42 (0.007 sec/batch)\n",
      "epoch 26, step 500 / 900, loss = 0.37 (0.008 sec/batch)\n",
      "epoch 26, step 550 / 900, loss = 0.38 (0.008 sec/batch)\n",
      "epoch 26, step 600 / 900, loss = 0.39 (0.008 sec/batch)\n",
      "epoch 26, step 650 / 900, loss = 0.43 (0.007 sec/batch)\n",
      "epoch 26, step 700 / 900, loss = 0.41 (0.008 sec/batch)\n",
      "epoch 26, step 750 / 900, loss = 0.45 (0.008 sec/batch)\n",
      "epoch 26, step 800 / 900, loss = 0.47 (0.007 sec/batch)\n",
      "epoch 26, step 850 / 900, loss = 0.35 (0.008 sec/batch)\n",
      "epoch 26, step 900 / 900, loss = 0.24 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 86.71\n",
      " avg loss = 0.45\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.96\n",
      " avg loss = 0.94\n",
      "\n",
      "Epoch time: 9.569688320159912\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 27, step 50 / 900, loss = 0.44 (0.007 sec/batch)\n",
      "epoch 27, step 100 / 900, loss = 0.33 (0.008 sec/batch)\n",
      "epoch 27, step 150 / 900, loss = 0.41 (0.008 sec/batch)\n",
      "epoch 27, step 200 / 900, loss = 0.61 (0.008 sec/batch)\n",
      "epoch 27, step 250 / 900, loss = 0.44 (0.008 sec/batch)\n",
      "epoch 27, step 300 / 900, loss = 0.49 (0.007 sec/batch)\n",
      "epoch 27, step 350 / 900, loss = 0.58 (0.008 sec/batch)\n",
      "epoch 27, step 400 / 900, loss = 0.31 (0.008 sec/batch)\n",
      "epoch 27, step 450 / 900, loss = 0.32 (0.007 sec/batch)\n",
      "epoch 27, step 500 / 900, loss = 0.55 (0.008 sec/batch)\n",
      "epoch 27, step 550 / 900, loss = 0.43 (0.007 sec/batch)\n",
      "epoch 27, step 600 / 900, loss = 0.46 (0.008 sec/batch)\n",
      "epoch 27, step 650 / 900, loss = 0.52 (0.007 sec/batch)\n",
      "epoch 27, step 700 / 900, loss = 0.37 (0.007 sec/batch)\n",
      "epoch 27, step 750 / 900, loss = 0.51 (0.008 sec/batch)\n",
      "epoch 27, step 800 / 900, loss = 0.47 (0.007 sec/batch)\n",
      "epoch 27, step 850 / 900, loss = 0.41 (0.008 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 27, step 900 / 900, loss = 0.53 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 86.78\n",
      " avg loss = 0.44\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.76\n",
      " avg loss = 0.94\n",
      "\n",
      "Epoch time: 9.514052629470825\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 28, step 50 / 900, loss = 0.39 (0.007 sec/batch)\n",
      "epoch 28, step 100 / 900, loss = 0.49 (0.008 sec/batch)\n",
      "epoch 28, step 150 / 900, loss = 0.33 (0.008 sec/batch)\n",
      "epoch 28, step 200 / 900, loss = 0.52 (0.008 sec/batch)\n",
      "epoch 28, step 250 / 900, loss = 0.42 (0.007 sec/batch)\n",
      "epoch 28, step 300 / 900, loss = 0.49 (0.008 sec/batch)\n",
      "epoch 28, step 350 / 900, loss = 0.38 (0.008 sec/batch)\n",
      "epoch 28, step 400 / 900, loss = 0.42 (0.007 sec/batch)\n",
      "epoch 28, step 450 / 900, loss = 0.39 (0.007 sec/batch)\n",
      "epoch 28, step 500 / 900, loss = 0.51 (0.007 sec/batch)\n",
      "epoch 28, step 550 / 900, loss = 0.41 (0.007 sec/batch)\n",
      "epoch 28, step 600 / 900, loss = 0.51 (0.007 sec/batch)\n",
      "epoch 28, step 650 / 900, loss = 0.44 (0.007 sec/batch)\n",
      "epoch 28, step 700 / 900, loss = 0.43 (0.007 sec/batch)\n",
      "epoch 28, step 750 / 900, loss = 0.47 (0.008 sec/batch)\n",
      "epoch 28, step 800 / 900, loss = 0.44 (0.007 sec/batch)\n",
      "epoch 28, step 850 / 900, loss = 0.53 (0.008 sec/batch)\n",
      "epoch 28, step 900 / 900, loss = 0.33 (0.008 sec/batch)\n",
      "Train error:\n",
      " accuracy = 86.67\n",
      " avg loss = 0.44\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.76\n",
      " avg loss = 0.95\n",
      "\n",
      "Epoch time: 9.47847867012024\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 29, step 50 / 900, loss = 0.31 (0.008 sec/batch)\n",
      "epoch 29, step 100 / 900, loss = 0.39 (0.007 sec/batch)\n",
      "epoch 29, step 150 / 900, loss = 0.48 (0.008 sec/batch)\n",
      "epoch 29, step 200 / 900, loss = 0.35 (0.007 sec/batch)\n",
      "epoch 29, step 250 / 900, loss = 0.59 (0.007 sec/batch)\n",
      "epoch 29, step 300 / 900, loss = 0.31 (0.008 sec/batch)\n",
      "epoch 29, step 350 / 900, loss = 0.39 (0.008 sec/batch)\n",
      "epoch 29, step 400 / 900, loss = 0.39 (0.007 sec/batch)\n",
      "epoch 29, step 450 / 900, loss = 0.43 (0.007 sec/batch)\n",
      "epoch 29, step 500 / 900, loss = 0.69 (0.007 sec/batch)\n",
      "epoch 29, step 550 / 900, loss = 0.40 (0.008 sec/batch)\n",
      "epoch 29, step 600 / 900, loss = 0.25 (0.008 sec/batch)\n",
      "epoch 29, step 650 / 900, loss = 0.37 (0.007 sec/batch)\n",
      "epoch 29, step 700 / 900, loss = 0.38 (0.012 sec/batch)\n",
      "epoch 29, step 750 / 900, loss = 0.32 (0.007 sec/batch)\n",
      "epoch 29, step 800 / 900, loss = 0.53 (0.008 sec/batch)\n",
      "epoch 29, step 850 / 900, loss = 0.40 (0.008 sec/batch)\n",
      "epoch 29, step 900 / 900, loss = 0.40 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 87.18\n",
      " avg loss = 0.43\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.76\n",
      " avg loss = 0.95\n",
      "\n",
      "Epoch time: 9.483714818954468\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 30, step 50 / 900, loss = 0.28 (0.007 sec/batch)\n",
      "epoch 30, step 100 / 900, loss = 0.51 (0.007 sec/batch)\n",
      "epoch 30, step 150 / 900, loss = 0.62 (0.008 sec/batch)\n",
      "epoch 30, step 200 / 900, loss = 0.37 (0.008 sec/batch)\n",
      "epoch 30, step 250 / 900, loss = 0.45 (0.008 sec/batch)\n",
      "epoch 30, step 300 / 900, loss = 0.31 (0.007 sec/batch)\n",
      "epoch 30, step 350 / 900, loss = 0.45 (0.007 sec/batch)\n",
      "epoch 30, step 400 / 900, loss = 0.37 (0.008 sec/batch)\n",
      "epoch 30, step 450 / 900, loss = 0.47 (0.007 sec/batch)\n",
      "epoch 30, step 500 / 900, loss = 0.45 (0.008 sec/batch)\n",
      "epoch 30, step 550 / 900, loss = 0.57 (0.007 sec/batch)\n",
      "epoch 30, step 600 / 900, loss = 0.50 (0.008 sec/batch)\n",
      "epoch 30, step 650 / 900, loss = 0.35 (0.007 sec/batch)\n",
      "epoch 30, step 700 / 900, loss = 0.41 (0.008 sec/batch)\n",
      "epoch 30, step 750 / 900, loss = 0.52 (0.008 sec/batch)\n",
      "epoch 30, step 800 / 900, loss = 0.41 (0.007 sec/batch)\n",
      "epoch 30, step 850 / 900, loss = 0.52 (0.007 sec/batch)\n",
      "epoch 30, step 900 / 900, loss = 0.46 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 87.44\n",
      " avg loss = 0.43\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.88\n",
      " avg loss = 0.95\n",
      "\n",
      "Epoch time: 9.421269178390503\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 31, step 50 / 900, loss = 0.38 (0.008 sec/batch)\n",
      "epoch 31, step 100 / 900, loss = 0.42 (0.007 sec/batch)\n",
      "epoch 31, step 150 / 900, loss = 0.43 (0.007 sec/batch)\n",
      "epoch 31, step 200 / 900, loss = 0.26 (0.007 sec/batch)\n",
      "epoch 31, step 250 / 900, loss = 0.44 (0.007 sec/batch)\n",
      "epoch 31, step 300 / 900, loss = 0.42 (0.007 sec/batch)\n",
      "epoch 31, step 350 / 900, loss = 0.35 (0.007 sec/batch)\n",
      "epoch 31, step 400 / 900, loss = 0.62 (0.007 sec/batch)\n",
      "epoch 31, step 450 / 900, loss = 0.46 (0.007 sec/batch)\n",
      "epoch 31, step 500 / 900, loss = 0.58 (0.008 sec/batch)\n",
      "epoch 31, step 550 / 900, loss = 0.39 (0.007 sec/batch)\n",
      "epoch 31, step 600 / 900, loss = 0.37 (0.007 sec/batch)\n",
      "epoch 31, step 650 / 900, loss = 0.39 (0.008 sec/batch)\n",
      "epoch 31, step 700 / 900, loss = 0.30 (0.007 sec/batch)\n",
      "epoch 31, step 750 / 900, loss = 0.53 (0.007 sec/batch)\n",
      "epoch 31, step 800 / 900, loss = 0.79 (0.007 sec/batch)\n",
      "epoch 31, step 850 / 900, loss = 0.29 (0.007 sec/batch)\n",
      "epoch 31, step 900 / 900, loss = 0.45 (0.008 sec/batch)\n",
      "Train error:\n",
      " accuracy = 87.58\n",
      " avg loss = 0.42\n",
      "\n",
      "Validation error:\n",
      " accuracy = 72.06\n",
      " avg loss = 0.95\n",
      "\n",
      "Epoch time: 9.374969959259033\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 32, step 50 / 900, loss = 0.30 (0.007 sec/batch)\n",
      "epoch 32, step 100 / 900, loss = 0.43 (0.008 sec/batch)\n",
      "epoch 32, step 150 / 900, loss = 0.27 (0.007 sec/batch)\n",
      "epoch 32, step 200 / 900, loss = 0.48 (0.008 sec/batch)\n",
      "epoch 32, step 250 / 900, loss = 0.37 (0.007 sec/batch)\n",
      "epoch 32, step 300 / 900, loss = 0.46 (0.008 sec/batch)\n",
      "epoch 32, step 350 / 900, loss = 0.38 (0.008 sec/batch)\n",
      "epoch 32, step 400 / 900, loss = 0.35 (0.008 sec/batch)\n",
      "epoch 32, step 450 / 900, loss = 0.34 (0.008 sec/batch)\n",
      "epoch 32, step 500 / 900, loss = 0.38 (0.008 sec/batch)\n",
      "epoch 32, step 550 / 900, loss = 0.50 (0.008 sec/batch)\n",
      "epoch 32, step 600 / 900, loss = 0.35 (0.008 sec/batch)\n",
      "epoch 32, step 650 / 900, loss = 0.41 (0.007 sec/batch)\n",
      "epoch 32, step 700 / 900, loss = 0.47 (0.007 sec/batch)\n",
      "epoch 32, step 750 / 900, loss = 0.44 (0.008 sec/batch)\n",
      "epoch 32, step 800 / 900, loss = 0.33 (0.007 sec/batch)\n",
      "epoch 32, step 850 / 900, loss = 0.35 (0.007 sec/batch)\n",
      "epoch 32, step 900 / 900, loss = 0.35 (0.008 sec/batch)\n",
      "Train error:\n",
      " accuracy = 87.82\n",
      " avg loss = 0.42\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.92\n",
      " avg loss = 0.95\n",
      "\n",
      "Epoch time: 9.478518009185791\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 33, step 50 / 900, loss = 0.56 (0.007 sec/batch)\n",
      "epoch 33, step 100 / 900, loss = 0.51 (0.008 sec/batch)\n",
      "epoch 33, step 150 / 900, loss = 0.41 (0.008 sec/batch)\n",
      "epoch 33, step 200 / 900, loss = 0.37 (0.007 sec/batch)\n",
      "epoch 33, step 250 / 900, loss = 0.51 (0.008 sec/batch)\n",
      "epoch 33, step 300 / 900, loss = 0.44 (0.007 sec/batch)\n",
      "epoch 33, step 350 / 900, loss = 0.52 (0.007 sec/batch)\n",
      "epoch 33, step 400 / 900, loss = 0.35 (0.008 sec/batch)\n",
      "epoch 33, step 450 / 900, loss = 0.34 (0.007 sec/batch)\n",
      "epoch 33, step 500 / 900, loss = 0.52 (0.008 sec/batch)\n",
      "epoch 33, step 550 / 900, loss = 0.28 (0.008 sec/batch)\n",
      "epoch 33, step 600 / 900, loss = 0.32 (0.007 sec/batch)\n",
      "epoch 33, step 650 / 900, loss = 0.35 (0.008 sec/batch)\n",
      "epoch 33, step 700 / 900, loss = 0.31 (0.007 sec/batch)\n",
      "epoch 33, step 750 / 900, loss = 0.43 (0.007 sec/batch)\n",
      "epoch 33, step 800 / 900, loss = 0.44 (0.008 sec/batch)\n",
      "epoch 33, step 850 / 900, loss = 0.35 (0.008 sec/batch)\n",
      "epoch 33, step 900 / 900, loss = 0.49 (0.008 sec/batch)\n",
      "Train error:\n",
      " accuracy = 87.77\n",
      " avg loss = 0.42\n",
      "\n",
      "Validation error:\n",
      " accuracy = 72.00\n",
      " avg loss = 0.95\n",
      "\n",
      "Epoch time: 9.44293212890625\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 34, step 50 / 900, loss = 0.53 (0.007 sec/batch)\n",
      "epoch 34, step 100 / 900, loss = 0.23 (0.008 sec/batch)\n",
      "epoch 34, step 150 / 900, loss = 0.35 (0.007 sec/batch)\n",
      "epoch 34, step 200 / 900, loss = 0.48 (0.007 sec/batch)\n",
      "epoch 34, step 250 / 900, loss = 0.50 (0.008 sec/batch)\n",
      "epoch 34, step 300 / 900, loss = 0.33 (0.008 sec/batch)\n",
      "epoch 34, step 350 / 900, loss = 0.33 (0.008 sec/batch)\n",
      "epoch 34, step 400 / 900, loss = 0.36 (0.007 sec/batch)\n",
      "epoch 34, step 450 / 900, loss = 0.62 (0.007 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 34, step 500 / 900, loss = 0.34 (0.008 sec/batch)\n",
      "epoch 34, step 550 / 900, loss = 0.26 (0.007 sec/batch)\n",
      "epoch 34, step 600 / 900, loss = 0.34 (0.008 sec/batch)\n",
      "epoch 34, step 650 / 900, loss = 0.47 (0.007 sec/batch)\n",
      "epoch 34, step 700 / 900, loss = 0.28 (0.007 sec/batch)\n",
      "epoch 34, step 750 / 900, loss = 0.58 (0.007 sec/batch)\n",
      "epoch 34, step 800 / 900, loss = 0.42 (0.008 sec/batch)\n",
      "epoch 34, step 850 / 900, loss = 0.37 (0.008 sec/batch)\n",
      "epoch 34, step 900 / 900, loss = 0.33 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 87.90\n",
      " avg loss = 0.41\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.96\n",
      " avg loss = 0.95\n",
      "\n",
      "Epoch time: 9.363494157791138\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 35, step 50 / 900, loss = 0.52 (0.008 sec/batch)\n",
      "epoch 35, step 100 / 900, loss = 0.67 (0.008 sec/batch)\n",
      "epoch 35, step 150 / 900, loss = 0.63 (0.007 sec/batch)\n",
      "epoch 35, step 200 / 900, loss = 0.29 (0.007 sec/batch)\n",
      "epoch 35, step 250 / 900, loss = 0.41 (0.007 sec/batch)\n",
      "epoch 35, step 300 / 900, loss = 0.35 (0.007 sec/batch)\n",
      "epoch 35, step 350 / 900, loss = 0.46 (0.007 sec/batch)\n",
      "epoch 35, step 400 / 900, loss = 0.62 (0.007 sec/batch)\n",
      "epoch 35, step 450 / 900, loss = 0.48 (0.007 sec/batch)\n",
      "epoch 35, step 500 / 900, loss = 0.72 (0.007 sec/batch)\n",
      "epoch 35, step 550 / 900, loss = 0.39 (0.007 sec/batch)\n",
      "epoch 35, step 600 / 900, loss = 0.27 (0.007 sec/batch)\n",
      "epoch 35, step 650 / 900, loss = 0.48 (0.007 sec/batch)\n",
      "epoch 35, step 700 / 900, loss = 0.34 (0.007 sec/batch)\n",
      "epoch 35, step 750 / 900, loss = 0.41 (0.008 sec/batch)\n",
      "epoch 35, step 800 / 900, loss = 0.74 (0.007 sec/batch)\n",
      "epoch 35, step 850 / 900, loss = 0.35 (0.007 sec/batch)\n",
      "epoch 35, step 900 / 900, loss = 0.62 (0.008 sec/batch)\n",
      "Train error:\n",
      " accuracy = 88.08\n",
      " avg loss = 0.41\n",
      "\n",
      "Validation error:\n",
      " accuracy = 72.00\n",
      " avg loss = 0.96\n",
      "\n",
      "Epoch time: 9.337905883789062\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 36, step 50 / 900, loss = 0.43 (0.007 sec/batch)\n",
      "epoch 36, step 100 / 900, loss = 0.25 (0.008 sec/batch)\n",
      "epoch 36, step 150 / 900, loss = 0.24 (0.008 sec/batch)\n",
      "epoch 36, step 200 / 900, loss = 0.39 (0.008 sec/batch)\n",
      "epoch 36, step 250 / 900, loss = 0.37 (0.007 sec/batch)\n",
      "epoch 36, step 300 / 900, loss = 0.30 (0.008 sec/batch)\n",
      "epoch 36, step 350 / 900, loss = 0.42 (0.008 sec/batch)\n",
      "epoch 36, step 400 / 900, loss = 0.44 (0.009 sec/batch)\n",
      "epoch 36, step 450 / 900, loss = 0.63 (0.007 sec/batch)\n",
      "epoch 36, step 500 / 900, loss = 0.39 (0.008 sec/batch)\n",
      "epoch 36, step 550 / 900, loss = 0.28 (0.007 sec/batch)\n",
      "epoch 36, step 600 / 900, loss = 0.34 (0.008 sec/batch)\n",
      "epoch 36, step 650 / 900, loss = 0.46 (0.007 sec/batch)\n",
      "epoch 36, step 700 / 900, loss = 0.38 (0.007 sec/batch)\n",
      "epoch 36, step 750 / 900, loss = 0.29 (0.008 sec/batch)\n",
      "epoch 36, step 800 / 900, loss = 0.63 (0.007 sec/batch)\n",
      "epoch 36, step 850 / 900, loss = 0.64 (0.007 sec/batch)\n",
      "epoch 36, step 900 / 900, loss = 0.42 (0.008 sec/batch)\n",
      "Train error:\n",
      " accuracy = 88.13\n",
      " avg loss = 0.41\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.92\n",
      " avg loss = 0.96\n",
      "\n",
      "Epoch time: 9.386979818344116\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 37, step 50 / 900, loss = 0.30 (0.008 sec/batch)\n",
      "epoch 37, step 100 / 900, loss = 0.59 (0.008 sec/batch)\n",
      "epoch 37, step 150 / 900, loss = 0.40 (0.007 sec/batch)\n",
      "epoch 37, step 200 / 900, loss = 0.61 (0.007 sec/batch)\n",
      "epoch 37, step 250 / 900, loss = 0.44 (0.007 sec/batch)\n",
      "epoch 37, step 300 / 900, loss = 0.53 (0.008 sec/batch)\n",
      "epoch 37, step 350 / 900, loss = 0.56 (0.008 sec/batch)\n",
      "epoch 37, step 400 / 900, loss = 0.61 (0.007 sec/batch)\n",
      "epoch 37, step 450 / 900, loss = 0.54 (0.008 sec/batch)\n",
      "epoch 37, step 500 / 900, loss = 0.36 (0.008 sec/batch)\n",
      "epoch 37, step 550 / 900, loss = 0.48 (0.008 sec/batch)\n",
      "epoch 37, step 600 / 900, loss = 0.28 (0.007 sec/batch)\n",
      "epoch 37, step 650 / 900, loss = 0.42 (0.007 sec/batch)\n",
      "epoch 37, step 700 / 900, loss = 0.41 (0.008 sec/batch)\n",
      "epoch 37, step 750 / 900, loss = 0.24 (0.008 sec/batch)\n",
      "epoch 37, step 800 / 900, loss = 0.28 (0.008 sec/batch)\n",
      "epoch 37, step 850 / 900, loss = 0.52 (0.008 sec/batch)\n",
      "epoch 37, step 900 / 900, loss = 0.46 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 88.19\n",
      " avg loss = 0.41\n",
      "\n",
      "Validation error:\n",
      " accuracy = 72.10\n",
      " avg loss = 0.96\n",
      "\n",
      "Epoch time: 9.323684692382812\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 38, step 50 / 900, loss = 0.20 (0.008 sec/batch)\n",
      "epoch 38, step 100 / 900, loss = 0.24 (0.008 sec/batch)\n",
      "epoch 38, step 150 / 900, loss = 0.69 (0.007 sec/batch)\n",
      "epoch 38, step 200 / 900, loss = 0.22 (0.007 sec/batch)\n",
      "epoch 38, step 250 / 900, loss = 0.46 (0.008 sec/batch)\n",
      "epoch 38, step 300 / 900, loss = 0.44 (0.008 sec/batch)\n",
      "epoch 38, step 350 / 900, loss = 0.28 (0.007 sec/batch)\n",
      "epoch 38, step 400 / 900, loss = 0.36 (0.007 sec/batch)\n",
      "epoch 38, step 450 / 900, loss = 0.35 (0.007 sec/batch)\n",
      "epoch 38, step 500 / 900, loss = 0.34 (0.008 sec/batch)\n",
      "epoch 38, step 550 / 900, loss = 0.63 (0.007 sec/batch)\n",
      "epoch 38, step 600 / 900, loss = 0.28 (0.008 sec/batch)\n",
      "epoch 38, step 650 / 900, loss = 0.49 (0.007 sec/batch)\n",
      "epoch 38, step 700 / 900, loss = 0.31 (0.008 sec/batch)\n",
      "epoch 38, step 750 / 900, loss = 0.43 (0.008 sec/batch)\n",
      "epoch 38, step 800 / 900, loss = 0.38 (0.008 sec/batch)\n",
      "epoch 38, step 850 / 900, loss = 0.42 (0.007 sec/batch)\n",
      "epoch 38, step 900 / 900, loss = 0.55 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 88.33\n",
      " avg loss = 0.41\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.90\n",
      " avg loss = 0.96\n",
      "\n",
      "Epoch time: 9.397969961166382\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 39, step 50 / 900, loss = 0.64 (0.007 sec/batch)\n",
      "epoch 39, step 100 / 900, loss = 0.52 (0.008 sec/batch)\n",
      "epoch 39, step 150 / 900, loss = 0.39 (0.008 sec/batch)\n",
      "epoch 39, step 200 / 900, loss = 0.40 (0.007 sec/batch)\n",
      "epoch 39, step 250 / 900, loss = 0.63 (0.007 sec/batch)\n",
      "epoch 39, step 300 / 900, loss = 0.47 (0.008 sec/batch)\n",
      "epoch 39, step 350 / 900, loss = 0.64 (0.008 sec/batch)\n",
      "epoch 39, step 400 / 900, loss = 0.58 (0.008 sec/batch)\n",
      "epoch 39, step 450 / 900, loss = 0.38 (0.007 sec/batch)\n",
      "epoch 39, step 500 / 900, loss = 0.45 (0.007 sec/batch)\n",
      "epoch 39, step 550 / 900, loss = 0.46 (0.007 sec/batch)\n",
      "epoch 39, step 600 / 900, loss = 0.42 (0.008 sec/batch)\n",
      "epoch 39, step 650 / 900, loss = 0.53 (0.007 sec/batch)\n",
      "epoch 39, step 700 / 900, loss = 0.18 (0.007 sec/batch)\n",
      "epoch 39, step 750 / 900, loss = 0.32 (0.008 sec/batch)\n",
      "epoch 39, step 800 / 900, loss = 0.42 (0.007 sec/batch)\n",
      "epoch 39, step 850 / 900, loss = 0.57 (0.008 sec/batch)\n",
      "epoch 39, step 900 / 900, loss = 0.52 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 88.37\n",
      " avg loss = 0.40\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.98\n",
      " avg loss = 0.96\n",
      "\n",
      "Epoch time: 9.310829162597656\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 40, step 50 / 900, loss = 0.33 (0.007 sec/batch)\n",
      "epoch 40, step 100 / 900, loss = 0.32 (0.008 sec/batch)\n",
      "epoch 40, step 150 / 900, loss = 0.32 (0.008 sec/batch)\n",
      "epoch 40, step 200 / 900, loss = 0.26 (0.008 sec/batch)\n",
      "epoch 40, step 250 / 900, loss = 0.57 (0.007 sec/batch)\n",
      "epoch 40, step 300 / 900, loss = 0.61 (0.007 sec/batch)\n",
      "epoch 40, step 350 / 900, loss = 0.54 (0.007 sec/batch)\n",
      "epoch 40, step 400 / 900, loss = 0.39 (0.007 sec/batch)\n",
      "epoch 40, step 450 / 900, loss = 0.48 (0.007 sec/batch)\n",
      "epoch 40, step 500 / 900, loss = 0.61 (0.007 sec/batch)\n",
      "epoch 40, step 550 / 900, loss = 0.47 (0.007 sec/batch)\n",
      "epoch 40, step 600 / 900, loss = 0.46 (0.007 sec/batch)\n",
      "epoch 40, step 650 / 900, loss = 0.48 (0.007 sec/batch)\n",
      "epoch 40, step 700 / 900, loss = 0.31 (0.007 sec/batch)\n",
      "epoch 40, step 750 / 900, loss = 0.34 (0.008 sec/batch)\n",
      "epoch 40, step 800 / 900, loss = 0.43 (0.007 sec/batch)\n",
      "epoch 40, step 850 / 900, loss = 0.43 (0.007 sec/batch)\n",
      "epoch 40, step 900 / 900, loss = 0.44 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 88.44\n",
      " avg loss = 0.40\n",
      "\n",
      "Validation error:\n",
      " accuracy = 72.32\n",
      " avg loss = 0.96\n",
      "\n",
      "Epoch time: 9.367578268051147\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "Total train time: 390.4536085128784\n"
     ]
    }
   ],
   "source": [
    "session = tf.Session()\n",
    "train_start = time.time()\n",
    "\n",
    "plot_data={}\n",
    "train_cifar(session, train_x, train_y, valid_x, valid_y, config, plot_data)\n",
    "\n",
    "print(\"Total train time:\", time.time() - train_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7YAAAHiCAYAAADcelBQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nM7GVJEmSJNW1/w8Qmtu0mWfnEgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2c60c7ef3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_training_progress(plot_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOP 20 worst samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEYCAYAAABsuVKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17e88596a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct class: airplane\n",
      "Predictions: ['horse' 'cat' 'dog'] [  9.99739468e-01   9.74641371e-05   8.75552141e-05]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEYCAYAAABsuVKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSSjYiEgSCjYikoSCjYgkoWAjIkn8f+e0E0lbrqeIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17f89d0e940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct class: cat\n",
      "Predictions: ['frog' 'bird' 'deer'] [  9.99967933e-01   2.01723269e-05   6.18917466e-06]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEYCAYAAABsuVKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKXogt15lAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17eea197668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct class: automobile\n",
      "Predictions: ['cat' 'bird' 'airplane'] [ 0.36930302  0.27390957  0.19647795]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEYCAYAAABsuVKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmGyIKAgmGyIKgsmGiIJgsiGiIP4/aN56M6AIyJ4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17f890d8a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct class: horse\n",
      "Predictions: ['bird' 'cat' 'frog'] [ 0.80542415  0.15984803  0.02156841]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEYCAYAAABsuVKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17eea1bc630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct class: ship\n",
      "Predictions: ['horse' 'truck' 'automobile'] [ 0.54567158  0.44412461  0.00595216]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEYCAYAAABsuVKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKNmISBJKNiKSxP8H+Plwhd9kWG8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17eea197cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct class: ship\n",
      "Predictions: ['horse' 'deer' 'cat'] [  9.80924249e-01   1.85880158e-02   2.54482380e-04]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEYCAYAAABsuVKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAKMN4ZbRCaNeAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17e889f0780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct class: dog\n",
      "Predictions: ['airplane' 'ship' 'bird'] [ 0.96621758  0.01631741  0.01473842]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEYCAYAAABsuVKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17f88fe7358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct class: bird\n",
      "Predictions: ['dog' 'cat' 'horse'] [ 0.47077549  0.19616599  0.19369781]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEYCAYAAABsuVKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17ee9f142e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct class: frog\n",
      "Predictions: ['horse' 'bird' 'dog'] [ 0.87164414  0.10383365  0.01343391]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEYCAYAAABsuVKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRCQLJRsRyULJRkSyULIRkSz+D2opgdyzgsqVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17e886b0668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct class: airplane\n",
      "Predictions: ['truck' 'cat' 'automobile'] [ 0.98806775  0.00313511  0.00250824]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEYCAYAAABsuVKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17eee6822b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct class: deer\n",
      "Predictions: ['dog' 'frog' 'horse'] [ 0.98990017  0.00438304  0.00317229]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEYCAYAAABsuVKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17eea1ac518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct class: ship\n",
      "Predictions: ['automobile' 'bird' 'cat'] [ 0.81574732  0.13052218  0.04060157]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEYCAYAAABsuVKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noWQjIkko2YhIEko2IpLE/wPHQfjqNm0vFAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17f89383e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct class: dog\n",
      "Predictions: ['deer' 'bird' 'airplane'] [ 0.67643625  0.16384377  0.14045107]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEYCAYAAABsuVKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17f893047b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct class: truck\n",
      "Predictions: ['ship' 'cat' 'airplane'] [  9.98607337e-01   9.56409203e-04   1.60390948e-04]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEYCAYAAABsuVKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAGjVJREFUeJzt3Xts3NWVB/DvmbffsZNJ4oQQJ4RHKY9AvSlVKaWlsIC6\nAvoHKtrtpl1W6a5oVaSudquutKXV/oFWfaja7aKGhxpaSqEhFNrSB2ShFFQBDg0JIVACBMjLdhI7\nthOPxzNz9o/5RQoh99yJPb4znnw/khV7ztz53fnZOf557pl7RFVBRDTTYrWeABGdGphsiCgIJhsi\nCoLJhoiCYLIhoiCYbIgoCCYbIgqCyYaIgmCyIaIgEiEPNm/ePO3p6Ql5SKpTk5OTzlixWDLHptIp\nMx4TccYKhYI5dmhoyIzH4+7fz51zOs2xEmvM3+2bNm3ar6pZ3/2mlWxE5GoA3wcQB3CXqt5u3b+n\npwd9fX3TOSTNFp53wezds88ZOzQ2Zo5d0nO6GW8xktHggQPm2IfWP2jG57S3OmPXX3+DOTbT5B47\nm4nI25Xcb8qpVkTiAH4A4BoA5wK4SUTOnerjEVFjm8513SoAO1T1TVXNA/gZgOuqMy0iajTTSTaL\nAbx7zNe7otveQ0TWiEifiPQNDg5O43BENJvN+CtWqrpWVXtVtTeb9b6GREQNajrJZjeAJcd8fVp0\nGxHR+0wn2bwA4EwRWSYiKQCfBfBodaZFRI1mykvfqloQkS8B+B3KS9/3qOq26UzG2jVQjNoJmhnW\n6rV4lra1VDTjv3/8d87YM396zhx70Yc/bMavuuJyZ6zn9Pe9rPgeo6N2nc1vf7PBGTv3A2ebYy9Y\nucqMa9E+qWJcGqjYtUniva6Y+f9f06qzUdXHADxWpbkQUQNrzJJGIqo7TDZEFASTDREFwWRDREEw\n2RBREEG3mPDh8nZ9sb8bnmXaeNyMW7stvLZ9szl29NCAGT/U734T8ocuvsAcu2h+mxlfOK/ZGfvl\nwz8xxy5dvNSMd2QXmHEtGed8FvzX4ZUNEQXBZENEQTDZEFEQTDZEFASTDREFwWRDREEw2RBREHVV\nZ8MtJmYPzw4T3rKPFWf0OGMdGXv0glb76MN7djhjv3rrZXNsqXjYjPcsmuuM5XP95tiHf36PGf+b\nz3zBjM9duNAZs0pwAKAe/vvwyoaIgmCyIaIgmGyIKAgmGyIKgsmGiIJgsiGiIJhsiCiIuqqzoUZi\nF36cdc45ztglq+yWJ8Pv2B2DmuLuopJkc5M5dujgATO+Y9tWZ2xuV6c59q2R5834z8cmzfjnvnir\nM9bS0W6OVd/+Q2a0OnhlQ0RBMNkQURBMNkQUBJMNEQXBZENEQTDZEFEQXPqmKfFtWaBaMONz5mad\nsZtW32yO/cl//6cZnzw05g5mUubYw4ftLSYkX3TGDuy3l83ntNvLzy9uesqMp+53t5FZ/Y+3mGPj\nCXvJP4RpJRsR2QlgFEARQEFVe6sxKSJqPNW4svmEqu6vwuMQUQPjazZEFMR0k40CeEJENonImhPd\nQUTWiEifiPQNDg5O83BENFtNN9lcqqorAVwD4BYRuez4O6jqWlXtVdXebNb9oiARNbZpJRtV3R39\nOwDgYQD2O+iI6JQ15WQjIi0i0nb0cwBXAbC3rieiU9Z0VqMWAHg4arGSAPBTVf1tVWZFDcAuxFEj\nfvoK9/YTAHDWSvsCetuzTzljhUm7jqZUmDDjna0tzlg+Zj/nN/bZrV7y9g4TeOzXG5yx5SvOMMde\n/qkbzHiINkpTTjaq+iaAC6syCyJqeFz6JqIgmGyIKAgmGyIKgsmGiIJgsiGiIJhsiCgI7mdDU+Sr\nvfD9HrPGl8yRy5cvNeNHdi9yxt7ev8cc29aaMePNiaQzVirZz1ng3o8GAPLjdg1QQtx7BP30R/eZ\nY+ctsOtwzjv/AmfMqsE5GbyyIaIgmGyIKAgmGyIKgsmGiIJgsiGiIJhsiCgILn3TjFC1l8at3RgO\njx40xx7Y9ZoZl6K7lUt+wmjzAqB7wVwznjaWtzOT7jYvgP8/WyHbYcZzefcS9K533jDH/nDtWjP+\nzW9+yxnr6uoyx1aKVzZEFASTDREFwWRDREEw2RBREEw2RBQEkw0RBcFkQ0RBNFCdjb0tgc3Oub53\n2Fvhkhn1t8nwbeRgxavTgMPBc05inrYmIyOHnLFfPPBTc+w7WzeZ8eaYux1LR1uTObatvd2MZ2Ip\nZ6y96N4CAgBamu1zMjbmaTMD99yTyfnm2De3v2TGf/3IL5yxv1v9BXNspXhlQ0RBMNkQURBMNkQU\nBJMNEQXBZENEQTDZEFEQTDZEFIS3zkZE7gHwaQADqnpedFsXgAcA9ADYCeBGVR2auWlWwLN/CsRd\nGKLqqdHx1JSIUVMS91S7iO/BYe+RoiX33P0dODxzE+N3Ucz+PTU6tN+Mr1//E2fsqSd+ZY5t9pyT\njoz7eXV22XU2yWTcjMcS7rgU7PPZke00420daTM+MuJ+3gdGRs2xzam8Gf/9bx5xxlZd8hFzbKUq\nubL5EYCrj7vtawA2quqZADZGXxMROXmTjao+DeD4rdOuA7Au+nwdgOurPC8iajBTfc1mgarujT7f\nB2BBleZDRA1q2i8Qa7k3p/PVARFZIyJ9ItI3ODg43cMR0Sw11WTTLyLdABD9O+C6o6quVdVeVe3N\nZrNTPBwRzXZTTTaPAlgdfb4agPulbCIiVJBsROR+AH8CcLaI7BKRmwHcDuBKEXkdwKeir4mInLx1\nNqp6kyN0RZXnMk2eOhuj5kSMGhwAKJXsfUaG+p1/RWLPzrfMsfv2vGPGRw/Z9Sqi7tqLiby9v0q+\n6DlniYwzFE/b9Sq5I3bdx1N/2OiMDXuec6LVrkfRuLv/UlHtOpqSUbcEAMnmFmcsnrDnJXHP3kYF\nuxZm19DbzthuT11TS2urGR8edf8MP/pL9143J4MVxEQUBJMNEQXBZENEQTDZEFEQTDZEFASTDREF\n0TCtXHzbKVjbQIyN2MuGj//uYTO+te85Z+zg7j3m2FLhiB0v2vFU0v28YmIv8xbE8+1Pu5dLR8ft\nZfUjh+1l3IPDY85YUe3fgYVme4l5smSck0TSHJvO2Ev6i3rOdMYWn7HSHFss+rblOP79zu81EfuL\nM9a64HxzbEuzGcbWra85Y3/Z8ao9uEK8siGiIJhsiCgIJhsiCoLJhoiCYLIhoiCYbIgoCCYbIgqi\nYepsrK4jADDQv9sZW3f3/5pjn/m/35rxiZy7ZkTE3sYhGbfjMc/2F80p97ewOWN/e5Np9xYSANBm\nlJwszrq3cQCAnbl9Znwil3PGmlrbzLES9/zYGqU0mWZ7bFubfeyeFRc6Y0vO+rA5VkqeeU/atUvn\nnv9xdzBh/xzFY/Zj79nzrjM2kbO33bjrznvN+FG8siGiIJhsiCgIJhsiCoLJhoiCYLIhoiCYbIgo\nCCYbIgqiYeps8nl735cf33uXM7bhofXm2ImxETNeVHcNgybtfJ6Ke1qLeNqxLJg31xlr77RrYTLp\nlBkfGOh3xlKeVi7z57ab8Vx+woi5a3DKx7Z/bJua3Ju3JNP296Oja54Zz85fZkTtvXJ8LYPiSfv7\n0WY87yLsWhh4WtisWD7HHl8FvLIhoiCYbIgoCCYbIgqCyYaIgmCyIaIgmGyIKIi6Wvq22rF4dmrA\nSy9uMuOP/+YxZyw/MWmOPZyzl5+tpe+WuN12pKvLXp7OzrG3PMjOcy9ZLl063xzb2dZixt/dPeiM\nbXntbXOsepaBl3a759Y/sNcc25qyl907m93lAE1N9hJwtnu5GW9pcc87Bl8/Ic8PsSds3SEO+3n5\njq1mLyTP86qQ98pGRO4RkQERefmY224Tkd0isjn6uLYqsyGihlXJn1E/AnD1CW7/nqqujD7clw1E\nRKgg2ajq0wDsVn1ERB7TeYH4yyKyJfozq9N1JxFZIyJ9ItI3OOh+DYCIGttUk80dAJYDWAlgL4Dv\nuO6oqmtVtVdVe7PZ7BQPR0Sz3ZSSjar2q2pRVUsA7gSwqrrTIqJGM6VkIyLdx3x5A4CXXfclIgIq\nqLMRkfsBXA5gnojsAvANAJeLyEqUF+B3AvhidaZjvU3ezot/eOpJMz48NOR+5JhnGwhPy5OSUWfT\n3GTXhLS1tprxjna7ziY3Pu6MvfWuXa8SO32xGT9jeY/7uJP2lgbbXtlhHzvprsNp7XDXyQBAwrO9\nRarZ/dhdWbv2qHvJWWY8ZtVN+cpRvHU0PtN+APcjm3U41TmuN9mo6k0nuPnuqhydiE4ZfLsCEQXB\nZENEQTDZEFEQTDZEFASTDREFwWRDREHU1X42VrnL8PB+c+z2V7eZ8WKh6D6up51KMmmfpkLBXXNS\nKNp74Rw8YL/HdXzEjncZ7VrSnvKIPQP2Y6eM1iJnLVtkjt07YL8Pbv/QsDPW2Wm3FcnE7IKWli53\nHc7SFReaY7uyZ5jxGSx1aXi8siGiIJhsiCgIJhsiCoLJhoiCYLIhoiCYbIgoiLpa+rYcHBow47t3\nv2vGJyYmnLF0xt5Cwn77PZBMuZeIE/HprZWm03YrmETc/fsiVrBb1BTd1QAAgOGxI85Yc7P7OQPA\nWStON+OvbHe3gtFJe94t6WYz3r2k2xk7bdlKc2wyZS+7m/tI+Fq1nOJ4ZUNEQTDZEFEQTDZEFAST\nDREFwWRDREEw2RBREEw2RBTErKmzef31V834waED9gMYNRC+Vi4ZTx1O3DiLMbFbnmQSdm1GR0e7\nGZ/T6p5bHHa9yuGxUTPe3OauZxmftLfOWLTQbsdyeMTdgmZiwn5sKeXN+IKF7hY1nV1LzLFe5vfT\n97v71K7D4ZUNEQXBZENEQTDZEFEQTDZEFASTDREFwWRDREEw2RBREN46GxFZAuBeAAtQ3sxjrap+\nX0S6ADwAoAfATgA3qurQdCaTz7nrJ575w7Pm2Mm8XXsBcbdriYmdc5sznpys7roQUbvtSFPK/hZk\n0nY8kXDPLdNk1+i0pez6oWTMfc4mc3YtTLrF3U4FAOZ2uWt4Dg67a3AAoKBJM97ecaYzlky1mGO1\nZH+/xPz9fGrX0fhUcmVTAPBVVT0XwCUAbhGRcwF8DcBGVT0TwMboayKiE/ImG1Xdq6ovRp+PAtgO\nYDGA6wCsi+62DsD1MzVJIpr9Tuo1GxHpAXARgOcALFDVvVFoH8p/Zp1ozBoR6RORvsFBu0siETWu\nipONiLQCeAjArao6cmxMVRWOzVlVda2q9qpqbzabndZkiWj2qijZiEgS5URzn6puiG7uF5HuKN4N\nwN6RnIhOad5kI+XWAncD2K6q3z0m9CiA1dHnqwE8Uv3pEVGjqGSLiY8C+ByArSKyObrt6wBuB/Cg\niNwM4G0AN053MgP97td0Nr/4Z3NsPm9vp5CMuVui5HLuNi8A0GyvECOddOfsuGf7ilTCvbwM2K1a\nAODwkTFnbG7W3ubh7LPPNuOlkns7hYMHD5pjfaUI6bS7Fcx4zn7s9q6FZry1dZ4R9SxPi730zeXt\nqfMmG1V9Bu4zfEV1p0NEjYoVxEQUBJMNEQXBZENEQTDZEFEQTDZEFASTDREFUVetXPb173LG9u+3\nC5QTVj8VAKWiu2akNGnX6Izn7NqLGNxbHiQz9lYLJc8WFLlczoynU8bvCy3axy7Yz7uzs9MZi8Fu\nUXN4bNiMFwvu533kiP2cEbdb0AwOuOu1Fp1m1xZ5Wd8uluCYeGVDREEw2RBREEw2RBQEkw0RBcFk\nQ0RBMNkQURBMNkQURNA6m1KphPFxd43E03/c6IzlJuzai5KnBceEsWdNzFPrUrS7liA/6R6fTNin\nOJ1y7+tSPrZdK9Pa2eaMtaTtjXjy40fM+ISx50xnu/u4AFCctPcIGhp2HzuTdrd5AYCc0fIHAJ5/\n/gVn7JwP9ppj055zZtbZkIlXNkQUBJMNEQXBZENEQTDZEFEQTDZEFASTDREFEXTpe2xsFM8++0dn\nfP36nzljsZj9/v143G6JYo0vTdrLy3lPW5K2FvcysBrtUACgVLSPLTF7aRzGsn0iYZ8z8bQtGRsb\nccYSCfv3lHpKEY6MjztjqZS77Q4AFOxvB9544w33cY/Yy/3ptL0lCE0dr2yIKAgmGyIKgsmGiIJg\nsiGiIJhsiCgIJhsiCoLJhoiCCFpnUygUMNjvbrMxMuSu64iLXa8Si7vbqQBAKuWOF2DXunh2oEDM\naCMT95TJFEt2OxWBXfeRsJ63p7VIMml/+ycn3Xtr5MbtLT8OjdntVg7n3HU2Kva8imrv+XH2OR9w\nxpqaWsyxXmzXMmXeKxsRWSIiT4rIKyKyTUS+Et1+m4jsFpHN0ce1Mz9dIpqtKrmyKQD4qqq+KCJt\nADaJyONR7Huq+u2Zmx4RNQpvslHVvQD2Rp+Pish2AItnemJE1FhO6gViEekBcBGA56KbviwiW0Tk\nHhE5Ya9WEVkjIn0i0jc6Yv8dT0SNq+JkIyKtAB4CcKuqjgC4A8ByACtRvvL5zonGqepaVe1V1d42\nz761RNS4Kko2IpJEOdHcp6obAEBV+1W1qKolAHcCWDVz0ySi2a6S1SgBcDeA7ar63WNu7z7mbjcA\neLn60yOiRlHJatRHAXwOwFYR2Rzd9nUAN4nISpSbW+wE8EXfA6VTaSxbttwZb8q4ayBGhvebj93c\natfZWD04vHvlxOy9cqzii3jSzue+R/a1DskYrWBSnj1nfO1WxsfctTSHht01UQCwZ+iwGR885K4v\nShg1UQBw5bWfMeNXXHW9M5ZO23vl0MypZDXqGZz4f9Nj1Z8OETUqvl2BiIJgsiGiIJhsiCgIJhsi\nCoLJhoiCYLIhoiCC7mfT3NKCD/X+lTP+t3//eWfsrh/eYT52SX0bjbgLVsp1i8Zjl+z9biaNvlPp\non2K00123UciYY+3+jelPX2jxo/YtTB79vQ7Y5Kw99nRVNaMz1/S44x95JJLzbFX/vU1ZjyRdJ9T\n9W1ORDOGVzZEFASTDREFwWRDREEw2RBREEw2RBQEkw0RBRF06VtEkExlnPF/+ucvOWMpYzkTAH7w\nP98z4wljG4lE0t7SIBGzl0tjMff4/IQ9Nh+3l9VT7XYvGGspt2B3v8Hh8bwZPzDqbrcyb+F8c+xl\nV3zajH/g/F5nbHF3tzMG+EsVrNVt31iaObyyIaIgmGyIKAgmGyIKgsmGiIJgsiGiIJhsiCgIJhsi\nCiJonY1POtPsjH3hH9aYY3ftetuMP7LhQWes5OmXkml21wYBQCLurrOJeeo6igX72PG43exlIu9u\nx7L/0Jg5dt/+Q2Y8B3eNz/IPXmyOveSjl5nxttY5zphvSw8xWucAgAh/h9YjfleIKAgmGyIKgsmG\niIJgsiGiIJhsiCgIJhsiCoLJhoiC8NbZiEgGwNMA0tH916vqN0SkC8ADAHoA7ARwo6oOTWcypZK7\n5qSppcUc+y//+nUz3tbW5oz98tEN5tjx8REzHi/knLFMwt6HZyw3acYP21vOYH7HImesfeFic2xq\nrl3js6TnDGfsYx//lDm2tdV9vgG7lsa/5YynHYsaG/mwBqdmKjnzEwA+qaoXAlgJ4GoRuQTA1wBs\nVNUzAWyMviYiOiFvstGyo6WoyehDAVwHYF10+zoA18/IDImoIVR0TSkicRHZDGAAwOOq+hyABaq6\nN7rLPgALHGPXiEifiPQNDg5WZdJENPtUlGxUtaiqKwGcBmCViJx3XFzh+ENaVdeqaq+q9mazdktW\nImpcJ/VqmaoOA3gSwNUA+kWkGwCifweqPz0iahTeZCMiWRGZE33eBOBKAK8CeBTA6uhuqwE8MlOT\nJKLZT6xWIAAgIheg/AJwHOXk9KCqfktE5gJ4EMDpAN5Geen7oPVYvb292tfXV5WJn6zJSfdWDM89\n/6w59s+bnzPjI8PuFX/N29sl5HLuZXMAuPhieyuHj338E85YS/tCc2wiYVc+pFJ2GxkiABCRTarq\n7s0T8dbZqOoWABed4PYDAK6Y2vSI6FTDCiciCoLJhoiCYLIhoiCYbIgoCCYbIgqCyYaIgvDW2VT1\nYCKDKNfkHDUPwP5gE6hcvc4LqN+5cV4nr17ndrLzWqqq3vciBU027zu4SF8lxUCh1eu8gPqdG+d1\n8up1bjM1L/4ZRURBMNkQURC1TjZra3x8l3qdF1C/c+O8Tl69zm1G5lXT12yI6NRR6ysbIjpFMNkQ\nURA1STYicrWIvCYiO0SkrroyiMhOEdkqIptFpDab75TncY+IDIjIy8fc1iUij4vI69G/nXU0t9tE\nZHd03jaLyLU1mNcSEXlSRF4RkW0i8pXo9pqeN2Ne9XDOMiLyvIi8FM3tm9HtVT9nwV+zEZE4gL+g\nvOPfLgAvALhJVV8JOhEHEdkJoFdVa1psJSKXARgDcK+qnhfd9l8ADqrq7VGS7lTVf6uTud0GYExV\nvx16PsfMqxtAt6q+KCJtADah3PXj86jheTPmdSNqf84EQIuqjolIEsAzAL4C4DOo8jmrxZXNKgA7\nVPVNVc0D+BnKbWHoGKr6NIDjdz6si/Y5jrnVnKruVdUXo89HAWwHsBg1Pm/GvGouZKumWiSbxQDe\nPebrXaiTEx9RAE+IyCYRWVPryRynovY5NfRlEdkS/ZlVkz/xjhKRHpR3mKy47VAIx80LqINzNp1W\nTSeDLxC/36VR25prANwS/clQd6z2OTVyB4DlKHdN3QvgO7WaiIi0AngIwK2q+p7eybU8byeYV12c\ns+m0ajoZtUg2uwEsOebr06Lb6oKq7o7+HQDwMMp/9tWLum2fo6r90Q9tCcCdqNF5i153eAjAfap6\ntIl7zc/bieZVL+fsqJlu1VSLZPMCgDNFZJmIpAB8FuW2MDUnIi3RC3gQkRYAVwF42R4VVN22zzn6\ngxm5ATU4b9GLnXcD2K6q3z0mVNPz5ppXnZyzcK2aVDX4B4BrUV6RegPAv9diDo55LQfwUvSxrZZz\nA3A/ypfWkyi/rnUzgLkANgJ4HcATALrqaG4/BrAVwJboB7W7BvO6FOXL/S0ANkcf19b6vBnzqodz\ndgGAP0dzeBnAf0S3V/2c8e0KRBQEXyAmoiCYbIgoCCYbIgqCyYaIgmCyIaIgmGyIKAgmGyIK4v8B\nj4vSadQ1t8EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17f89c58048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct class: deer\n",
      "Predictions: ['cat' 'dog' 'horse'] [ 0.76926351  0.11822023  0.08392049]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEYCAYAAABsuVKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCSUbEUni/wPDm95yjNvOIAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17e886b8550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct class: automobile\n",
      "Predictions: ['truck' 'horse' 'dog'] [  9.84286904e-01   1.51919518e-02   3.53193784e-04]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEYCAYAAABsuVKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17e88a4c1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct class: truck\n",
      "Predictions: ['bird' 'dog' 'horse'] [  9.98521388e-01   6.92658476e-04   2.98972009e-04]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEYCAYAAABsuVKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3VmMnOd1JuD3VFXvC5vdTTaX5iIuoiVqoeS2JC9xvCQZ\nWfGM7AQjRIMJNIAGysxkBBvIxRgJMPHcGYPYQS4GBuSxYzmRt/EeR3ZibZBk0xIpiaS4SCRFUtya\nbDbJ3teqOnPRJYCWed6/yG5+3Wq9D0CQrNNf1d9/VZ+uqu/UOebuEBG51nLzfQAi8u6gZCMiSSjZ\niEgSSjYikoSSjYgkoWQjIkko2YhIEko2IpKEko2IJFFIeWPtnZ3evXZd/AVeJqtnV+lcKsbXPTU1\nxdeW+W3X1MSnMatAe3JqksYt4wryOYuD9HxmXzfIVWd+YxnY6lwuT9c2NDXSuOXJw9rZNwV45uPs\n2lXcGz3hXNYnATzzuuO4ZTwlefWVV/rdfVnGDcwu2ZjZ3QD+DkAewP919y+wr+9euw4/fe5XYbw8\nPRHGclakx5Lxc4WBgdEwdvz4Cbp2ZCw+LgBY3rUijBWLJbr2zaNHaDxX4olwSUMNWZuRyIo8nrP4\nAVwuZZzwjB/KMvnhaGxpo2tvvON2Gq9vbY9vt8gTWTnjgeTl+HGYlahyGT+1ZhkJgYSLxazzzX/U\ny+S2C+SXKQCsb218k35BxVW/jDKzPID/A+ATAG4EcL+Z3Xi11ycii9ts3rO5A8Bhdz/i7lMAvg3g\n3rk5LBFZbGaTbFYDuPT1x8nKZb/BzB4ys51mtvNC/7lZ3JyIvJNd890od3/E3Xvcvae9M/M9JBFZ\npGaTbE4BWHPJ/7srl4mI/JbZJJsdADab2XVmVgvgTwD8ZG4OS0QWm6ve+nb3opn9dwD/gpmt76+5\n+76sdblcnN8sH2/jFqf4FvLp0700fq7vYhjLk9sFgHXrNtJ4fUNDGMvaIc7XNtP4ud7jND48eCGM\n1WT8LimUM+IkTGtZkL2NmyPxyXF+0kbOj9N4fXN8f5YtY+sbWXcYO2cZ28/8mjGZUe919MgbYWxi\nnJ+TLZu30ngdq13K2pKv0qzqbNz9cQCPz8mRiMiipo8riEgSSjYikoSSjYgkoWQjIkko2YhIEklb\nTBgAI+0aLg4MhrHjx/kHS4eGRmi8sz3+ZPbSpfGnhAGgppZvjbNt3ovnztO153r7afx8H19//MjB\nMNZ/9jRdW5PnW5rt5Lw0NTXx667h56y2tjaM5TM+eX1+kG/z3lnfGsaaOzrp2ulp3l0gz7a+jZdn\n5HJ8a3z3rt00/q1/fCyM1Rf4+f7kH/17Gr/lve+Nr7suvq+uhJ7ZiEgSSjYikoSSjYgkoWQjIkko\n2YhIEko2IpKEko2IJJG0zmZ6agpnTsX9tQ69EU8aKJC6DADoXkNGxABoIR37s7reF9i4FADucX3F\n0//6c7p2+xPP03hW64CBC31hbHRkiK5147UZDY0tYay+vp6uzdFWDEADWZ8zPvWhUM+vu2PtmjB2\newevqcpltIlwj297epq3iBgfjduBAMDuXS/R+P49r4axTjJRAgCGR/njCKTNStbkkmrpmY2IJKFk\nIyJJKNmISBJKNiKShJKNiCShZCMiSSjZiEgSSetshoaH8dTTT4fxG7beHMa6Vqyi111XV0fjTooF\nyuA9TCzHx38UyHiagYt85PDLL/I6m0IhY2QK+b6y6oNQG4+gAYAiKXcp2jRdW5PRX2V8ajSMlUq8\nJuSuD95F48u64smrfWf4HMUS6bcEAMtXxPVcA6QfEwD87J++R+MvvbCDxlmfn0LGY3TJkiX8usn9\nVSrx+7paemYjIkko2YhIEko2IpKEko2IJKFkIyJJKNmISBJJt74LhQI6l3aE8e6Vq+PFOb6VCudb\nljkyZoOvBLzMP2Ofy8en8bqNcbsDAFi1MR4xAwBtrXzL8nc+8MEwtmHTJrp2YHSYxvM18ffVnjES\npaW1mcb37t0bxspFfr7v+sAHaHxJazxm5qmnfkHXZtzV+OQf3x/GDh4+QNfu3r2LxsdG43IAABgb\nj+Mt6/lonY72uMUKAHgx3t6uIaOKrsSsko2ZHQMwDKAEoOjuPXNxUCKy+MzFM5uPujuftCYi73p6\nz0ZEkphtsnEAT5jZS2b20OW+wMweMrOdZrZzZIi3qRSRxWu2L6M+5O6nzGw5gF+Y2Wvu/uylX+Du\njwB4BADWb9yU9V6siCxSs3pm4+6nKn/3AfghgDvm4qBEZPG56mRjZk1m1vLWvwH8AYB4P1NE3tVm\n8zKqC8APbWYPvgDgm+5O55YU8gV0LInrM/JkP99zvA0EwMdoFMgn8M2zanj4x/cNcXxpJx+x0bmK\n16s0F/gImzVd8frajPE3rYV4VAsArOhaHsY2bLyerj0/NEDjK7vj+qJuMooFALKqPh77+t+HsVPH\nj9K1q9aQWi8Ag8Onw9jp0/vo2vEx/p7l73zkozT+YvMvw9jZ/mN07cB5/n13tMd1URN53r6lWled\nbNz9CIBb5+QoRGTR09a3iCShZCMiSSjZiEgSSjYikoSSjYgkoWQjIkkk7WcDdxTLcb2MFeKeM6U8\nr6OZGB+h8dZc3M+jtpbX2ZTL8XEBgOXj76k4PkbX9h/jo0XeHOI9Z8bOxB+4z+rNUtdWT+N/eO8n\nw9jaFV107Uj/GRrvXhb36VnWtpSu/dWzv6bx73zjm2FsKel1AwDD5y/Q+D9987Ewdr7/Dbq2buI8\njTeU+Dl7z3Xx6J0Ted4/6NWdz9H46/v2hLHaZt5TqVp6ZiMiSSjZiEgSSjYikoSSjYgkoWQjIkko\n2YhIEkm3vqenJ9B3Kh530dUVt2o4cfY4ve4zp+OP/gPAnTfFI0+6uuLxMgAwPTVI4xdG4/j4Ob61\nPXySb4eOl+MRGwBwaPrNMFYPvmXfPt5I40Pn4nN+aO8OvnaEt1NYviJujfH4dr613Xuyj8YxGZdB\nTA1P0qUDGVf9q2fi7enOdv67e93yeOsaAPpPvELjZYzHt13gW99Dxw/T+MhwfN25Gl4iUS09sxGR\nJJRsRCQJJRsRSULJRkSSULIRkSSUbEQkCSUbEUkiaZ3N5PgwDu95OowfOxSPqjhw8BC97rHhCRof\nuO1gGOvo5KMqWtoyRrnUxjn77BneNmBpK69haPFWGi8vie/CpnpeZ9NU4ANKd7+0PYydOhrXSwFA\nYwuv4Tn6Zvx9Hz/dS9dakf+O3HbLyjCWy2W0C8mYWlLXHp/vVSt4KwYf5j0/+s/xmqvmrrg9xtJN\nfGRQPsfH+uQm4zYrJc8anlMdPbMRkSSUbEQkCSUbEUlCyUZEklCyEZEklGxEJAklGxFJIrPOxsy+\nBuCTAPrc/abKZe0AvgNgPYBjAO5z94tZ11UqTWF46EQYH+2Nx54USb8NAOg7do7Gd5dHw9jWm+O6\nDAAYG+enqdAY9ymZKPPjbu/g+d6HeHyyKb7t7i0r6NqWMq+zObBvdxjrWMJrj9asX07j03VxvUvH\n2m66FrykCqXJeMzM+FT8OACAwSk+Oscb4++7sZnXsry8L671AoDTp3jfpLvW3hbGWlfxGp+JCd7H\nx6bjcUT5uSmzqeqZzdcB3P22yz4H4El33wzgycr/RURCmcnG3Z8F8PbJXfcCeLTy70cBfGqOj0tE\nFpmrfc+my93fqik/A4CPRxSRd71Zv0Hs7g4gfPFvZg+Z2U4z2zkxwfvpisjidbXJ5qyZrQSAyt9h\nm2h3f8Tde9y9p76ez9QWkcXrapPNTwA8UPn3AwB+PDeHIyKLVWayMbNvAdgOYIuZnTSzBwF8AcDv\nm9khAL9X+b+ISCizzsbd7w9CH7/iG6vNY+m6uD/LcotnCdnBt2+I/abXXj9L480r4n4f7ev4+9u5\n2rgGAQC8EL88bM/oI7K+i98Fvaf6afz8RDyfqZ+3lEF9B++B0kni7Z18TlG+ib8/N1o3FQed932p\nyXjYFhric15X4vdHzQj//Vssx4+FiRFeauYZv9qXrlhK4w2ktVFpnNfo1IAXy9SRsqnawty0vVIF\nsYgkoWQjIkko2YhIEko2IpKEko2IJKFkIyJJJB3lUrYyJgpxf4BiKd4ubWjjh3rzHVtovGNNRxgb\nK/OP35fLfCu2OB23aqhr5Pl89Tq+hWy7eeuMPPkIyNlDx+jaA33xtjkAFOrj72tsOG4HAgDj5/n3\nPbEkbjGRc976YkUjb1+RI+UGY8ave7DAW2fkx+Mt5FyJf89bNq+lcQd/nGEyvr8uXByhS2tqMrb8\n8/FonWJdxnybKumZjYgkoWQjIkko2YhIEko2IpKEko2IJKFkIyJJKNmISBJJ62y8XEZpNK7PKOTi\nGoelrbzLX2d7G4031MT1KLXjfNzKGKmjAYByLs7ZllHfUFuTMRIlF9ejAMA6xOfliPPrfmWI18rk\n6+JzWhriLSTK/byGp6EmfujlJvhxt57hNSX5jrgXQzN5jAHAxTH++3d6Oq6FyWX97q7ndTSTUxn1\nXqNxe4vhIn+cuPHHeCEf178VyH11JfTMRkSSULIRkSSUbEQkCSUbEUlCyUZEklCyEZEklGxEJImk\ndTY5AKwzhpF+NoUCz4u1NbzOoJlMsmguNNC1rY18Jorn4l4g5UleozMJPiamSGp4AKA1nnyMVuN3\nr2X0bskti+tVujr4OfN6/n2VRkh8kNcmHdj+Go13bL0ujN24Zh1d2956PY2X6+LHmbe00LWja5fR\n+L7e4zSem4xve3WBnzMYH+VSXx8/xpubec+lL+N5ftsVemYjIkko2YhIEko2IpKEko2IJKFkIyJJ\nKNmISBJJt74LuTw6WuNtNMvH27hsPAcANDTE27QAUCAjOMqTPOe2Na+h8aPH4nErO186QNd2NPAt\n5LZmvu0+WhwOY80Z5QJbC/F4GwAYLsb3x/U330zXLruBjy0Z6bsYxppH+H396yceo/Gzr/eGsQ1n\neCnC0ozt69x0vP1cWBdvuQPA1p5baHzzJh6vbY5bfjQ0ddK1AN/6zuXiVFBbmzXK5a8y4pXbyPoC\nM/uamfWZ2d5LLvu8mZ0ys12VP/dUdWsi8q5VzcuorwO4+zKX/627b6v8eXxuD0tEFpvMZOPuzwK4\nkOBYRGQRm80bxA+b2Z7Ky6yl0ReZ2UNmttPMdo6OTs3i5kTknexqk82XAWwAsA1AL4AvRl/o7o+4\ne4+79zQ1ZXx+Q0QWratKNu5+1t1L7l4G8BUAd8ztYYnIYnNVycbMVl7y308D2Bt9rYgIUEWdjZl9\nC8BHAHSa2UkAfw3gI2a2DYADOAbgz6q5sVzO0NgQv5TK18ZjSYy0cQCAoSE+JuOFZw6GsXMn+NiR\n9VviWhYAGB+NW2O88MIbdO208/Edd7bwOpulxfh9sPZpXluxpoGP91gxHbeB6D/B9wxu6fkojWNV\nHDq/dx9f67ydyNDJ/jB29OBpuvZsibfGGCW1YMN7d9C1XY2DNP7B/3gvjXc2rQxj5YYuurZU5j8f\nXo7bjbjzMUrVykw27n7/ZS7+6pzcuoi8a+jjCiKShJKNiCShZCMiSSjZiEgSSjYikoSSjYgkkXaU\nS74WjS1xbxj3uC5kcpzXCex/9RCNDw7GtQLH3uR1NvuPbKfx6zdtCmOb12yka88fO0bjdYO87mNZ\nKb4Lu2p4H5K2Mq/DGXjzVBh7PWPsyMDeo/y2l8S9dEb6TtK1tZig8faJuHbJyegbAPAaPt7Ga+O6\np8k8rwXb/wqvuWrr5rWxm7fFNWodt3bTtc6/bZRL8c9X1hilaumZjYgkoWQjIkko2YhIEko2IpKE\nko2IJKFkIyJJJN36bmxaip73/3EYr7X4cH797E563TdtiT9+DwDdLWfDWO8B3nagNB23kACAvuNH\nwth1HXzExnuWLKHx9YPxyBMAaCcVAY1lvt/ZUOCtGgqluH1FYYifk+Iru2l8rBBvy9d283PW0MLH\n33QNxcd9jrTkAICpHN/6bq+LH6NDZPsYAI6+Hj9OAKD34rdp/NPF+LnB3Xd9hK6dKvISCnZaCvyU\nVE3PbEQkCSUbEUlCyUZEklCyEZEklGxEJAklGxFJQslGRJJIWmdTX9eETdfdGcbLk3EtQLF4gF/5\n5BgNn937Whjb1MrHpZzI8xYUY6PxSJSps+fo2ppG3pZg0nntxnhDcxgrZrRTmKjhv2tq6uOHR2Oe\nt68YGOd1HUMTcXz4FK8tqsmY4txGpuOczKg9KtbxczI8Ht/X56d43dIF4yNRRsd5vVf364fD2O+N\n8nFDuQZ+f+VJLY3qbETkHUXJRkSSULIRkSSUbEQkCSUbEUlCyUZEklCyEZEkMutszGwNgG8A6ALg\nAB5x978zs3YA3wGwHsAxAPe5Oy+QgCFfjsdRFItxDcSq1avoNT/x/75H43dtjEeq3PP+2+naH32f\nX/f+4bjG56Lx2guAF40czfM6m7ampjD2/vdupWtv+NCHaLxscYFFXcb3lT9+nsb3PRuPxzlz9gRd\nWxoeoPHaprjfTam5la4dJDVTAK8f6ijEj20AWJbx45bnLYKwZ8crYez0Ed4rZ+XNW2i8TJ52lDPq\ntapVzTObIoC/cPcbAdwF4M/N7EYAnwPwpLtvBvBk5f8iIpeVmWzcvdfdX678exjAAQCrAdwL4NHK\nlz0K4FPX6iBF5J3vit6zMbP1AG4D8AKALnfvrYTOYOZl1uXWPGRmO81sZ39//ywOVUTeyapONmbW\nDOD7AD7r7r/xYSF3d+DyL+zc/RF373H3ns5O3ltWRBavqpKNmdVgJtE85u4/qFx81sxWVuIrAfRd\nm0MUkcUgM9mYmQH4KoAD7v6lS0I/AfBA5d8PAPjx3B+eiCwW1bSY+CCAPwXwqpntqlz2lwC+AOC7\nZvYggDcB3FfNDRrZyq1fEm9ZbrxuHb3e5hr+Ofj3fbgnjPVu/xVdu65/hMYv5OOcvb+ebxu2dbTT\nuGeMkTlPtiXHO/mYmGU3xOUAANC9Mj7nA/2DdG3DulEaH122LIw9u+M5uralnrdq2Hr7rWFsYnKC\nrv3uV/6BxtdYvL29pYG3C+nI+NXetuk9NP7c8eNh7NDBg3Ttqlv41nfwLsicykw27v48AAvCH5/b\nwxGRxUoVxCKShJKNiCShZCMiSSjZiEgSSjYikoSSjYgkkXSUC+Aoe9yaoEzqVQaHePeKpow6m1w+\nbg1gvWfp2taMkSf5Uvw9dTe00bX3fuweGj948HUa33hjXD8xPsrbPOx45kc0fnp5Sxgb7OetGCZy\n/P5Ye3183P/1vzxI165YuYbGW1fGH4s5tHsfXfvM3/+AxreSc3JLE6//KYxdoPGVq+LaIwB44fDR\nMHbixEm6dnqaj9Zxj+8vzxgnVC09sxGRJJRsRCQJJRsRSULJRkSSULIRkSSUbEQkCSUbEUkicZ2N\n0f38qXLcUyNfH/e6AYCpaT5axMbjvjANnUvp2pp1y2m848JwGLswwvvR/OvLL9D4Lbdvo/F/+5//\nQxibnOJ9eGyS1y415uP7o1iKuo7M6BuIx9sAQFtXPJpnw4Yb6VpzftvllrivTG1dI13bWMN70hRL\nk2FscJLXHrU38R+3wyfeoPHBkbhO58J53l/Iy7zuyf3aP+/QMxsRSULJRkSSULIRkSSUbEQkCSUb\nEUlCyUZEkki69e1lx/RE/HH1icl4m7ilnU/TzDU20fiBna+FsbVr19K1a3J8q7V3z5Ew9uIA384c\n6j9D4z2N8egQANi3f08Ya2rmLQ8a65tpfKo2bqdQW8+3iFu7+EOrWIjXHzp5iq7NkZYeAJBrjcsk\nzg4P0LWlVr413rokPicnet+ka08P8jEy7Q18rE/XqrhcoK6WH3fW1raHA1QAy3j8V0vPbEQkCSUb\nEUlCyUZEklCyEZEklGxEJAklGxFJQslGRJJIW2cDoESmQkxMTIWxkvGPyG/q6aHxJ370z2Fs62le\nw1Mc5bUZL5+Ja2W6unkNz7rVXTQ+corXnOwbj9tbNDXyu7emiY+Z8UJcu5GfZemFF+L6Ic+o6yhM\n8bEkpUL8O/TCRd5WY2B4iF93W3zObv3dj9G1dfW8ZmpijNfh7N/+Shi7fjNvy1GcjtuFAEDJ4xq3\nugKv16pW5jMbM1tjZk+b2X4z22dmn6lc/nkzO2Vmuyp/+AAkEXlXq+aZTRHAX7j7y2bWAuAlM/tF\nJfa37v431+7wRGSxyEw27t4LoLfy72EzOwBg9bU+MBFZXK7oDWIzWw/gNgBv9bJ82Mz2mNnXzOyy\nvTXN7CEz22lmO8+f5+NgRWTxqjrZmFkzgO8D+Ky7DwH4MoANALZh5pnPFy+3zt0fcfced+/p6OiY\ng0MWkXeiqpKNmdVgJtE85u4/AAB3P+vuJZ+ZOv4VAHdcu8MUkXe6anajDMBXARxw9y9dcvnKS77s\n0wD2zv3hichiUc1u1AcB/CmAV81sV+WyvwRwv5ltw0z5zDEAf5Z1RWUrY7QQjxcZr40PZ2Cc11bc\nvo3X2fQeiXuNvLx7N107MT5K400brwtjd9z5Abp22TLew6S+iY+wMcT1E+78nOXyvO7Dye+iUpFf\n9/RUPPIEAC5cjGuXLvbx9/amh/mYmKLHxVzTGce9pLmVxrfvPxDGlnfwkT/v2Rj3owGAp154gsbL\nXXGNz3vet5WuHRrop/GauvixME1qoq5ENbtRzwOX7azz+JwcgYi8K+jjCiKShJKNiCShZCMiSSjZ\niEgSSjYikoSSjYgkkbSfjU1Oof7oyTDe3H19HGvlH3XwHj436uEtG8JY1me2JqfiXh8A0NZK5is1\n8jqZUkZjmJmayliR1I1MTsX9gQAA03z+khfjeLHIz0kpY7bTxHjcu2ViOK7FAoDJ8XEaHybrx8Z4\nzdS66zfR+K+f+2UY++lzz9O1P3vmGRqvXcvrdB56+L+FsemauK8RALy4/UUaf1/PnWFscKCPrq2W\nntmISBJKNiKShJKNiCShZCMiSSjZiEgSSjYikkTSre9yuYSh8XhURhvi7dTGGp4XPaNdQp60amhd\nxbccs7af3eM2D6VyxvZymY/YyOUyfh+QQ8u6bjpXB4CR9eVyxtqMc8aUM5aWSQsJAJgiW/6Tk7z1\nxVRGmcOH/80nwljfyeN07akzR2m80FpP49OFuMzh5//yU7r25z/7OY0ffuNgGBsc4C09qqVnNiKS\nhJKNiCShZCMiSSjZiEgSSjYikoSSjYgkoWQjIkkkrbO5ODKEH/0yHlfx6c2bw1hTLW8hUSa1LgBQ\nJiNPkFG3kcvKyawuJJfnS3kYyKizYfUsuaxal+LV19nkMs531m0b+b4mjR8X2H0JIE/GktRljMZp\nzLivOzpWhrHVW+KRPgDQPRC3OQGAH3z32zT+jcf+MYwNDMSjcQDg3Dk+yuXYG2fC2Mhw3A7kSuiZ\njYgkoWQjIkko2YhIEko2IpKEko2IJKFkIyJJKNmISBKZdTZmVg/gWQB1la//nrv/tZm1A/gOgPUA\njgG4z90vsuuavjiM3u88HcZHP/rvwljjzTfQ46yd4LUXZvG3msuohcliJGd7xlWXjR93Rkcaft0Z\ni9lxz8TJFWT1nJlFPxsr8+PKurdYH5/MHj8ZJT7Oxt9k/DQdPnqMxn/+eFyDBgBv7D0cxkrOz1kp\no//Q2CAZBZPVYKhK1TyzmQTwMXe/FcA2AHeb2V0APgfgSXffDODJyv9FRC4rM9n4jLemftVU/jiA\newE8Wrn8UQCfuiZHKCKLQlXv2ZhZ3sx2AegD8At3fwFAl7v3Vr7kDICuYO1DZrbTzHaOk+mNIrK4\nVZVs3L3k7tsAdAO4w8xuelvcEby94O6PuHuPu/c0FJJ+FEtEFpAr2o1y9wEATwO4G8BZM1sJAJW/\n52YgsIgsSpnJxsyWmVlb5d8NAH4fwGsAfgLggcqXPQDgx9fqIEXkna+a1zUrATxqZnnMJKfvuvtP\nzWw7gO+a2YMA3gRwX9YVTU5O4fCRE2H8qSeeCmP3bVhPr7uYuT1H2iXksloacHRsScYWMG19AWRu\nMc9mKRtBM/MFs7jtrPE3ZAu6SEaxAEBxmr/3x8bMlEp8bdYpYT1BSgU+tmfHr16k8cMHj/CbzsWt\nM4oT/LZLGT8fZXJaMh8nVcpMNu6+B8Btl7n8PICPz8lRiMiipwpiEUlCyUZEklCyEZEklGxEJAkl\nGxFJQslGRJKwudpDr+rGzM5hpibnLZ0A+IyJ+bFQjwtYuMem47pyC/XYrvS41rn7sqwvSppsfuvG\nzXa6e8+8HUBgoR4XsHCPTcd15RbqsV2r49LLKBFJQslGRJKY72TzyDzffmShHhewcI9Nx3XlFuqx\nXZPjmtf3bETk3WO+n9mIyLuEko2IJDEvycbM7jaz183ssJktqKkMZnbMzF41s11mtnMej+NrZtZn\nZnsvuazdzH5hZocqfy9dQMf2eTM7VTlvu8zsnnk4rjVm9rSZ7TezfWb2mcrl83reyHEthHNWb2Yv\nmtnuyrH9r8rlc37Okr9nU2nCdRAzHf9OAtgB4H5335/0QAJmdgxAj7vPa7GVmX0YwAiAb7j7TZXL\n/jeAC+7+hUqSXuru/2OBHNvnAYy4+9+kPp5LjmslgJXu/rKZtQB4CTNTP/4T5vG8keO6D/N/zgxA\nk7uPmFkNgOcBfAbAH2GOz9l8PLO5A8Bhdz/i7lMAvo2ZsTByCXd/FsCFt128IMbnBMc279y9191f\nrvx7GMABAKsxz+eNHNe8SzmqaT6SzWoAl/YGPYkFcuIrHMATZvaSmT003wfzNlWNz5lHD5vZnsrL\nrHl5ifcWM1uPmQ6TVY8dSuFtxwUsgHM2m1FNV0JvEP+2D1XG1nwCwJ9XXjIsOGx8zjz5MoANmJma\n2gvgi/N1IGbWDOD7AD7r7kOXxubzvF3muBbEOZvNqKYrMR/J5hSANZf8v7ty2YLg7qcqf/cB+CFm\nXvYtFAt2fI67n608aMsAvoJ5Om+V9x2+D+Axd/9B5eJ5P2+XO66Fcs7ecq1HNc1HstkBYLOZXWdm\ntQD+BDNjYeadmTVV3sCDmTUB+AMAe/mqpBbs+Jy3HpgVn8Y8nLfKm51fBXDA3b90SWhez1t0XAvk\nnKUb1eSLJX7QAAAAk0lEQVTuyf8AuAczO1JvAPir+TiG4Lg2ANhd+bNvPo8NwLcw89R6GjPvaz0I\noAPAkwAOAXgCQPsCOrZ/APAqgD2VB+rKeTiuD2Hm6f4eALsqf+6Z7/NGjmshnLNbALxSOYa9AP5n\n5fI5P2f6uIKIJKE3iEUkCSUbEUlCyUZEklCyEZEklGxEJAklGxFJQslGRJL4/3wwQQINrPcDAAAA\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17f89bad908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct class: automobile\n",
      "Predictions: ['truck' 'cat' 'horse'] [ 0.9849745   0.0131389   0.00147518]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEYCAYAAABsuVKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyYaIomCyIaIomGyIKAomGyKK4v8DdePeKRKkW20AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17f88febcf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct class: ship\n",
      "Predictions: ['airplane' 'truck' 'deer'] [ 0.98544532  0.00985646  0.00249108]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEYCAYAAABsuVKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+d3PAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17e889e7ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct class: deer\n",
      "Predictions: ['automobile' 'truck' 'horse'] [ 0.99187875  0.00626307  0.00136699]\n"
     ]
    }
   ],
   "source": [
    "worst = worst_samples(session, test_x, test_y, config)\n",
    "\n",
    "for sample_id, l, predicted in worst:\n",
    "    show_image(test_x[sample_id], data_mean, data_std)\n",
    "    probas = session.run(tf.nn.softmax(logits), feed_dict={X: np.array([test_x[sample_id]])})\n",
    "    probas = probas[0]\n",
    "    predictions  = np.argsort(-probas)\n",
    "\n",
    "    print(\"Correct class:\", class_names[test_y[sample_id]])\n",
    "    print(\"Predictions:\", class_names[predictions[:3]], probas[predictions[:3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS task - Multiclass hinge loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multi_class_hinge_loss(logits, label, n_classes, delta=10):\n",
    "    label_oh = tf.one_hot(label, depth=n_classes, dtype=tf.float32)\n",
    "    mask = 1-label_oh\n",
    "    \n",
    "    correct_logits = tf.diag(tf.reduce_sum(label_oh * logits, 1))\n",
    "    correct_logits_mat = tf.matmul(correct_logits, tf.ones_like(logits))\n",
    "    \n",
    "    errors = tf.nn.relu(logits - correct_logits_mat + delta)\n",
    "    return errors * mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Graph\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "Y_ = tf.placeholder(tf.int32, [None,])\n",
    "logits = convnet(X, weights, biases)\n",
    "\n",
    "# loss\n",
    "regularizers = l2_loss(weights.values())\n",
    "loss_per_sample = multi_class_hinge_loss(logits, Y_, n_classes)\n",
    "data_loss = tf.reduce_mean(loss_per_sample)\n",
    "loss = data_loss + weight_decay*regularizers\n",
    "\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(0.01, global_step, 900, 0.9, staircase=True)\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, step 50 / 900, loss = 7.51 (0.007 sec/batch)\n",
      "epoch 1, step 100 / 900, loss = 6.23 (0.006 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Korisnik\\Anaconda3\\lib\\site-packages\\skimage\\util\\dtype.py:122: UserWarning: Possible precision loss when converting from float64 to uint8\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, step 150 / 900, loss = 4.86 (0.006 sec/batch)\n",
      "epoch 1, step 200 / 900, loss = 4.44 (0.006 sec/batch)\n",
      "epoch 1, step 250 / 900, loss = 4.82 (0.008 sec/batch)\n",
      "epoch 1, step 300 / 900, loss = 4.16 (0.007 sec/batch)\n",
      "epoch 1, step 350 / 900, loss = 4.65 (0.008 sec/batch)\n",
      "epoch 1, step 400 / 900, loss = 3.61 (0.009 sec/batch)\n",
      "epoch 1, step 450 / 900, loss = 3.45 (0.007 sec/batch)\n",
      "epoch 1, step 500 / 900, loss = 3.54 (0.007 sec/batch)\n",
      "epoch 1, step 550 / 900, loss = 3.37 (0.008 sec/batch)\n",
      "epoch 1, step 600 / 900, loss = 2.82 (0.008 sec/batch)\n",
      "epoch 1, step 650 / 900, loss = 2.56 (0.007 sec/batch)\n",
      "epoch 1, step 700 / 900, loss = 3.11 (0.007 sec/batch)\n",
      "epoch 1, step 750 / 900, loss = 2.81 (0.006 sec/batch)\n",
      "epoch 1, step 800 / 900, loss = 4.42 (0.007 sec/batch)\n",
      "epoch 1, step 850 / 900, loss = 3.11 (0.007 sec/batch)\n",
      "epoch 1, step 900 / 900, loss = 3.24 (0.009 sec/batch)\n",
      "Train error:\n",
      " accuracy = 50.23\n",
      " avg loss = 2.92\n",
      "\n",
      "Validation error:\n",
      " accuracy = 48.92\n",
      " avg loss = 2.98\n",
      "\n",
      "Epoch time: 9.073502540588379\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 2, step 50 / 900, loss = 3.44 (0.006 sec/batch)\n",
      "epoch 2, step 100 / 900, loss = 3.18 (0.007 sec/batch)\n",
      "epoch 2, step 150 / 900, loss = 3.85 (0.007 sec/batch)\n",
      "epoch 2, step 200 / 900, loss = 3.59 (0.007 sec/batch)\n",
      "epoch 2, step 250 / 900, loss = 2.79 (0.007 sec/batch)\n",
      "epoch 2, step 300 / 900, loss = 2.77 (0.009 sec/batch)\n",
      "epoch 2, step 350 / 900, loss = 2.38 (0.007 sec/batch)\n",
      "epoch 2, step 400 / 900, loss = 2.29 (0.006 sec/batch)\n",
      "epoch 2, step 450 / 900, loss = 3.38 (0.007 sec/batch)\n",
      "epoch 2, step 500 / 900, loss = 3.47 (0.007 sec/batch)\n",
      "epoch 2, step 550 / 900, loss = 1.90 (0.007 sec/batch)\n",
      "epoch 2, step 600 / 900, loss = 2.09 (0.006 sec/batch)\n",
      "epoch 2, step 650 / 900, loss = 2.46 (0.007 sec/batch)\n",
      "epoch 2, step 700 / 900, loss = 2.80 (0.007 sec/batch)\n",
      "epoch 2, step 750 / 900, loss = 2.90 (0.008 sec/batch)\n",
      "epoch 2, step 800 / 900, loss = 2.76 (0.007 sec/batch)\n",
      "epoch 2, step 850 / 900, loss = 3.19 (0.007 sec/batch)\n",
      "epoch 2, step 900 / 900, loss = 2.35 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 57.97\n",
      " avg loss = 2.31\n",
      "\n",
      "Validation error:\n",
      " accuracy = 57.36\n",
      " avg loss = 2.46\n",
      "\n",
      "Epoch time: 9.08277678489685\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 3, step 50 / 900, loss = 2.63 (0.008 sec/batch)\n",
      "epoch 3, step 100 / 900, loss = 2.53 (0.007 sec/batch)\n",
      "epoch 3, step 150 / 900, loss = 1.81 (0.007 sec/batch)\n",
      "epoch 3, step 200 / 900, loss = 1.65 (0.007 sec/batch)\n",
      "epoch 3, step 250 / 900, loss = 3.04 (0.008 sec/batch)\n",
      "epoch 3, step 300 / 900, loss = 2.80 (0.007 sec/batch)\n",
      "epoch 3, step 350 / 900, loss = 3.05 (0.007 sec/batch)\n",
      "epoch 3, step 400 / 900, loss = 1.98 (0.009 sec/batch)\n",
      "epoch 3, step 450 / 900, loss = 2.69 (0.007 sec/batch)\n",
      "epoch 3, step 500 / 900, loss = 2.24 (0.009 sec/batch)\n",
      "epoch 3, step 550 / 900, loss = 2.21 (0.007 sec/batch)\n",
      "epoch 3, step 600 / 900, loss = 2.51 (0.009 sec/batch)\n",
      "epoch 3, step 650 / 900, loss = 1.92 (0.007 sec/batch)\n",
      "epoch 3, step 700 / 900, loss = 2.15 (0.007 sec/batch)\n",
      "epoch 3, step 750 / 900, loss = 1.92 (0.008 sec/batch)\n",
      "epoch 3, step 800 / 900, loss = 1.73 (0.009 sec/batch)\n",
      "epoch 3, step 850 / 900, loss = 1.50 (0.007 sec/batch)\n",
      "epoch 3, step 900 / 900, loss = 1.23 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 62.55\n",
      " avg loss = 1.96\n",
      "\n",
      "Validation error:\n",
      " accuracy = 60.68\n",
      " avg loss = 2.16\n",
      "\n",
      "Epoch time: 9.118268251419067\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 4, step 50 / 900, loss = 1.63 (0.007 sec/batch)\n",
      "epoch 4, step 100 / 900, loss = 2.26 (0.008 sec/batch)\n",
      "epoch 4, step 150 / 900, loss = 1.99 (0.007 sec/batch)\n",
      "epoch 4, step 200 / 900, loss = 2.12 (0.007 sec/batch)\n",
      "epoch 4, step 250 / 900, loss = 1.77 (0.007 sec/batch)\n",
      "epoch 4, step 300 / 900, loss = 1.92 (0.007 sec/batch)\n",
      "epoch 4, step 350 / 900, loss = 2.34 (0.007 sec/batch)\n",
      "epoch 4, step 400 / 900, loss = 2.37 (0.007 sec/batch)\n",
      "epoch 4, step 450 / 900, loss = 1.43 (0.009 sec/batch)\n",
      "epoch 4, step 500 / 900, loss = 1.45 (0.007 sec/batch)\n",
      "epoch 4, step 550 / 900, loss = 1.85 (0.006 sec/batch)\n",
      "epoch 4, step 600 / 900, loss = 1.70 (0.006 sec/batch)\n",
      "epoch 4, step 650 / 900, loss = 2.88 (0.007 sec/batch)\n",
      "epoch 4, step 700 / 900, loss = 2.01 (0.009 sec/batch)\n",
      "epoch 4, step 750 / 900, loss = 1.82 (0.006 sec/batch)\n",
      "epoch 4, step 800 / 900, loss = 1.64 (0.007 sec/batch)\n",
      "epoch 4, step 850 / 900, loss = 1.27 (0.007 sec/batch)\n",
      "epoch 4, step 900 / 900, loss = 1.93 (0.008 sec/batch)\n",
      "Train error:\n",
      " accuracy = 65.03\n",
      " avg loss = 1.76\n",
      "\n",
      "Validation error:\n",
      " accuracy = 62.18\n",
      " avg loss = 2.05\n",
      "\n",
      "Epoch time: 9.131751298904419\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 5, step 50 / 900, loss = 1.93 (0.010 sec/batch)\n",
      "epoch 5, step 100 / 900, loss = 1.73 (0.007 sec/batch)\n",
      "epoch 5, step 150 / 900, loss = 1.16 (0.007 sec/batch)\n",
      "epoch 5, step 200 / 900, loss = 1.52 (0.006 sec/batch)\n",
      "epoch 5, step 250 / 900, loss = 1.77 (0.008 sec/batch)\n",
      "epoch 5, step 300 / 900, loss = 1.65 (0.007 sec/batch)\n",
      "epoch 5, step 350 / 900, loss = 2.20 (0.007 sec/batch)\n",
      "epoch 5, step 400 / 900, loss = 1.46 (0.007 sec/batch)\n",
      "epoch 5, step 450 / 900, loss = 1.62 (0.007 sec/batch)\n",
      "epoch 5, step 500 / 900, loss = 1.54 (0.009 sec/batch)\n",
      "epoch 5, step 550 / 900, loss = 1.42 (0.007 sec/batch)\n",
      "epoch 5, step 600 / 900, loss = 1.71 (0.007 sec/batch)\n",
      "epoch 5, step 650 / 900, loss = 2.33 (0.006 sec/batch)\n",
      "epoch 5, step 700 / 900, loss = 1.21 (0.009 sec/batch)\n",
      "epoch 5, step 750 / 900, loss = 0.95 (0.007 sec/batch)\n",
      "epoch 5, step 800 / 900, loss = 2.19 (0.007 sec/batch)\n",
      "epoch 5, step 850 / 900, loss = 2.04 (0.007 sec/batch)\n",
      "epoch 5, step 900 / 900, loss = 1.22 (0.009 sec/batch)\n",
      "Train error:\n",
      " accuracy = 67.93\n",
      " avg loss = 1.58\n",
      "\n",
      "Validation error:\n",
      " accuracy = 65.20\n",
      " avg loss = 1.89\n",
      "\n",
      "Epoch time: 9.094632863998413\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 6, step 50 / 900, loss = 1.98 (0.006 sec/batch)\n",
      "epoch 6, step 100 / 900, loss = 1.48 (0.006 sec/batch)\n",
      "epoch 6, step 150 / 900, loss = 1.04 (0.007 sec/batch)\n",
      "epoch 6, step 200 / 900, loss = 1.09 (0.007 sec/batch)\n",
      "epoch 6, step 250 / 900, loss = 1.02 (0.006 sec/batch)\n",
      "epoch 6, step 300 / 900, loss = 1.35 (0.007 sec/batch)\n",
      "epoch 6, step 350 / 900, loss = 1.70 (0.007 sec/batch)\n",
      "epoch 6, step 400 / 900, loss = 1.67 (0.007 sec/batch)\n",
      "epoch 6, step 450 / 900, loss = 1.55 (0.007 sec/batch)\n",
      "epoch 6, step 500 / 900, loss = 0.95 (0.007 sec/batch)\n",
      "epoch 6, step 550 / 900, loss = 1.09 (0.007 sec/batch)\n",
      "epoch 6, step 600 / 900, loss = 0.97 (0.007 sec/batch)\n",
      "epoch 6, step 650 / 900, loss = 1.12 (0.007 sec/batch)\n",
      "epoch 6, step 700 / 900, loss = 1.03 (0.007 sec/batch)\n",
      "epoch 6, step 750 / 900, loss = 2.15 (0.007 sec/batch)\n",
      "epoch 6, step 800 / 900, loss = 2.03 (0.008 sec/batch)\n",
      "epoch 6, step 850 / 900, loss = 1.67 (0.007 sec/batch)\n",
      "epoch 6, step 900 / 900, loss = 1.66 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 68.76\n",
      " avg loss = 1.50\n",
      "\n",
      "Validation error:\n",
      " accuracy = 65.40\n",
      " avg loss = 1.90\n",
      "\n",
      "Epoch time: 9.130816221237183\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 7, step 50 / 900, loss = 1.37 (0.006 sec/batch)\n",
      "epoch 7, step 100 / 900, loss = 1.64 (0.007 sec/batch)\n",
      "epoch 7, step 150 / 900, loss = 1.55 (0.006 sec/batch)\n",
      "epoch 7, step 200 / 900, loss = 1.11 (0.007 sec/batch)\n",
      "epoch 7, step 250 / 900, loss = 1.07 (0.007 sec/batch)\n",
      "epoch 7, step 300 / 900, loss = 0.67 (0.009 sec/batch)\n",
      "epoch 7, step 350 / 900, loss = 1.38 (0.008 sec/batch)\n",
      "epoch 7, step 400 / 900, loss = 1.53 (0.007 sec/batch)\n",
      "epoch 7, step 450 / 900, loss = 0.81 (0.007 sec/batch)\n",
      "epoch 7, step 500 / 900, loss = 1.28 (0.007 sec/batch)\n",
      "epoch 7, step 550 / 900, loss = 1.69 (0.006 sec/batch)\n",
      "epoch 7, step 600 / 900, loss = 1.09 (0.007 sec/batch)\n",
      "epoch 7, step 650 / 900, loss = 1.61 (0.007 sec/batch)\n",
      "epoch 7, step 700 / 900, loss = 1.89 (0.007 sec/batch)\n",
      "epoch 7, step 750 / 900, loss = 1.29 (0.007 sec/batch)\n",
      "epoch 7, step 800 / 900, loss = 1.26 (0.006 sec/batch)\n",
      "epoch 7, step 850 / 900, loss = 1.34 (0.006 sec/batch)\n",
      "epoch 7, step 900 / 900, loss = 1.82 (0.007 sec/batch)\n",
      "Train error:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy = 72.02\n",
      " avg loss = 1.31\n",
      "\n",
      "Validation error:\n",
      " accuracy = 67.40\n",
      " avg loss = 1.72\n",
      "\n",
      "Epoch time: 9.067446231842041\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 8, step 50 / 900, loss = 1.05 (0.009 sec/batch)\n",
      "epoch 8, step 100 / 900, loss = 1.60 (0.009 sec/batch)\n",
      "epoch 8, step 150 / 900, loss = 0.85 (0.007 sec/batch)\n",
      "epoch 8, step 200 / 900, loss = 1.44 (0.007 sec/batch)\n",
      "epoch 8, step 250 / 900, loss = 1.64 (0.007 sec/batch)\n",
      "epoch 8, step 300 / 900, loss = 1.91 (0.007 sec/batch)\n",
      "epoch 8, step 350 / 900, loss = 0.67 (0.007 sec/batch)\n",
      "epoch 8, step 400 / 900, loss = 1.88 (0.006 sec/batch)\n",
      "epoch 8, step 450 / 900, loss = 1.30 (0.009 sec/batch)\n",
      "epoch 8, step 500 / 900, loss = 1.71 (0.009 sec/batch)\n",
      "epoch 8, step 550 / 900, loss = 1.36 (0.007 sec/batch)\n",
      "epoch 8, step 600 / 900, loss = 2.08 (0.007 sec/batch)\n",
      "epoch 8, step 650 / 900, loss = 1.00 (0.007 sec/batch)\n",
      "epoch 8, step 700 / 900, loss = 1.22 (0.007 sec/batch)\n",
      "epoch 8, step 750 / 900, loss = 1.90 (0.008 sec/batch)\n",
      "epoch 8, step 800 / 900, loss = 1.53 (0.007 sec/batch)\n",
      "epoch 8, step 850 / 900, loss = 1.09 (0.007 sec/batch)\n",
      "epoch 8, step 900 / 900, loss = 2.21 (0.008 sec/batch)\n",
      "Train error:\n",
      " accuracy = 73.29\n",
      " avg loss = 1.23\n",
      "\n",
      "Validation error:\n",
      " accuracy = 68.42\n",
      " avg loss = 1.70\n",
      "\n",
      "Epoch time: 9.059067726135254\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 9, step 50 / 900, loss = 0.72 (0.007 sec/batch)\n",
      "epoch 9, step 100 / 900, loss = 0.76 (0.007 sec/batch)\n",
      "epoch 9, step 150 / 900, loss = 1.14 (0.007 sec/batch)\n",
      "epoch 9, step 200 / 900, loss = 1.03 (0.007 sec/batch)\n",
      "epoch 9, step 250 / 900, loss = 1.42 (0.009 sec/batch)\n",
      "epoch 9, step 300 / 900, loss = 1.21 (0.008 sec/batch)\n",
      "epoch 9, step 350 / 900, loss = 1.58 (0.010 sec/batch)\n",
      "epoch 9, step 400 / 900, loss = 0.93 (0.007 sec/batch)\n",
      "epoch 9, step 450 / 900, loss = 1.28 (0.007 sec/batch)\n",
      "epoch 9, step 500 / 900, loss = 1.02 (0.006 sec/batch)\n",
      "epoch 9, step 550 / 900, loss = 1.21 (0.007 sec/batch)\n",
      "epoch 9, step 600 / 900, loss = 1.06 (0.007 sec/batch)\n",
      "epoch 9, step 650 / 900, loss = 1.42 (0.007 sec/batch)\n",
      "epoch 9, step 700 / 900, loss = 0.95 (0.007 sec/batch)\n",
      "epoch 9, step 750 / 900, loss = 1.46 (0.007 sec/batch)\n",
      "epoch 9, step 800 / 900, loss = 1.15 (0.007 sec/batch)\n",
      "epoch 9, step 850 / 900, loss = 1.38 (0.007 sec/batch)\n",
      "epoch 9, step 900 / 900, loss = 1.48 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 74.71\n",
      " avg loss = 1.12\n",
      "\n",
      "Validation error:\n",
      " accuracy = 68.66\n",
      " avg loss = 1.64\n",
      "\n",
      "Epoch time: 9.096686363220215\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 10, step 50 / 900, loss = 0.89 (0.007 sec/batch)\n",
      "epoch 10, step 100 / 900, loss = 1.34 (0.007 sec/batch)\n",
      "epoch 10, step 150 / 900, loss = 0.92 (0.007 sec/batch)\n",
      "epoch 10, step 200 / 900, loss = 1.13 (0.007 sec/batch)\n",
      "epoch 10, step 250 / 900, loss = 0.83 (0.006 sec/batch)\n",
      "epoch 10, step 300 / 900, loss = 1.50 (0.006 sec/batch)\n",
      "epoch 10, step 350 / 900, loss = 0.83 (0.007 sec/batch)\n",
      "epoch 10, step 400 / 900, loss = 1.08 (0.007 sec/batch)\n",
      "epoch 10, step 450 / 900, loss = 1.08 (0.007 sec/batch)\n",
      "epoch 10, step 500 / 900, loss = 0.80 (0.009 sec/batch)\n",
      "epoch 10, step 550 / 900, loss = 1.27 (0.007 sec/batch)\n",
      "epoch 10, step 600 / 900, loss = 1.20 (0.006 sec/batch)\n",
      "epoch 10, step 650 / 900, loss = 0.76 (0.009 sec/batch)\n",
      "epoch 10, step 700 / 900, loss = 0.90 (0.007 sec/batch)\n",
      "epoch 10, step 750 / 900, loss = 1.33 (0.007 sec/batch)\n",
      "epoch 10, step 800 / 900, loss = 1.41 (0.006 sec/batch)\n",
      "epoch 10, step 850 / 900, loss = 0.84 (0.008 sec/batch)\n",
      "epoch 10, step 900 / 900, loss = 0.57 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 76.33\n",
      " avg loss = 1.05\n",
      "\n",
      "Validation error:\n",
      " accuracy = 70.22\n",
      " avg loss = 1.61\n",
      "\n",
      "Epoch time: 9.07310152053833\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 11, step 50 / 900, loss = 0.86 (0.009 sec/batch)\n",
      "epoch 11, step 100 / 900, loss = 1.08 (0.008 sec/batch)\n",
      "epoch 11, step 150 / 900, loss = 1.45 (0.007 sec/batch)\n",
      "epoch 11, step 200 / 900, loss = 1.08 (0.007 sec/batch)\n",
      "epoch 11, step 250 / 900, loss = 0.58 (0.007 sec/batch)\n",
      "epoch 11, step 300 / 900, loss = 0.83 (0.006 sec/batch)\n",
      "epoch 11, step 350 / 900, loss = 1.11 (0.006 sec/batch)\n",
      "epoch 11, step 400 / 900, loss = 0.90 (0.007 sec/batch)\n",
      "epoch 11, step 450 / 900, loss = 1.33 (0.007 sec/batch)\n",
      "epoch 11, step 500 / 900, loss = 1.16 (0.007 sec/batch)\n",
      "epoch 11, step 550 / 900, loss = 1.12 (0.006 sec/batch)\n",
      "epoch 11, step 600 / 900, loss = 0.79 (0.008 sec/batch)\n",
      "epoch 11, step 650 / 900, loss = 0.87 (0.007 sec/batch)\n",
      "epoch 11, step 700 / 900, loss = 1.38 (0.007 sec/batch)\n",
      "epoch 11, step 750 / 900, loss = 1.41 (0.007 sec/batch)\n",
      "epoch 11, step 800 / 900, loss = 0.94 (0.007 sec/batch)\n",
      "epoch 11, step 850 / 900, loss = 1.08 (0.006 sec/batch)\n",
      "epoch 11, step 900 / 900, loss = 0.92 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 77.24\n",
      " avg loss = 0.98\n",
      "\n",
      "Validation error:\n",
      " accuracy = 70.38\n",
      " avg loss = 1.59\n",
      "\n",
      "Epoch time: 9.034096240997314\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 12, step 50 / 900, loss = 0.85 (0.006 sec/batch)\n",
      "epoch 12, step 100 / 900, loss = 0.80 (0.007 sec/batch)\n",
      "epoch 12, step 150 / 900, loss = 0.70 (0.007 sec/batch)\n",
      "epoch 12, step 200 / 900, loss = 1.28 (0.007 sec/batch)\n",
      "epoch 12, step 250 / 900, loss = 1.04 (0.008 sec/batch)\n",
      "epoch 12, step 300 / 900, loss = 0.77 (0.006 sec/batch)\n",
      "epoch 12, step 350 / 900, loss = 0.75 (0.006 sec/batch)\n",
      "epoch 12, step 400 / 900, loss = 0.95 (0.008 sec/batch)\n",
      "epoch 12, step 450 / 900, loss = 1.00 (0.007 sec/batch)\n",
      "epoch 12, step 500 / 900, loss = 1.09 (0.007 sec/batch)\n",
      "epoch 12, step 550 / 900, loss = 0.72 (0.009 sec/batch)\n",
      "epoch 12, step 600 / 900, loss = 0.67 (0.009 sec/batch)\n",
      "epoch 12, step 650 / 900, loss = 0.82 (0.006 sec/batch)\n",
      "epoch 12, step 700 / 900, loss = 0.85 (0.007 sec/batch)\n",
      "epoch 12, step 750 / 900, loss = 2.10 (0.007 sec/batch)\n",
      "epoch 12, step 800 / 900, loss = 1.05 (0.006 sec/batch)\n",
      "epoch 12, step 850 / 900, loss = 1.20 (0.007 sec/batch)\n",
      "epoch 12, step 900 / 900, loss = 1.63 (0.009 sec/batch)\n",
      "Train error:\n",
      " accuracy = 75.64\n",
      " avg loss = 1.04\n",
      "\n",
      "Validation error:\n",
      " accuracy = 68.92\n",
      " avg loss = 1.67\n",
      "\n",
      "Epoch time: 9.014873266220093\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 13, step 50 / 900, loss = 0.84 (0.007 sec/batch)\n",
      "epoch 13, step 100 / 900, loss = 0.83 (0.007 sec/batch)\n",
      "epoch 13, step 150 / 900, loss = 0.72 (0.007 sec/batch)\n",
      "epoch 13, step 200 / 900, loss = 0.71 (0.007 sec/batch)\n",
      "epoch 13, step 250 / 900, loss = 0.78 (0.007 sec/batch)\n",
      "epoch 13, step 300 / 900, loss = 0.86 (0.008 sec/batch)\n",
      "epoch 13, step 350 / 900, loss = 1.15 (0.007 sec/batch)\n",
      "epoch 13, step 400 / 900, loss = 1.34 (0.007 sec/batch)\n",
      "epoch 13, step 450 / 900, loss = 0.89 (0.009 sec/batch)\n",
      "epoch 13, step 500 / 900, loss = 0.97 (0.006 sec/batch)\n",
      "epoch 13, step 550 / 900, loss = 0.74 (0.007 sec/batch)\n",
      "epoch 13, step 600 / 900, loss = 1.42 (0.007 sec/batch)\n",
      "epoch 13, step 650 / 900, loss = 1.37 (0.009 sec/batch)\n",
      "epoch 13, step 700 / 900, loss = 0.66 (0.006 sec/batch)\n",
      "epoch 13, step 750 / 900, loss = 1.48 (0.007 sec/batch)\n",
      "epoch 13, step 800 / 900, loss = 1.06 (0.006 sec/batch)\n",
      "epoch 13, step 850 / 900, loss = 1.32 (0.008 sec/batch)\n",
      "epoch 13, step 900 / 900, loss = 0.74 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 78.93\n",
      " avg loss = 0.87\n",
      "\n",
      "Validation error:\n",
      " accuracy = 70.86\n",
      " avg loss = 1.55\n",
      "\n",
      "Epoch time: 9.094472408294678\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 14, step 50 / 900, loss = 0.62 (0.009 sec/batch)\n",
      "epoch 14, step 100 / 900, loss = 0.94 (0.009 sec/batch)\n",
      "epoch 14, step 150 / 900, loss = 0.85 (0.007 sec/batch)\n",
      "epoch 14, step 200 / 900, loss = 1.36 (0.010 sec/batch)\n",
      "epoch 14, step 250 / 900, loss = 0.32 (0.007 sec/batch)\n",
      "epoch 14, step 300 / 900, loss = 0.78 (0.007 sec/batch)\n",
      "epoch 14, step 350 / 900, loss = 0.98 (0.007 sec/batch)\n",
      "epoch 14, step 400 / 900, loss = 0.54 (0.007 sec/batch)\n",
      "epoch 14, step 450 / 900, loss = 0.94 (0.007 sec/batch)\n",
      "epoch 14, step 500 / 900, loss = 0.73 (0.007 sec/batch)\n",
      "epoch 14, step 550 / 900, loss = 0.83 (0.007 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14, step 600 / 900, loss = 0.66 (0.008 sec/batch)\n",
      "epoch 14, step 650 / 900, loss = 0.92 (0.007 sec/batch)\n",
      "epoch 14, step 700 / 900, loss = 0.82 (0.008 sec/batch)\n",
      "epoch 14, step 750 / 900, loss = 0.69 (0.006 sec/batch)\n",
      "epoch 14, step 800 / 900, loss = 0.67 (0.007 sec/batch)\n",
      "epoch 14, step 850 / 900, loss = 0.66 (0.007 sec/batch)\n",
      "epoch 14, step 900 / 900, loss = 0.80 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 79.63\n",
      " avg loss = 0.84\n",
      "\n",
      "Validation error:\n",
      " accuracy = 70.74\n",
      " avg loss = 1.55\n",
      "\n",
      "Epoch time: 9.249760389328003\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 15, step 50 / 900, loss = 1.07 (0.007 sec/batch)\n",
      "epoch 15, step 100 / 900, loss = 1.18 (0.007 sec/batch)\n",
      "epoch 15, step 150 / 900, loss = 0.68 (0.007 sec/batch)\n",
      "epoch 15, step 200 / 900, loss = 1.01 (0.006 sec/batch)\n",
      "epoch 15, step 250 / 900, loss = 0.65 (0.007 sec/batch)\n",
      "epoch 15, step 300 / 900, loss = 0.66 (0.007 sec/batch)\n",
      "epoch 15, step 350 / 900, loss = 0.66 (0.006 sec/batch)\n",
      "epoch 15, step 400 / 900, loss = 0.67 (0.007 sec/batch)\n",
      "epoch 15, step 450 / 900, loss = 0.78 (0.007 sec/batch)\n",
      "epoch 15, step 500 / 900, loss = 0.40 (0.009 sec/batch)\n",
      "epoch 15, step 550 / 900, loss = 1.74 (0.009 sec/batch)\n",
      "epoch 15, step 600 / 900, loss = 1.02 (0.007 sec/batch)\n",
      "epoch 15, step 650 / 900, loss = 0.74 (0.007 sec/batch)\n",
      "epoch 15, step 700 / 900, loss = 0.41 (0.006 sec/batch)\n",
      "epoch 15, step 750 / 900, loss = 1.02 (0.008 sec/batch)\n",
      "epoch 15, step 800 / 900, loss = 1.01 (0.007 sec/batch)\n",
      "epoch 15, step 850 / 900, loss = 0.88 (0.007 sec/batch)\n",
      "epoch 15, step 900 / 900, loss = 0.54 (0.008 sec/batch)\n",
      "Train error:\n",
      " accuracy = 80.51\n",
      " avg loss = 0.79\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.04\n",
      " avg loss = 1.53\n",
      "\n",
      "Epoch time: 9.048942565917969\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 16, step 50 / 900, loss = 0.88 (0.007 sec/batch)\n",
      "epoch 16, step 100 / 900, loss = 0.77 (0.006 sec/batch)\n",
      "epoch 16, step 150 / 900, loss = 1.08 (0.006 sec/batch)\n",
      "epoch 16, step 200 / 900, loss = 0.36 (0.006 sec/batch)\n",
      "epoch 16, step 250 / 900, loss = 0.84 (0.006 sec/batch)\n",
      "epoch 16, step 300 / 900, loss = 0.56 (0.007 sec/batch)\n",
      "epoch 16, step 350 / 900, loss = 0.79 (0.009 sec/batch)\n",
      "epoch 16, step 400 / 900, loss = 1.03 (0.007 sec/batch)\n",
      "epoch 16, step 450 / 900, loss = 0.67 (0.007 sec/batch)\n",
      "epoch 16, step 500 / 900, loss = 0.53 (0.007 sec/batch)\n",
      "epoch 16, step 550 / 900, loss = 0.62 (0.007 sec/batch)\n",
      "epoch 16, step 600 / 900, loss = 0.80 (0.007 sec/batch)\n",
      "epoch 16, step 650 / 900, loss = 0.93 (0.009 sec/batch)\n",
      "epoch 16, step 700 / 900, loss = 0.62 (0.009 sec/batch)\n",
      "epoch 16, step 750 / 900, loss = 0.83 (0.007 sec/batch)\n",
      "epoch 16, step 800 / 900, loss = 1.10 (0.006 sec/batch)\n",
      "epoch 16, step 850 / 900, loss = 0.55 (0.009 sec/batch)\n",
      "epoch 16, step 900 / 900, loss = 0.60 (0.009 sec/batch)\n",
      "Train error:\n",
      " accuracy = 81.11\n",
      " avg loss = 0.75\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.00\n",
      " avg loss = 1.54\n",
      "\n",
      "Epoch time: 9.067134141921997\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 17, step 50 / 900, loss = 0.60 (0.008 sec/batch)\n",
      "epoch 17, step 100 / 900, loss = 0.60 (0.007 sec/batch)\n",
      "epoch 17, step 150 / 900, loss = 0.56 (0.006 sec/batch)\n",
      "epoch 17, step 200 / 900, loss = 0.55 (0.006 sec/batch)\n",
      "epoch 17, step 250 / 900, loss = 0.99 (0.007 sec/batch)\n",
      "epoch 17, step 300 / 900, loss = 0.65 (0.007 sec/batch)\n",
      "epoch 17, step 350 / 900, loss = 0.90 (0.007 sec/batch)\n",
      "epoch 17, step 400 / 900, loss = 0.73 (0.007 sec/batch)\n",
      "epoch 17, step 450 / 900, loss = 0.90 (0.008 sec/batch)\n",
      "epoch 17, step 500 / 900, loss = 0.78 (0.007 sec/batch)\n",
      "epoch 17, step 550 / 900, loss = 0.79 (0.007 sec/batch)\n",
      "epoch 17, step 600 / 900, loss = 0.84 (0.007 sec/batch)\n",
      "epoch 17, step 650 / 900, loss = 0.40 (0.007 sec/batch)\n",
      "epoch 17, step 700 / 900, loss = 0.40 (0.006 sec/batch)\n",
      "epoch 17, step 750 / 900, loss = 0.94 (0.006 sec/batch)\n",
      "epoch 17, step 800 / 900, loss = 0.87 (0.007 sec/batch)\n",
      "epoch 17, step 850 / 900, loss = 0.77 (0.008 sec/batch)\n",
      "epoch 17, step 900 / 900, loss = 0.65 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 81.86\n",
      " avg loss = 0.72\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.60\n",
      " avg loss = 1.52\n",
      "\n",
      "Epoch time: 9.062285900115967\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 18, step 50 / 900, loss = 0.55 (0.006 sec/batch)\n",
      "epoch 18, step 100 / 900, loss = 0.90 (0.008 sec/batch)\n",
      "epoch 18, step 150 / 900, loss = 0.64 (0.007 sec/batch)\n",
      "epoch 18, step 200 / 900, loss = 0.54 (0.007 sec/batch)\n",
      "epoch 18, step 250 / 900, loss = 0.80 (0.006 sec/batch)\n",
      "epoch 18, step 300 / 900, loss = 1.13 (0.007 sec/batch)\n",
      "epoch 18, step 350 / 900, loss = 0.56 (0.006 sec/batch)\n",
      "epoch 18, step 400 / 900, loss = 0.95 (0.007 sec/batch)\n",
      "epoch 18, step 450 / 900, loss = 0.90 (0.007 sec/batch)\n",
      "epoch 18, step 500 / 900, loss = 1.06 (0.006 sec/batch)\n",
      "epoch 18, step 550 / 900, loss = 0.48 (0.007 sec/batch)\n",
      "epoch 18, step 600 / 900, loss = 1.19 (0.007 sec/batch)\n",
      "epoch 18, step 650 / 900, loss = 0.68 (0.007 sec/batch)\n",
      "epoch 18, step 700 / 900, loss = 0.61 (0.007 sec/batch)\n",
      "epoch 18, step 750 / 900, loss = 0.82 (0.007 sec/batch)\n",
      "epoch 18, step 800 / 900, loss = 0.93 (0.007 sec/batch)\n",
      "epoch 18, step 850 / 900, loss = 0.94 (0.009 sec/batch)\n",
      "epoch 18, step 900 / 900, loss = 0.62 (0.009 sec/batch)\n",
      "Train error:\n",
      " accuracy = 82.24\n",
      " avg loss = 0.70\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.28\n",
      " avg loss = 1.53\n",
      "\n",
      "Epoch time: 9.083269119262695\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 19, step 50 / 900, loss = 0.75 (0.009 sec/batch)\n",
      "epoch 19, step 100 / 900, loss = 0.64 (0.007 sec/batch)\n",
      "epoch 19, step 150 / 900, loss = 0.51 (0.007 sec/batch)\n",
      "epoch 19, step 200 / 900, loss = 0.59 (0.006 sec/batch)\n",
      "epoch 19, step 250 / 900, loss = 0.60 (0.007 sec/batch)\n",
      "epoch 19, step 300 / 900, loss = 0.61 (0.007 sec/batch)\n",
      "epoch 19, step 350 / 900, loss = 0.59 (0.007 sec/batch)\n",
      "epoch 19, step 400 / 900, loss = 0.75 (0.007 sec/batch)\n",
      "epoch 19, step 450 / 900, loss = 0.67 (0.006 sec/batch)\n",
      "epoch 19, step 500 / 900, loss = 1.10 (0.007 sec/batch)\n",
      "epoch 19, step 550 / 900, loss = 0.71 (0.007 sec/batch)\n",
      "epoch 19, step 600 / 900, loss = 0.66 (0.007 sec/batch)\n",
      "epoch 19, step 650 / 900, loss = 1.05 (0.006 sec/batch)\n",
      "epoch 19, step 700 / 900, loss = 0.60 (0.006 sec/batch)\n",
      "epoch 19, step 750 / 900, loss = 0.69 (0.007 sec/batch)\n",
      "epoch 19, step 800 / 900, loss = 0.64 (0.007 sec/batch)\n",
      "epoch 19, step 850 / 900, loss = 0.70 (0.007 sec/batch)\n",
      "epoch 19, step 900 / 900, loss = 0.65 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 82.84\n",
      " avg loss = 0.67\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.86\n",
      " avg loss = 1.52\n",
      "\n",
      "Epoch time: 9.063470125198364\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 20, step 50 / 900, loss = 0.76 (0.007 sec/batch)\n",
      "epoch 20, step 100 / 900, loss = 0.54 (0.008 sec/batch)\n",
      "epoch 20, step 150 / 900, loss = 0.54 (0.007 sec/batch)\n",
      "epoch 20, step 200 / 900, loss = 0.83 (0.007 sec/batch)\n",
      "epoch 20, step 250 / 900, loss = 0.70 (0.008 sec/batch)\n",
      "epoch 20, step 300 / 900, loss = 0.37 (0.007 sec/batch)\n",
      "epoch 20, step 350 / 900, loss = 0.58 (0.007 sec/batch)\n",
      "epoch 20, step 400 / 900, loss = 0.86 (0.007 sec/batch)\n",
      "epoch 20, step 450 / 900, loss = 0.72 (0.006 sec/batch)\n",
      "epoch 20, step 500 / 900, loss = 0.40 (0.007 sec/batch)\n",
      "epoch 20, step 550 / 900, loss = 0.42 (0.006 sec/batch)\n",
      "epoch 20, step 600 / 900, loss = 0.82 (0.006 sec/batch)\n",
      "epoch 20, step 650 / 900, loss = 0.66 (0.007 sec/batch)\n",
      "epoch 20, step 700 / 900, loss = 0.84 (0.007 sec/batch)\n",
      "epoch 20, step 750 / 900, loss = 0.55 (0.007 sec/batch)\n",
      "epoch 20, step 800 / 900, loss = 0.62 (0.007 sec/batch)\n",
      "epoch 20, step 850 / 900, loss = 0.22 (0.007 sec/batch)\n",
      "epoch 20, step 900 / 900, loss = 0.39 (0.009 sec/batch)\n",
      "Train error:\n",
      " accuracy = 82.94\n",
      " avg loss = 0.66\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.68\n",
      " avg loss = 1.52\n",
      "\n",
      "Epoch time: 9.026525497436523\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 21, step 50 / 900, loss = 0.65 (0.007 sec/batch)\n",
      "epoch 21, step 100 / 900, loss = 0.45 (0.007 sec/batch)\n",
      "epoch 21, step 150 / 900, loss = 0.88 (0.007 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21, step 200 / 900, loss = 0.65 (0.009 sec/batch)\n",
      "epoch 21, step 250 / 900, loss = 0.57 (0.008 sec/batch)\n",
      "epoch 21, step 300 / 900, loss = 0.53 (0.007 sec/batch)\n",
      "epoch 21, step 350 / 900, loss = 0.91 (0.007 sec/batch)\n",
      "epoch 21, step 400 / 900, loss = 0.28 (0.008 sec/batch)\n",
      "epoch 21, step 450 / 900, loss = 0.50 (0.006 sec/batch)\n",
      "epoch 21, step 500 / 900, loss = 0.60 (0.007 sec/batch)\n",
      "epoch 21, step 550 / 900, loss = 0.46 (0.007 sec/batch)\n",
      "epoch 21, step 600 / 900, loss = 0.44 (0.007 sec/batch)\n",
      "epoch 21, step 650 / 900, loss = 0.60 (0.007 sec/batch)\n",
      "epoch 21, step 700 / 900, loss = 0.60 (0.007 sec/batch)\n",
      "epoch 21, step 750 / 900, loss = 0.53 (0.006 sec/batch)\n",
      "epoch 21, step 800 / 900, loss = 0.66 (0.007 sec/batch)\n",
      "epoch 21, step 850 / 900, loss = 0.59 (0.007 sec/batch)\n",
      "epoch 21, step 900 / 900, loss = 0.68 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 83.66\n",
      " avg loss = 0.63\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.74\n",
      " avg loss = 1.51\n",
      "\n",
      "Epoch time: 9.025930404663086\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 22, step 50 / 900, loss = 0.57 (0.008 sec/batch)\n",
      "epoch 22, step 100 / 900, loss = 0.40 (0.007 sec/batch)\n",
      "epoch 22, step 150 / 900, loss = 0.65 (0.007 sec/batch)\n",
      "epoch 22, step 200 / 900, loss = 0.72 (0.007 sec/batch)\n",
      "epoch 22, step 250 / 900, loss = 0.83 (0.007 sec/batch)\n",
      "epoch 22, step 300 / 900, loss = 0.45 (0.009 sec/batch)\n",
      "epoch 22, step 350 / 900, loss = 0.35 (0.009 sec/batch)\n",
      "epoch 22, step 400 / 900, loss = 0.46 (0.007 sec/batch)\n",
      "epoch 22, step 450 / 900, loss = 1.07 (0.007 sec/batch)\n",
      "epoch 22, step 500 / 900, loss = 0.68 (0.006 sec/batch)\n",
      "epoch 22, step 550 / 900, loss = 1.18 (0.007 sec/batch)\n",
      "epoch 22, step 600 / 900, loss = 0.76 (0.007 sec/batch)\n",
      "epoch 22, step 650 / 900, loss = 0.74 (0.007 sec/batch)\n",
      "epoch 22, step 700 / 900, loss = 0.63 (0.006 sec/batch)\n",
      "epoch 22, step 750 / 900, loss = 0.69 (0.010 sec/batch)\n",
      "epoch 22, step 800 / 900, loss = 0.74 (0.007 sec/batch)\n",
      "epoch 22, step 850 / 900, loss = 0.72 (0.006 sec/batch)\n",
      "epoch 22, step 900 / 900, loss = 0.62 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 83.98\n",
      " avg loss = 0.62\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.82\n",
      " avg loss = 1.51\n",
      "\n",
      "Epoch time: 9.034543752670288\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 23, step 50 / 900, loss = 1.06 (0.008 sec/batch)\n",
      "epoch 23, step 100 / 900, loss = 0.57 (0.007 sec/batch)\n",
      "epoch 23, step 150 / 900, loss = 0.68 (0.006 sec/batch)\n",
      "epoch 23, step 200 / 900, loss = 0.83 (0.007 sec/batch)\n",
      "epoch 23, step 250 / 900, loss = 0.49 (0.008 sec/batch)\n",
      "epoch 23, step 300 / 900, loss = 0.71 (0.007 sec/batch)\n",
      "epoch 23, step 350 / 900, loss = 0.70 (0.007 sec/batch)\n",
      "epoch 23, step 400 / 900, loss = 0.70 (0.007 sec/batch)\n",
      "epoch 23, step 450 / 900, loss = 0.38 (0.007 sec/batch)\n",
      "epoch 23, step 500 / 900, loss = 0.63 (0.007 sec/batch)\n",
      "epoch 23, step 550 / 900, loss = 0.79 (0.007 sec/batch)\n",
      "epoch 23, step 600 / 900, loss = 0.50 (0.006 sec/batch)\n",
      "epoch 23, step 650 / 900, loss = 0.77 (0.007 sec/batch)\n",
      "epoch 23, step 700 / 900, loss = 0.74 (0.007 sec/batch)\n",
      "epoch 23, step 750 / 900, loss = 0.43 (0.007 sec/batch)\n",
      "epoch 23, step 800 / 900, loss = 0.63 (0.007 sec/batch)\n",
      "epoch 23, step 850 / 900, loss = 0.77 (0.006 sec/batch)\n",
      "epoch 23, step 900 / 900, loss = 0.44 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 84.18\n",
      " avg loss = 0.60\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.20\n",
      " avg loss = 1.52\n",
      "\n",
      "Epoch time: 9.008318185806274\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 24, step 50 / 900, loss = 0.98 (0.007 sec/batch)\n",
      "epoch 24, step 100 / 900, loss = 0.58 (0.006 sec/batch)\n",
      "epoch 24, step 150 / 900, loss = 0.46 (0.006 sec/batch)\n",
      "epoch 24, step 200 / 900, loss = 0.46 (0.006 sec/batch)\n",
      "epoch 24, step 250 / 900, loss = 0.58 (0.009 sec/batch)\n",
      "epoch 24, step 300 / 900, loss = 0.36 (0.007 sec/batch)\n",
      "epoch 24, step 350 / 900, loss = 0.40 (0.007 sec/batch)\n",
      "epoch 24, step 400 / 900, loss = 0.38 (0.007 sec/batch)\n",
      "epoch 24, step 450 / 900, loss = 1.06 (0.007 sec/batch)\n",
      "epoch 24, step 500 / 900, loss = 0.73 (0.007 sec/batch)\n",
      "epoch 24, step 550 / 900, loss = 0.44 (0.007 sec/batch)\n",
      "epoch 24, step 600 / 900, loss = 0.59 (0.006 sec/batch)\n",
      "epoch 24, step 650 / 900, loss = 0.52 (0.007 sec/batch)\n",
      "epoch 24, step 700 / 900, loss = 0.32 (0.009 sec/batch)\n",
      "epoch 24, step 750 / 900, loss = 0.33 (0.007 sec/batch)\n",
      "epoch 24, step 800 / 900, loss = 0.62 (0.007 sec/batch)\n",
      "epoch 24, step 850 / 900, loss = 0.42 (0.007 sec/batch)\n",
      "epoch 24, step 900 / 900, loss = 1.00 (0.009 sec/batch)\n",
      "Train error:\n",
      " accuracy = 84.40\n",
      " avg loss = 0.59\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.58\n",
      " avg loss = 1.53\n",
      "\n",
      "Epoch time: 9.046659708023071\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 25, step 50 / 900, loss = 0.35 (0.008 sec/batch)\n",
      "epoch 25, step 100 / 900, loss = 0.49 (0.007 sec/batch)\n",
      "epoch 25, step 150 / 900, loss = 0.45 (0.006 sec/batch)\n",
      "epoch 25, step 200 / 900, loss = 0.51 (0.009 sec/batch)\n",
      "epoch 25, step 250 / 900, loss = 0.65 (0.007 sec/batch)\n",
      "epoch 25, step 300 / 900, loss = 1.27 (0.006 sec/batch)\n",
      "epoch 25, step 350 / 900, loss = 0.50 (0.007 sec/batch)\n",
      "epoch 25, step 400 / 900, loss = 0.57 (0.007 sec/batch)\n",
      "epoch 25, step 450 / 900, loss = 0.71 (0.007 sec/batch)\n",
      "epoch 25, step 500 / 900, loss = 0.32 (0.009 sec/batch)\n",
      "epoch 25, step 550 / 900, loss = 0.47 (0.009 sec/batch)\n",
      "epoch 25, step 600 / 900, loss = 0.90 (0.007 sec/batch)\n",
      "epoch 25, step 650 / 900, loss = 0.78 (0.006 sec/batch)\n",
      "epoch 25, step 700 / 900, loss = 0.35 (0.007 sec/batch)\n",
      "epoch 25, step 750 / 900, loss = 0.63 (0.007 sec/batch)\n",
      "epoch 25, step 800 / 900, loss = 0.50 (0.007 sec/batch)\n",
      "epoch 25, step 850 / 900, loss = 0.64 (0.009 sec/batch)\n",
      "epoch 25, step 900 / 900, loss = 0.45 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 84.73\n",
      " avg loss = 0.58\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.64\n",
      " avg loss = 1.53\n",
      "\n",
      "Epoch time: 9.034089803695679\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 26, step 50 / 900, loss = 0.65 (0.007 sec/batch)\n",
      "epoch 26, step 100 / 900, loss = 0.85 (0.006 sec/batch)\n",
      "epoch 26, step 150 / 900, loss = 0.53 (0.009 sec/batch)\n",
      "epoch 26, step 200 / 900, loss = 0.46 (0.007 sec/batch)\n",
      "epoch 26, step 250 / 900, loss = 0.66 (0.007 sec/batch)\n",
      "epoch 26, step 300 / 900, loss = 0.58 (0.006 sec/batch)\n",
      "epoch 26, step 350 / 900, loss = 0.50 (0.007 sec/batch)\n",
      "epoch 26, step 400 / 900, loss = 0.74 (0.007 sec/batch)\n",
      "epoch 26, step 450 / 900, loss = 0.41 (0.007 sec/batch)\n",
      "epoch 26, step 500 / 900, loss = 0.55 (0.006 sec/batch)\n",
      "epoch 26, step 550 / 900, loss = 0.73 (0.009 sec/batch)\n",
      "epoch 26, step 600 / 900, loss = 0.36 (0.009 sec/batch)\n",
      "epoch 26, step 650 / 900, loss = 1.27 (0.007 sec/batch)\n",
      "epoch 26, step 700 / 900, loss = 0.54 (0.007 sec/batch)\n",
      "epoch 26, step 750 / 900, loss = 0.53 (0.007 sec/batch)\n",
      "epoch 26, step 800 / 900, loss = 0.45 (0.007 sec/batch)\n",
      "epoch 26, step 850 / 900, loss = 0.50 (0.007 sec/batch)\n",
      "epoch 26, step 900 / 900, loss = 0.56 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 85.01\n",
      " avg loss = 0.57\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.46\n",
      " avg loss = 1.52\n",
      "\n",
      "Epoch time: 9.025359630584717\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 27, step 50 / 900, loss = 0.77 (0.006 sec/batch)\n",
      "epoch 27, step 100 / 900, loss = 0.51 (0.006 sec/batch)\n",
      "epoch 27, step 150 / 900, loss = 0.58 (0.006 sec/batch)\n",
      "epoch 27, step 200 / 900, loss = 0.56 (0.007 sec/batch)\n",
      "epoch 27, step 250 / 900, loss = 0.66 (0.007 sec/batch)\n",
      "epoch 27, step 300 / 900, loss = 0.70 (0.006 sec/batch)\n",
      "epoch 27, step 350 / 900, loss = 0.50 (0.006 sec/batch)\n",
      "epoch 27, step 400 / 900, loss = 0.31 (0.007 sec/batch)\n",
      "epoch 27, step 450 / 900, loss = 0.40 (0.007 sec/batch)\n",
      "epoch 27, step 500 / 900, loss = 0.73 (0.007 sec/batch)\n",
      "epoch 27, step 550 / 900, loss = 1.00 (0.009 sec/batch)\n",
      "epoch 27, step 600 / 900, loss = 0.52 (0.006 sec/batch)\n",
      "epoch 27, step 650 / 900, loss = 0.69 (0.007 sec/batch)\n",
      "epoch 27, step 700 / 900, loss = 0.56 (0.008 sec/batch)\n",
      "epoch 27, step 750 / 900, loss = 0.74 (0.007 sec/batch)\n",
      "epoch 27, step 800 / 900, loss = 0.77 (0.006 sec/batch)\n",
      "epoch 27, step 850 / 900, loss = 0.68 (0.007 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 27, step 900 / 900, loss = 0.64 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 85.08\n",
      " avg loss = 0.57\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.58\n",
      " avg loss = 1.53\n",
      "\n",
      "Epoch time: 9.036214828491211\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 28, step 50 / 900, loss = 0.67 (0.006 sec/batch)\n",
      "epoch 28, step 100 / 900, loss = 0.71 (0.007 sec/batch)\n",
      "epoch 28, step 150 / 900, loss = 0.36 (0.008 sec/batch)\n",
      "epoch 28, step 200 / 900, loss = 0.79 (0.007 sec/batch)\n",
      "epoch 28, step 250 / 900, loss = 0.65 (0.006 sec/batch)\n",
      "epoch 28, step 300 / 900, loss = 0.45 (0.007 sec/batch)\n",
      "epoch 28, step 350 / 900, loss = 0.28 (0.007 sec/batch)\n",
      "epoch 28, step 400 / 900, loss = 0.34 (0.010 sec/batch)\n",
      "epoch 28, step 450 / 900, loss = 0.75 (0.007 sec/batch)\n",
      "epoch 28, step 500 / 900, loss = 0.54 (0.006 sec/batch)\n",
      "epoch 28, step 550 / 900, loss = 0.53 (0.007 sec/batch)\n",
      "epoch 28, step 600 / 900, loss = 0.42 (0.007 sec/batch)\n",
      "epoch 28, step 650 / 900, loss = 0.35 (0.007 sec/batch)\n",
      "epoch 28, step 700 / 900, loss = 0.44 (0.007 sec/batch)\n",
      "epoch 28, step 750 / 900, loss = 0.52 (0.007 sec/batch)\n",
      "epoch 28, step 800 / 900, loss = 0.54 (0.009 sec/batch)\n",
      "epoch 28, step 850 / 900, loss = 0.92 (0.008 sec/batch)\n",
      "epoch 28, step 900 / 900, loss = 0.53 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 85.28\n",
      " avg loss = 0.56\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.20\n",
      " avg loss = 1.54\n",
      "\n",
      "Epoch time: 9.052425622940063\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 29, step 50 / 900, loss = 0.93 (0.007 sec/batch)\n",
      "epoch 29, step 100 / 900, loss = 0.51 (0.006 sec/batch)\n",
      "epoch 29, step 150 / 900, loss = 0.36 (0.007 sec/batch)\n",
      "epoch 29, step 200 / 900, loss = 0.52 (0.007 sec/batch)\n",
      "epoch 29, step 250 / 900, loss = 0.55 (0.008 sec/batch)\n",
      "epoch 29, step 300 / 900, loss = 0.52 (0.008 sec/batch)\n",
      "epoch 29, step 350 / 900, loss = 0.39 (0.007 sec/batch)\n",
      "epoch 29, step 400 / 900, loss = 0.58 (0.007 sec/batch)\n",
      "epoch 29, step 450 / 900, loss = 0.52 (0.007 sec/batch)\n",
      "epoch 29, step 500 / 900, loss = 0.44 (0.007 sec/batch)\n",
      "epoch 29, step 550 / 900, loss = 0.45 (0.007 sec/batch)\n",
      "epoch 29, step 600 / 900, loss = 0.51 (0.008 sec/batch)\n",
      "epoch 29, step 650 / 900, loss = 0.70 (0.007 sec/batch)\n",
      "epoch 29, step 700 / 900, loss = 0.55 (0.007 sec/batch)\n",
      "epoch 29, step 750 / 900, loss = 0.57 (0.007 sec/batch)\n",
      "epoch 29, step 800 / 900, loss = 0.66 (0.009 sec/batch)\n",
      "epoch 29, step 850 / 900, loss = 0.52 (0.006 sec/batch)\n",
      "epoch 29, step 900 / 900, loss = 0.26 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 85.58\n",
      " avg loss = 0.55\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.82\n",
      " avg loss = 1.52\n",
      "\n",
      "Epoch time: 9.026522397994995\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 30, step 50 / 900, loss = 0.54 (0.007 sec/batch)\n",
      "epoch 30, step 100 / 900, loss = 0.56 (0.007 sec/batch)\n",
      "epoch 30, step 150 / 900, loss = 0.73 (0.006 sec/batch)\n",
      "epoch 30, step 200 / 900, loss = 0.52 (0.008 sec/batch)\n",
      "epoch 30, step 250 / 900, loss = 0.82 (0.007 sec/batch)\n",
      "epoch 30, step 300 / 900, loss = 0.68 (0.007 sec/batch)\n",
      "epoch 30, step 350 / 900, loss = 0.47 (0.007 sec/batch)\n",
      "epoch 30, step 400 / 900, loss = 0.28 (0.006 sec/batch)\n",
      "epoch 30, step 450 / 900, loss = 0.22 (0.006 sec/batch)\n",
      "epoch 30, step 500 / 900, loss = 0.51 (0.009 sec/batch)\n",
      "epoch 30, step 550 / 900, loss = 0.59 (0.009 sec/batch)\n",
      "epoch 30, step 600 / 900, loss = 0.61 (0.007 sec/batch)\n",
      "epoch 30, step 650 / 900, loss = 0.60 (0.007 sec/batch)\n",
      "epoch 30, step 700 / 900, loss = 0.31 (0.007 sec/batch)\n",
      "epoch 30, step 750 / 900, loss = 0.73 (0.009 sec/batch)\n",
      "epoch 30, step 800 / 900, loss = 0.52 (0.007 sec/batch)\n",
      "epoch 30, step 850 / 900, loss = 0.49 (0.007 sec/batch)\n",
      "epoch 30, step 900 / 900, loss = 0.45 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 85.76\n",
      " avg loss = 0.54\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.44\n",
      " avg loss = 1.52\n",
      "\n",
      "Epoch time: 9.102189779281616\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 31, step 50 / 900, loss = 0.67 (0.007 sec/batch)\n",
      "epoch 31, step 100 / 900, loss = 0.51 (0.007 sec/batch)\n",
      "epoch 31, step 150 / 900, loss = 0.42 (0.007 sec/batch)\n",
      "epoch 31, step 200 / 900, loss = 0.48 (0.007 sec/batch)\n",
      "epoch 31, step 250 / 900, loss = 0.58 (0.006 sec/batch)\n",
      "epoch 31, step 300 / 900, loss = 0.66 (0.007 sec/batch)\n",
      "epoch 31, step 350 / 900, loss = 0.52 (0.007 sec/batch)\n",
      "epoch 31, step 400 / 900, loss = 0.45 (0.008 sec/batch)\n",
      "epoch 31, step 450 / 900, loss = 0.52 (0.006 sec/batch)\n",
      "epoch 31, step 500 / 900, loss = 0.29 (0.007 sec/batch)\n",
      "epoch 31, step 550 / 900, loss = 0.53 (0.009 sec/batch)\n",
      "epoch 31, step 600 / 900, loss = 0.70 (0.009 sec/batch)\n",
      "epoch 31, step 650 / 900, loss = 0.64 (0.007 sec/batch)\n",
      "epoch 31, step 700 / 900, loss = 0.48 (0.009 sec/batch)\n",
      "epoch 31, step 750 / 900, loss = 0.57 (0.007 sec/batch)\n",
      "epoch 31, step 800 / 900, loss = 0.77 (0.007 sec/batch)\n",
      "epoch 31, step 850 / 900, loss = 0.75 (0.007 sec/batch)\n",
      "epoch 31, step 900 / 900, loss = 0.45 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 85.82\n",
      " avg loss = 0.54\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.78\n",
      " avg loss = 1.53\n",
      "\n",
      "Epoch time: 9.194083452224731\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 32, step 50 / 900, loss = 0.49 (0.007 sec/batch)\n",
      "epoch 32, step 100 / 900, loss = 0.45 (0.007 sec/batch)\n",
      "epoch 32, step 150 / 900, loss = 0.33 (0.007 sec/batch)\n",
      "epoch 32, step 200 / 900, loss = 0.58 (0.007 sec/batch)\n",
      "epoch 32, step 250 / 900, loss = 0.28 (0.007 sec/batch)\n",
      "epoch 32, step 300 / 900, loss = 0.47 (0.010 sec/batch)\n",
      "epoch 32, step 350 / 900, loss = 0.36 (0.007 sec/batch)\n",
      "epoch 32, step 400 / 900, loss = 0.65 (0.007 sec/batch)\n",
      "epoch 32, step 450 / 900, loss = 0.47 (0.009 sec/batch)\n",
      "epoch 32, step 500 / 900, loss = 0.43 (0.007 sec/batch)\n",
      "epoch 32, step 550 / 900, loss = 0.74 (0.008 sec/batch)\n",
      "epoch 32, step 600 / 900, loss = 0.64 (0.006 sec/batch)\n",
      "epoch 32, step 650 / 900, loss = 0.46 (0.007 sec/batch)\n",
      "epoch 32, step 700 / 900, loss = 0.68 (0.006 sec/batch)\n",
      "epoch 32, step 750 / 900, loss = 0.57 (0.008 sec/batch)\n",
      "epoch 32, step 800 / 900, loss = 0.39 (0.006 sec/batch)\n",
      "epoch 32, step 850 / 900, loss = 0.49 (0.007 sec/batch)\n",
      "epoch 32, step 900 / 900, loss = 0.83 (0.010 sec/batch)\n",
      "Train error:\n",
      " accuracy = 85.91\n",
      " avg loss = 0.53\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.80\n",
      " avg loss = 1.53\n",
      "\n",
      "Epoch time: 9.232450485229492\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 33, step 50 / 900, loss = 0.41 (0.010 sec/batch)\n",
      "epoch 33, step 100 / 900, loss = 0.33 (0.007 sec/batch)\n",
      "epoch 33, step 150 / 900, loss = 0.55 (0.007 sec/batch)\n",
      "epoch 33, step 200 / 900, loss = 0.64 (0.008 sec/batch)\n",
      "epoch 33, step 250 / 900, loss = 0.40 (0.007 sec/batch)\n",
      "epoch 33, step 300 / 900, loss = 0.65 (0.007 sec/batch)\n",
      "epoch 33, step 350 / 900, loss = 0.55 (0.008 sec/batch)\n",
      "epoch 33, step 400 / 900, loss = 0.43 (0.007 sec/batch)\n",
      "epoch 33, step 450 / 900, loss = 0.44 (0.007 sec/batch)\n",
      "epoch 33, step 500 / 900, loss = 0.38 (0.007 sec/batch)\n",
      "epoch 33, step 550 / 900, loss = 0.26 (0.006 sec/batch)\n",
      "epoch 33, step 600 / 900, loss = 0.51 (0.007 sec/batch)\n",
      "epoch 33, step 650 / 900, loss = 0.67 (0.007 sec/batch)\n",
      "epoch 33, step 700 / 900, loss = 0.57 (0.008 sec/batch)\n",
      "epoch 33, step 750 / 900, loss = 0.66 (0.007 sec/batch)\n",
      "epoch 33, step 800 / 900, loss = 0.71 (0.007 sec/batch)\n",
      "epoch 33, step 850 / 900, loss = 0.53 (0.007 sec/batch)\n",
      "epoch 33, step 900 / 900, loss = 0.16 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 86.07\n",
      " avg loss = 0.52\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.68\n",
      " avg loss = 1.53\n",
      "\n",
      "Epoch time: 9.303394794464111\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 34, step 50 / 900, loss = 0.42 (0.007 sec/batch)\n",
      "epoch 34, step 100 / 900, loss = 0.58 (0.009 sec/batch)\n",
      "epoch 34, step 150 / 900, loss = 0.56 (0.009 sec/batch)\n",
      "epoch 34, step 200 / 900, loss = 0.85 (0.007 sec/batch)\n",
      "epoch 34, step 250 / 900, loss = 0.46 (0.009 sec/batch)\n",
      "epoch 34, step 300 / 900, loss = 0.27 (0.009 sec/batch)\n",
      "epoch 34, step 350 / 900, loss = 0.36 (0.009 sec/batch)\n",
      "epoch 34, step 400 / 900, loss = 0.36 (0.008 sec/batch)\n",
      "epoch 34, step 450 / 900, loss = 0.75 (0.009 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 34, step 500 / 900, loss = 0.37 (0.009 sec/batch)\n",
      "epoch 34, step 550 / 900, loss = 0.53 (0.007 sec/batch)\n",
      "epoch 34, step 600 / 900, loss = 0.50 (0.007 sec/batch)\n",
      "epoch 34, step 650 / 900, loss = 0.60 (0.007 sec/batch)\n",
      "epoch 34, step 700 / 900, loss = 0.28 (0.008 sec/batch)\n",
      "epoch 34, step 750 / 900, loss = 0.64 (0.007 sec/batch)\n",
      "epoch 34, step 800 / 900, loss = 0.56 (0.007 sec/batch)\n",
      "epoch 34, step 850 / 900, loss = 0.81 (0.008 sec/batch)\n",
      "epoch 34, step 900 / 900, loss = 0.34 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 86.17\n",
      " avg loss = 0.52\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.80\n",
      " avg loss = 1.52\n",
      "\n",
      "Epoch time: 9.538930177688599\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 35, step 50 / 900, loss = 0.44 (0.007 sec/batch)\n",
      "epoch 35, step 100 / 900, loss = 0.26 (0.007 sec/batch)\n",
      "epoch 35, step 150 / 900, loss = 0.46 (0.008 sec/batch)\n",
      "epoch 35, step 200 / 900, loss = 0.40 (0.007 sec/batch)\n",
      "epoch 35, step 250 / 900, loss = 0.49 (0.009 sec/batch)\n",
      "epoch 35, step 300 / 900, loss = 0.89 (0.007 sec/batch)\n",
      "epoch 35, step 350 / 900, loss = 0.28 (0.007 sec/batch)\n",
      "epoch 35, step 400 / 900, loss = 0.24 (0.008 sec/batch)\n",
      "epoch 35, step 450 / 900, loss = 0.47 (0.007 sec/batch)\n",
      "epoch 35, step 500 / 900, loss = 0.46 (0.007 sec/batch)\n",
      "epoch 35, step 550 / 900, loss = 0.31 (0.007 sec/batch)\n",
      "epoch 35, step 600 / 900, loss = 0.45 (0.008 sec/batch)\n",
      "epoch 35, step 650 / 900, loss = 0.41 (0.007 sec/batch)\n",
      "epoch 35, step 700 / 900, loss = 0.54 (0.007 sec/batch)\n",
      "epoch 35, step 750 / 900, loss = 0.47 (0.007 sec/batch)\n",
      "epoch 35, step 800 / 900, loss = 0.32 (0.007 sec/batch)\n",
      "epoch 35, step 850 / 900, loss = 0.42 (0.007 sec/batch)\n",
      "epoch 35, step 900 / 900, loss = 0.42 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 86.24\n",
      " avg loss = 0.51\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.80\n",
      " avg loss = 1.53\n",
      "\n",
      "Epoch time: 9.280953645706177\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 36, step 50 / 900, loss = 0.51 (0.007 sec/batch)\n",
      "epoch 36, step 100 / 900, loss = 0.38 (0.007 sec/batch)\n",
      "epoch 36, step 150 / 900, loss = 0.44 (0.008 sec/batch)\n",
      "epoch 36, step 200 / 900, loss = 0.59 (0.006 sec/batch)\n",
      "epoch 36, step 250 / 900, loss = 0.43 (0.007 sec/batch)\n",
      "epoch 36, step 300 / 900, loss = 0.26 (0.006 sec/batch)\n",
      "epoch 36, step 350 / 900, loss = 0.58 (0.007 sec/batch)\n",
      "epoch 36, step 400 / 900, loss = 0.57 (0.007 sec/batch)\n",
      "epoch 36, step 450 / 900, loss = 0.37 (0.007 sec/batch)\n",
      "epoch 36, step 500 / 900, loss = 0.46 (0.006 sec/batch)\n",
      "epoch 36, step 550 / 900, loss = 0.33 (0.007 sec/batch)\n",
      "epoch 36, step 600 / 900, loss = 0.48 (0.007 sec/batch)\n",
      "epoch 36, step 650 / 900, loss = 0.58 (0.007 sec/batch)\n",
      "epoch 36, step 700 / 900, loss = 0.41 (0.007 sec/batch)\n",
      "epoch 36, step 750 / 900, loss = 0.47 (0.007 sec/batch)\n",
      "epoch 36, step 800 / 900, loss = 0.60 (0.007 sec/batch)\n",
      "epoch 36, step 850 / 900, loss = 0.59 (0.008 sec/batch)\n",
      "epoch 36, step 900 / 900, loss = 0.59 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 86.35\n",
      " avg loss = 0.51\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.56\n",
      " avg loss = 1.53\n",
      "\n",
      "Epoch time: 9.242517709732056\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 37, step 50 / 900, loss = 0.64 (0.006 sec/batch)\n",
      "epoch 37, step 100 / 900, loss = 0.52 (0.007 sec/batch)\n",
      "epoch 37, step 150 / 900, loss = 0.60 (0.008 sec/batch)\n",
      "epoch 37, step 200 / 900, loss = 0.42 (0.007 sec/batch)\n",
      "epoch 37, step 250 / 900, loss = 0.28 (0.006 sec/batch)\n",
      "epoch 37, step 300 / 900, loss = 0.67 (0.007 sec/batch)\n",
      "epoch 37, step 350 / 900, loss = 0.49 (0.009 sec/batch)\n",
      "epoch 37, step 400 / 900, loss = 0.45 (0.007 sec/batch)\n",
      "epoch 37, step 450 / 900, loss = 0.45 (0.006 sec/batch)\n",
      "epoch 37, step 500 / 900, loss = 0.55 (0.008 sec/batch)\n",
      "epoch 37, step 550 / 900, loss = 0.51 (0.007 sec/batch)\n",
      "epoch 37, step 600 / 900, loss = 0.36 (0.007 sec/batch)\n",
      "epoch 37, step 650 / 900, loss = 0.48 (0.007 sec/batch)\n",
      "epoch 37, step 700 / 900, loss = 0.74 (0.007 sec/batch)\n",
      "epoch 37, step 750 / 900, loss = 0.45 (0.007 sec/batch)\n",
      "epoch 37, step 800 / 900, loss = 0.39 (0.007 sec/batch)\n",
      "epoch 37, step 850 / 900, loss = 0.77 (0.006 sec/batch)\n",
      "epoch 37, step 900 / 900, loss = 0.69 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 86.42\n",
      " avg loss = 0.51\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.60\n",
      " avg loss = 1.53\n",
      "\n",
      "Epoch time: 9.202286720275879\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 38, step 50 / 900, loss = 0.70 (0.007 sec/batch)\n",
      "epoch 38, step 100 / 900, loss = 0.60 (0.007 sec/batch)\n",
      "epoch 38, step 150 / 900, loss = 0.27 (0.007 sec/batch)\n",
      "epoch 38, step 200 / 900, loss = 0.67 (0.006 sec/batch)\n",
      "epoch 38, step 250 / 900, loss = 0.82 (0.007 sec/batch)\n",
      "epoch 38, step 300 / 900, loss = 0.44 (0.009 sec/batch)\n",
      "epoch 38, step 350 / 900, loss = 0.59 (0.007 sec/batch)\n",
      "epoch 38, step 400 / 900, loss = 0.52 (0.007 sec/batch)\n",
      "epoch 38, step 450 / 900, loss = 0.49 (0.008 sec/batch)\n",
      "epoch 38, step 500 / 900, loss = 0.64 (0.007 sec/batch)\n",
      "epoch 38, step 550 / 900, loss = 0.55 (0.007 sec/batch)\n",
      "epoch 38, step 600 / 900, loss = 0.39 (0.007 sec/batch)\n",
      "epoch 38, step 650 / 900, loss = 0.35 (0.008 sec/batch)\n",
      "epoch 38, step 700 / 900, loss = 0.43 (0.007 sec/batch)\n",
      "epoch 38, step 750 / 900, loss = 0.43 (0.007 sec/batch)\n",
      "epoch 38, step 800 / 900, loss = 0.62 (0.006 sec/batch)\n",
      "epoch 38, step 850 / 900, loss = 0.57 (0.007 sec/batch)\n",
      "epoch 38, step 900 / 900, loss = 0.50 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 86.50\n",
      " avg loss = 0.50\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.62\n",
      " avg loss = 1.53\n",
      "\n",
      "Epoch time: 9.121679306030273\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 39, step 50 / 900, loss = 0.22 (0.007 sec/batch)\n",
      "epoch 39, step 100 / 900, loss = 0.53 (0.006 sec/batch)\n",
      "epoch 39, step 150 / 900, loss = 0.31 (0.009 sec/batch)\n",
      "epoch 39, step 200 / 900, loss = 0.52 (0.007 sec/batch)\n",
      "epoch 39, step 250 / 900, loss = 0.63 (0.007 sec/batch)\n",
      "epoch 39, step 300 / 900, loss = 0.30 (0.006 sec/batch)\n",
      "epoch 39, step 350 / 900, loss = 0.59 (0.007 sec/batch)\n",
      "epoch 39, step 400 / 900, loss = 0.73 (0.008 sec/batch)\n",
      "epoch 39, step 450 / 900, loss = 0.31 (0.006 sec/batch)\n",
      "epoch 39, step 500 / 900, loss = 0.47 (0.008 sec/batch)\n",
      "epoch 39, step 550 / 900, loss = 0.54 (0.007 sec/batch)\n",
      "epoch 39, step 600 / 900, loss = 0.47 (0.009 sec/batch)\n",
      "epoch 39, step 650 / 900, loss = 0.42 (0.007 sec/batch)\n",
      "epoch 39, step 700 / 900, loss = 0.44 (0.007 sec/batch)\n",
      "epoch 39, step 750 / 900, loss = 0.27 (0.007 sec/batch)\n",
      "epoch 39, step 800 / 900, loss = 0.76 (0.006 sec/batch)\n",
      "epoch 39, step 850 / 900, loss = 0.24 (0.007 sec/batch)\n",
      "epoch 39, step 900 / 900, loss = 0.43 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 86.48\n",
      " avg loss = 0.50\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.50\n",
      " avg loss = 1.53\n",
      "\n",
      "Epoch time: 9.080658435821533\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 40, step 50 / 900, loss = 0.54 (0.007 sec/batch)\n",
      "epoch 40, step 100 / 900, loss = 0.34 (0.007 sec/batch)\n",
      "epoch 40, step 150 / 900, loss = 0.51 (0.007 sec/batch)\n",
      "epoch 40, step 200 / 900, loss = 0.73 (0.009 sec/batch)\n",
      "epoch 40, step 250 / 900, loss = 0.55 (0.006 sec/batch)\n",
      "epoch 40, step 300 / 900, loss = 0.41 (0.007 sec/batch)\n",
      "epoch 40, step 350 / 900, loss = 0.57 (0.007 sec/batch)\n",
      "epoch 40, step 400 / 900, loss = 0.53 (0.009 sec/batch)\n",
      "epoch 40, step 450 / 900, loss = 0.37 (0.007 sec/batch)\n",
      "epoch 40, step 500 / 900, loss = 0.42 (0.006 sec/batch)\n",
      "epoch 40, step 550 / 900, loss = 0.35 (0.007 sec/batch)\n",
      "epoch 40, step 600 / 900, loss = 0.29 (0.008 sec/batch)\n",
      "epoch 40, step 650 / 900, loss = 0.44 (0.006 sec/batch)\n",
      "epoch 40, step 700 / 900, loss = 0.48 (0.007 sec/batch)\n",
      "epoch 40, step 750 / 900, loss = 0.61 (0.007 sec/batch)\n",
      "epoch 40, step 800 / 900, loss = 0.66 (0.009 sec/batch)\n",
      "epoch 40, step 850 / 900, loss = 0.46 (0.007 sec/batch)\n",
      "epoch 40, step 900 / 900, loss = 0.65 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 86.59\n",
      " avg loss = 0.50\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.60\n",
      " avg loss = 1.54\n",
      "\n",
      "Epoch time: 9.159557104110718\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "Total train time: 378.35075664520264\n"
     ]
    }
   ],
   "source": [
    "session = tf.Session()\n",
    "train_start = time.time()\n",
    "\n",
    "plot_data={}\n",
    "train_cifar(session, train_x, train_y, valid_x, valid_y, config, plot_data)\n",
    "\n",
    "print(\"Total train time:\", time.time() - train_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7YAAAHiCAYAAADcelBQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17f89ead278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_training_progress(plot_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaGrad optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Graph\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "Y_ = tf.placeholder(tf.int32, [None,])\n",
    "logits = convnet(X, weights, biases)\n",
    "\n",
    "# loss\n",
    "regularizers = l2_loss(weights.values())\n",
    "loss_per_sample = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=Y_)\n",
    "data_loss = tf.reduce_mean(loss_per_sample)\n",
    "loss = data_loss + weight_decay*regularizers\n",
    "\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(0.01, global_step, 900, 0.9, staircase=True)\n",
    "train_step = tf.train.AdagradOptimizer(learning_rate).minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, step 50 / 900, loss = 2.28 (0.006 sec/batch)\n",
      "epoch 1, step 100 / 900, loss = 1.71 (0.007 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Korisnik\\Anaconda3\\lib\\site-packages\\skimage\\util\\dtype.py:122: UserWarning: Possible precision loss when converting from float64 to uint8\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, step 150 / 900, loss = 1.97 (0.008 sec/batch)\n",
      "epoch 1, step 200 / 900, loss = 1.77 (0.007 sec/batch)\n",
      "epoch 1, step 250 / 900, loss = 1.66 (0.007 sec/batch)\n",
      "epoch 1, step 300 / 900, loss = 1.56 (0.006 sec/batch)\n",
      "epoch 1, step 350 / 900, loss = 1.55 (0.006 sec/batch)\n",
      "epoch 1, step 400 / 900, loss = 1.78 (0.006 sec/batch)\n",
      "epoch 1, step 450 / 900, loss = 1.55 (0.006 sec/batch)\n",
      "epoch 1, step 500 / 900, loss = 1.40 (0.009 sec/batch)\n",
      "epoch 1, step 550 / 900, loss = 1.33 (0.007 sec/batch)\n",
      "epoch 1, step 600 / 900, loss = 1.31 (0.006 sec/batch)\n",
      "epoch 1, step 650 / 900, loss = 1.39 (0.006 sec/batch)\n",
      "epoch 1, step 700 / 900, loss = 1.24 (0.008 sec/batch)\n",
      "epoch 1, step 750 / 900, loss = 1.41 (0.007 sec/batch)\n",
      "epoch 1, step 800 / 900, loss = 1.26 (0.008 sec/batch)\n",
      "epoch 1, step 850 / 900, loss = 1.49 (0.007 sec/batch)\n",
      "epoch 1, step 900 / 900, loss = 1.35 (0.008 sec/batch)\n",
      "Train error:\n",
      " accuracy = 56.42\n",
      " avg loss = 1.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 55.68\n",
      " avg loss = 1.29\n",
      "\n",
      "Epoch time: 9.518801927566528\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 2, step 50 / 900, loss = 1.38 (0.007 sec/batch)\n",
      "epoch 2, step 100 / 900, loss = 1.31 (0.008 sec/batch)\n",
      "epoch 2, step 150 / 900, loss = 1.31 (0.006 sec/batch)\n",
      "epoch 2, step 200 / 900, loss = 1.18 (0.009 sec/batch)\n",
      "epoch 2, step 250 / 900, loss = 1.36 (0.006 sec/batch)\n",
      "epoch 2, step 300 / 900, loss = 1.48 (0.006 sec/batch)\n",
      "epoch 2, step 350 / 900, loss = 1.21 (0.008 sec/batch)\n",
      "epoch 2, step 400 / 900, loss = 1.33 (0.007 sec/batch)\n",
      "epoch 2, step 450 / 900, loss = 0.90 (0.008 sec/batch)\n",
      "epoch 2, step 500 / 900, loss = 0.91 (0.010 sec/batch)\n",
      "epoch 2, step 550 / 900, loss = 1.10 (0.006 sec/batch)\n",
      "epoch 2, step 600 / 900, loss = 1.09 (0.007 sec/batch)\n",
      "epoch 2, step 650 / 900, loss = 1.09 (0.007 sec/batch)\n",
      "epoch 2, step 700 / 900, loss = 1.27 (0.006 sec/batch)\n",
      "epoch 2, step 750 / 900, loss = 0.87 (0.008 sec/batch)\n",
      "epoch 2, step 800 / 900, loss = 1.28 (0.007 sec/batch)\n",
      "epoch 2, step 850 / 900, loss = 1.08 (0.007 sec/batch)\n",
      "epoch 2, step 900 / 900, loss = 1.26 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 64.81\n",
      " avg loss = 1.05\n",
      "\n",
      "Validation error:\n",
      " accuracy = 62.28\n",
      " avg loss = 1.12\n",
      "\n",
      "Epoch time: 9.453373670578003\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 3, step 50 / 900, loss = 0.94 (0.009 sec/batch)\n",
      "epoch 3, step 100 / 900, loss = 1.08 (0.008 sec/batch)\n",
      "epoch 3, step 150 / 900, loss = 0.94 (0.008 sec/batch)\n",
      "epoch 3, step 200 / 900, loss = 1.14 (0.007 sec/batch)\n",
      "epoch 3, step 250 / 900, loss = 0.98 (0.009 sec/batch)\n",
      "epoch 3, step 300 / 900, loss = 0.82 (0.010 sec/batch)\n",
      "epoch 3, step 350 / 900, loss = 1.19 (0.009 sec/batch)\n",
      "epoch 3, step 400 / 900, loss = 1.08 (0.006 sec/batch)\n",
      "epoch 3, step 450 / 900, loss = 1.31 (0.008 sec/batch)\n",
      "epoch 3, step 500 / 900, loss = 1.07 (0.007 sec/batch)\n",
      "epoch 3, step 550 / 900, loss = 0.64 (0.007 sec/batch)\n",
      "epoch 3, step 600 / 900, loss = 1.14 (0.007 sec/batch)\n",
      "epoch 3, step 650 / 900, loss = 1.26 (0.007 sec/batch)\n",
      "epoch 3, step 700 / 900, loss = 1.06 (0.009 sec/batch)\n",
      "epoch 3, step 750 / 900, loss = 0.93 (0.007 sec/batch)\n",
      "epoch 3, step 800 / 900, loss = 0.67 (0.007 sec/batch)\n",
      "epoch 3, step 850 / 900, loss = 1.08 (0.007 sec/batch)\n",
      "epoch 3, step 900 / 900, loss = 1.09 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 68.45\n",
      " avg loss = 0.94\n",
      "\n",
      "Validation error:\n",
      " accuracy = 64.54\n",
      " avg loss = 1.05\n",
      "\n",
      "Epoch time: 9.603414535522461\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 4, step 50 / 900, loss = 1.28 (0.009 sec/batch)\n",
      "epoch 4, step 100 / 900, loss = 0.91 (0.007 sec/batch)\n",
      "epoch 4, step 150 / 900, loss = 0.76 (0.011 sec/batch)\n",
      "epoch 4, step 200 / 900, loss = 1.21 (0.006 sec/batch)\n",
      "epoch 4, step 250 / 900, loss = 0.94 (0.011 sec/batch)\n",
      "epoch 4, step 300 / 900, loss = 1.01 (0.007 sec/batch)\n",
      "epoch 4, step 350 / 900, loss = 0.69 (0.007 sec/batch)\n",
      "epoch 4, step 400 / 900, loss = 0.95 (0.007 sec/batch)\n",
      "epoch 4, step 450 / 900, loss = 0.99 (0.007 sec/batch)\n",
      "epoch 4, step 500 / 900, loss = 0.93 (0.006 sec/batch)\n",
      "epoch 4, step 550 / 900, loss = 1.11 (0.008 sec/batch)\n",
      "epoch 4, step 600 / 900, loss = 1.01 (0.007 sec/batch)\n",
      "epoch 4, step 650 / 900, loss = 1.16 (0.008 sec/batch)\n",
      "epoch 4, step 700 / 900, loss = 1.02 (0.007 sec/batch)\n",
      "epoch 4, step 750 / 900, loss = 0.93 (0.007 sec/batch)\n",
      "epoch 4, step 800 / 900, loss = 1.00 (0.006 sec/batch)\n",
      "epoch 4, step 850 / 900, loss = 0.83 (0.010 sec/batch)\n",
      "epoch 4, step 900 / 900, loss = 1.05 (0.008 sec/batch)\n",
      "Train error:\n",
      " accuracy = 69.73\n",
      " avg loss = 0.90\n",
      "\n",
      "Validation error:\n",
      " accuracy = 64.64\n",
      " avg loss = 1.05\n",
      "\n",
      "Epoch time: 9.545189619064331\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 5, step 50 / 900, loss = 1.17 (0.010 sec/batch)\n",
      "epoch 5, step 100 / 900, loss = 0.69 (0.007 sec/batch)\n",
      "epoch 5, step 150 / 900, loss = 0.81 (0.008 sec/batch)\n",
      "epoch 5, step 200 / 900, loss = 0.98 (0.006 sec/batch)\n",
      "epoch 5, step 250 / 900, loss = 0.78 (0.008 sec/batch)\n",
      "epoch 5, step 300 / 900, loss = 0.71 (0.007 sec/batch)\n",
      "epoch 5, step 350 / 900, loss = 0.74 (0.007 sec/batch)\n",
      "epoch 5, step 400 / 900, loss = 0.80 (0.007 sec/batch)\n",
      "epoch 5, step 450 / 900, loss = 0.79 (0.007 sec/batch)\n",
      "epoch 5, step 500 / 900, loss = 0.84 (0.007 sec/batch)\n",
      "epoch 5, step 550 / 900, loss = 1.01 (0.009 sec/batch)\n",
      "epoch 5, step 600 / 900, loss = 0.78 (0.008 sec/batch)\n",
      "epoch 5, step 650 / 900, loss = 0.70 (0.007 sec/batch)\n",
      "epoch 5, step 700 / 900, loss = 0.96 (0.007 sec/batch)\n",
      "epoch 5, step 750 / 900, loss = 0.67 (0.007 sec/batch)\n",
      "epoch 5, step 800 / 900, loss = 0.85 (0.007 sec/batch)\n",
      "epoch 5, step 850 / 900, loss = 0.57 (0.007 sec/batch)\n",
      "epoch 5, step 900 / 900, loss = 0.96 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 73.85\n",
      " avg loss = 0.80\n",
      "\n",
      "Validation error:\n",
      " accuracy = 67.32\n",
      " avg loss = 0.96\n",
      "\n",
      "Epoch time: 9.55604100227356\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 6, step 50 / 900, loss = 0.63 (0.007 sec/batch)\n",
      "epoch 6, step 100 / 900, loss = 0.88 (0.008 sec/batch)\n",
      "epoch 6, step 150 / 900, loss = 0.98 (0.007 sec/batch)\n",
      "epoch 6, step 200 / 900, loss = 0.71 (0.006 sec/batch)\n",
      "epoch 6, step 250 / 900, loss = 0.74 (0.008 sec/batch)\n",
      "epoch 6, step 300 / 900, loss = 0.51 (0.008 sec/batch)\n",
      "epoch 6, step 350 / 900, loss = 0.99 (0.007 sec/batch)\n",
      "epoch 6, step 400 / 900, loss = 1.03 (0.006 sec/batch)\n",
      "epoch 6, step 450 / 900, loss = 1.03 (0.006 sec/batch)\n",
      "epoch 6, step 500 / 900, loss = 0.68 (0.006 sec/batch)\n",
      "epoch 6, step 550 / 900, loss = 0.84 (0.013 sec/batch)\n",
      "epoch 6, step 600 / 900, loss = 0.65 (0.009 sec/batch)\n",
      "epoch 6, step 650 / 900, loss = 0.90 (0.007 sec/batch)\n",
      "epoch 6, step 700 / 900, loss = 0.72 (0.008 sec/batch)\n",
      "epoch 6, step 750 / 900, loss = 0.61 (0.009 sec/batch)\n",
      "epoch 6, step 800 / 900, loss = 0.68 (0.008 sec/batch)\n",
      "epoch 6, step 850 / 900, loss = 0.66 (0.007 sec/batch)\n",
      "epoch 6, step 900 / 900, loss = 0.70 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 76.20\n",
      " avg loss = 0.73\n",
      "\n",
      "Validation error:\n",
      " accuracy = 69.12\n",
      " avg loss = 0.92\n",
      "\n",
      "Epoch time: 9.521225452423096\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 7, step 50 / 900, loss = 0.53 (0.007 sec/batch)\n",
      "epoch 7, step 100 / 900, loss = 0.56 (0.007 sec/batch)\n",
      "epoch 7, step 150 / 900, loss = 0.80 (0.009 sec/batch)\n",
      "epoch 7, step 200 / 900, loss = 0.72 (0.007 sec/batch)\n",
      "epoch 7, step 250 / 900, loss = 0.57 (0.007 sec/batch)\n",
      "epoch 7, step 300 / 900, loss = 0.69 (0.009 sec/batch)\n",
      "epoch 7, step 350 / 900, loss = 0.73 (0.018 sec/batch)\n",
      "epoch 7, step 400 / 900, loss = 0.44 (0.007 sec/batch)\n",
      "epoch 7, step 450 / 900, loss = 0.49 (0.007 sec/batch)\n",
      "epoch 7, step 500 / 900, loss = 0.99 (0.006 sec/batch)\n",
      "epoch 7, step 550 / 900, loss = 0.69 (0.009 sec/batch)\n",
      "epoch 7, step 600 / 900, loss = 0.58 (0.006 sec/batch)\n",
      "epoch 7, step 650 / 900, loss = 0.72 (0.007 sec/batch)\n",
      "epoch 7, step 700 / 900, loss = 0.53 (0.007 sec/batch)\n",
      "epoch 7, step 750 / 900, loss = 1.07 (0.007 sec/batch)\n",
      "epoch 7, step 800 / 900, loss = 0.68 (0.009 sec/batch)\n",
      "epoch 7, step 850 / 900, loss = 0.98 (0.007 sec/batch)\n",
      "epoch 7, step 900 / 900, loss = 1.02 (0.007 sec/batch)\n",
      "Train error:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy = 77.42\n",
      " avg loss = 0.70\n",
      "\n",
      "Validation error:\n",
      " accuracy = 69.34\n",
      " avg loss = 0.92\n",
      "\n",
      "Epoch time: 9.489799499511719\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 8, step 50 / 900, loss = 0.69 (0.007 sec/batch)\n",
      "epoch 8, step 100 / 900, loss = 0.60 (0.008 sec/batch)\n",
      "epoch 8, step 150 / 900, loss = 0.71 (0.008 sec/batch)\n",
      "epoch 8, step 200 / 900, loss = 0.63 (0.009 sec/batch)\n",
      "epoch 8, step 250 / 900, loss = 0.68 (0.009 sec/batch)\n",
      "epoch 8, step 300 / 900, loss = 0.60 (0.008 sec/batch)\n",
      "epoch 8, step 350 / 900, loss = 0.60 (0.007 sec/batch)\n",
      "epoch 8, step 400 / 900, loss = 0.69 (0.006 sec/batch)\n",
      "epoch 8, step 450 / 900, loss = 0.63 (0.007 sec/batch)\n",
      "epoch 8, step 500 / 900, loss = 0.82 (0.006 sec/batch)\n",
      "epoch 8, step 550 / 900, loss = 0.75 (0.006 sec/batch)\n",
      "epoch 8, step 600 / 900, loss = 0.54 (0.006 sec/batch)\n",
      "epoch 8, step 650 / 900, loss = 0.51 (0.007 sec/batch)\n",
      "epoch 8, step 700 / 900, loss = 0.88 (0.008 sec/batch)\n",
      "epoch 8, step 750 / 900, loss = 0.73 (0.006 sec/batch)\n",
      "epoch 8, step 800 / 900, loss = 0.54 (0.008 sec/batch)\n",
      "epoch 8, step 850 / 900, loss = 0.97 (0.006 sec/batch)\n",
      "epoch 8, step 900 / 900, loss = 0.63 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 78.80\n",
      " avg loss = 0.66\n",
      "\n",
      "Validation error:\n",
      " accuracy = 69.70\n",
      " avg loss = 0.90\n",
      "\n",
      "Epoch time: 9.450426816940308\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 9, step 50 / 900, loss = 0.49 (0.006 sec/batch)\n",
      "epoch 9, step 100 / 900, loss = 0.65 (0.008 sec/batch)\n",
      "epoch 9, step 150 / 900, loss = 0.76 (0.012 sec/batch)\n",
      "epoch 9, step 200 / 900, loss = 0.71 (0.007 sec/batch)\n",
      "epoch 9, step 250 / 900, loss = 0.68 (0.007 sec/batch)\n",
      "epoch 9, step 300 / 900, loss = 0.62 (0.011 sec/batch)\n",
      "epoch 9, step 350 / 900, loss = 0.62 (0.007 sec/batch)\n",
      "epoch 9, step 400 / 900, loss = 0.61 (0.007 sec/batch)\n",
      "epoch 9, step 450 / 900, loss = 0.61 (0.006 sec/batch)\n",
      "epoch 9, step 500 / 900, loss = 0.70 (0.008 sec/batch)\n",
      "epoch 9, step 550 / 900, loss = 0.66 (0.006 sec/batch)\n",
      "epoch 9, step 600 / 900, loss = 0.66 (0.008 sec/batch)\n",
      "epoch 9, step 650 / 900, loss = 0.79 (0.006 sec/batch)\n",
      "epoch 9, step 700 / 900, loss = 0.68 (0.009 sec/batch)\n",
      "epoch 9, step 750 / 900, loss = 0.58 (0.007 sec/batch)\n",
      "epoch 9, step 800 / 900, loss = 0.61 (0.009 sec/batch)\n",
      "epoch 9, step 850 / 900, loss = 0.55 (0.007 sec/batch)\n",
      "epoch 9, step 900 / 900, loss = 0.72 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 79.78\n",
      " avg loss = 0.64\n",
      "\n",
      "Validation error:\n",
      " accuracy = 70.56\n",
      " avg loss = 0.89\n",
      "\n",
      "Epoch time: 9.400395393371582\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 10, step 50 / 900, loss = 0.58 (0.008 sec/batch)\n",
      "epoch 10, step 100 / 900, loss = 0.69 (0.007 sec/batch)\n",
      "epoch 10, step 150 / 900, loss = 0.42 (0.009 sec/batch)\n",
      "epoch 10, step 200 / 900, loss = 0.42 (0.008 sec/batch)\n",
      "epoch 10, step 250 / 900, loss = 0.52 (0.009 sec/batch)\n",
      "epoch 10, step 300 / 900, loss = 0.75 (0.007 sec/batch)\n",
      "epoch 10, step 350 / 900, loss = 0.69 (0.007 sec/batch)\n",
      "epoch 10, step 400 / 900, loss = 0.64 (0.006 sec/batch)\n",
      "epoch 10, step 450 / 900, loss = 0.60 (0.006 sec/batch)\n",
      "epoch 10, step 500 / 900, loss = 0.77 (0.009 sec/batch)\n",
      "epoch 10, step 550 / 900, loss = 0.58 (0.007 sec/batch)\n",
      "epoch 10, step 600 / 900, loss = 0.43 (0.008 sec/batch)\n",
      "epoch 10, step 650 / 900, loss = 0.61 (0.007 sec/batch)\n",
      "epoch 10, step 700 / 900, loss = 0.58 (0.009 sec/batch)\n",
      "epoch 10, step 750 / 900, loss = 0.83 (0.007 sec/batch)\n",
      "epoch 10, step 800 / 900, loss = 0.57 (0.007 sec/batch)\n",
      "epoch 10, step 850 / 900, loss = 0.68 (0.006 sec/batch)\n",
      "epoch 10, step 900 / 900, loss = 0.64 (0.009 sec/batch)\n",
      "Train error:\n",
      " accuracy = 81.19\n",
      " avg loss = 0.60\n",
      "\n",
      "Validation error:\n",
      " accuracy = 70.86\n",
      " avg loss = 0.87\n",
      "\n",
      "Epoch time: 9.494818925857544\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 11, step 50 / 900, loss = 0.60 (0.010 sec/batch)\n",
      "epoch 11, step 100 / 900, loss = 0.66 (0.013 sec/batch)\n",
      "epoch 11, step 150 / 900, loss = 0.65 (0.007 sec/batch)\n",
      "epoch 11, step 200 / 900, loss = 0.45 (0.006 sec/batch)\n",
      "epoch 11, step 250 / 900, loss = 0.44 (0.006 sec/batch)\n",
      "epoch 11, step 300 / 900, loss = 0.44 (0.009 sec/batch)\n",
      "epoch 11, step 350 / 900, loss = 0.54 (0.007 sec/batch)\n",
      "epoch 11, step 400 / 900, loss = 0.64 (0.008 sec/batch)\n",
      "epoch 11, step 450 / 900, loss = 0.62 (0.006 sec/batch)\n",
      "epoch 11, step 500 / 900, loss = 0.45 (0.008 sec/batch)\n",
      "epoch 11, step 550 / 900, loss = 0.87 (0.007 sec/batch)\n",
      "epoch 11, step 600 / 900, loss = 0.67 (0.007 sec/batch)\n",
      "epoch 11, step 650 / 900, loss = 0.58 (0.009 sec/batch)\n",
      "epoch 11, step 700 / 900, loss = 0.65 (0.007 sec/batch)\n",
      "epoch 11, step 750 / 900, loss = 0.54 (0.007 sec/batch)\n",
      "epoch 11, step 800 / 900, loss = 0.63 (0.007 sec/batch)\n",
      "epoch 11, step 850 / 900, loss = 0.54 (0.008 sec/batch)\n",
      "epoch 11, step 900 / 900, loss = 0.65 (0.008 sec/batch)\n",
      "Train error:\n",
      " accuracy = 81.22\n",
      " avg loss = 0.59\n",
      "\n",
      "Validation error:\n",
      " accuracy = 70.78\n",
      " avg loss = 0.89\n",
      "\n",
      "Epoch time: 9.557794570922852\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 12, step 50 / 900, loss = 0.58 (0.008 sec/batch)\n",
      "epoch 12, step 100 / 900, loss = 0.63 (0.007 sec/batch)\n",
      "epoch 12, step 150 / 900, loss = 0.47 (0.006 sec/batch)\n",
      "epoch 12, step 200 / 900, loss = 0.52 (0.007 sec/batch)\n",
      "epoch 12, step 250 / 900, loss = 0.64 (0.007 sec/batch)\n",
      "epoch 12, step 300 / 900, loss = 0.48 (0.006 sec/batch)\n",
      "epoch 12, step 350 / 900, loss = 0.71 (0.008 sec/batch)\n",
      "epoch 12, step 400 / 900, loss = 0.56 (0.008 sec/batch)\n",
      "epoch 12, step 450 / 900, loss = 0.55 (0.007 sec/batch)\n",
      "epoch 12, step 500 / 900, loss = 0.40 (0.006 sec/batch)\n",
      "epoch 12, step 550 / 900, loss = 0.44 (0.009 sec/batch)\n",
      "epoch 12, step 600 / 900, loss = 0.66 (0.006 sec/batch)\n",
      "epoch 12, step 650 / 900, loss = 0.87 (0.007 sec/batch)\n",
      "epoch 12, step 700 / 900, loss = 0.77 (0.007 sec/batch)\n",
      "epoch 12, step 750 / 900, loss = 0.66 (0.009 sec/batch)\n",
      "epoch 12, step 800 / 900, loss = 0.47 (0.009 sec/batch)\n",
      "epoch 12, step 850 / 900, loss = 0.58 (0.008 sec/batch)\n",
      "epoch 12, step 900 / 900, loss = 0.32 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 82.40\n",
      " avg loss = 0.56\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.50\n",
      " avg loss = 0.87\n",
      "\n",
      "Epoch time: 9.441704750061035\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 13, step 50 / 900, loss = 0.42 (0.008 sec/batch)\n",
      "epoch 13, step 100 / 900, loss = 0.58 (0.006 sec/batch)\n",
      "epoch 13, step 150 / 900, loss = 0.50 (0.007 sec/batch)\n",
      "epoch 13, step 200 / 900, loss = 0.95 (0.007 sec/batch)\n",
      "epoch 13, step 250 / 900, loss = 0.60 (0.007 sec/batch)\n",
      "epoch 13, step 300 / 900, loss = 0.42 (0.008 sec/batch)\n",
      "epoch 13, step 350 / 900, loss = 0.70 (0.007 sec/batch)\n",
      "epoch 13, step 400 / 900, loss = 0.57 (0.007 sec/batch)\n",
      "epoch 13, step 450 / 900, loss = 0.63 (0.006 sec/batch)\n",
      "epoch 13, step 500 / 900, loss = 0.37 (0.007 sec/batch)\n",
      "epoch 13, step 550 / 900, loss = 0.54 (0.007 sec/batch)\n",
      "epoch 13, step 600 / 900, loss = 0.56 (0.006 sec/batch)\n",
      "epoch 13, step 650 / 900, loss = 0.57 (0.006 sec/batch)\n",
      "epoch 13, step 700 / 900, loss = 0.48 (0.008 sec/batch)\n",
      "epoch 13, step 750 / 900, loss = 0.67 (0.007 sec/batch)\n",
      "epoch 13, step 800 / 900, loss = 0.49 (0.007 sec/batch)\n",
      "epoch 13, step 850 / 900, loss = 0.44 (0.007 sec/batch)\n",
      "epoch 13, step 900 / 900, loss = 0.51 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 83.20\n",
      " avg loss = 0.54\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.72\n",
      " avg loss = 0.87\n",
      "\n",
      "Epoch time: 9.358803749084473\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 14, step 50 / 900, loss = 0.41 (0.007 sec/batch)\n",
      "epoch 14, step 100 / 900, loss = 0.43 (0.006 sec/batch)\n",
      "epoch 14, step 150 / 900, loss = 0.43 (0.008 sec/batch)\n",
      "epoch 14, step 200 / 900, loss = 0.44 (0.006 sec/batch)\n",
      "epoch 14, step 250 / 900, loss = 0.47 (0.007 sec/batch)\n",
      "epoch 14, step 300 / 900, loss = 0.61 (0.009 sec/batch)\n",
      "epoch 14, step 350 / 900, loss = 0.50 (0.010 sec/batch)\n",
      "epoch 14, step 400 / 900, loss = 0.66 (0.006 sec/batch)\n",
      "epoch 14, step 450 / 900, loss = 0.82 (0.012 sec/batch)\n",
      "epoch 14, step 500 / 900, loss = 0.67 (0.008 sec/batch)\n",
      "epoch 14, step 550 / 900, loss = 0.34 (0.008 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14, step 600 / 900, loss = 0.55 (0.006 sec/batch)\n",
      "epoch 14, step 650 / 900, loss = 0.65 (0.008 sec/batch)\n",
      "epoch 14, step 700 / 900, loss = 0.48 (0.010 sec/batch)\n",
      "epoch 14, step 750 / 900, loss = 0.58 (0.007 sec/batch)\n",
      "epoch 14, step 800 / 900, loss = 0.62 (0.006 sec/batch)\n",
      "epoch 14, step 850 / 900, loss = 0.30 (0.007 sec/batch)\n",
      "epoch 14, step 900 / 900, loss = 0.62 (0.008 sec/batch)\n",
      "Train error:\n",
      " accuracy = 83.88\n",
      " avg loss = 0.53\n",
      "\n",
      "Validation error:\n",
      " accuracy = 72.06\n",
      " avg loss = 0.87\n",
      "\n",
      "Epoch time: 9.401389122009277\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 15, step 50 / 900, loss = 0.39 (0.007 sec/batch)\n",
      "epoch 15, step 100 / 900, loss = 0.73 (0.006 sec/batch)\n",
      "epoch 15, step 150 / 900, loss = 0.44 (0.007 sec/batch)\n",
      "epoch 15, step 200 / 900, loss = 0.52 (0.006 sec/batch)\n",
      "epoch 15, step 250 / 900, loss = 0.61 (0.007 sec/batch)\n",
      "epoch 15, step 300 / 900, loss = 0.27 (0.007 sec/batch)\n",
      "epoch 15, step 350 / 900, loss = 0.54 (0.007 sec/batch)\n",
      "epoch 15, step 400 / 900, loss = 0.64 (0.007 sec/batch)\n",
      "epoch 15, step 450 / 900, loss = 0.36 (0.007 sec/batch)\n",
      "epoch 15, step 500 / 900, loss = 0.57 (0.006 sec/batch)\n",
      "epoch 15, step 550 / 900, loss = 0.49 (0.007 sec/batch)\n",
      "epoch 15, step 600 / 900, loss = 0.56 (0.006 sec/batch)\n",
      "epoch 15, step 650 / 900, loss = 0.47 (0.007 sec/batch)\n",
      "epoch 15, step 700 / 900, loss = 0.50 (0.007 sec/batch)\n",
      "epoch 15, step 750 / 900, loss = 0.65 (0.007 sec/batch)\n",
      "epoch 15, step 800 / 900, loss = 0.50 (0.009 sec/batch)\n",
      "epoch 15, step 850 / 900, loss = 0.31 (0.006 sec/batch)\n",
      "epoch 15, step 900 / 900, loss = 0.61 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 84.31\n",
      " avg loss = 0.51\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.02\n",
      " avg loss = 0.87\n",
      "\n",
      "Epoch time: 9.377566576004028\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 16, step 50 / 900, loss = 0.67 (0.006 sec/batch)\n",
      "epoch 16, step 100 / 900, loss = 0.65 (0.007 sec/batch)\n",
      "epoch 16, step 150 / 900, loss = 0.59 (0.007 sec/batch)\n",
      "epoch 16, step 200 / 900, loss = 0.34 (0.012 sec/batch)\n",
      "epoch 16, step 250 / 900, loss = 0.54 (0.006 sec/batch)\n",
      "epoch 16, step 300 / 900, loss = 0.51 (0.006 sec/batch)\n",
      "epoch 16, step 350 / 900, loss = 0.66 (0.007 sec/batch)\n",
      "epoch 16, step 400 / 900, loss = 0.57 (0.007 sec/batch)\n",
      "epoch 16, step 450 / 900, loss = 0.49 (0.006 sec/batch)\n",
      "epoch 16, step 500 / 900, loss = 0.36 (0.008 sec/batch)\n",
      "epoch 16, step 550 / 900, loss = 0.42 (0.009 sec/batch)\n",
      "epoch 16, step 600 / 900, loss = 0.44 (0.007 sec/batch)\n",
      "epoch 16, step 650 / 900, loss = 0.58 (0.007 sec/batch)\n",
      "epoch 16, step 700 / 900, loss = 0.68 (0.007 sec/batch)\n",
      "epoch 16, step 750 / 900, loss = 0.63 (0.007 sec/batch)\n",
      "epoch 16, step 800 / 900, loss = 0.46 (0.006 sec/batch)\n",
      "epoch 16, step 850 / 900, loss = 0.55 (0.011 sec/batch)\n",
      "epoch 16, step 900 / 900, loss = 0.73 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 84.79\n",
      " avg loss = 0.50\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.90\n",
      " avg loss = 0.87\n",
      "\n",
      "Epoch time: 9.393240213394165\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 17, step 50 / 900, loss = 0.35 (0.006 sec/batch)\n",
      "epoch 17, step 100 / 900, loss = 0.40 (0.007 sec/batch)\n",
      "epoch 17, step 150 / 900, loss = 0.60 (0.007 sec/batch)\n",
      "epoch 17, step 200 / 900, loss = 0.61 (0.008 sec/batch)\n",
      "epoch 17, step 250 / 900, loss = 0.73 (0.006 sec/batch)\n",
      "epoch 17, step 300 / 900, loss = 0.45 (0.007 sec/batch)\n",
      "epoch 17, step 350 / 900, loss = 0.58 (0.008 sec/batch)\n",
      "epoch 17, step 400 / 900, loss = 0.51 (0.007 sec/batch)\n",
      "epoch 17, step 450 / 900, loss = 0.54 (0.007 sec/batch)\n",
      "epoch 17, step 500 / 900, loss = 0.45 (0.008 sec/batch)\n",
      "epoch 17, step 550 / 900, loss = 0.60 (0.007 sec/batch)\n",
      "epoch 17, step 600 / 900, loss = 0.60 (0.007 sec/batch)\n",
      "epoch 17, step 650 / 900, loss = 0.46 (0.009 sec/batch)\n",
      "epoch 17, step 700 / 900, loss = 0.50 (0.007 sec/batch)\n",
      "epoch 17, step 750 / 900, loss = 0.53 (0.006 sec/batch)\n",
      "epoch 17, step 800 / 900, loss = 0.45 (0.009 sec/batch)\n",
      "epoch 17, step 850 / 900, loss = 0.33 (0.009 sec/batch)\n",
      "epoch 17, step 900 / 900, loss = 0.42 (0.010 sec/batch)\n",
      "Train error:\n",
      " accuracy = 85.28\n",
      " avg loss = 0.49\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.96\n",
      " avg loss = 0.87\n",
      "\n",
      "Epoch time: 9.535875797271729\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 18, step 50 / 900, loss = 0.55 (0.007 sec/batch)\n",
      "epoch 18, step 100 / 900, loss = 0.72 (0.007 sec/batch)\n",
      "epoch 18, step 150 / 900, loss = 0.58 (0.007 sec/batch)\n",
      "epoch 18, step 200 / 900, loss = 0.33 (0.006 sec/batch)\n",
      "epoch 18, step 250 / 900, loss = 0.57 (0.008 sec/batch)\n",
      "epoch 18, step 300 / 900, loss = 0.57 (0.008 sec/batch)\n",
      "epoch 18, step 350 / 900, loss = 0.51 (0.006 sec/batch)\n",
      "epoch 18, step 400 / 900, loss = 0.40 (0.006 sec/batch)\n",
      "epoch 18, step 450 / 900, loss = 0.40 (0.007 sec/batch)\n",
      "epoch 18, step 500 / 900, loss = 0.38 (0.007 sec/batch)\n",
      "epoch 18, step 550 / 900, loss = 0.68 (0.007 sec/batch)\n",
      "epoch 18, step 600 / 900, loss = 0.37 (0.006 sec/batch)\n",
      "epoch 18, step 650 / 900, loss = 0.57 (0.006 sec/batch)\n",
      "epoch 18, step 700 / 900, loss = 0.47 (0.006 sec/batch)\n",
      "epoch 18, step 750 / 900, loss = 0.41 (0.006 sec/batch)\n",
      "epoch 18, step 800 / 900, loss = 0.46 (0.007 sec/batch)\n",
      "epoch 18, step 850 / 900, loss = 0.56 (0.007 sec/batch)\n",
      "epoch 18, step 900 / 900, loss = 0.68 (0.009 sec/batch)\n",
      "Train error:\n",
      " accuracy = 85.48\n",
      " avg loss = 0.48\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.76\n",
      " avg loss = 0.87\n",
      "\n",
      "Epoch time: 9.514918804168701\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 19, step 50 / 900, loss = 0.45 (0.007 sec/batch)\n",
      "epoch 19, step 100 / 900, loss = 0.39 (0.007 sec/batch)\n",
      "epoch 19, step 150 / 900, loss = 0.48 (0.009 sec/batch)\n",
      "epoch 19, step 200 / 900, loss = 0.64 (0.009 sec/batch)\n",
      "epoch 19, step 250 / 900, loss = 0.38 (0.006 sec/batch)\n",
      "epoch 19, step 300 / 900, loss = 0.56 (0.009 sec/batch)\n",
      "epoch 19, step 350 / 900, loss = 0.56 (0.007 sec/batch)\n",
      "epoch 19, step 400 / 900, loss = 0.54 (0.007 sec/batch)\n",
      "epoch 19, step 450 / 900, loss = 0.41 (0.007 sec/batch)\n",
      "epoch 19, step 500 / 900, loss = 0.42 (0.007 sec/batch)\n",
      "epoch 19, step 550 / 900, loss = 0.51 (0.006 sec/batch)\n",
      "epoch 19, step 600 / 900, loss = 0.41 (0.007 sec/batch)\n",
      "epoch 19, step 650 / 900, loss = 0.61 (0.008 sec/batch)\n",
      "epoch 19, step 700 / 900, loss = 0.45 (0.008 sec/batch)\n",
      "epoch 19, step 750 / 900, loss = 0.39 (0.009 sec/batch)\n",
      "epoch 19, step 800 / 900, loss = 0.43 (0.006 sec/batch)\n",
      "epoch 19, step 850 / 900, loss = 0.43 (0.007 sec/batch)\n",
      "epoch 19, step 900 / 900, loss = 0.59 (0.009 sec/batch)\n",
      "Train error:\n",
      " accuracy = 85.98\n",
      " avg loss = 0.48\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.80\n",
      " avg loss = 0.87\n",
      "\n",
      "Epoch time: 9.440505743026733\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 20, step 50 / 900, loss = 0.45 (0.006 sec/batch)\n",
      "epoch 20, step 100 / 900, loss = 0.46 (0.008 sec/batch)\n",
      "epoch 20, step 150 / 900, loss = 0.53 (0.007 sec/batch)\n",
      "epoch 20, step 200 / 900, loss = 0.58 (0.008 sec/batch)\n",
      "epoch 20, step 250 / 900, loss = 0.39 (0.008 sec/batch)\n",
      "epoch 20, step 300 / 900, loss = 0.46 (0.009 sec/batch)\n",
      "epoch 20, step 350 / 900, loss = 0.52 (0.007 sec/batch)\n",
      "epoch 20, step 400 / 900, loss = 0.47 (0.008 sec/batch)\n",
      "epoch 20, step 450 / 900, loss = 0.54 (0.008 sec/batch)\n",
      "epoch 20, step 500 / 900, loss = 0.50 (0.007 sec/batch)\n",
      "epoch 20, step 550 / 900, loss = 0.35 (0.007 sec/batch)\n",
      "epoch 20, step 600 / 900, loss = 0.72 (0.007 sec/batch)\n",
      "epoch 20, step 650 / 900, loss = 0.43 (0.008 sec/batch)\n",
      "epoch 20, step 700 / 900, loss = 0.49 (0.006 sec/batch)\n",
      "epoch 20, step 750 / 900, loss = 0.47 (0.008 sec/batch)\n",
      "epoch 20, step 800 / 900, loss = 0.56 (0.007 sec/batch)\n",
      "epoch 20, step 850 / 900, loss = 0.55 (0.008 sec/batch)\n",
      "epoch 20, step 900 / 900, loss = 0.54 (0.008 sec/batch)\n",
      "Train error:\n",
      " accuracy = 86.20\n",
      " avg loss = 0.47\n",
      "\n",
      "Validation error:\n",
      " accuracy = 72.06\n",
      " avg loss = 0.87\n",
      "\n",
      "Epoch time: 9.406922578811646\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 21, step 50 / 900, loss = 0.61 (0.008 sec/batch)\n",
      "epoch 21, step 100 / 900, loss = 0.51 (0.006 sec/batch)\n",
      "epoch 21, step 150 / 900, loss = 0.51 (0.008 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21, step 200 / 900, loss = 0.47 (0.007 sec/batch)\n",
      "epoch 21, step 250 / 900, loss = 0.55 (0.011 sec/batch)\n",
      "epoch 21, step 300 / 900, loss = 0.32 (0.006 sec/batch)\n",
      "epoch 21, step 350 / 900, loss = 0.34 (0.006 sec/batch)\n",
      "epoch 21, step 400 / 900, loss = 0.60 (0.008 sec/batch)\n",
      "epoch 21, step 450 / 900, loss = 0.45 (0.007 sec/batch)\n",
      "epoch 21, step 500 / 900, loss = 0.53 (0.008 sec/batch)\n",
      "epoch 21, step 550 / 900, loss = 0.47 (0.007 sec/batch)\n",
      "epoch 21, step 600 / 900, loss = 0.46 (0.007 sec/batch)\n",
      "epoch 21, step 650 / 900, loss = 0.47 (0.008 sec/batch)\n",
      "epoch 21, step 700 / 900, loss = 0.57 (0.008 sec/batch)\n",
      "epoch 21, step 750 / 900, loss = 0.33 (0.006 sec/batch)\n",
      "epoch 21, step 800 / 900, loss = 0.39 (0.007 sec/batch)\n",
      "epoch 21, step 850 / 900, loss = 0.44 (0.007 sec/batch)\n",
      "epoch 21, step 900 / 900, loss = 0.48 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 86.26\n",
      " avg loss = 0.46\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.64\n",
      " avg loss = 0.87\n",
      "\n",
      "Epoch time: 9.465367555618286\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 22, step 50 / 900, loss = 0.41 (0.008 sec/batch)\n",
      "epoch 22, step 100 / 900, loss = 0.39 (0.007 sec/batch)\n",
      "epoch 22, step 150 / 900, loss = 0.49 (0.006 sec/batch)\n",
      "epoch 22, step 200 / 900, loss = 0.45 (0.013 sec/batch)\n",
      "epoch 22, step 250 / 900, loss = 0.24 (0.009 sec/batch)\n",
      "epoch 22, step 300 / 900, loss = 0.26 (0.008 sec/batch)\n",
      "epoch 22, step 350 / 900, loss = 0.56 (0.008 sec/batch)\n",
      "epoch 22, step 400 / 900, loss = 0.42 (0.009 sec/batch)\n",
      "epoch 22, step 450 / 900, loss = 0.49 (0.006 sec/batch)\n",
      "epoch 22, step 500 / 900, loss = 0.61 (0.007 sec/batch)\n",
      "epoch 22, step 550 / 900, loss = 0.41 (0.007 sec/batch)\n",
      "epoch 22, step 600 / 900, loss = 0.38 (0.008 sec/batch)\n",
      "epoch 22, step 650 / 900, loss = 0.84 (0.008 sec/batch)\n",
      "epoch 22, step 700 / 900, loss = 0.50 (0.009 sec/batch)\n",
      "epoch 22, step 750 / 900, loss = 0.45 (0.006 sec/batch)\n",
      "epoch 22, step 800 / 900, loss = 0.54 (0.009 sec/batch)\n",
      "epoch 22, step 850 / 900, loss = 0.33 (0.007 sec/batch)\n",
      "epoch 22, step 900 / 900, loss = 0.49 (0.008 sec/batch)\n",
      "Train error:\n",
      " accuracy = 86.79\n",
      " avg loss = 0.46\n",
      "\n",
      "Validation error:\n",
      " accuracy = 72.00\n",
      " avg loss = 0.87\n",
      "\n",
      "Epoch time: 9.41845154762268\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 23, step 50 / 900, loss = 0.35 (0.007 sec/batch)\n",
      "epoch 23, step 100 / 900, loss = 0.46 (0.006 sec/batch)\n",
      "epoch 23, step 150 / 900, loss = 0.37 (0.007 sec/batch)\n",
      "epoch 23, step 200 / 900, loss = 0.71 (0.006 sec/batch)\n",
      "epoch 23, step 250 / 900, loss = 0.53 (0.009 sec/batch)\n",
      "epoch 23, step 300 / 900, loss = 0.49 (0.009 sec/batch)\n",
      "epoch 23, step 350 / 900, loss = 0.41 (0.006 sec/batch)\n",
      "epoch 23, step 400 / 900, loss = 0.38 (0.006 sec/batch)\n",
      "epoch 23, step 450 / 900, loss = 0.36 (0.009 sec/batch)\n",
      "epoch 23, step 500 / 900, loss = 0.45 (0.008 sec/batch)\n",
      "epoch 23, step 550 / 900, loss = 0.41 (0.007 sec/batch)\n",
      "epoch 23, step 600 / 900, loss = 0.34 (0.006 sec/batch)\n",
      "epoch 23, step 650 / 900, loss = 0.33 (0.006 sec/batch)\n",
      "epoch 23, step 700 / 900, loss = 0.34 (0.008 sec/batch)\n",
      "epoch 23, step 750 / 900, loss = 0.35 (0.008 sec/batch)\n",
      "epoch 23, step 800 / 900, loss = 0.49 (0.006 sec/batch)\n",
      "epoch 23, step 850 / 900, loss = 0.72 (0.006 sec/batch)\n",
      "epoch 23, step 900 / 900, loss = 0.48 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 87.03\n",
      " avg loss = 0.45\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.96\n",
      " avg loss = 0.87\n",
      "\n",
      "Epoch time: 9.460394144058228\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 24, step 50 / 900, loss = 0.56 (0.006 sec/batch)\n",
      "epoch 24, step 100 / 900, loss = 0.46 (0.008 sec/batch)\n",
      "epoch 24, step 150 / 900, loss = 0.38 (0.010 sec/batch)\n",
      "epoch 24, step 200 / 900, loss = 0.67 (0.007 sec/batch)\n",
      "epoch 24, step 250 / 900, loss = 0.30 (0.008 sec/batch)\n",
      "epoch 24, step 300 / 900, loss = 0.53 (0.006 sec/batch)\n",
      "epoch 24, step 350 / 900, loss = 0.64 (0.007 sec/batch)\n",
      "epoch 24, step 400 / 900, loss = 0.49 (0.007 sec/batch)\n",
      "epoch 24, step 450 / 900, loss = 0.57 (0.007 sec/batch)\n",
      "epoch 24, step 500 / 900, loss = 0.48 (0.009 sec/batch)\n",
      "epoch 24, step 550 / 900, loss = 0.32 (0.009 sec/batch)\n",
      "epoch 24, step 600 / 900, loss = 0.34 (0.010 sec/batch)\n",
      "epoch 24, step 650 / 900, loss = 0.46 (0.007 sec/batch)\n",
      "epoch 24, step 700 / 900, loss = 0.51 (0.008 sec/batch)\n",
      "epoch 24, step 750 / 900, loss = 0.36 (0.007 sec/batch)\n",
      "epoch 24, step 800 / 900, loss = 0.46 (0.007 sec/batch)\n",
      "epoch 24, step 850 / 900, loss = 0.50 (0.007 sec/batch)\n",
      "epoch 24, step 900 / 900, loss = 0.47 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 87.07\n",
      " avg loss = 0.45\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.60\n",
      " avg loss = 0.87\n",
      "\n",
      "Epoch time: 9.38111138343811\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 25, step 50 / 900, loss = 0.39 (0.006 sec/batch)\n",
      "epoch 25, step 100 / 900, loss = 0.50 (0.006 sec/batch)\n",
      "epoch 25, step 150 / 900, loss = 0.39 (0.006 sec/batch)\n",
      "epoch 25, step 200 / 900, loss = 0.53 (0.006 sec/batch)\n",
      "epoch 25, step 250 / 900, loss = 0.54 (0.007 sec/batch)\n",
      "epoch 25, step 300 / 900, loss = 0.34 (0.006 sec/batch)\n",
      "epoch 25, step 350 / 900, loss = 0.45 (0.007 sec/batch)\n",
      "epoch 25, step 400 / 900, loss = 0.41 (0.007 sec/batch)\n",
      "epoch 25, step 450 / 900, loss = 0.44 (0.006 sec/batch)\n",
      "epoch 25, step 500 / 900, loss = 0.56 (0.008 sec/batch)\n",
      "epoch 25, step 550 / 900, loss = 0.68 (0.013 sec/batch)\n",
      "epoch 25, step 600 / 900, loss = 0.51 (0.007 sec/batch)\n",
      "epoch 25, step 650 / 900, loss = 0.51 (0.007 sec/batch)\n",
      "epoch 25, step 700 / 900, loss = 0.29 (0.006 sec/batch)\n",
      "epoch 25, step 750 / 900, loss = 0.28 (0.007 sec/batch)\n",
      "epoch 25, step 800 / 900, loss = 0.57 (0.008 sec/batch)\n",
      "epoch 25, step 850 / 900, loss = 0.48 (0.008 sec/batch)\n",
      "epoch 25, step 900 / 900, loss = 0.57 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 87.28\n",
      " avg loss = 0.44\n",
      "\n",
      "Validation error:\n",
      " accuracy = 72.14\n",
      " avg loss = 0.87\n",
      "\n",
      "Epoch time: 9.35793662071228\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 26, step 50 / 900, loss = 0.62 (0.007 sec/batch)\n",
      "epoch 26, step 100 / 900, loss = 0.53 (0.007 sec/batch)\n",
      "epoch 26, step 150 / 900, loss = 0.44 (0.009 sec/batch)\n",
      "epoch 26, step 200 / 900, loss = 0.41 (0.007 sec/batch)\n",
      "epoch 26, step 250 / 900, loss = 0.34 (0.006 sec/batch)\n",
      "epoch 26, step 300 / 900, loss = 0.49 (0.007 sec/batch)\n",
      "epoch 26, step 350 / 900, loss = 0.62 (0.010 sec/batch)\n",
      "epoch 26, step 400 / 900, loss = 0.41 (0.009 sec/batch)\n",
      "epoch 26, step 450 / 900, loss = 0.35 (0.012 sec/batch)\n",
      "epoch 26, step 500 / 900, loss = 0.36 (0.006 sec/batch)\n",
      "epoch 26, step 550 / 900, loss = 0.64 (0.006 sec/batch)\n",
      "epoch 26, step 600 / 900, loss = 0.42 (0.007 sec/batch)\n",
      "epoch 26, step 650 / 900, loss = 0.49 (0.008 sec/batch)\n",
      "epoch 26, step 700 / 900, loss = 0.51 (0.007 sec/batch)\n",
      "epoch 26, step 750 / 900, loss = 0.55 (0.007 sec/batch)\n",
      "epoch 26, step 800 / 900, loss = 0.47 (0.007 sec/batch)\n",
      "epoch 26, step 850 / 900, loss = 0.40 (0.009 sec/batch)\n",
      "epoch 26, step 900 / 900, loss = 0.40 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 87.38\n",
      " avg loss = 0.44\n",
      "\n",
      "Validation error:\n",
      " accuracy = 72.04\n",
      " avg loss = 0.87\n",
      "\n",
      "Epoch time: 9.382044315338135\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 27, step 50 / 900, loss = 0.43 (0.008 sec/batch)\n",
      "epoch 27, step 100 / 900, loss = 0.46 (0.008 sec/batch)\n",
      "epoch 27, step 150 / 900, loss = 0.47 (0.009 sec/batch)\n",
      "epoch 27, step 200 / 900, loss = 0.43 (0.007 sec/batch)\n",
      "epoch 27, step 250 / 900, loss = 0.52 (0.008 sec/batch)\n",
      "epoch 27, step 300 / 900, loss = 0.48 (0.007 sec/batch)\n",
      "epoch 27, step 350 / 900, loss = 0.35 (0.007 sec/batch)\n",
      "epoch 27, step 400 / 900, loss = 0.53 (0.009 sec/batch)\n",
      "epoch 27, step 450 / 900, loss = 0.45 (0.006 sec/batch)\n",
      "epoch 27, step 500 / 900, loss = 0.31 (0.008 sec/batch)\n",
      "epoch 27, step 550 / 900, loss = 0.42 (0.007 sec/batch)\n",
      "epoch 27, step 600 / 900, loss = 0.38 (0.006 sec/batch)\n",
      "epoch 27, step 650 / 900, loss = 0.53 (0.007 sec/batch)\n",
      "epoch 27, step 700 / 900, loss = 0.28 (0.006 sec/batch)\n",
      "epoch 27, step 750 / 900, loss = 0.58 (0.006 sec/batch)\n",
      "epoch 27, step 800 / 900, loss = 0.54 (0.007 sec/batch)\n",
      "epoch 27, step 850 / 900, loss = 0.32 (0.007 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 27, step 900 / 900, loss = 0.53 (0.008 sec/batch)\n",
      "Train error:\n",
      " accuracy = 87.43\n",
      " avg loss = 0.44\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.98\n",
      " avg loss = 0.87\n",
      "\n",
      "Epoch time: 9.328996181488037\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 28, step 50 / 900, loss = 0.60 (0.010 sec/batch)\n",
      "epoch 28, step 100 / 900, loss = 0.50 (0.009 sec/batch)\n",
      "epoch 28, step 150 / 900, loss = 0.28 (0.006 sec/batch)\n",
      "epoch 28, step 200 / 900, loss = 0.49 (0.008 sec/batch)\n",
      "epoch 28, step 250 / 900, loss = 0.37 (0.006 sec/batch)\n",
      "epoch 28, step 300 / 900, loss = 0.44 (0.013 sec/batch)\n",
      "epoch 28, step 350 / 900, loss = 0.39 (0.007 sec/batch)\n",
      "epoch 28, step 400 / 900, loss = 0.57 (0.008 sec/batch)\n",
      "epoch 28, step 450 / 900, loss = 0.56 (0.007 sec/batch)\n",
      "epoch 28, step 500 / 900, loss = 0.53 (0.008 sec/batch)\n",
      "epoch 28, step 550 / 900, loss = 0.45 (0.008 sec/batch)\n",
      "epoch 28, step 600 / 900, loss = 0.33 (0.008 sec/batch)\n",
      "epoch 28, step 650 / 900, loss = 0.42 (0.006 sec/batch)\n",
      "epoch 28, step 700 / 900, loss = 0.40 (0.008 sec/batch)\n",
      "epoch 28, step 750 / 900, loss = 0.51 (0.007 sec/batch)\n",
      "epoch 28, step 800 / 900, loss = 0.42 (0.009 sec/batch)\n",
      "epoch 28, step 850 / 900, loss = 0.49 (0.009 sec/batch)\n",
      "epoch 28, step 900 / 900, loss = 0.52 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 87.48\n",
      " avg loss = 0.44\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.98\n",
      " avg loss = 0.87\n",
      "\n",
      "Epoch time: 9.358166217803955\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 29, step 50 / 900, loss = 0.36 (0.009 sec/batch)\n",
      "epoch 29, step 100 / 900, loss = 0.58 (0.006 sec/batch)\n",
      "epoch 29, step 150 / 900, loss = 0.45 (0.007 sec/batch)\n",
      "epoch 29, step 200 / 900, loss = 0.50 (0.006 sec/batch)\n",
      "epoch 29, step 250 / 900, loss = 0.55 (0.007 sec/batch)\n",
      "epoch 29, step 300 / 900, loss = 0.38 (0.006 sec/batch)\n",
      "epoch 29, step 350 / 900, loss = 0.50 (0.009 sec/batch)\n",
      "epoch 29, step 400 / 900, loss = 0.38 (0.006 sec/batch)\n",
      "epoch 29, step 450 / 900, loss = 0.55 (0.009 sec/batch)\n",
      "epoch 29, step 500 / 900, loss = 0.32 (0.006 sec/batch)\n",
      "epoch 29, step 550 / 900, loss = 0.44 (0.007 sec/batch)\n",
      "epoch 29, step 600 / 900, loss = 0.36 (0.007 sec/batch)\n",
      "epoch 29, step 650 / 900, loss = 0.41 (0.006 sec/batch)\n",
      "epoch 29, step 700 / 900, loss = 0.32 (0.007 sec/batch)\n",
      "epoch 29, step 750 / 900, loss = 0.43 (0.007 sec/batch)\n",
      "epoch 29, step 800 / 900, loss = 0.37 (0.007 sec/batch)\n",
      "epoch 29, step 850 / 900, loss = 0.28 (0.007 sec/batch)\n",
      "epoch 29, step 900 / 900, loss = 0.38 (0.012 sec/batch)\n",
      "Train error:\n",
      " accuracy = 87.64\n",
      " avg loss = 0.43\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.86\n",
      " avg loss = 0.87\n",
      "\n",
      "Epoch time: 9.328883171081543\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 30, step 50 / 900, loss = 0.51 (0.010 sec/batch)\n",
      "epoch 30, step 100 / 900, loss = 0.45 (0.007 sec/batch)\n",
      "epoch 30, step 150 / 900, loss = 0.36 (0.008 sec/batch)\n",
      "epoch 30, step 200 / 900, loss = 0.55 (0.007 sec/batch)\n",
      "epoch 30, step 250 / 900, loss = 0.25 (0.008 sec/batch)\n",
      "epoch 30, step 300 / 900, loss = 0.35 (0.007 sec/batch)\n",
      "epoch 30, step 350 / 900, loss = 0.48 (0.007 sec/batch)\n",
      "epoch 30, step 400 / 900, loss = 0.35 (0.006 sec/batch)\n",
      "epoch 30, step 450 / 900, loss = 0.30 (0.010 sec/batch)\n",
      "epoch 30, step 500 / 900, loss = 0.37 (0.008 sec/batch)\n",
      "epoch 30, step 550 / 900, loss = 0.51 (0.009 sec/batch)\n",
      "epoch 30, step 600 / 900, loss = 0.54 (0.008 sec/batch)\n",
      "epoch 30, step 650 / 900, loss = 0.37 (0.007 sec/batch)\n",
      "epoch 30, step 700 / 900, loss = 0.39 (0.009 sec/batch)\n",
      "epoch 30, step 750 / 900, loss = 0.35 (0.008 sec/batch)\n",
      "epoch 30, step 800 / 900, loss = 0.43 (0.008 sec/batch)\n",
      "epoch 30, step 850 / 900, loss = 0.40 (0.007 sec/batch)\n",
      "epoch 30, step 900 / 900, loss = 0.27 (0.008 sec/batch)\n",
      "Train error:\n",
      " accuracy = 87.70\n",
      " avg loss = 0.43\n",
      "\n",
      "Validation error:\n",
      " accuracy = 72.08\n",
      " avg loss = 0.87\n",
      "\n",
      "Epoch time: 9.411394357681274\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 31, step 50 / 900, loss = 0.29 (0.012 sec/batch)\n",
      "epoch 31, step 100 / 900, loss = 0.38 (0.006 sec/batch)\n",
      "epoch 31, step 150 / 900, loss = 0.48 (0.006 sec/batch)\n",
      "epoch 31, step 200 / 900, loss = 0.32 (0.006 sec/batch)\n",
      "epoch 31, step 250 / 900, loss = 0.46 (0.006 sec/batch)\n",
      "epoch 31, step 300 / 900, loss = 0.31 (0.008 sec/batch)\n",
      "epoch 31, step 350 / 900, loss = 0.29 (0.009 sec/batch)\n",
      "epoch 31, step 400 / 900, loss = 0.47 (0.011 sec/batch)\n",
      "epoch 31, step 450 / 900, loss = 0.37 (0.007 sec/batch)\n",
      "epoch 31, step 500 / 900, loss = 0.46 (0.009 sec/batch)\n",
      "epoch 31, step 550 / 900, loss = 0.54 (0.007 sec/batch)\n",
      "epoch 31, step 600 / 900, loss = 0.50 (0.007 sec/batch)\n",
      "epoch 31, step 650 / 900, loss = 0.31 (0.007 sec/batch)\n",
      "epoch 31, step 700 / 900, loss = 0.44 (0.006 sec/batch)\n",
      "epoch 31, step 750 / 900, loss = 0.45 (0.008 sec/batch)\n",
      "epoch 31, step 800 / 900, loss = 0.42 (0.007 sec/batch)\n",
      "epoch 31, step 850 / 900, loss = 0.32 (0.013 sec/batch)\n",
      "epoch 31, step 900 / 900, loss = 0.62 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 87.72\n",
      " avg loss = 0.43\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.86\n",
      " avg loss = 0.87\n",
      "\n",
      "Epoch time: 9.363265037536621\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 32, step 50 / 900, loss = 0.39 (0.008 sec/batch)\n",
      "epoch 32, step 100 / 900, loss = 0.48 (0.006 sec/batch)\n",
      "epoch 32, step 150 / 900, loss = 0.49 (0.007 sec/batch)\n",
      "epoch 32, step 200 / 900, loss = 0.46 (0.008 sec/batch)\n",
      "epoch 32, step 250 / 900, loss = 0.41 (0.008 sec/batch)\n",
      "epoch 32, step 300 / 900, loss = 0.39 (0.006 sec/batch)\n",
      "epoch 32, step 350 / 900, loss = 0.29 (0.008 sec/batch)\n",
      "epoch 32, step 400 / 900, loss = 0.46 (0.011 sec/batch)\n",
      "epoch 32, step 450 / 900, loss = 0.47 (0.007 sec/batch)\n",
      "epoch 32, step 500 / 900, loss = 0.32 (0.007 sec/batch)\n",
      "epoch 32, step 550 / 900, loss = 0.27 (0.009 sec/batch)\n",
      "epoch 32, step 600 / 900, loss = 0.39 (0.012 sec/batch)\n",
      "epoch 32, step 650 / 900, loss = 0.46 (0.008 sec/batch)\n",
      "epoch 32, step 700 / 900, loss = 0.25 (0.006 sec/batch)\n",
      "epoch 32, step 750 / 900, loss = 0.36 (0.006 sec/batch)\n",
      "epoch 32, step 800 / 900, loss = 0.42 (0.007 sec/batch)\n",
      "epoch 32, step 850 / 900, loss = 0.35 (0.010 sec/batch)\n",
      "epoch 32, step 900 / 900, loss = 0.45 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 87.86\n",
      " avg loss = 0.43\n",
      "\n",
      "Validation error:\n",
      " accuracy = 72.12\n",
      " avg loss = 0.87\n",
      "\n",
      "Epoch time: 9.34594464302063\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 33, step 50 / 900, loss = 0.36 (0.008 sec/batch)\n",
      "epoch 33, step 100 / 900, loss = 0.51 (0.006 sec/batch)\n",
      "epoch 33, step 150 / 900, loss = 0.51 (0.006 sec/batch)\n",
      "epoch 33, step 200 / 900, loss = 0.44 (0.007 sec/batch)\n",
      "epoch 33, step 250 / 900, loss = 0.44 (0.009 sec/batch)\n",
      "epoch 33, step 300 / 900, loss = 0.55 (0.008 sec/batch)\n",
      "epoch 33, step 350 / 900, loss = 0.44 (0.007 sec/batch)\n",
      "epoch 33, step 400 / 900, loss = 0.42 (0.007 sec/batch)\n",
      "epoch 33, step 450 / 900, loss = 0.36 (0.006 sec/batch)\n",
      "epoch 33, step 500 / 900, loss = 0.60 (0.006 sec/batch)\n",
      "epoch 33, step 550 / 900, loss = 0.41 (0.009 sec/batch)\n",
      "epoch 33, step 600 / 900, loss = 0.33 (0.007 sec/batch)\n",
      "epoch 33, step 650 / 900, loss = 0.47 (0.008 sec/batch)\n",
      "epoch 33, step 700 / 900, loss = 0.27 (0.008 sec/batch)\n",
      "epoch 33, step 750 / 900, loss = 0.48 (0.007 sec/batch)\n",
      "epoch 33, step 800 / 900, loss = 0.37 (0.007 sec/batch)\n",
      "epoch 33, step 850 / 900, loss = 0.33 (0.008 sec/batch)\n",
      "epoch 33, step 900 / 900, loss = 0.43 (0.009 sec/batch)\n",
      "Train error:\n",
      " accuracy = 87.83\n",
      " avg loss = 0.43\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.96\n",
      " avg loss = 0.87\n",
      "\n",
      "Epoch time: 9.381444692611694\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 34, step 50 / 900, loss = 0.29 (0.008 sec/batch)\n",
      "epoch 34, step 100 / 900, loss = 0.51 (0.009 sec/batch)\n",
      "epoch 34, step 150 / 900, loss = 0.46 (0.006 sec/batch)\n",
      "epoch 34, step 200 / 900, loss = 0.38 (0.009 sec/batch)\n",
      "epoch 34, step 250 / 900, loss = 0.62 (0.006 sec/batch)\n",
      "epoch 34, step 300 / 900, loss = 0.34 (0.008 sec/batch)\n",
      "epoch 34, step 350 / 900, loss = 0.34 (0.008 sec/batch)\n",
      "epoch 34, step 400 / 900, loss = 0.55 (0.007 sec/batch)\n",
      "epoch 34, step 450 / 900, loss = 0.41 (0.007 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 34, step 500 / 900, loss = 0.28 (0.008 sec/batch)\n",
      "epoch 34, step 550 / 900, loss = 0.40 (0.009 sec/batch)\n",
      "epoch 34, step 600 / 900, loss = 0.37 (0.007 sec/batch)\n",
      "epoch 34, step 650 / 900, loss = 0.42 (0.007 sec/batch)\n",
      "epoch 34, step 700 / 900, loss = 0.44 (0.008 sec/batch)\n",
      "epoch 34, step 750 / 900, loss = 0.49 (0.008 sec/batch)\n",
      "epoch 34, step 800 / 900, loss = 0.28 (0.007 sec/batch)\n",
      "epoch 34, step 850 / 900, loss = 0.42 (0.007 sec/batch)\n",
      "epoch 34, step 900 / 900, loss = 0.47 (0.009 sec/batch)\n",
      "Train error:\n",
      " accuracy = 87.95\n",
      " avg loss = 0.43\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.94\n",
      " avg loss = 0.87\n",
      "\n",
      "Epoch time: 9.360340595245361\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 35, step 50 / 900, loss = 0.26 (0.008 sec/batch)\n",
      "epoch 35, step 100 / 900, loss = 0.36 (0.006 sec/batch)\n",
      "epoch 35, step 150 / 900, loss = 0.32 (0.008 sec/batch)\n",
      "epoch 35, step 200 / 900, loss = 0.49 (0.007 sec/batch)\n",
      "epoch 35, step 250 / 900, loss = 0.51 (0.006 sec/batch)\n",
      "epoch 35, step 300 / 900, loss = 0.33 (0.007 sec/batch)\n",
      "epoch 35, step 350 / 900, loss = 0.36 (0.006 sec/batch)\n",
      "epoch 35, step 400 / 900, loss = 0.34 (0.007 sec/batch)\n",
      "epoch 35, step 450 / 900, loss = 0.47 (0.007 sec/batch)\n",
      "epoch 35, step 500 / 900, loss = 0.43 (0.008 sec/batch)\n",
      "epoch 35, step 550 / 900, loss = 0.40 (0.006 sec/batch)\n",
      "epoch 35, step 600 / 900, loss = 0.40 (0.008 sec/batch)\n",
      "epoch 35, step 650 / 900, loss = 0.36 (0.007 sec/batch)\n",
      "epoch 35, step 700 / 900, loss = 0.44 (0.008 sec/batch)\n",
      "epoch 35, step 750 / 900, loss = 0.31 (0.007 sec/batch)\n",
      "epoch 35, step 800 / 900, loss = 0.43 (0.007 sec/batch)\n",
      "epoch 35, step 850 / 900, loss = 0.37 (0.007 sec/batch)\n",
      "epoch 35, step 900 / 900, loss = 0.33 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 87.91\n",
      " avg loss = 0.43\n",
      "\n",
      "Validation error:\n",
      " accuracy = 72.10\n",
      " avg loss = 0.87\n",
      "\n",
      "Epoch time: 9.32069182395935\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 36, step 50 / 900, loss = 0.33 (0.007 sec/batch)\n",
      "epoch 36, step 100 / 900, loss = 0.32 (0.007 sec/batch)\n",
      "epoch 36, step 150 / 900, loss = 0.57 (0.008 sec/batch)\n",
      "epoch 36, step 200 / 900, loss = 0.40 (0.008 sec/batch)\n",
      "epoch 36, step 250 / 900, loss = 0.54 (0.006 sec/batch)\n",
      "epoch 36, step 300 / 900, loss = 0.39 (0.012 sec/batch)\n",
      "epoch 36, step 350 / 900, loss = 0.35 (0.006 sec/batch)\n",
      "epoch 36, step 400 / 900, loss = 0.36 (0.006 sec/batch)\n",
      "epoch 36, step 450 / 900, loss = 0.33 (0.009 sec/batch)\n",
      "epoch 36, step 500 / 900, loss = 0.49 (0.007 sec/batch)\n",
      "epoch 36, step 550 / 900, loss = 0.39 (0.009 sec/batch)\n",
      "epoch 36, step 600 / 900, loss = 0.54 (0.007 sec/batch)\n",
      "epoch 36, step 650 / 900, loss = 0.27 (0.006 sec/batch)\n",
      "epoch 36, step 700 / 900, loss = 0.31 (0.008 sec/batch)\n",
      "epoch 36, step 750 / 900, loss = 0.48 (0.007 sec/batch)\n",
      "epoch 36, step 800 / 900, loss = 0.28 (0.007 sec/batch)\n",
      "epoch 36, step 850 / 900, loss = 0.51 (0.006 sec/batch)\n",
      "epoch 36, step 900 / 900, loss = 0.34 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 87.92\n",
      " avg loss = 0.42\n",
      "\n",
      "Validation error:\n",
      " accuracy = 72.00\n",
      " avg loss = 0.88\n",
      "\n",
      "Epoch time: 9.439465045928955\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 37, step 50 / 900, loss = 0.44 (0.010 sec/batch)\n",
      "epoch 37, step 100 / 900, loss = 0.27 (0.007 sec/batch)\n",
      "epoch 37, step 150 / 900, loss = 0.41 (0.007 sec/batch)\n",
      "epoch 37, step 200 / 900, loss = 0.40 (0.007 sec/batch)\n",
      "epoch 37, step 250 / 900, loss = 0.72 (0.007 sec/batch)\n",
      "epoch 37, step 300 / 900, loss = 0.52 (0.007 sec/batch)\n",
      "epoch 37, step 350 / 900, loss = 0.41 (0.011 sec/batch)\n",
      "epoch 37, step 400 / 900, loss = 0.52 (0.007 sec/batch)\n",
      "epoch 37, step 450 / 900, loss = 0.35 (0.006 sec/batch)\n",
      "epoch 37, step 500 / 900, loss = 0.41 (0.006 sec/batch)\n",
      "epoch 37, step 550 / 900, loss = 0.53 (0.007 sec/batch)\n",
      "epoch 37, step 600 / 900, loss = 0.52 (0.007 sec/batch)\n",
      "epoch 37, step 650 / 900, loss = 0.38 (0.007 sec/batch)\n",
      "epoch 37, step 700 / 900, loss = 0.60 (0.008 sec/batch)\n",
      "epoch 37, step 750 / 900, loss = 0.44 (0.012 sec/batch)\n",
      "epoch 37, step 800 / 900, loss = 0.26 (0.009 sec/batch)\n",
      "epoch 37, step 850 / 900, loss = 0.34 (0.007 sec/batch)\n",
      "epoch 37, step 900 / 900, loss = 0.58 (0.008 sec/batch)\n",
      "Train error:\n",
      " accuracy = 87.96\n",
      " avg loss = 0.42\n",
      "\n",
      "Validation error:\n",
      " accuracy = 71.98\n",
      " avg loss = 0.87\n",
      "\n",
      "Epoch time: 9.62856650352478\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 38, step 50 / 900, loss = 0.54 (0.007 sec/batch)\n",
      "epoch 38, step 100 / 900, loss = 0.66 (0.009 sec/batch)\n",
      "epoch 38, step 150 / 900, loss = 0.54 (0.007 sec/batch)\n",
      "epoch 38, step 200 / 900, loss = 0.33 (0.006 sec/batch)\n",
      "epoch 38, step 250 / 900, loss = 0.40 (0.006 sec/batch)\n",
      "epoch 38, step 300 / 900, loss = 0.44 (0.007 sec/batch)\n",
      "epoch 38, step 350 / 900, loss = 0.63 (0.006 sec/batch)\n",
      "epoch 38, step 400 / 900, loss = 0.54 (0.008 sec/batch)\n",
      "epoch 38, step 450 / 900, loss = 0.48 (0.007 sec/batch)\n",
      "epoch 38, step 500 / 900, loss = 0.53 (0.006 sec/batch)\n",
      "epoch 38, step 550 / 900, loss = 0.40 (0.007 sec/batch)\n",
      "epoch 38, step 600 / 900, loss = 0.39 (0.007 sec/batch)\n",
      "epoch 38, step 650 / 900, loss = 0.47 (0.006 sec/batch)\n",
      "epoch 38, step 700 / 900, loss = 0.49 (0.013 sec/batch)\n",
      "epoch 38, step 750 / 900, loss = 0.60 (0.007 sec/batch)\n",
      "epoch 38, step 800 / 900, loss = 0.45 (0.006 sec/batch)\n",
      "epoch 38, step 850 / 900, loss = 0.40 (0.007 sec/batch)\n",
      "epoch 38, step 900 / 900, loss = 0.44 (0.007 sec/batch)\n",
      "Train error:\n",
      " accuracy = 88.02\n",
      " avg loss = 0.42\n",
      "\n",
      "Validation error:\n",
      " accuracy = 72.20\n",
      " avg loss = 0.87\n",
      "\n",
      "Epoch time: 9.33177638053894\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 39, step 50 / 900, loss = 0.47 (0.006 sec/batch)\n",
      "epoch 39, step 100 / 900, loss = 0.42 (0.007 sec/batch)\n",
      "epoch 39, step 150 / 900, loss = 0.34 (0.006 sec/batch)\n",
      "epoch 39, step 200 / 900, loss = 0.34 (0.007 sec/batch)\n",
      "epoch 39, step 250 / 900, loss = 0.42 (0.008 sec/batch)\n",
      "epoch 39, step 300 / 900, loss = 0.35 (0.006 sec/batch)\n",
      "epoch 39, step 350 / 900, loss = 0.45 (0.007 sec/batch)\n",
      "epoch 39, step 400 / 900, loss = 0.34 (0.007 sec/batch)\n",
      "epoch 39, step 450 / 900, loss = 0.47 (0.008 sec/batch)\n",
      "epoch 39, step 500 / 900, loss = 0.49 (0.008 sec/batch)\n",
      "epoch 39, step 550 / 900, loss = 0.50 (0.007 sec/batch)\n",
      "epoch 39, step 600 / 900, loss = 0.49 (0.007 sec/batch)\n",
      "epoch 39, step 650 / 900, loss = 0.48 (0.010 sec/batch)\n",
      "epoch 39, step 700 / 900, loss = 0.52 (0.006 sec/batch)\n",
      "epoch 39, step 750 / 900, loss = 0.42 (0.006 sec/batch)\n",
      "epoch 39, step 800 / 900, loss = 0.47 (0.006 sec/batch)\n",
      "epoch 39, step 850 / 900, loss = 0.55 (0.006 sec/batch)\n",
      "epoch 39, step 900 / 900, loss = 0.34 (0.008 sec/batch)\n",
      "Train error:\n",
      " accuracy = 88.07\n",
      " avg loss = 0.42\n",
      "\n",
      "Validation error:\n",
      " accuracy = 72.12\n",
      " avg loss = 0.87\n",
      "\n",
      "Epoch time: 9.416100263595581\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 40, step 50 / 900, loss = 0.54 (0.006 sec/batch)\n",
      "epoch 40, step 100 / 900, loss = 0.45 (0.007 sec/batch)\n",
      "epoch 40, step 150 / 900, loss = 0.49 (0.008 sec/batch)\n",
      "epoch 40, step 200 / 900, loss = 0.37 (0.006 sec/batch)\n",
      "epoch 40, step 250 / 900, loss = 0.30 (0.008 sec/batch)\n",
      "epoch 40, step 300 / 900, loss = 0.27 (0.006 sec/batch)\n",
      "epoch 40, step 350 / 900, loss = 0.38 (0.007 sec/batch)\n",
      "epoch 40, step 400 / 900, loss = 0.36 (0.007 sec/batch)\n",
      "epoch 40, step 450 / 900, loss = 0.49 (0.007 sec/batch)\n",
      "epoch 40, step 500 / 900, loss = 0.44 (0.007 sec/batch)\n",
      "epoch 40, step 550 / 900, loss = 0.42 (0.007 sec/batch)\n",
      "epoch 40, step 600 / 900, loss = 0.31 (0.006 sec/batch)\n",
      "epoch 40, step 650 / 900, loss = 0.36 (0.007 sec/batch)\n",
      "epoch 40, step 700 / 900, loss = 0.43 (0.013 sec/batch)\n",
      "epoch 40, step 750 / 900, loss = 0.37 (0.009 sec/batch)\n",
      "epoch 40, step 800 / 900, loss = 0.30 (0.007 sec/batch)\n",
      "epoch 40, step 850 / 900, loss = 0.28 (0.009 sec/batch)\n",
      "epoch 40, step 900 / 900, loss = 0.58 (0.006 sec/batch)\n",
      "Train error:\n",
      " accuracy = 88.02\n",
      " avg loss = 0.42\n",
      "\n",
      "Validation error:\n",
      " accuracy = 72.10\n",
      " avg loss = 0.87\n",
      "\n",
      "Epoch time: 9.320418119430542\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "Total train time: 391.99248576164246\n"
     ]
    }
   ],
   "source": [
    "session = tf.Session()\n",
    "train_start = time.time()\n",
    "\n",
    "plot_data={}\n",
    "train_cifar(session, train_x, train_y, valid_x, valid_y, config, plot_data)\n",
    "\n",
    "print(\"Total train time:\", time.time() - train_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7YAAAHiCAYAAADcelBQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3Xl8XGXVwPHfmSUzSZqkadKka7Yu6U5bSkvZLIIKyK4C\n0rJLUVTA7RWpL+JSRWVVtrcoKFpAZBFUkEVaQEiRFkopJOmSNmszTZu1TTLJZJ73jztJJ8lM9mSS\n5nw/n37amfvMvWdubvrcc59NjDEopZRSSimllFIjlS3SASillFJKKaWUUv2hia1SSimllFJKqRFN\nE1ullFJKKaWUUiOaJrZKKaWUUkoppUY0TWyVUkoppZRSSo1omtgqpZRSSimllBrRNLFVSg0KEblN\nRP4c6TiUUkqpwSQiK0SkJNJxKDXaaWKrRjURuVRENovIIRHZJyIvichJkY6rL0QkQ0SMiDgiHYtS\nSiklIhtFpEpEXJGORSl19NPEVo1aIvJt4B7g50AqkAbcD5wbpvyITxiPhu+glFJq+BORDOBkwBCm\nXh2AY2idFgEiYo90DEqFoomtGpVEJAH4CfB1Y8yzxpjDxphmY8w/jDH/Eyhzm4g8LSJ/FpFa4EoR\ncYnIPSJSFvhzT+uTaBFJFpF/iEi1iFSKyFsiYgts+76IlIpInYjki8hpXcR2vIi8E9jPhyKyImjb\nRhH5qYi8HdjXKyKSHNj8ZuDv6kAL9HIRuTJQ9m4ROQjcJiI2EfmhiBSKyH4ReSxwPoJbfVcHvt8+\nEfluYNsEEakXkaSgeBaLSIWIOHtwzs8VkY8D32ujiMwO2hby/IjI0kCLeq2IeETkrm5/uEoppYaD\ny4FNwB+AK1rfFJFlIlIenByJyAUisi3wb5uI3Cwiu0XkoIg8JSLjAtta66hrRKQIeD3w/l8D+6wR\nkTdFZG7QvpNE5O+BeuQ9EfmZiPwnaPssEXk1UG/ni8hF4b6QiIwTkUcD9WOViPwtTLnW+OtE5BMR\nuSBo23QReSMQ6wER+UvgfQnU1fsDsX4kIvPC7P8qEckN7L9ARK7rsP08Edka2M9uETmjq/gD9wr/\n6bAPIyLTA//+g4g8KCIvishh4FQR+byIfBA4RrGI3Nbh8yfJkXuZ4sAxjgvU5cE/+wtF5MNw51yp\n3tDEVo1WywE38Fw35c4DngbGAuuBNcDxwELgGGAp8MNA2e8AJcB4rBbgWwAjItnAN4DjjDFxwOeA\nvaEOJiKTgX8CPwPGAd8FnhGR8UHFLgWuAlKAqEAZgFMCf481xowxxuQEXi8DCgIxrQWuDPw5FcgC\nxgD3dQjlVGAG8Fng+yJyujGmHNgIBFf6lwFPGmOaQ32foO81E3gCuClwfl4E/i4iUd2cn3uBe40x\n8cA04KmujqOUUmrYuByr3lwPfE5EUgGMMe8Ch4FPB5W9FHg88O9vAucDnwImAVVYvamCfQqYjVVf\nALyEVWelAO8Hjtnq/sDxJmAl2MFJdizwauDYKcAlwAMiMifMd/oTEAPMDZS/O0y53Vit1QnAj4E/\ni8jEwLafAq8AicAU4LeB9z+LVY/PDHzuIuBgmP3vB84G4rHuB+4WkcWB77QUeAz4Hta9yykcqVN7\nGn8ol2LdQ8QB/8E6p5cHjvF54Gsicn4ghnSsn8lvser8hcBWY8x7ge/02aD9XhaIV6l+08RWjVZJ\nwAFjjK+bcjnGmL8ZY/zGmAZgJfATY8x+Y0wFVoV1WaBsMzARSA+0/r5ljDFAC+AC5oiI0xiz1xiz\nO8zxVgEvGmNeDBzzVWAzcFZQmUeNMTsC8TyFVWF0pcwY81tjjC/oO9xljCkwxhwCfgBcIu27dP04\n0Ir9EfAo8OXA+38MxNjaFenLWBVldy4G/mmMeTWQBN8BRAMn0PX5aQami0iyMeaQMWZTD46llFIq\ngsSaqyIdeMoYswUr0bs0qMgTBOoVEYnDquOeCGz7KrDGGFNijPECtwFf7FBH3RaooxoAjDGPGGPq\ngsofIyIJgXrqC8CPjDH1xphPsOqxVmcDe40xjwbqyA+AZ4AvhfhOE4Ezga8aY6oC9fwbob6/Meav\nxpiyQD3+F2An1oNwsOq1dGCSMabRGPOfoPfjgFmAGGNyjTH7wuz/n8aY3cbyBlaifHJg8zXAI4H6\n1m+MKTXG5PUm/jCeN8a8HdhnozFmozHmo8DrbVg/v08Fyl4KvGaMeSJwnIPGmK2BbcH3EeOwHk48\n3vFgSvWFJrZqtDoIJEv343OKO7yeBBQGvS4MvAfwa2AX8Eqga9DNAMaYXVgtlbcB+0XkSRGZBCBW\nl+HWP2lYld2XAl13qkWkGjgJK2FuVR7073qsFtf+fgcHVotuqM8Ef8fnsRLQTOAzQI0x5r/dHL/T\nMY0x/sAxJnd1frAq6JlAXqAL2dk9OJZSSqnIugJ4xRhzIPD6cYJaSgOvLxRrKM+FwPvGmNY6Ih14\nLqgOzMV6ABqyjhIRu4jcHuhyW8uR1slkrNZCB+3rtOB/pwPLOtS5K7FadzuaClQaY6q6+/Iicnmg\nK3DrPucF4gH4H0CA/4o1POdqAGPM61i9p+7HqgvXiUh8mP2fKSKbxOo+XY31YKB1/1OxHiT0Of4w\n2t1LiNWlfINYw5FqsB5IdBcDwJ+BcwKt5RcBb4VL4JXqLU1s1WiVA3ixujt1xXR4XYZVEbZKC7xH\n4Gnxd4wxWVgTZXxbAmNFjTGPG2Nan2Ab4JeB98cE/SnCqjj+ZIwZG/Qn1hhzew++U8dYe/MdfIAn\n6L2pYb5jI1Yr8SqsluqetNZ2OqaISOAYpYH9hjs/O40xX8bqMvVL4OlAZaiUUmoYEpForITlU2KN\ney0HvoXVinoMQKDltBCrBTG4GzJY9eCZHepBtzGmNKhMcL12KdawodOxuvBmtIYCVGDVb1OCygfX\nb8XAGx2ONcYY87UQX60YGCciY7v5/unAw1hDbJKMMWOB7YF4MMaUG2OuNcZMAq7D6vo8PbDtN8aY\nY4E5WA91vxdi/y6sVuU7gNTA/l9s3X8gzmm9jP8wVhfl1mOESuw73ks8DrwATDXGJAAP9SAGAj/H\nHKwHGr25j1CqW5rYqlHJGFMD3ArcLyLni0iMiDgDT0F/1cVHnwB+KCLjxZq06Vasp4+IyNliTQoh\nQA3WE2a/iGSLyKcDlVEj0AD4w+y/9Unm5wJPod1irY83JUz5YBWB/WZ1U+4J4FsikikiY7Bmhf6L\nad8t+38D52Qu1vidvwRtewxrjO659LxCegr4vIicJtZEU9/BerDwTlfnR0RWicj4QAtvdWBf4c6d\nUkqpyDsfq/6bgzVUZiHWeNi3sMZktnocuBFrDOhfg95/CFgbSBAJ1LfndXG8OKz65CBWcvbz1g3G\nmBbgWayJE2NEZFaHGP4BzBSRywL3AE6xJjiaTQeBVsWXsBLRxEDZUzqWA2KxksCKQPxXYbXYEnj9\npaA6vSpQ1h847rJAHXkYqz4MVd9FYQ3fqQB8InIm7ces/h64KlDf2kRksojM6ib+D4G5IrJQRNxY\nPai6E4fVAtwo1rje4K7m64HTReQiEXGINYFX8LCpx7Barudj/XyUGhCa2KpRyxhzJ/BtrMmfKrCe\nMH4DCDnLYcDPsMa8bgM+wpqk4meBbTOA14BDWE8jHzDGbMCqgG4HDmB1I07BGtcaKqZirCfPtwTF\n9D168LtqjKnHmtjh7UD3p+PDFH0EKyF9E9iDVXl+s0OZN7C6Vf8buMMY80rQcd7GqmyDu451F1s+\nVivvb7HOwznAOcaYJro+P2cAH4vIIayJpC5pHVOllFJqWLoCay6IokDrZLmxJh+8D1gZNASodUzm\n60FdlsH6v/4FrGE9dVgzKy/r4niPYbX+lgKfBMoH+wZWS245Vt33BFYijDGmDispvASrZ1E5Vu+g\ncOvuXoY1FjYPawKnmzoWCLRG34l1H+DBSt7eDipyHPBuoF57AbjRGFOANRHUw1jJbiFWov7rEPuv\nA27AemBchZVQvhC0/b8EJpTCesj+Bkd6TIWM3xizA2uliNewxgO3myE5jOuBnwR+RrcSNLljoAfa\nWVgPsSuBrVgTbrZ6LhDTc4F7F6UGhBgTrveiUmq0EWvdwT2A03QxsZaIvA48boz53RCFppRSSvWb\niPwSmGCMuaLbwmrQiMhu4DpjzGuRjkUdPbTFVinVKyJyHLCY9t2TlVJKqWFHrHVqF4hlKdakhN0t\n9acGkYh8AasL9uuRjkUdXbqbEVYppdqIyB+xxk/dGOgOpZRSSg1ncVjdjydhdQ2+E2uGfxUBIrIR\na/z1ZYH5M5QaMNoVWSmllFJKKaXUiKZdkZVSSimllFJKjWia2CqllFJKKaWUGtFG1Bjb5ORkk5GR\nEekwlFJKHSW2bNlywBgzPtJxjGRaNyullBpIfa2bR1Rim5GRwebNmyMdhlJKqaOEiPRoLWYVntbN\nSimlBlJf62btiqyUUkoppZRSakTTxFYppZRSSiml1Iimia1SSimllFJKqRFtRI2xVUqp0aC5uZmS\nkhIaGxsjHcpRw+12M2XKFJxOZ6RDGRX0Gh5Yev0qpVT3RlViu97jYU1BAUVeL2kuF2uzsliZmhrp\nsJRSqp2SkhLi4uLIyMhARCIdzohnjOHgwYOUlJSQmZkZ6XBGBb2GB45ev0qpgeZZ76FgTQHeIi+u\nNBdZa7NIXdk5J+pJuZ7uayiMmq7I6z0eVufnU+j1YoBCr5fV+fms93giHZpSSrXT2NhIUlKSJgQD\nRERISkrS1sMhpNfwwNHrV6mh51nvIScjh422jeRk5OBZ3zlf6EmZgdzXQB3Ps95D/up8vIVeMOAt\n9JK/Or9dOWMM+/64j/xrO5crX1/eq30NpVHTYrumoIB6v7/de/V+P2sKCrTVVik17GhCMLD0fA49\nPecDR8+lGs2GunWxNVnz11t5Q2uyBrSVDVfG+A1JZyXRXNmMr8rH/qf3U3pvKabJtJXLuzqPmndr\nGHfaOMQpVL9VTcndJRhvUJlr8jicf5hxnxkHfjj4ykFK7uxcpmZTDfHL4jFNBn+Tn5p3aqh4sgLT\nHFTuyjxKHy7FPdmNv8FP5UuV+Bvb50T+ej+5l+ey4+s78Df6247Tkb/eT96qPPKvzsfmstFyuAX8\nncsUrCmISKvtqElsi7zeXr2vlFKjWXV1NY8//jjXX399rz531lln8fjjjzN27NhBikyp7un1q1Rk\nDVQy2pMkM2y5a/PxHfYx/sLx4If9f91PwfcK8DccKZN3dR6Vr1biTnPTtL+JZk8zB1862Cmx89f7\nyb0ilz0/2oPNaaNhd0Nb8hhcJu/yvG7PjWkylP22jLLfloUv4zUU/bSIop8WdVmm7L4yyu4Lvx8A\n4zPUvlFLLbVdB+aHlpqWrsu07rPJ0NIUvqy3KDL51ahJbNNcLgpDJLFpLlcEolFKqYEzGONbqqur\neeCBBzolBj6fD4cjfNXx4osv9uu4anQa6GtYr1+lem+4JKPefV7ij4unydPEzm/ubNveyl/vJ++q\nPArXFtJS34K/wU9zRTN0aGT0N/jZed1Odl63M+x3Nk0Gzx972G22BRp3dz8kwDHWgWOcA+c4J3Wb\n68KWSzo3CdNkqPxXZdgyCSclgA1q3qwJWyZlZQq2KBsSJez7v31hy8360yzs0XZ2fG2Hdb46iJoS\nxXEfHYfNZcPmsrEpa5PVxbgDV5qLpXlL8Xv9bJ6/GW9J6DKRMGoS27VZWVz7cR4NtiNXvdsvrM3K\nimBUSinVPz29geitm2++md27d7Nw4UKcTidut5vExETy8vLYsWMH559/PsXFxTQ2NnLjjTeyevVq\nADIyMti8eTOHDh3izDPP5KSTTuKdd95h8uTJPP/880RHR/f/S6ujymBcw3r9KtVef5NRv89PS00L\n5Y+VU3BLAaaxfdfaqjeriF8cj7/Rz97b9oZMRvOvy+fACwfwe62urlUbqjq3jDb4KfheQbffxzQb\n6nPre/TdHeMciE1oPtA5mWuVfms6UalROFOc7PzGTpo9IRK/SVEs3LAQ02z48DMf0rSvqVMZV5qL\n5YXL217nZOSETg7TXcx/fn63ZRa9tajbMnP+PKftdeW/KsOWm7BqAgD+Rn+7nzWALcbGtNun4Rx7\nZOb1rLVZIctl/TwLe7Qde7SdrNvDlFkbmfxq1CS2p78G31lvePhy8KQCAp9+xXD6PmBlpKNTSqnQ\nNsrGXn/GX+8nd1Uuuatyw5ZZYVZ0uY/bb7+d7du3s3XrVjZu3MjnP/95tm/f3jYr6yOPPMK4ceNo\naGjguOOO4wtf+AJJSUnt9rFz506eeOIJHn74YS666CKeeeYZVq1a1evvo0a2SFzDev2q0aI/Y0aN\n3zB2xVga9zay88bQLaO5l+d2Slw6Mk2G8nXllFMetgyA/7CfiqcqevS94k+MJ2pCFFUvV9FyqHOX\nV+cEJwtfW4gtxoYt2sb7S9/HWxw6oVu+10o0u0oOM398ZMZx4zWhE79fTSNmZgwA0349LWzSFyxs\nchiU+A1UmZ6Wa70+urtuelKup/saKqMmsS1YU8BphXDaS/DCOXD3t+Gwm4gNblZKqZFk6dKl7ZYa\n+c1vfsNzzz0HQHFxMTt37uyUGGRmZrJw4UIAjj32WPbu3Ttk8SoVTK9fNRL1qJX12vxOY0arNlYR\ntzgO02xNKFS4tjB0d94ejAfFb5XFBo54B75qX9iiE6+diM1to/yP5bTUdk5GHUkOZtw3o62ra95V\neTTv79wy6kp3sfg/i498xxCJ2vQ7phM7N7btvaxfDFxyOJAJ3UDtayCP11quJ/lPT8r1dF9DYdQk\ntsGDmJdstv5+fzHU/0wnj1JKDV/dtax29QS69Sn1QIiNPXIDsXHjRl577TVycnKIiYlhxYoVIZci\ncQXNYWC322loaBiweNTIMRyuYb1+1UgTqpU17+o8Kv5WgT3WTsOuBmpzajvNSGuaDOW/6771tFXU\nhCjcGW4ObTsUslU2anIUS/OWYo+1IyJd/r5mr8sGIH5ZfMgEcsa9M0i95EgCNP2u6cO2dXEgE7qB\n2tdAHu9oNWoSW1eaq+0XcdI+mFgG+ybB3pOd3XxSKaWGr54+ge6tuLg46upCT3pRU1NDYmIiMTEx\n5OXlsWnTpn4dS41ug3EN6/WrRjK/z8+u7+zqlGiaJsOBpw/0aB8Tr5t4ZEKhh/eFbEF1TXWxvMh6\neBSuZXTaL6fhGHMkXRjqrq6t5Y7G1kU18EZNYtvxF/HYLfCPSbD7Jp3SXyk1cg3W+JakpCROPPFE\n5s2bR3R0NKlB632fccYZPPTQQ8yePZvs7GyOP/74fh1LjW6DcQ3r9asipa8zC6ffmo4zwcmB5w9w\n8J8H8VWG7/I78/9mEj09mtzLc2kqDTGBUbqL7Iey217HLYoLnYz+YmQko0r1lBgTegHe4WjJkiVm\n8+bNff68Z72HglusX8Q3ToHbfgwrxo5lQ2AMjVJKDQe5ubnMnj070mEcdUKdVxHZYoxZEqGQjgqh\n6ma9hgeentPhL1yrZ/a67HbJW/n6cnas3tHlhEziEIyv8z16cBf9nh6vtexwmeBHqe70tW4eNS22\ncOSp0LYzt7Ho7UrEwNs1NRxuaSHWbo90eEoppZRSapgKlRyOv3g83iIvDQUN7LwhzMzCV+ay+/u7\n8Tf42/6EIlFC5k8ySToviUNbDg1Yl9/WsprIqqPdqEpsW8UtiyP+X5XMr3ayLbGZN6urObPDbIhK\nKaWUUkpBoJX1KzvwNx6ZzCn3slxyL8uF7jo/+gjZZbgj02xI+34aALGzrAnPBqrLr1KjwahMbOOX\nxQOw5ENh2wp4tapKE1ullFJKqVEoXDddX52PqteqqHyxkn2P7oOO8y8FElrXFBfuLDd1W+rwHw4x\ns/CkKBZvWowt2obNbeO9ue+1W62jlSvN1e61Jq1K9c7oTGyXWontghebYYWV2CqllFJKqaNLj9aC\n7biszlV57F27l8ZdjZjmbppjBZYXdzOz8K+m4Z7qbnsv6+eDM5u9UqOdLdIBRIIzyUn0jGhmbzXE\nYGP74cPs8+p6tkoppUYPEfmWiHwsIttF5AkRcYvIbSJSKiJbA3/OinScSvVVa6LpLfSCsZLW/NX5\nlD5YSu3mWiqerWDnNzuPizXNhobcBkyLIf7EeDLXZhI1MSrkMYJbWVNXppK9LhtXugvkyNquoboP\n96ScUqp3BqXFVkQeAc4G9htj5oXYvhL4PiBAHfA1Y8yHgxFLOPHL4mnY2cCyOjcb4up5raqKyyZM\nGMoQlFJKqYgQkcnADcAcY0yDiDwFXBLYfLcx5o7IRafUwCj4QUHIyZx2Xr+z+w8LnFhxIs5xTgDc\n6e4etbLqMjdKRc5gtdj+ATiji+17gE8ZY+YDPwXWDVIcYcUtiwNg6SfWKdDuyEop1XdjxowBoKys\njC9+8Yshy6xYsYLulmy75557qK+vb3t91llnUV1dPXCBqmAOIFpEHEAMUBbheCJGr9+Rx7PeQ05G\nDhttG8nJyMGz3oMxhvr8eorvKmbraVvxFofvjRd7TCxJ5yRhHxN6VQxXmqstqQVtZVVqJBiUFltj\nzJsiktHF9neCXm4CpgxGHF1pnUBqwcvNsAxeq6rCGIOIDHUoSinVL+s9HtYUFFDk9ZLmcrE2K4uV\nqZG52Zo0aRJPP/10nz9/zz33sGrVKmJiYgB48cUXByo0FcQYUyoidwBFQAPwijHmFRE5AfimiFwO\nbAa+Y4wZ9Ce/w+Ua1ut3ZAg5LvbKPHZ+eye+/b5uP+9Kd3Hc1uNC7gvCj3fVVlalhrfhMMb2GuCl\noT7omGPGIC5h4gYvEx1O9jU18fHhw0MdhlJK9ct6j4fV+fkUer0YoNDrZXV+Pus9nn7t9+abb+b+\n++9ve33bbbfxs5/9jNNOO43Fixczf/58nn/++U6f27t3L/PmWSNQGhoauOSSS5g9ezYXXHABDQ0N\nbeW+9rWvsWTJEubOncuPfvQjAH7zm99QVlbGqaeeyqmnngpARkYGBw4cAOCuu+5i3rx5zJs3j3vu\nuafteLNnz+baa69l7ty5fPazn213HBWaiCQC5wGZwCQgVkRWAQ8CWcBCYB9wZ5jPrxaRzSKyuaKi\nol+xDMY1rNfv0W33/+zuPC7WZ/Dt9+EY5yBlZQqz189mxkMzsMW0v9UNtRastsQqdXSI6KzIInIq\nVmJ7UhdlVgOrAdLS0gbs2LYoG3GL4qjdVMvJ3lieslfzalUV8wLdkZRSajiQjRt7/Zl6v59Vubms\nys0NW8asWNHlPi6++GJuuukmvv71rwPw1FNP8fLLL3PDDTcQHx/PgQMHOP744zn33HPD9nR58MEH\niYmJITc3l23btrF48eK2bWvXrmXcuHG0tLRw2mmnsW3bNm644QbuuusuNmzYQHJycrt9bdmyhUcf\nfZR3330XYwzLli3jU5/6FImJiezcuZMnnniChx9+mIsuuohnnnmGVatW9fBsjVqnA3uMMRUAIvIs\ncIIx5s+tBUTkYeAfoT5sjFlHYBjRkiVLupw2NhLXsF6/I1vHmYzT/zedqPFRVP6rksqXK2kqC7Mm\nrMCJ+09E7Ed+po4xDl0LVqlRImIttiKyAPgdcJ4x5mC4csaYdcaYJcaYJePHjx/QGFrH2S7bZeX3\nOs5WKaUsixYtYv/+/ZSVlfHhhx+SmJjIhAkTuOWWW1iwYAGnn346paWleLpoVXvzzTfbbtAXLFjA\nggUL2rY99dRTLF68mEWLFvHxxx/zySefdBnPf/7zHy644AJiY2MZM2YMF154IW+99RYAmZmZLFy4\nEIBjjz2WvXv39vPbjwpFwPEiEiNWZncakCsiE4PKXABsj0h0/aTX78gVaibjHV/ZwfbztlP2YBmN\nBY1h715daa52SS1YSevyvctZ4V/B8r3LNYFV6igWkRZbEUkDngUuM8bsiEQMYI2zLaWUY173QTa8\nUV2N1+/HZRsOPbSVUqr7ltWMnBwKQyxXlu5ysXf58n4d+0tf+hJPP/005eXlXHzxxaxfv56Kigq2\nbNmC0+kkIyODxsbGXu93z5493HHHHbz33nskJiZy5ZVX9mk/rVyuI8tt2O127crZA8aYd0XkaeB9\nwAd8gNUC+zsRWQgYYC9wXb+PFaFrWK/f4SfcmrLGbzi07RDVG6rZs2YP/gZ/p89KlJB+SzrjzhhH\n/Y56dnx1h64Dq5RqZ1AyOBF5AsgBskWkRESuEZGvishXA0VuBZKABwLr5HU9zeAgaZ1Ayv36YebF\nxlLv95NTUxOJUJRSqk/WZmUR0+FhXIzNxtqs/t/gXXzxxTz55JM8/fTTfOlLX6KmpoaUlBScTicb\nNmygsLCwy8+fcsopPP744wBs376dbdu2AVBbW0tsbCwJCQl4PB5eeunINAtxcXHU1dV12tfJJ5/M\n3/72N+rr6zl8+DDPPfccJ598cr+/42hmjPmRMWaWMWaeMeYyY4w38Pd8Y8wCY8y5xph9gx3HYF3D\nev0OL6FaYvOuyuO9Y9/j7eS32bJoC7u/vTtkUgvW2rIZP8ogflk8Ey6boONilVKdDNasyF/uZvtX\ngK8MxrF7w53pxpnspPlAM6faktjOYV6tqmJFYmKkQ1NKqR5pnTl2MGaUnTt3LnV1dUyePJmJEyey\ncuVKzjnnHObPn8+SJUuYNWtWl5//2te+xlVXXcXs2bOZPXs2xx57LADHHHMMixYtYtasWUydOpUT\nTzyx7TOrV6/mjDPOYNKkSWzYsKHt/cWLF3PllVeydOlSAL7yla+waNGiUd9t82gwWNewXr/DS8Et\nndeUNc2Gw+9bE3e6M9yMXTGWA38/gO9g55mNXWmudq91XKxSqiMxpss5H4aVJUuWmO7WkOutbWdv\no/KflZQ9M5WV44pZGhfHu4HKSymlIiE3N5fZs2dHOoyjTqjzKiJbjDFLIhTSUSFU3azX8MAbqefU\nV+PD87iHndfvDFtm2Z5lRGdEA+GX39EWWaVGj77WzRGdFXk4iF8WT+U/K5n7to+oc4XNdXVUNTeT\n6HR2/2GllFJKKdV+/OxUFxOunoC3yMv+J/d3aqkN5kp3tSW1QFvy2t1Mxkop1ZEmtoFxti3vHOaE\nyxLYWF0w8gE2AAAgAElEQVTN69XVfGGAZ2BWSimllDoadWxl9RZ5KbztyBjmsSvGEjM3hvJHy3s0\n4ZN2M1ZK9cWon/43bqm15E/dB3WcHj8WgFcrKyMZklJKKaXUiLH7+7tDtsra4+0szV/Kwg0LmXnf\nTJ3wSSk1qEZ9i61zrJPo7Gga8hs4Yb81MYGuZ6uUijRjDNbyomogjKT5JI4Weg0PnOF6/dbn11P0\n6yKaSptCbm+payFmZkzba22JVUoNplGf2ILVHbkhv4GszT4SFzooaGykoKGBrOjo7j+slFIDzO12\nc/DgQZKSkjQxGADGGA4ePIjb7Y50KKOGXsMDJ9LXb6i1Z6NnRlP0yyIOPHvAWvE4jI4zGSul1GDS\nxBYrsfU85qH+3UOcdmoiT1dU8GpVFddpYquUioApU6ZQUlJCRUVFpEM5arjdbqZMmRLpMEYNvYYH\nVqSu305jZwu95F6eC4FexxIlTLhiAtHZ0ey9dW+Pxs8qpdRg0cSWIxNI1b5by2cSp1qJbWUl102a\nFOHIlFKjkdPpJDMzM9JhKNVneg0fHQrWdF57Fj8gMPW7U5ly0xRck6xWWdcEl85krJSKKE1sgdgF\nsdjcNhp2NrDCZk0m9Xp1NS3GYNcuVEoppZQahbxF3rDbpv1qWrvXOn5WKRVpo35WZACb08aYxWMA\nSNraxHiHgyqfD+cbb5CRk8N6jyfCESqllFJKDZ1DHx5CHKEf7uvYWaXUcKSJbUBrd+Q/FeyjyucD\nrPkQCr1eVufna3KrlFJKqaOeaTEU/bKILcdtwTQb6JDb6thZpdRwpYltQGti+6vJlfg6bKv3+1lT\nUDD0QSmllFJKDZGGgga2rthKwc0FmGbDpK9OIvt3uvasUmpk0DG2AXHLrLG15XGdFxgHKPKGH2ei\nlFJKKTWchFqmp2NCGlzGkeig5XALxmuImhBF9iPZJJ2ZBMDEqydG4isopVSvaGIb4E5340xxkrK/\nGc+EztvTXDqeRCmllFLDX6hlevJX52MwpHwpBdNsKH+8nN037sbfYJXxVVr91eKWxrHgxQU4k5wR\ni18ppfpCE9sAESF+WTxf+d1B7rpFaLAdWXHcBqzVZQuUUkopNQKEWqbHX+8nb1UeeavyuvxsU3mT\nJrVKqRFJx9gGiV8Wz+n/hp9uTiDd5UKwTpAf8BrTzaeVUkoppSLLtBi8hV0Mn7KDLTr87Z+3WIde\nKaVGJk1sg7SOs13xTAt7ly/Hv2IFj82eDcC3du2iuLExkuEppZRSSoXlq/Hx0dkfhd3uSnOxwreC\nU+pPsSaEClNGKaVGIk1sg8QfFw8Ch7Yewu+1uvBcmpLCeUlJ1La0cG1+PkZbbpVSSik1zNTvrGfL\nsi1U/qsS2xgb4mq/To8txkbWz48s05O1NgtbjK1zGV3KRyk1QmliG8SR4CBmVgymyXBo6yHAGnv7\n0MyZjHM4eLmqit/v2xfhKJVSSqn+E5FvicjHIrJdRJ4QEbeIjBORV0VkZ+DvxEjHqbpX+Wol7y99\nn4b8BmLnx3LcR8cx6/ezulymJ3VlKtnrdCkfpdTRQyeP6iB+WTz1ufXUvlvbtrbtBJeL+2bM4NLc\nXL69ezefHTeONLc7wpEqpZRSfSMik4EbgDnGmAYReQq4BJgD/NsYc7uI3AzcDHw/gqGqLhhjKL2v\nlF3f2gUtkHReErP/NBtHnIPojOhuk9TUlamayCqljhraYttB6zjb2ndr271/SUoKFyQnU9fSwle0\nS7JSSqmRzwFEi4gDiAHKgPOAPwa2/xE4P0KxqTA86z3kZOSw0baRt+LfYtcNVlKbdksa856dhyNO\n2yyUUqOTJrYdtLbSdkxsRYQHZ84kyeHg1aoqHtYuyUoppUYoY0wpcAdQBOwDaowxrwCpxpjWCq4c\n0Oa8YaR1fVpvoRcM+A9Z84FMun4SWWuzEJt0swellDp6aWLbQez8WGzRNhp3N9J0oKndttSoKO6b\nMQOA7+zeTaHOkqyUUmoECoydPQ/IBCYBsSKyKriMsbomheyeJCKrRWSziGyuqKgY9HiVpeCWzuvT\nAhz858EIRKOUUsOLJrYd2Bw2XFOtqe7fSXmHnIwcPOs9bdsvTknhwuRkDrW0MOvdd7Ft3EhGTg7r\nPZ6Q+1vv8ZCRk9NtOaWUUmoInQ7sMcZUGGOagWeBEwCPiEwECPy9P9SHjTHrjDFLjDFLxo8fP2RB\nj2Y1OTV4i0KvMRvufaWUGk00se3As95DY0GgJdaAt9BL/ur8tuRWRDg90ZokstEYDFDo9bI6P79T\n0rre42F1fj6FXm+X5ZRSSqkhVgQcLyIxIiLAaUAu8AJwRaDMFcDzEYpPBTSWNPLJqk/44IQPwpbR\ntWeVUkpnRe6kYE0Bxte+55W/3k/BmoK2mQN/WVTU6XP1fj/X5OXxWHk5LpsNl83GiwcPUu/3dyq3\npqCAlak6bEkppVRkGGPeFZGngfcBH/ABsA4YAzwlItcAhcBFkYty9PGs91CwpgBvkRfXFBdxx8VR\n+a9K/PV+xCUknpFI9SvV+BuO3Fvo2rNKKWXRxLaDnnTzKfKGKWMMr1RVdXuMcJ9XSimlhoox5kfA\njzq87cVqvVVDrHViqNYxtN5iL95i635h/BfHk/WrLKIzo9snv2kustZm6ZI9SimFJraduNJc1myD\nId5vleZyURgiOU1xOvnjrFl4jcHr93P9jh0c9Pk6lUtzaZchpZRSSh1RsCb0xFDOVCdz/zq37bWu\nPauUUqHpGNsOstZmYYtpf1o6dvNZm5VFjK19mRibjbumT+eMpCTOS07mopQU7p0xo1O5aJuNtVna\nZUgppZRSR4TrMda8v3mII1FKqZFJE9sOUlemkr0uu10L7YQrJrR7OroyNZV12dmku1wIkO5ysS47\nu9O42eByrU5NSNDxtUoppZQCwBhD8d3FYRZW0omhlFKqp7Qrcgit3XzKHi5jx+od1G2uwxiDNXGk\nZWVqao8S1NZyH9TVsXjLFv5dXU1xYyNT3e7B/ApKKaWUGuZaGlrYcd0OPH8KrLzgkHYTWOrEUEop\n1XPaYtuF1JWpOMY5qHuvjtpNtf3a16K4OC5JScFrDLft3TswASqllFJqRGosaWTrKVvx/MmDLcbG\nnKfmMOsPs3Clu0DAle4ie122jqdVSqke0hbbLthj7ExaPYmi24soubeEhOUJ/drfTzMyeLqigj+U\nl/OdqVOZExs7QJEqpZRSaqSoeaeG7Rdup9nTjDvDzby/zWPMMWMANJFVSqk+GpQWWxF5RET2i8j2\nMNtFRH4jIrtEZJuILB6MOAbCpOsngR0qnq6gsaSxX/uaHhPDtRMn4gd+uGfPwASolFJKqWHNs95D\nTkYOG20b+U/Sf/jg5A9o9jQz9tSxLH5vcVtSq5RSqu8GqyvyH4Azuth+JjAj8Gc18OAgxdFv7qlu\nxn9hPLRA2QNl/d7f/6anE2Oz8dyBA2yqqRmACJVSSik1XLWuT+st9IIBX6UP/DD2c2NZ8PICopKj\nIh2iUkodFQYlsTXGvAlUdlHkPOAxY9kEjBWRiYMRy0CYcuMUAMrWldHS0NKvfU10ubhpirW/mwsK\nMCbMNIhKKaWUGvHCrU/bkNeAzalTnSil1ECJ1P+ok4HioNclgfc6EZHVIrJZRDZXVFQMSXAdxS+P\nJ25JHL6DPjzrPf3e3/emTiXR4eCNmhperuwq/1dKKaXUSBZufdpw7yullOqbYf+o0BizzhizxBiz\nZPz48RGJQUSYfKOVd5fcU9LvVtaxTie3pKUB8IM9e/Brq61SSil1VPEd8pF3dZ6uT6uUUkMkUolt\nKTA16PWUwHvDVspFKURNiKL+43qqX6/u9/6+PnkyU1wuth46xF/27x+ACJVSSik1HNS+V8uWRVso\nf7QcHCBR0m67rk+rlFIDL1KJ7QvA5YHZkY8Haowx+yIUS4/YomzWDMlAyb0l/d5ftN3ObRkZgDVD\ncpO/8/gbpZRSSo0cpsVQeHshH5zwAQ27GohdEMtxHx7HrEd0fVqllBpsg7KOrYg8AawAkkWkBPgR\n4AQwxjwEvAicBewC6oGrBiOOgTbpukkU/qyQg/84SMPuBqKnRfdrf1ekpnJHcTF59fX8bt8+rp8c\ncpixUkoppYYpz3oPBWsK8BZ5kSjBeK2+x1NumkLmLzKxu+3EzonVRFYppQbZYM2K/GVjzERjjNMY\nM8UY83tjzEOBpJbAbMhfN8ZMM8bMN8ZsHow4BlpUShSpl6aCgZLf9r/V1mGzsTYzE4Bv7tyJbeNG\nMnJyWO/p/wRVSimllBpcHZfyaUtqvzeF6XdPx+62RzhCpZQaPYb95FHDTeskUuWPlOOr9fV7fw0t\nLdgAP9b8EoVeL6vz8zW5VUoppYa5cEv5VDwVmVUclFJqNNPEtpfiFsaRcEoCLXUtlP+hvN/7W7Nn\nDx2rxHq/nzUFBf3et1JKKaUGjy7lo5RSw4cmtn0w5cYpAJT+thTj799SPUXe0JVfuPeVUkopFXn+\nJj/ilJDbdCkfpZQaeprY9kHyecnYk+w07GrgDccb5GTk4Fnft67Daa7QlV+y09mfEJVSSik1iPb8\ncA+myUCH3FaX8lFKqcjQxLYP9j+5H39toAOxAW+hl/zV+X1KbtdmZRFj6/xjqGhu5v7SYb20r1JK\nqRFKRLJFZGvQn1oRuUlEbhOR0qD3z4p0rMNR5SuVFP+6GOyQfmu6LuWjlFLDwKAs93O0K1hTgGlu\n3wXZX++nYE1BryuzlalW+TUFBRR5vUx1uVgSF8ezBw7wjZ07KW5s5OdZWdgkdHcnpZRSqreMMfnA\nQgARsQOlwHNYy+/dbYy5I4LhDWtNniZyL88FIOO2DDJ+mEHmbZkRjkoppZQmtn0w0JNFrExNbUtw\nWz26bx+rd+zgl8XFFHm9PDprFq4QLbtKKaVUP50G7DbGFIo+RO2S8Rtyr8il2dPM2BVjSf9BeqRD\nUkopFaCZUh+EmxTCkTRwzwmumjiRf8yfzxi7nSf27+eMbdt4uLSUjJwcXe9WKaXUQLoEeCLo9TdF\nZJuIPCIiiaE+ICKrRWSziGyuqBg9S9uU3F1C1ctVOJIczP7zbMSuDwKUUmq40MS2D7LWZmGL6Xzq\nfAd87Htk34Ad53PjxvHmwoVMiIpiY3U11+3cSaHXq+vdKqWUGhAiEgWcC/w18NaDQBZWN+V9wJ2h\nPmeMWWeMWWKMWTJ+/PghiTXSajfXUvADaym+WY/OwjVZZz5WSqnhRBPbPkhdmUr2uuwjk0WkuUj+\nQjIA+dfkU3xP8YAda1FcHJsWL8YhQseFher9fm7psN7teo9HW3WVUkr11JnA+8YYD4AxxmOMaTHG\n+IGHgaURjW6Y8NX5+OSSTzDNhsnfnEzyOcmRDkkppVQHOsa2j1JXpnaaKKrk3hJ23bSL3d/aTUtN\nC+m3pjMQ45XS3W5aTOj1cou8XjI3bSLL7cZvDG/X1tIcKNvaqgt0GsOrlFJKAV8mqBuyiEw0xrR2\nPboA2B6RqIaZnV/fSePuRmKPiSXrV7qUj1JKDUea2A6gKTdOwR5nJ//afPbethdfrY9pd0wbkOQ2\nzeWi0Bt6cqq9jY3sbWwMua3e7+cHBQWa2CqllGpHRGKBzwDXBb39KxFZCBhgb4dto4pnvYeCNQV4\nCwN1rxPmPDkHu9se2cCUUkqFpF2RB9jEqycy58k5iFMouauErZ/eSk56DhttG8nJyOnTWrcQer3b\nGJuNP2Rns2PpUv61YEHYzxZ7vVyTl0dOTQ0m0JqrXZaVUmp0M8YcNsYkGWNqgt67zBgz3xizwBhz\nblDr7ajiWe8hf3X+kaQWEBEObTkUwaiUUkp1RVtsB0HKl1Kwx9n56NyPqNnYdr+At9BL/mqra3B/\n17tNc7lYm5XV9v6MmBjSu2jVfaS8nEfKy5kbE8PCMWN49sABGvx+QLssK6WUUsEK1hTgr/e3e880\nmT6tV6+UUmpoaGI7SJLOSMI5zkmzp7nd+/56f58rxlDr3QZbm5XF6vx86v1HKuMYm42fZGSwv7mZ\nP5SX83F9PR/X13f6bL3fzxrtsqyUUkoN+Hr1SimlBp92RR5EzfubQ74/WBXjytRU1mVnk+5yIUC6\ny8W67Gy+k5bGL6dNo3j5cp6ZOzfs5wu9XrYfOjQo3ZW167NSSqmRItx69eHeV0opFXnaYjuIXGmu\nduNzgt8fLF216kbZbFw4fnyXXZbnb97MVJeLGW43b9fW4h2AGZbXezztWpK167NSSqnhLP3WdHZc\ns6Pde7YYG1lrdUZkpZQarrTFdhBlrc3CFtP+FNuiI18xhpqIKkqEk+LjSXE6KfZ6eb2mpi2pbRVq\n3dyuGGPIqanhazt2tOse3bqvG3fupDwowdZWXaWGh+HaW0P/j1BDJtDhSlxirVef7iJ7XbaOr1VK\nqWFMW2wHUWsFGLxcQPIXkiNeMXY1EZXfGN6vq+O4998P+dkir5fr8vM5c9w4Pp2YSLzDwXqPp21f\nU10urp04kdqWFv6yfz9FYVqGAQ76fEzMyWFJXBxTo6J4qbKSxh60EAcfr+MkWr01kPsayOP1pNxA\n7isS33GoDfU5Han76k0Pi+G6L6X6wxhD6QOlAMx6dBapX9brSymlRgIxHVrlhrMlS5aYzZs3RzqM\nPql4roKPL/wYd6abZTuXIfb+r207mDJycsJ2V27lEGGG282uxkaaw1xHk6OiqG1poa6lpdM2d6DV\nuLFDa26wFKeTDQsXMsXlakuiQ02QtS47u9NNcE9u8AdqXz0p15vjdVeuqzKXpqRwuKWF/c3N/Km8\nnF8UFbVrfR8u33Eok77+ntPuvmO0zcavs7L4dGIiNT4fz1ZUcG9pKU1B5z1KhK9OmsQZ48YRbbMR\nbbezoaqKHxcWtvsdiLbZ+HlmJucmJ+M3hucPHOCHe/e2K+MW4XtTp3JqYiLNxuAzhteqqrg/xDGv\nTE1lWUICfmPIqa3lzx5PuzJOET6XmEhmdDSNfj+PezwcDvE7GWuzcdmECcTYbETbbOyor+dvBw+2\n+913inBhcjJzY2NpNoZ7S0qoDfG7H2+3c9OUKThFiLLZ2HboEE9VVHTa10Xjx3PMmDH4jOGXRUXU\nhNhXusvF3uXLO73fUyKyxRizpM87UCO6bg6l5u0aPjjpA5wpTpYXLcfm0s5tSik1lPpaN2tiO0RM\ni+Hdme/SWNDI3GfnMv6C8ZEOqUvhbvJvnjoVEeGlyko21dYSLiUdY7fz4vz5nJiQwBP794dNGC5I\nTmZDdTVnf/RRtzHF2e00+P34QlyzqU4nby1axISoKF44cIDVHbo/x9hs3DFtGovj4tjb2MjexkZ+\nVljIoRA3yrE2G1dNnEiiw8Huhgaerqholwi4bTbuzMriiokTibHZEJGw5+uXWVmckJCAp6mJy3Jz\nOejzhTxXV0+YgF0EG/Dwvn0hk4E4u51rJk7EbwyPlpeHfFhgB5w2W5cPC8BKeFampjI7JobypiYe\nKCtr95kYm42HZs7kguRk6v1+Gvx+nvJ4uHXv3rZWdbCSj/OTksiIjqbG5+PPHk+nbuet+/tySgqx\ndjt7Gxp4qaqqXRITJcI1EydySkICdhHsIvynpoYHSkvbJeUuEa6fPJmTExLwAxurqli3b1+7n49D\nhJPj4xnndHLQ5+NgczOfHD5M57MFAox3OnHbbJQ1NYW8tqJEmBMbS4sxtBjDjoaGkOVUZAjgX7Gi\n75/XxLbfRnLdHMonqz5h//r9pN2SFvGhQ0oNpeHa4yoSjvZzMVx7/bXSxHYEKPlNCbtu3EXCSQks\nemtRpMPpVncXc1VzM0lvv02oK6jjzWZ3+wrXQuwWId3tptjrDZkwhSIQMqbBYAPiHQ7qfL6QiVOk\nuG02UpzOLruCq4E3MzqasQ4H/62rC1vmc4mJbQ8LNndRLtPtxgbsbmwMW+ZTCQk4bTYcIvyrsjJs\nuasmTMAG/L68PGyZe6ZPJ9pm45aCgpAPYJIcDn6SmUl9SwsNfj+37t0bdl9r0tKIstm4u7iY6hAP\nYMba7dw0dSpNfj/NxvDr4uKw+/ru1Kk4RHiwtFRbbIepkV43B2va30TO1BxMs+H4PcfjTndHOqSj\nSiRukkfycJyBHKrSk2P1pNfSQMc1HIeXDefeZ8OtB2Fv4uoNTWxHAF+dj5wpObTUtrD4vcXEL4mP\ndEj9Fi4h7e3NZne/PMYYqn0+5r73Hvuamjp9PkqEyS4X5U1NNHSRAC8aM4YMt5sMt5s/lJdTFeYG\n/taMDKp8Pm7r4uY92mbr8litFsTGkhoVxTs1NSG7eCY6HNyano4faDGGnxcWhk4GHA7+Nz0dG/CT\nwsKQsU+KiiJ/6VJi7XZEJOzPJ8Xp5EcZGeTV1/Pb0tKwscfYbMTY7UTbbBR3kST/IjPTim/PHg6E\niCvZ4eDnWVkcamnh27t3h93PRePHWy2jwN8OHAhb7vzkZGzAs12UeXLOHJKdTpIcDs7+6CNKQ1w3\nU10u3jv2WBpaWjjxgw8oC1FmgtPJPxcswAbYRfjctm0hr8Hga76nvxc9KTfU++ppRTZc99Vbmtj2\n30ivm4MV3l7Inh/sIemcJOa/MD/S4RxVBvp3eLgOObo2P7/dvUFfv+NADqHpiXD/D8fb7Xw/LY0E\nh4MEu50PDh3i/g69qaJtNu6fMYMrJ0zoshfbYCVOfmN4oLSU7+7e3Smuh3uxL5/fz97GRk744AMq\nmjsv2ZnqdPJmoHfg38P0Duzxd5w5kzOSkihvamJ9eTl3lpR0Gkp0w+TJnD9+PAl2O69XVXHznj3d\nXluhjue22fj6pEmku93saWzkobKykPevNmBadDRj7HbG2O28V1cXsgdgnN3O6okTsYuQd/hwp154\nkaybNbEdYru+u4uSO0tIuTSFOevnRDqcfhvI/1QHopIyxpC+aVPIJKxjItCT2Lu74W72+6lraWHB\ne++FTJz6cmM+2ONBe/od01wuCnuZOA3EOe3N8Xq6r6E+p0fDvobyhnEkPhVWRxwNdTNYQ4Y2TduE\nt9DL/Jfmk3RGUqRDGlE6/X5mZnLS2LFsrqtjS10dd5eUhLxJ7kuvi1D/ZzhFOCcpiRnR0TQEesWE\nmzOgdf6ONJeL5w8e7PP/sa7AXADxDgf5DQ1sqKoK2XtrrMPBS/Pnc2xcHM7A/CLh/j/z+v1sPXSI\nM7dtC/kA2yHCzOhoomw2Pjl8uF0y1GpyVBQlJ5zQ6Zx1PN6lKSm8V1fHcwcOcHtRUfgT3gtum40m\nvz/kUDWnCNkxMW2v8+vrQ87REme3852pUxnncJBbX88j+/a1S1idIpySkIBDhD2NjRQ2NnZayaOV\nDVgWH0+m281hn4+Xqqo6DV86JjaWer+f3Q0NIc9nKOF6B0bbbJydlGQNq8J6SB/qGhwoNmBiVFTb\nMK5ir3dYDJeKVG8qTWyHWGNhI5umbUJEWLZnGe4pI7+b03Dr5jOQ3Wki1V3jaO6SEolEbajP6dGw\nr54YrvvqDU1s++9oqJsBDvzjANvP2Y47KzDJo214T/I4VLr73TTG8H9lZXxr1652czD0xuGTTybG\nbu9x+SnvvBPyYXJf2CBkEhZts3Hq2LE0GUOT38+m2toeJz3hxNpsnJCQQILdzt8PHmyXjDlEmBoV\nRUlTU9gJOXtjclQUy+LjWRofT21zM3eXlrZrpXOIEGezURWih1iwsXY7102aRE1LCzU+H0/s39/v\n2IazKS4XB5qaQl7LLhGmuFzsa2rq8fC4riTY7UyIiiK/oSFsmePj46nx+citr+/38a6bOJHM6Gju\nKCoK2btuclQUry9cyKGWFg61tPDFjz8O2XKd6HBwS1oaLcDNYZYBjdT8F5rYRsDHF39MxVMVTP3+\nVKbdPi3S4RyVInHDPZInGhjq7zhcEzU1+mhi239HS9287fPbqHyxkqxfZ5H23bRIhzMshGsZ/XRC\nAlF2O3saG9nT0BC2RcoGfCYxkWPj4li3bx8HQtwkg9W689sZMzgnObnLeDxNTfyisJB7uxhC8/PM\nzLaZ59eEmTPALcJUt5uiLlr6euPOadPIjolhdX5+yCEtY2w2JrtcXSYwrQSYHRNDYWNjyPM6KSqK\nlxcsoNkYztq2jfIQ57Q3c41Mcbk4PzmZOJuNe0tL+zV0ZPfxx9Po9zPrv/+lJESZiYHYW4Ub2pPo\ncPCNyZOpbG7m/rKysLG/MG8emYHhZfPeey9kXJOjovjz7NnsaWzk6sAScR0JsOXYY5kRHc2YHq7A\nkZaTE7J3YLLDwX0zZ7ZNOPmtXbtCXoNTXS6KetH7LFyZyVFR5Cxe3DaM65Qww6oGqwfhQA1J7EgT\n2xGkZlMNHyz/AEeig+XFy7HH9vwppVJKqYGjiW3/HQ11c0NBA+9OfxeJEk4oPQFnkjPSIQ0L4W7e\neyq41SZcV94Up5PiwI34uUlJ3Dt9OhnR0e32U9nczK+Kivhth8Sro94OQzHGkLZpU8gkLNnp5NHs\nbKJsNqJEuOSTT/CESCJ7kzCUe728WVPDxZ98EvZ8VZ90Uo+XNwxX5v9mzuS4+Hj+W1vLf+vquC/M\ngwABWj71KUSkbX+RXj6vr4nTQCdhw6332XDtQTjc5r9w9PmIqs8Sjk8gfnk8tTm1lP+xnMnXT450\nSEoppdSoVfZ/ZWAg5eKUoyap7U9vlo8PH+a+0tIuk9pn5s4l0+0m0+1m4ebNYedqaNV67I4xXTx+\nPA+UlfHDPXt44eBBXq2q4uxx43i3ro5ir5d4ux2v39/WNfTcpCSOj4/nZ4WFnW6m12a1X54p3DFb\n3xcRbs/KCnljfs/06Zwd1IJ85/TpIcsFH7O7401wubgoJYX/2b077PmKdzh6tK+elMmOieGyCRP4\n+4EDYY/XmtS27q+7a2Qg4upNubVhfj69/Vn3Zl89ORcD+R2H+pz25PsN9HkYKtpiGyH7/7qfTy76\nhOgZ0SzNW6pjeZRSKgK0xbb/Rnrd3NLYQs6UHHwHfSzetJj4ZSN/xYK+tNpMdbm4IDmZbYcPs6G6\nuqxv8bMAACAASURBVMv996W1rDtlXi/f2b2bJ8OM4ZwXE8PvZ81iaXx8p9hH2pCjwWrlGi7HG2hH\nw3wOqne0K/II4/f5eXf6u3gLvcx7YR7J53Q9rkQppdTA08S2/0Z63Vz+53LyLstjzOIxHLv52HYt\nWMNRVzfmxhjKm5pYuHkz+8NM+nLntGnWUh61tfymw7ItrWJtNq6YMIF0l4sfh2gZ7esEfT2R+vbb\nIWPv75i94Wa4Tbyp1HCiXZFHGJvDxpRvTmH3d3dTcneJJrZKKaVUBJQ9YE1OM/n6ySMiqQ1ueSv0\nerk6L4/H9u2jCfjo0KGQE9W0qvL5wk6g0yrR4WDP8ceTEOgSO9nt7lFC1NOujd0JNQsrQFE/xvoO\nRwN1vobr8ZSKBE1sI2jiVyay97a9VG+opm5rHXEL4yIdklJKqVFARLKBvwS9lQXcCjwWeD8D2Atc\nZIypGur4hkrdB3XU5tRiT7CTcklKpMPp1i0FBZ0mT2oyhleCug4n2O00+v0hW2Lj7Ha+MH48h1pa\neLqiIuQxqn2+tqQWhj4hSnO5uh2vq5RSodgGa8cicoaI5IvILhG5OcT2BBH5u4h8KCIfi8hVgxXL\ncOVIcDDhmgkAlNxdEuFolFJKjRbGmHxjzEJjzELgWKAeeA64Gfi3MWYG8O/A66NW2YNWa+2EKycM\n6xUKjDG8XFnZZavlP+fPp+j446k66SR+P2sWMbb2t3gxNhsPzpzJo7Nm8de5c0kPkyhGOoFcm5UV\nMvZQE/wopVSwQUlsRcQO3A+cCcwBviwiczoU+zrwiTHmGGAFcKeI/D979x4fZ1nmf/xzTQ7TJj2n\nbVqapmlom1KQAi2FyoogIqAIKrgLVAUPVETwxE8Bi6LrVkHwxIpiBZTVuqwiSJdlBVytqIRDKS2F\ntmmbNGnTQ5qmp6RpJoe5fn/MpKTJTHOcTCb5vl+vvDpzP/fzzDVPmty55j5lJiKegSzvc3kAVP1H\nFSsDKykuKKZqeVWSoxIRkSHkAqDU3SuAy4FHouWPAB9IWlQJ1nyw+Wh7O+UzA3N3AnfnuX37OOe1\n17j49dfj1psWDPLenBymDhuGmbEoN5dlRUVMCwax6PH282IHagLZldhFRGJJ1FDkBcAWdy8DMLNH\niTSWbTfucmCkRSa0jAD2AfEnhgxSh4oPQRrQAjiEKkKULI7Mf8ldpF/iIiKScFcB/xl9nOvuu6KP\ndwODsiGqWl7F5ps3E64PY8OM2lW1ZBVlJTusYxb4mZCRwZj0dDYdOQJATno67x47lhU1NRzpx+1K\nkkHzQUWkJxKV2E4Btrd5Xgmc1a7Oj4EVwE5gJPAv7h5/1+1BqmxJWSSpbSNcH6ZsSZkSWxERSajo\nSKnLgNvbH3N3N7OYWyeY2WJgMUB+fn5CY+xrVcurKFlcQrg+8ieHN/iA+EC5/cJQe5qa2NPURLYZ\nSwoKuGnKFEamp/fp6rZKIEVkMEnm4lEXAWuAdwEnAs+Z2d/c/VDbSqnceHZFaFvs+TLxykVERPrQ\nJcBqd2+dA1NlZpPdfZeZTQZibirq7suAZRDZ7qd/Qu0bZbeXHU1qWyX6A+VYyeg1EyeyLRTi5UOH\neLm2ln+vrIy54NO4zExunzbt6HMloyIisSUqsd0BTG3zPC9a1tbHgbs8spHuFjPbCswGXm5bKZUb\nz64I5gcJVXRMYoP5Wv1PREQS7mreGoYMkZFU1wJ3Rf99MhlBJUrdG3WEtvftB8qd9aDG2qLn2g0b\n+ExJCbXhzgeqVQ6ybW5ERBIlUYntK8BMM5tOJKG9CrimXZ1tRBas+JuZ5QJFQFmC4hmwCpcWHjMk\nCgCDgq8XJC0mEREZ/MwsG7gQ+HSb4ruA35rZJ4EK4J+TEVtfc3d2/2I3m2/aHLdOTz5Qjrev7H/v\n3cuY9HS2NjTw5wMHaG7XE9sC1IbD5KSns2DUKBaMHMlPdu6MuYdrslcpFhFJFQlJbN292cxuAp4h\nsjTSw+7+ppndED3+APAt4Jdmtg4w4FZ335uIeAay1mFPZUvKCG0LYemGNzl1a+qSHJmIiAxm7n4Y\nyGlXVkPkQ+dBo7mumc03bqbqV5HR1qPeOYq6l+sIH3nrA+VAVoDCpd1fDXhJnH1l/yvOHrFtGVB9\nzjlE1tCEmVlZxyTJMDBWKRYRSRUJm2Pr7k8DT7cre6DN453AexL1+qkkd1Hu0QS39rVaVi9YzY5/\n38H4D41n7HljkxydiIhIaqp7o471H15P/cZ6AlkBZv10FpM+Nomq5VVHP1AO5gcpXFrYo/m1x9tX\n9kczZjB92DA+vWkTuxobOxzPDwaPJrUwsFcpFhFJBclcPEpiGHn6SKbdMY3yb5RT8vES5r8+n/SR\n+jaJiIh0pm3Cmj42nebaZmiCrJOzOPl3J5N9UjZw7AfKPbX+8GGMyN6F7U0LBvlcXmSf+kMtLV3u\nidXCUCIiPRfovIr0t/yv5jPijBE0lDdQ9pUhN+1YRESk21q38QlVhMCheV8kqR31zlHMe3ne0aS2\nL5QeOcK7164lTGS+VVvtk9ZFubksKypiWjCIEUl6lxUVKYEVEeljSmwHoEBGgNmPzMYyjJ0P7GTf\nc/uSHZKIiMiAVrak4zY+AKHyEGlZ7dPPnqtsaODda9eyq7GR88eM4eddSFoX5eZSvnAh4fPOo3zh\nQiW1IiIJoDGuA9SIU0ZQ8M0Ctn51KyWfKOHMN84kfbS+XSIiIrH0x77wexobeffatZQ3NHDWyJE8\necopjExP5+OTJ/fZa4iISM+ox3YAm/rlqYxcMJJQZYgtX9qS7HBEREQGrHjb9fTVvvD7m5p4z9q1\nlBw5wqnZ2fzvqacyMl0fOIuIDBRKbAewQHp0SHLQ2P3wbmr+pybZIYmIiAxIeV/K61DW02182qtr\nbuZ969ax9vBhZg0fzrNz5zI2I6PX1xURkb6jjxoHuOzZ2RQuLaT0/5WyftF60kemE9rRu+0JRERE\nBpvQ1siQ40BWgPCRcK/byeVVVUe33sk0I+ROfjDIn+bOJTczsy9DFxGRPqDENgXkfSGPHT/bQcPm\nBloOtgAQqghRsrgEQMmtiIgMaU01TexcthOA0/9xOiNPG9mr6y2vqjpmi56QRzb1+fyUKUwdNqx3\nwYqISEJoKHIKsDQjfLjjSo/h+jBlS7QdkIiIDG07fryDcH2YcReP63VSC7CkrOyYfWdb3bdjR6+v\nLSIiiaHENkU07mqMWd6Xqz2KiIikmpbDLVT+eyUA+bfl98k1t4Vit63xykVEJPmU2KaIeKs6ZkzQ\n4hUiIjJ07XpoF801zYw6exSjzx3dJ9fMC8Zuc/PjlIuISPIpsU0RhUsLCWR1/HY17Wli002baDnc\nkoSoREREkifcFGb7vduBSG+tmfXJdd+WldWhLCsQYGlh71dYFhGRxFBimyJyF+VStKyI4LQgWKQH\nd/yV47EMY+f9O3ll7isc+PuBZIcpIiLSb/b85x5C20NknZRFzvtz+uSar9bW8sf9+wGYlJGBAdOC\nQZYVFbEoV4s1iogMVFoVOYXkLsrtsAJy3do6NnxsA4dfP8yac9cw9pKx1K+rJ1SpLYFERGTw8rCz\n7e5tAOTfmo8Fet9b2xwOc31JCWHgC3l5/GDGjF5fU0RE+od6bFPciLkjmPfKPPKXRBbM2P/0fkLb\nQ+BvbQlUtbwqyVGKiIj0rZqnaqhfX09wapCJV0/sk2v+sLKS1+rqmBYM8q2Cgj65poiI9A8ltoNA\nIDNA4b8VkpHbcSEpbQkkIiKDjbuz7TuR3tqpt0wlkNn7P2fKjhzh6+XlAPx01ixGpGtQm4hIKlFi\nO4g0VTXFLNeWQCIiMpgc/NtBDr14iPRx6Uz+1OReX8/duWHTJo6Ew1w9cSKX5PTNfF0REek/SmwH\nkXhbAgWGB2hp0KrJIiIyOLTOrc37XB5p2Wm9vt6vq6p4bv9+xqWn80PNqxURSUlKbAeReFsChevD\nrL1gLY3VjUmISkREBiIzG2Nmj5nZRjPbYGYLzewbZrbDzNZEv96b7Djbq3u9jn1P7yOQFWDKTVN6\nfb3qxka+uGULAN878UQmZmb2+poiItL/lNgOIh22BJoWZPp3phOcGuTQC4dYffZq6kvqkx2miIgM\nDD8C/ujus4G5wIZo+Q/c/bTo19PJCy+21t7ayddPJiOn49oS3XVLaSk1zc28a8wYrp00qdfXExGR\n5NDKCINMrC2BJl07iXXvX0fdq3WsXriakx8/mbHnjU1ShCIikmxmNho4F7gOwN0bgUaz3m+ZkyhV\ny6so/UopjTsjo4+GFQ7r9TWf3bePX1VVMSwQ4GezZjGQ37+IiByfemyHgODkIKf/9XRyLs+heX8z\nr7/ndUpuKKG4oJiVgZUUFxRrSyARkaFlOlAN/MLMXjOzB80sO3rsZjN73cweNrMB8Slo1fIqShaX\nHE1qAbbevrVD27W8qoqC4mICK1dSUFzM8qrYbdvyqiryi4u56PXXAbhs3DhmZGUl7g2IiEjCKbEd\nItKy0zjl96eQ98U8vMnZ9bNdhCq0362IyBCVDpwB/NTdTwcOA7cBPwUKgdOAXcD3Yp1sZovNbJWZ\nraqurk54sGVLygjXh48pa7+d3fKqKhaXlFARCuFARSjE4pKSDsnt8t27ub6khO2ht3YMeGrfvrhJ\nsIiIpAYltkOIpRkzvj+D9LEdR6Brv1sRkSGlEqh095eizx8DznD3Kndvcfcw8HNgQayT3X2Zu893\n9/kTJkxIeLDxtq1rW76krIz68LHJb304zCc3bmTeqlXMePFFJvzjH3xk40aOxKi3pExtoIhIKlNi\nOwQ1H2iOWa79bkVEhgZ33w1sN7OiaNEFwHoza7sp7AeBN/o9uBjibWfXtnxbKE7y687qujpKGxrY\n2xR7v/fjnS8iIqlBie0QFO8PBBxKv1JK04H4Db+IiAwaNwPLzex1IkOPvw1818zWRcvOB76YzABb\nFS4thHbb1QayApHyqKnB2G3bxIwMXjrjDDYuWMCuhQvj1suPUy4iIqlBie0QFHO/2+gfDNvv2c5L\nJ75E5Y8q2fXILi0wJSIySLn7muhw4lPd/QPuvt/dP+rub4uWXebuu5IdJ8DEayaSlh1tqKLb2RUt\nKzpmF4D5I0Z0OC8rEOD7M2awYNQoirKymBQM8p3CQrICgQ71lhYWdjhfRERSh7b7GYJa/xAoW1JG\naFuIYH6QwqWFDJ81nNIvl3LwrwfZ8oUtYIBHzmldYKrt+SIiIv2hcWcjLYdaSB+Tzjn7zumwLc9z\n+/bxRE0NEOmhrW5qIj8YZGlhIYtyj22zWp8vKStjWygUt56IiKQWJbZDVKz9bgFO+8tp1Px3DW9e\n8Sbe7Mcca11gSomtiIj0p9pVtQCMnD+yQ1K7MxRi0YYNOPCNggLuLCjo9HqLcnOVyIqIDDIaiizH\nMDPGXzYeb/GYx7XAlIiI9Le2iW1bzeEwV69fT3VTE+8eO5Y7pk1LRngiIjIAKLGVmOItMJWRk9HP\nkYiIyFAXL7G9s7yc5w8eZFJmJr8+6STS2vXmiojI0KHEVmKKucAU0LS3ifJvleMeu0dXRESkL7l7\nzMT2jzU1fHvbNgLAo3PmkJuZmaQIRURkIFBiKzHlLsqlaFkRwWnByAqU+UEmXj0RDMq/Xs6GazbQ\ncqQl2WGKiMggF9oWomlvExnjM46OJtre0MBHNmwA4F+nT+edY8YkM0QRERkAErZ4lJldDPyIyEYy\nD7r7XTHqnAf8EMgA9rr7OxMVj3RfrAWmJl4zkQ1Xb2DPo3s4UnqEU/5wCsETtPefiIgkRvuFo5rC\nYa5av56a5mYuGjuW2/PzkxyhiIgMBAlJbM0sDbgfuBCoBF4xsxXuvr5NnTHAT4CL3X2bmU1MRCzS\nt8ZfOp7Ti0/njfe/Qe0rtbx65quc8JkT2PXgrmO2DtLKySIi0hdaE9s/v8+4rLiYilBkEcMxaWn8\n6qSTCGherYiIkLihyAuALe5e5u6NwKPA5e3qXAM87u7bANx9T4JikT424pQRnPHyGYx+x2gadzZS\n/rVyQhUh8Lf2u61aXnXMOVXLqyguKGZlYCXFBcUdjouIiMRSu6qWP10AXz1539GkFuBIOMyz+/cn\nMTIRERlIEpXYTgG2t3leGS1raxYw1sxWmtmrZvaxBMUiCZA5IZO5f5pLILvjf6FwfZhNN25ix092\nsPepvVTcVUHJ9SWdJr8iIiJttS4c9eCn4Igdu2hhyJ0lZWVJikxERAaahM2x7eJrzwMuAIYDxWb2\nortvalvJzBYDiwHyNY9mQAlkBgjXh2MeaznUwubPbo57brg+TNmSMg1ZFhGRuI6UHqH5QDN74kxW\n2hbS3uoiIhKRqB7bHcDUNs/zomVtVQLPuPthd98LPA/MbX8hd1/m7vPdff6ECRMSFK70VLz9btNG\npzH5+smMfc/YuOeGtukPEhERia91fu2k2th/ruQHtXihiIhEJCqxfQWYaWbTzSwTuApY0a7Ok8A/\nmVm6mWUBZwEbEhSPJEis/W4DWQFm3T+LomVFzH1mbmTLoBgCwQANFQ39EaaIiKSg1sT2KzvG0X6J\nqKxAgKWFhf0flIiIDEgJSWzdvRm4CXiGSLL6W3d/08xuMLMbonU2AH8EXgdeJrIl0BuJiEcSp8N+\nt9OCFC0rOmaIcazkFyDcEOaVU15h58924u4djouIyNDWmti+u2A8Dlj0a1owyLKiIhblajqLiIhE\nJGyOrbs/DTzdruyBds/vAe5JVAzSP2Ltd9v+OEDZkrKjWwLl35rP/v/bz97f72XTDZuofqyacZeO\no/IHldo2SERE8LBT92odAKtmtMBOuGTcOP7n1FOTHJmIiAxEyVw8SoaQWMnvCTecQPXvqtn82c3s\n/9N+9v/prW0bWldObj1XRESGlvpN9bTUtRCcGuT5cKTn9oKx8ddtEBGRoS1Rc2xFOmVmTPzniZz5\n5pkEhsfeNqhsibZyEBEZilqHIY+YP4L/i+5X+64xY5IZkoiIDGBKbCXpMidmEm6IvW2QVk4WERma\nWhPb/ecMZ1soxLj0dE4dMSLJUYmIyEClxFYGhHjbBuGw7gPrqF1d278BiYhIUrUmtqvnRhYXPH/M\nGALWfm1kERGRCCW2MiDEWjnZ0g3SoebJGl6d9yrr3r+OQy8fomp5FcUFxawMrKS4oJiq5VVJilpE\nJHWZ2Rgze8zMNprZBjNbaGbjzOw5M9sc/Tcpk1rDzWHqXossHFWcExm58y7NrxURkeNQYisDQqxt\ng2b/cjYLty8k75Y8AlkBap6qYfVZq9nwsQ2EKkLgby0ypeRWRKTbfgT80d1nA3OJbM93G/B/7j4T\n+L/o835Xv7GecH2Y4PQgK+sPAppfKyIix6dVkWXAiLdt0Ix7Z5D/lXy2f38727+7HdpNxw3Xhym7\nvUyrJ4uIdJGZjQbOBa4DcPdGoNHMLgfOi1Z7BFgJ3Nrf8bUOQ95zYRZ7mvYzOTOToqys/g5DRERS\niHpsJSVkTszkxLtOjHs8tD3EmnetoWJpBQdfPMiuX+3ScGURkfimA9XAL8zsNTN70MyygVx33xWt\nsxtIyieGrYntmrdH/kx515gxmObXiojIcajHVlJKMD8YGYYcw4G/HODAXw7AHceWa09cEZEO0oEz\ngJvd/SUz+xHthh27u5uZxzrZzBYDiwHy8/P7PLjWxPbl/CZA+9eKiEjn1GMrKSXWIlOBrAAzfzqT\nOb+bwwk3nBBZdKqdcH2YzZ/fTHNtc3+FKiIykFUCle7+UvT5Y0QS3SozmwwQ/XdPrJPdfZm7z3f3\n+RMmTOjTwMJNYerW1NESgH+kHQa0cJSIiHROia2klFiLTBUtK2LKDVOYeOVEZv10Ft4Ss4OB5ppm\nXpj0AusXrafmjzWEm8NaYVlEhiR33w1sN7OiaNEFwHpgBXBttOxa4Mn+ju3wm4fxkLP9XUEOhlso\nHDaMacOG9XcYIiKSYjQUWVJOvEWmWsUbrmxBI1wfZs9v9rDnN3sIjArg9Y43RxJhDVkWkSHmZmC5\nmWUCZcDHiXzg/Vsz+yRQAfxzfwfVOgx53bvTgZB6a0VEpEuU2MqgU7i0kJLFJYTr31o+OZAVoGhZ\nEaPePoqq5VVU/aqKI5uOdDg3XB9m8+c2M3zmcLJPziYtO42q5VWULSkjtC1EMD9I4dJCJb4ikvLc\nfQ0wP8ahC/o7lrZaE9tXZ0d+h2ubHxER6QoltjLotCad8ZLRgjsKmLZkGn9N+yvEGLXcvK+Z1Wet\nBoP08ek072uGlsgx9eqKiCRW7apamtLhldGRkTfnK7EVEZEuUGIrg1Jnw5XNLO6Q5UBWgOEnDqd+\nYz3N1R0XmwrXh9l882ZGLRzF8MLhR8vVsysi0jvhUJjDrx9mwxyoJ8ycrCwmBYPJDktERFKAElsZ\nso43ZDl3US7hxjDPD3s+dq/u/mZeOvElsk7OYvz7x2NBY/s9249eSz27IiLdV7euDm9y3rgwA2jS\nNj8iItJlSmxlyOpsyHIgM3DcXl1LN+rfrGfbm9tiXj9cH6bs9rJjElv16oqIxNc6v/a1eZHnml8r\nIiJdpcRWhrTOhiwfr1d3wocncPBvB9n733vZ8aMdMc8PbQ/xwpQXGF44HHen9uVavKnzVZiVAIvI\nUFS7qpaGIKzNbcKAdyqxFRGRLlJiK3IcnfXqjr1gLGMvGMveJ/YS2taxZxegcWcjjTsbYx4L14fZ\n+MmNHHrxEFlzssiek83hDYcpvaVUw5pFZMipXVXLG6dAUwDmjRjB2IyMZIckIiIpQomtSCc669UF\nKPx27J7dmQ/MZMw7xtCwtYG171ob81wPOTt+HLvHt1W4PkzpraVMvHoiFjBAvboiMri01Ldw+I3D\nvPaJyHPtXysiIt2hxFakD3TWszu8YDjBabHn62ZMzGDql6dSv76ew+sPU/tSbczXaNzRyN+y/8bw\nWcMJDAtQ91qdhjWLyKBRt7YOWmDt2wNAWPNrRUSkW5TYivSRns7XnfH9GcecVzytOPaw5gCEGyJb\nYcQSrg+z4WMb2Hb3NjJPyCQ4OUjjvkb2/+/+LiXAIiLJVLuqlrps2JAfJt2Mfxo9OtkhiYhIClFi\nK9JPOuvVbRVvWHPRsiJy3p9DfUk9qxesjv0iYTi87jCH18VOfiGSAG/6zCa82Rlx+giyTsqi+rfV\n6tUVkaSqXVXL2rkQDsDbR41iRLr+RBERka5TqyHSj7oyX7ezBHjUmaPiDmvOzMvkbU++jcZdjYR2\nhdh0/aaYr9FS28LG6zZGnqQR2as3mkeHKkJs/ORGGmsamXLDFAKZgaPndWVYs4Y+i0hP1K6q5bXz\nI481DFlERLpLia3IANTTYc0n3nUiI88YebSs4t8qYibAaWPSGPfucdS+VktDaUOH4x5ySj9fSunn\nS8mYkEFwShB35/Abh6ElUqc1AT5SfoRJH5lExvgMqv9QzabFmzpd0VnJr4i01VzXTP2Gel67JfJc\nC0eJiEh3KbEVSUFdHtYcJwGe9eNZR+uuDKyM9NjGEoCm6iaaqptiHvaQU35HOeV3lMeNNVwfZtNn\nN9G0t4mM8RnUvV5H5X2VeEPfLHylJFkktVUtr2LLLVvYPxrKCmGYG2ePGpXssEREJMUosRVJUX0x\nrBkgmB97WHNwWpCzS8+mcU8joR2hyLzeOAlw5pRMmmuaCTeEYx5vOdjCli9siRtnuD7Mxk9sZM9v\n95CZm0lmbiZHth2h+tFqvLFNAnx9CeHGMJOum4SZUbW86pjEXUmySGpp+zO85rxI2clrnAM7q/Wz\nJyIi3aLEVmSQ6+mw5sKlhViaEZwcjHwdJwFeWL4QgOL8YkLbYwx9Hp1G7kdyadrbRPV/VceMwxud\nmhU1x30v4SNhSj5RQsmnSkgbkUbL4ZajQ6OP1qkPU7K4hP1/2U9aVhqB4QGObDlCzX/XHLM69MZP\nbuRIxRFyr8klfUw6NU/VsOnTnQ+jhr5NkpVwy1BWtqTs6M/ca6dHyk5fBWX/KNP/cRER6RYltiJD\nXG+HNRcuLXyrznfiDH2+/62hz8UvFsfezzc3g1n3z6JxTyONVY1UfLMiftBhaDnUEv9wfZjdD+0+\n7vv2kFO+pJzyJeXHvU7J9SXsXbGXtOw0AlkBGioa2P/MsVsobfzERvb/dT9jzh2DpRuHig+x82c7\n8VCb3uZPldBY08ikj04ibWQagfRAl3qck9ErraRc+kvbrc1WnxH59/TXiL3lmYiIyHGYe7zJdQPP\n/PnzfdWqVckOQ2TI6otEpn2iBm9tZ3TMfr4FsRPg4LQgZ205i/DhMC+f/DKNOxo71EnPSafwO4WE\n68O0HGlh6+1b476nYH6Q5gPNx02UE8GCFkmOY4zetqAx5p1jsAzjwJ8PED7SsVLa6DTyb82P9Epn\nBahdXcvuX+w+mky3Xif/tnxy3puDpRv7ntlHxb9WHDNkPDA8wInfO5GJV03EMozqx6rZ/NnNnX5/\nuvJ97Or3urVuMpJkM3vV3ef36iJDXG/a5uKCYv5nRoiffRr2TgALw23fgfdvfmskiIiIDC09bZuV\n2IpIv+tqEtNXidPxkuSjw6inFcfsJcqYkMGM+2YQPhympb6FLZ+LP1d44jUT8Wan+rexh1sDpI1K\no6WuJWZCO6AFIDg1SCAYIBAMUF9Sf3T+8zHVhgUYc/4YMDjwl9hJeSA7wKRrJxEIBrBMo35TPfue\n2ne0FxzAMowJV01g1IJRWMA49Moh9vxmzzGvGS9J7g4ltr3Xm7b5J49v4ktZOwkNe6ss2ADfrz+B\nGz80q48iFBGRVKLEVkQGnb4cWtuvSXInddyd8JEwLxe9TKgyRjI9MYPZj8zGm5yST5TQtLfjqtRp\no9M44YYTCNeHCR8Js+vBXTHvIcDI+SPxFqfutbq4ddJGp+HNTvhwamXcbe97Tyix7b3etM0FxcVU\nhDr+DEwLBilfqB5bEZGhqKdtc8Lm2JrZxcCPgDTgQXe/K069M4Fi4Cp3fyxR8YhI6unqys99G9wD\nIgAAIABJREFUsTp0n8417qSOmZGWlUbhXbHrzfj+DHIuzgGg5Yctnc5bBtj33L64yfS8V+YBvUvK\nM6dkcvrfTiccCuONztqL1tK0u2PCnTExg9kPz8Y9mpTH2CoqfVw6Bd8swBudcCjM1q/GHyp+wo0n\ngMPOn+6MeVxzMXvOzMqBWiJLsDW7+3wz+wZwPdA67OCr7v50omLYFiOpPV65iIhIPAlJbM0sDbgf\nuBCoBF4xsxXuvj5GvbuBZxMRh4hIq4GYJA+0hPt4dU68+0SGTx9+tGzGvTPiJ+XviyblP4idlM+8\nb+Yx8e/82c64Cfes+yPDUWueroldJz/YoUy65Xx339uu7Afufm9/vHh+MBizxzY/qO+riIh0T6J6\nbBcAW9y9DMDMHgUuB9a3q3cz8HvgzATFISLS5/oqSe7La/VVkpzKSbmknqWFhSwuKaE+/Nb3NSsQ\nYGmhvq8iItI9CZlja2ZXAhe7+6eizz8KnOXuN7WpMwX4DXA+8DDwVGdDkTXHVkRkcNKqyP3PzLYC\nB4kMRf6Zuy+LDkX+eLR8FXCLu++Pce5iYDFAfn7+vIqK42zP1YnlVVUsKStjWyhEfjDI0sJCFuVq\nSygRkaFqwM2x7YIfAre6e9jM4lZq13j2U2giItKf+rIXXLrsn9x9h5lNBJ4zs43AT4FvAR7993vA\nJ9qf6O7LgGUQ+dC5N0Esys1VIisiIr0WSNB1dwBT2zzPi5a1NR94NLp4xZXAT8zsA+0v5O7L3H2+\nu8+fMGFCgsIVEREZWtx9R/TfPcATwAJ3r3L3FncPAz8nMrVIRERkwEtUYvsKMNPMpptZJnAVsKJt\nBXef7u4F7l4APAbc6O5/SFA8IiIiEmVm2WY2svUx8B7gDTOb3KbaB4E3khGfiIhIdyVkKLK7N5vZ\nTcAzRLb7edjd3zSzG6LHH0jE64qIiEiX5AJPRKcCpQO/cfc/mtmvzOw0IkORy4FPJy9EERGRrkvY\nHNvovndPtyuLmdC6+3WJikNERESOFd21YG6M8o8mIRwREZFeS8iqyIliZtVAZ0svjgfa78mXKlI5\ndkjt+BV7cqRy7JDa8Sv2iGnurgUcekFt84CXyvEr9uRI5dghteNX7BE9aptTKrHtCjNblapbN6Ry\n7JDa8Sv25Ejl2CG141fs0p9S+XuWyrFDasev2JMjlWOH1I5fsfdOohaPEhEREREREekXSmxFRERE\nREQkpQ3GxHZZsgPohVSOHVI7fsWeHKkcO6R2/Ipd+lMqf89SOXZI7fgVe3KkcuyQ2vEr9l4YdHNs\nRUREREREZGgZjD22IiIiIiIiMoQMmsTWzC42sxIz22JmtyU7nu4ys3IzW2dma8xsVbLjOR4ze9jM\n9pjZG23KxpnZc2a2Ofrv2GTGeDxx4v+Gme2I3v81ZvbeZMYYj5lNNbO/mNl6M3vTzD4fLR/w9/84\nsQ/4e29mw8zsZTNbG439m9HyVLjv8WIf8Pe9lZmlmdlrZvZU9PmAv+8Soba5/6htTo5UbpdBbXOy\nqG1OUEyDYSiymaUBm4ALgUrgFeBqd1+f1MC6wczKgfnuPuD3rjKzc4E64D/c/ZRo2XeBfe5+V/SP\nl7Hufmsy44wnTvzfAOrc/d5kxtYZM5sMTHb31WY2EngV+ABwHQP8/h8n9n9mgN97MzMg293rzCwD\n+DvweeBDDPz7Hi/2ixng972VmX0JmA+McvdLU+n3zVCmtrl/qW1OjlRul0Ftc7KobU6MwdJjuwDY\n4u5l7t4IPApcnuSYBi13fx7Y1674cuCR6ONHiPxSHJDixJ8S3H2Xu6+OPq4FNgBTSIH7f5zYBzyP\nqIs+zYh+Oalx3+PFnhLMLA94H/Bgm+IBf98FUNvcr9Q2J0cqt8ugtjlZ1DYnxmBJbKcA29s8ryRF\nfijbcOBPZvaqmS1OdjA9kOvuu6KPdwO5yQymh242s9ejw6EG3LCV9sysADgdeIkUu//tYocUuPfR\nITdrgD3Ac+6eMvc9TuyQAvcd+CHwFSDcpiwl7ruobR4ABsPPSir8ngJSu10Gtc39TW1z3xssie1g\n8E/ufhpwCfDZ6JCclOSR8e0p86lT1E+BQuA0YBfwveSGc3xmNgL4PfAFdz/U9thAv/8xYk+Je+/u\nLdGf0TxggZmd0u74gL3vcWIf8PfdzC4F9rj7q/HqDOT7LoOC2ubkGvC/p1qlcrsMapuTQW1z3xss\nie0OYGqb53nRspTh7jui/+4BniAyhCuVVEXnabTO19iT5Hi6xd2ror9gwsDPGcD3PzoX4/fAcnd/\nPFqcEvc/VuypdO8B3P0A8Bci82BS4r63aht7itz3c4DLovMcHwXeZWa/JsXu+xCmtjn5UvpnJUV+\nT6V0uwxqm5NNbXPfGSyJ7SvATDObbmaZwFXAiiTH1GVmlh2dsI+ZZQPvAd44/lkDzgrg2ujja4En\nkxhLt7X+IEZ9kAF6/6OLDTwEbHD377c5NODvf7zYU+Hem9kEMxsTfTycyGI4G0mN+x4z9lS47+5+\nu7vnuXsBkd/rf3b3j5AC910Atc0DQUr/rKTC76lUbpdBbXOyqG1OjPT+fsFEcPdmM7sJeAZIAx52\n9zeTHFZ35AJPRH63kA78xt3/mNyQ4jOz/wTOA8abWSVwJ3AX8Fsz+yRQQWQ1vQEpTvznmdlpRIZN\nlAOfTlqAx3cO8FFgXXReBsBXSY37Hy/2q1Pg3k8GHrHIKq8B4Lfu/pSZFTPw73u82H+VAvc9nlT4\n/z7kqW3uX2qbkyaV22VQ25wsapsTYFBs9yMiIiIiIiJD12AZiiwiIiIiIiJDlBJbERERERERSWlK\nbEVERERERCSlKbEVERERERGRlKbEVkRERERERFKaElsRERERERFJaUpsRUREREREJKUpsRVJYWb2\nv2Z2bbLjEBERERFJJiW2Ij1gZuVm9u5kx+Hul7j7I8mOA8DMVprZp5Idh4iIiIgMPUpsRQYoM0tP\ndgytBlIsIiIiIiLtKbEV6WNmdqmZrTGzA2b2gpmd2ubYbWZWama1ZrbezD7Y5th1ZvYPM/uBmdUA\n34iW/d3M7jWz/Wa21cwuaXPO0V7SLtSdbmbPR1/7T2Z2v5n9Os57OM/MKs3sVjPbDfzCzMaa2VNm\nVh29/lNmlhetvxR4B/BjM6szsx9Hy2eb2XNmts/MSszsn/v2bouIiIiIKLEV6VNmdjrwMPBpIAf4\nGbDCzILRKqVEEsDRwDeBX5vZ5DaXOAsoA3KBpW3KSoDxwHeBh8zM4oRwvLq/AV6OxvUN4KOdvJ1J\nwDhgGrCYyO+LX0Sf5wNHgB8DuPsS4G/ATe4+wt1vMrNs4Lno604ErgJ+YmZzOnldEREREZFuUWIr\n0rcWAz9z95fcvSU6/zUEnA3g7r9z953uHnb3/wI2AwvanL/T3f/d3Zvd/Ui0rMLdf+7uLcAjwGQi\niW8sMeuaWT5wJvB1d290978DKzp5L2HgTncPufsRd69x99+7e7271xJJvN95nPMvBcrd/RfR9/Ma\n8Hvgw528roiIiIhIt2jenEjfmgZca2Y3tynLBE4AMLOPAV8CCqLHRhDpXW21PcY1d7c+cPf6aAfs\niDivH6/ueGCfu9e3e62px3kv1e7e0PrEzLKAHwAXA2OjxSPNLC2aSLc3DTjLzA60KUsHfnWc1xQR\nERER6TYltiJ9azuw1N2Xtj9gZtOAnwMXAMXu3mJma4C2w4o9QXHtAsaZWVab5PZ4SW2sWG4BioCz\n3H23mZ0GvMZb8bevvx34q7tf2Iu4RUREREQ6paHIIj2XYWbD2nylE0lcbzCzsywi28zeZ2YjgWwi\nyV81gJl9HDilPwJ19wpgFZEFqTLNbCHw/m5eZiSRebUHzGwccGe741VAYZvnTwGzzOyjZpYR/TrT\nzE7q4dsQEREREYlJia1Izz1NJNFr/fqGu68CrieyqNJ+YAtwHYC7rwe+BxQTSQLfBvyjH+NdBCwE\naoB/A/6LyPzfrvohMBzYC7wI/LHd8R8BV0ZXTL4vOg/3PUQWjdpJZJj03UAQEREREZE+ZO6JGvko\nIgOZmf0XsNHd2/e8ioiIiIikFPXYigwR0WHAJ5pZwMwuBi4H/pDsuEREREREekuJrcjQMQlYCdQB\n9wGfiW7BIyJDgJk9bGZ7zOyNOMfNzO4zsy1m9rqZndHfMYqIiPSUhiKLiIgMAWZ2LpEPtv7D3Tss\nXGdm7wVuBt4LnAX8yN3P6t8oRUREekY9tiIiIkOAuz8P7DtOlcuJJL3u7i8CY8xscv9EJyIi0jtK\nbEVERARgCpH9p1tVRstEREQGvPRkB9Ad48eP94KCgmSHISIig8Srr766190nJDuOVGNmi4HFANnZ\n2fNmz56d5IhERGSw6GnbnFKJbUFBAatWrUp2GCIiMkiYWUWyYxhAdgBT2zzPi5Z14O7LgGUA8+fP\nd7XNIiLSV3raNmsosoiIiACsAD4WXR35bOCgu+9KdlAiIiJdkVI9tiIiItIzZvafwHnAeDOrBO4E\nMgDc/QHgaSIrIm8B6oGPJydSERGR7lNiKyIiMgS4+9WdHHfgs/0UjoiISJ9K+cS2qamJyspKGhoa\nkh2KRA0bNoy8vDwyMjKSHYqIiIiIiAwBKZ/YVlZWMnLkSAoKCjCz49ZtqmkitCOENzqWaQSnBMnI\nUfLVl9ydmpoaKisrmT59erLDERERERGRISDlF49qaGggJyenS0ltQ0UD3ugAeKPTUNFAU01Tf4Q5\nZJgZOTk56kEXEREREZF+k/KJLdBpUgsQ2hGCcLvCcLRc+lRXvh8iIiIiIiJ9ZVAktl3R2lPb1XIR\nERERERFJDUMmsbXM2L2I8cq7Y8SIEb2+RmdWrFjBXXfdlfDXieUPf/gD69evT8pri4iIiIiIdGbI\nJLbBKUEIQM3/1rDu/et4dcGrrHv/Og4WH0x2aEe1tLTEPXbZZZdx2223JeW1ldiKiIiIiMhANmQS\n24ycDA69coht395G4+5GcGjc3UjpF0upWl7VZ69zzz33cOaZZ3Lqqady5513Hi3/wAc+wLx58zj5\n5JNZtmzZ0fIRI0Zwyy23MHfuXIqLiykoKODOO+/kjDPO4G1vexsbN24E4Je//CU33XQTANdddx2f\n+9znePvb305hYSGPPfYYAOFwmBtvvJHZs2dz4YUX8t73vvfosVgKCgq49dZbOeOMM/jd737Hz3/+\nc84880zmzp3LFVdcQX19PS+88AIrVqzgy1/+MqeddhqlpaWUlpZy8cUXM2/ePN7xjnccjVFERERE\nRCQZUn67n7ZW2spunxM+EmbDRzaw4SMb4tY5z8/r0rWeffZZNm/ezMsvv4y7c9lll/H8889z7rnn\n8vDDDzNu3DiOHDnCmWeeyRVXXEFOTg6HDx/mrLPO4nvf+97R64wfP57Vq1fzk5/8hHvvvZcHH3yw\nw2vt2rWLv//972zcuJHLLruMK6+8kscff5zy8nLWr1/Pnj17OOmkk/jEJz5x3JhzcnJYvXo1ADU1\nNVx//fUA3HHHHTz00EPcfPPNXHbZZVx66aVceeWVAFxwwQU88MADzJw5k5deeokbb7yRP//5z126\nRyIiIiIiIn1tUCW2yfbss8/y7LPPcvrppwNQV1fH5s2bOffcc7nvvvt44oknANi+fTubN28mJyeH\ntLQ0rrjiimOu86EPfQiAefPm8fjjj8d8rQ984AMEAgHmzJlDVVWkx/nvf/87H/7whwkEAkyaNInz\nzz+/05j/5V/+5ejjN954gzvuuIMDBw5QV1fHRRdd1KF+XV0dL7zwAh/+8IePloVCWllaRERERESS\nZ1Altp31rBYXFBOq6JiEZU7OZOGOhb3epsbduf322/n0pz99TPnKlSv505/+RHFxMVlZWZx33nlH\n93kdNmwYaWlpx9QPBoMApKWl0dzcHPO1Wuu0vm5PZWdnH3183XXX8Yc//IG5c+fyy1/+kpUrV3ao\nHw6HGTNmDGvWrOnxa4qIiIiIiPSlITPHFqBwaSGBrGPfcmBYgBM+cwLN+2InkN1x0UUX8fDDD1NX\nVwfAjh072LNnDwcPHmTs2LFkZWWxceNGXnzxxV6/ViznnHMOv//97wmHw1RVVcVMTI+ntraWyZMn\n09TUxPLly4+Wjxw5ktraWgBGjRrF9OnT+d3vfgdEkuq1a9f22XsQERERERHpriGV2OYuyqVoWRHB\naUEwCE4LcuIPTiTnkhxCO0O96vkEeM973sM111zDwoULedvb3saVV15JbW0tF198Mc3NzZx00knc\ndtttnH322X30jo51xRVXkJeXx5w5c/jIRz7CGWecwejRo7t8/re+9S3OOusszjnnHGbPnn20/Kqr\nruKee+7h9NNPp7S0lOXLl/PQQw8xd+5cTj75ZJ588slEvB0REREREZEusd4mc/1p/vz5vmrVqmPK\nNmzYwEknndTja3rYOfzmYTzkBAuCZI7P7G2YSVVXV8eIESOoqalhwYIF/OMf/2DSpEn9Hkdvvy8i\nIv3BzF519/nJjiOVxWqbRUREeqqnbfOgmmPbExYwgicEadjaQOPORjLGZWCB3s21TaZLL72UAwcO\n0NjYyNe+9rWkJLUiIiIiIiL9acgntgDp49IJ7AoQbgjTtLeJzImp22sba17tBz/4QbZu3XpM2d13\n3x1z1WMREREREZFUo8QWMDMyp2TSUNpA465GMnIysLTU7bVtr3WbIRERERERkcFoUCwe1RfzhNPH\npBPICuBNTmN1Yx9ENXSl0rxtERERERFJfSmf2A4bNoyamppeJ1NmRnBKZG/Yxt2NeIuSs55wd2pq\nahg2bFiyQxERERERkSEi5Yci5+XlUVlZSXV1dZ9cr7G2kXAoDFWAg6UZ6WPTSctO65PrDwXDhg0j\nLy8v2WGIiIiIiMgQkfKJbUZGBtOnT++z65UuL2X70u3HlAWyAhQtKyJ3UW6fvY6IiIiIiIj0jS4N\nRTazi82sxMy2mNltMY6bmd0XPf66mZ3R2blmdpqZvWhma8xslZkt6Ju31Dt7fr2nQ1m4PkzZkrIk\nRCMiIiIiIiKd6TSxNbM04H7gEmAOcLWZzWlX7RJgZvRrMfDTLpz7XeCb7n4a8PXo86QLbQt1q1xE\nRERERESSqys9tguALe5e5u6NwKPA5e3qXA78h0e8CIwxs8mdnOvAqOjj0cDOXr6XPhHMD3arXERE\nRERERJKrK4ntFKDtpNPKaFlX6hzv3C8A95jZduBe4PZYL25mi6NDlVf11QJRx1O4tJBA1rG3xYJG\n4dLChL+2iIiIiIiIdF8yt/v5DPBFd58KfBF4KFYld1/m7vPdff6ECRMSHlTuolyKlhURnPZWD23m\n5EwmXj0x4a8tIiIiIiIi3deVxHYHMLXN87xoWVfqHO/ca4HHo49/R2TY8oCQuyiXheULeUfdO8ic\nkkmoPMTuR3YnOywRERERERGJoSuJ7SvATDObbmaZwFXAinZ1VgAfi66OfDZw0N13dXLuTuCd0cfv\nAjb38r30ubTsNArvigxB3vrVrTTXNic5IhEREREREWmv08TW3ZuBm4BngA3Ab939TTO7wcxuiFZ7\nGigDtgA/B2483rnRc64Hvmdma4FvE1lNecDJvSaXUWePonF3I9u+vS3Z4YiIiIiIiEg75u7JjqHL\n5s+f76tWrer31z308iFWn7UayzQWrF/A8BOH93sMIiLS98zsVXefn+w4Ulmy2mYRERmceto2J3Px\nqJQxasEocj+aizc6pV8uTXY4IiIiIiIi0oYS2y4q/E4hgewAe5/Yy/6/7E92OCIiIt1mZhebWYmZ\nbTGz22IcH21m/21ma83sTTP7eDLiFBER6S4ltl0UnBJk2u3TANjyhS2Em8NJjkhERKTrzCwNuB+4\nBJgDXG1mc9pV+yyw3t3nAucRWQsjs18DFRER6QEltt2Q96U8gtOCHH79MLse3JXscERERLpjAbDF\n3cvcvRF4FLi8XR0HRpqZASOAfYC2BBARkQFPiW03pA1P48R7TwSg/GvlNB1oSnJEIiIiXTYF2N7m\neWW0rK0fAycR2ZJvHfB5d9cQJRERGfCU2HbThCsmMPrc0TTtbaLiXyuSHY6IiEhfughYA5wAnAb8\n2MxGta9kZovNbJWZraquru7vGEVERDpQYttNZsaMH84AoPIHlawMrKS4oJiq5VVJjkxEROS4dgBT\n2zzPi5a19XHgcY/YAmwFZre/kLsvc/f57j5/woQJCQtYRESkq5TY9kD9+npIjz5xCFWEKFlcouRW\nREQGsleAmWY2Pbog1FXAinZ1tgEXAJhZLlAElPVrlCIiIj2gxLYHypaUdVhKI1wfjpSLiIgMQO7e\nDNwEPANsAH7r7m+a2Q1mdkO02reAt5vZOuD/gFvdfW9yIhYREem69M6rSHuhbaFulYuIiAwE7v40\n8HS7sgfaPN4JvKe/4xIREekt9dj2QDA/2K1yERERERERSRwltj1QuLSQQFbHWzfpuklJiEZERERE\nRGRoU2LbA7mLcilaVkRwWhAM0kalAVD9WDXhRm33JyIiIiIi0p+U2PZQ7qJcFpYv5Lzwebx999sZ\nPmM49W/Ws+3ubckOTUREREREZEhRYtsH0oanMWvZLAAq/q2CwxsOJzkiERERERGRoUOJbR8Ze/5Y\nJn9qMt7obFq8CQ97skMSEREREREZEpTY9qHC7xaSkZvBwb8fZOeynckOR0REREREZEhQYtuHMsZm\nMPPHMwEo+0oZoR3a11ZERERERCTRlNj2sQlXTCDnshxaalvY9NlNuGtIsoiIiIiISCIpse1jZsbM\n+2eSNjKNmidr2Pv43mSHJCIiIiIiMqgpsU2AYXnDKLy7EIDNN22maX9TkiMSEREREREZvJTYJsgJ\nnz6BUeeMonF3I8X5xawMrKS4oJiq5VXJDk1ERERERGRQUWKbIBYwci7LASBcFwaHUEWIksUlSm5F\nRERERET6kBLbBNr5k45b/oTrw5QtKUtCNCIiIiIiIoOTEtsECm2Lvd1PvHIRERERERHpPiW2CRTM\nD3arXERERERERLpPiW0CFS4tJJDV8RbnfSkvCdGIiIiIiIgMTkpsEyh3US5Fy4oITguCQWB45HbX\nPFGDt3iSoxMRERERERkclNgmWO6iXBaWL+S88HmcXX42GRMzOLDyANu/tz3ZoYmIiIiIiAwKSmz7\nUebETGb/cjYAW+/YSu2rtUmOSEREREREJPUpse1nOZfkMOXmKXiTs37ReloOtyQ7JBERERERkZTW\npcTWzC42sxIz22Jmt8U4bmZ2X/T462Z2RlfONbObzWyjmb1pZt/t/dtJDYV3F5J1chZHSo6w5ZYt\nyQ5HREREREQkpXWa2JpZGnA/cAkwB7jazOa0q3YJMDP6tRj4aWfnmtn5wOXAXHc/Gbi3L95QKkgb\nnsac38zBMo1dP9vF3if3JjskERERERGRlNWVHtsFwBZ3L3P3RuBRIglpW5cD/+ERLwJjzGxyJ+d+\nBrjL3UMA7r6nD95Pyhhx6ggK7yoEYOMnNxLaFUpyRCIiIiIiIqmpK4ntFKDtEr6V0bKu1DneubOA\nd5jZS2b2VzM7szuBDwZ5n89j7IVjaa5p5qXCl1gZWElxQTFVy6uSHZqIiIiIiEjKSObiUenAOOBs\n4MvAb83M2lcys8VmtsrMVlVXV/d3jAllASPnshwAwg1hcAhVhChZXKLkVkREREREpIu6ktjuAKa2\neZ4XLetKneOdWwk8Hh2+/DIQBsa3f3F3X+bu8919/oQJE7oQbmrZfm/H/WzD9WHKlpQlIRoRERER\nEZHU05XE9hVgpplNN7NM4CpgRbs6K4CPRVdHPhs46O67Ojn3D8D5AGY2C8gEhtwqSqFtsefWxisX\nERERERGRY6V3VsHdm83sJuAZIA142N3fNLMboscfAJ4G3gtsAeqBjx/v3OilHwYeNrM3gEbgWnf3\nPn13KSCYHyRU0TGJDU4NJiEaERERERGR1NNpYgvg7k8TSV7blj3Q5rEDn+3qudHyRuAj3Ql2MCpc\nWkjJ4hLC9eFjyrNPy05SRCIiIiIiIqklmYtHCZC7KJeiZUUEpwXBIGNiBhjsW7GP6scH12JZIiKS\nXGZ2sZmVmNkWM7stTp3zzGyNmb1pZn/t7xhFRER6oks9tpJYuYtyyV2Ue/T59u9vp/SWUjZeu5Gs\nOVlkz1bvrYiI9I6ZpQH3AxcSWcDxFTNb4e7r29QZA/wEuNjdt5nZxOREKyIi0j3qsR2A8r6Yx4R/\nmUBLXQtvfvBNmg81JzskERFJfQuALe5eFp0O9Chwebs61xDZsWAbgLvv6ecYRUREekSJ7QBkZsx+\naDbZp2RTv7GejddtZAiuqyUiIn1rCtB2j7nKaFlbs4CxZrbSzF41s4/1W3QiIiK9oMR2gErLTuPk\nx08mbXQae5/Yy7a7tyU7JBERGfzSgXnA+4CLgK9Ft+Q7hpktNrNVZraqulrrQYiISPIpsR3AsmZm\ncdKvTwJg65Kt7Ht2X5IjEhGRFLYDmNrmeV60rK1K4Bl3P+zue4HngbntL+Tuy9x9vrvPnzBhQsIC\nFhER6SoltgPc+EvHM+3OaRCGdR9axwt5L7AysJLigmKqllclOzwREUkdrwAzzWy6mWUCVwEr2tV5\nEvgnM0s3syzgLGBDP8cpIiLSbUpsU0DB1wvInpuNH3YadzSCQ6giRMniEiW3IiLSJe7ezP9v796j\n4y7PA49/n5HksY0NNtjIxiALBUdAuYV1IKHphpSTBiinblI2B+INCUmOSxJIt9tzIA2b28l6yzZt\nt0lIoA5JcykNTUtIaUpCkrYupRH3gA0YI8f4gsGyje+WGUuad/+YkRH2jDSyZM2M9P2c42PN7zJ6\n5tXlN4/e9/c8cD1wP4Vk9fsppWci4rqIuK54zCrgJ8AK4BHgjpTS09WKWZKkStnupw5EJuh95fDK\nyPnuPGtvXvu6VkGSJJWTUroPuO+Qbbcf8viLwBfHMi5JkkbKGds6kduUK719Q+ntkiT35Y7wAAAf\nMklEQVRJkjRRmNjWiWxLdljbJUmSJGmiMLGtE21L28hMPfzLNftKq1FKkiRJmthMbOtE8+Jm2pe1\nk52fhYCG4xoAeOm2l9j92O4qRydJkiRJ1WNiW0eaFzfz1nVv5eL8xbxtx9tovqaZfHeelVesZP+6\n/dUOT5IkSZKqwsS2TkUE7V9vZ8YlM+jp6mHlZSvp2d5T7bAkSZIkacyZ2NaxzKQMZ919FsecdQzd\nz3Xz9LufJp/LVzssSZIkSRpTJrZ1rvG4Rs6+72wmzZvErgd2seoDq0j5VO2wJEmSJGnMmNiOA5NP\nmcw5/3wODdMb2Pp3W3lw5oMszyyno7WDrju7qh2eJEmSJB1VJrbjxLRzp3HSx04CoG93HyTIrc+x\neslqk1tJkiRJ45qJ7Tiy5a4th23Ld+dZe/PaKkQjSZIkSWPDxHYcyW3IDWu7JEmSJI0HJrbjSLYl\nW3J706ymMY5EkiRJksaOie040ra0jczUw7+kPa/08Mp9r1QhIkmSJEk6+kxsx5Hmxc20L2snOz8L\nUZjBnfGuGZCHp9/zNNt/tr3aIUqSJEnSqGusdgAaXc2Lm2le3HzwcUqJzo938tJtL/H0oqc5+76z\nmXnxzCpGKEmSJEmjyxnbcS4iWHDrAuZ8eA75/XlWXrGSXf+5q9phSZIkSdKoccZ2AohM0L6snXQg\n0fXdLlZctoKT/+hkNv/1ZnIbcmRbsrQtbXvdTK8kSZIk1QsT2wkiMsHpf306qSex5a4trP/c+oP7\ncutzrF6yGsDkVpIkSVLdcSnyBBINwenfOZ3MlMO/7PnuPGtvXluFqCRJkiRpZExsJ5hMU4b8q/mS\n+3IbcmMcjSRJkiSNnIntBJRtyQ5ruyRJkiTVMhPbCahtaRuZqYd/6U+4/IQqRCNJkiRJI1NRYhsR\nl0bE6ohYExGfLLE/IuLLxf0rIuL8YZz7RxGRImLWyF6KKtW8uJn2Ze1k52choOHYBgBeuu0lXrz1\nxSpHJ0mSJEnDM2RV5IhoAL4KvBN4EXg0Iu5NKT074LDLgAXFfxcCtwEXDnVuRJwC/BawYfRekirR\nvLj5dRWQN/zpBtbetJY1N6yh95Ve5n9mPhFRxQglSZIkqTKVzNheAKxJKa1NKR0A7gIWHXLMIuA7\nqeAhYEZEzK3g3P8H3Aikkb4QjUzLjS2039EOGVj3uXWs+cQaUt4viyRJkqTaV0kf23nAxgGPX6Qw\nKzvUMfMGOzciFgGbUkpPOTNYG+Z+eC6NMxt59upn2XTrJnY/vpsDmw6Q25gj25KlbWmbfW4lSZIk\n1ZyqFI+KiKnAp4DPVHDskoh4LCIe27p169EPboKb/Z7ZnHPfOUQ22NOxp9ACKEFufY7VS1bTdWdX\ntUOUJEmSpNepJLHdBJwy4PHJxW2VHFNu+xuAU4GnImJdcfsTETHn0E+eUlqWUlqYUlo4e/bsCsLV\nSM28ZCaNMw+fzM9351l789oqRCRJkiRJ5VWS2D4KLIiIUyNiEnAVcO8hx9wLXFOsjvwWYFdK6eVy\n56aUVqaUTkwptaaUWiksUT4/pbR5tF6YRqanq6fk9tyG3BhHIkmSJEmDG/Ie25RSb0RcD9wPNADf\nTCk9ExHXFfffDtwHXA6sAbqBawc796i8Eo2qbEuW3PrDk9imWU1ViEaSJEmSyqvoHtuU0n0ppTem\nlN6QUlpa3HZ7MamlWA3548X9Z6eUHhvs3BLP35pS2jYaL0ijo21pG5mph3979GztYcMXN5CSFZMl\nqd4M1Vt+wHFvjojeiLhyLOOTJOlIVaV4lGpf8+Jm2pe1k52fhSjM4M76vVkArL1xLas/tJp8Ll/l\nKCVJlRrQW/4y4Ezg6og4s8xx/xf46dhGKEnSkauk3Y8mqObFzYe199l691ZWXbOKzd/aTHdnN2f9\n4CwmnTipShFKkobhYG95gIjo7y3/7CHH3QDcDbx5bMOTJOnIOWOrYZn9e7N504NvIntylt3/uZvH\nL3ic9X+yno7WDpZnltPR2mFLIEmqTeV6zh8UEfOAdwO3jWFckiSNmImthm36m6Zz/iPnM/3C6eTW\n53jhUy8UCk3Z71aS6t1fAjellAa918Qe85KkWmNiqyOSnZvlvOXnlSwwZb9bSapJlfSlXwjcVewx\nfyXwtYj43UOfyB7zkqRa4z22OmINkxvI7y/9R3373UpSzTnYW55CQnsV8L6BB6SUTu3/OCK+Bfwo\npfTDsQxSkqQj4YytRiTbki25vWm2/W4lqZaklHqB/t7yq4Dv9/el7+9NL0lSvTKx1YiU7Xe7pYcX\nPvcCqc9+t5JUK4bqS3/IsR9MKf3D2EcpSdLwmdhqREr1uz3hPSdAwPrPr+epdz5FbrPLkiVJkiQd\nPd5jqxEr1e92x7/s4NnFz7Lz33by2LmPMedDc9jyvS3kNuTItmRpW9p22DmSJEmSdCScsdVRMfOS\nmSx8ciEzfnMGPVt62HjLRlsCSZIkSToqTGx11GTnZDn3p+fScFzDYftsCSRJkiRptJjY6qiKhqBv\nd1/JfbYEkiRJkjQaTGx11JVrCZTJZnh1w6tjHI0kSZKk8cbEVkdduZZA+VfzPHrWo7z8jZdJybZA\nkiRJko6Mia2OusNaAs3PsuCrC5j17ln07elj9UdWs/LylWz8ykY6WjtYnllOR2uHxaUkSZIkVcR2\nPxoTpVoCnfTRk9jyvS10Xt/J9p9sZ/tPth/c1185uf9cSZIkSSrHGVtVTUTQ/L5m3vzMm8lMKbFU\n2crJkiRJkipgYquqy87Nkn81X3KflZMlSZIkDcXEVjWhXOXkaAx2PrBzjKORJEmSVE9MbFUTSlZO\nDkg9iSff/iSrrlnFga4D1QlOkiRJUk2zeJRqQn+BqLU3ryW3IUe2JUvrZ1t5df2rbLhlA13f7WLb\nvdtoW9pGw7ENvPDpFw4e17a0zQJTkiRJ0gRmYquaUapyMsCc98+h84ZOtv94O53Xd0IAxba3Vk+W\nJEmS5FJk1bwpb5jC2f98Nr/2g1+DBg4mtf2snixJkiRNbCa2qgsRwex3z4bSxZOtnixJkiRNYCa2\nqivlqicT8OKtL5I/UCbzlSRJkjRumdiqrpSsnpwB8rDmhjU8+muPsuXvt7D5zs10tHawPLOcjtYO\nuu7sqkq8kiRJko4+i0eprpSqnnzq0lNpnNbIr276FftX7+fZ9z57MNkFC0xJkiRJ452JrepOuerJ\nx//28Wz+xmae/9jzh92L219gysRWkiRJGn9ciqxxI9OY4aTfP+mwqsn9LDAlSZIkjU8mthp3yhaY\nSrDi8hXs6tg1tgFJkiRJOqoqSmwj4tKIWB0RayLikyX2R0R8ubh/RUScP9S5EfHFiHiuePw9ETFj\ndF6SJrpSBaaiMWASbP/xdn550S956p1PsfOBnXTd2WWRKUmSJKnODZnYRkQD8FXgMuBM4OqIOPOQ\nwy4DFhT/LQFuq+DcnwFnpZTOAZ4H/njEr0aicA9u+7J2svOzEJCdn+X0b53ORZsuouXmFhqmN7Dj\n5zt48u1PsuqaVeTW5yC9VmTK5FaSJEmqL5UUj7oAWJNSWgsQEXcBi4BnBxyzCPhOSikBD0XEjIiY\nC7SWOzel9NMB5z8EXDnSFyP1K1dgqu1/t3HKH53Cpi9vYt3n11lkSpIkSRoHKlmKPA/YOODxi8Vt\nlRxTybkAHwJ+XEEs0og1zWyi9bOtZffn1ufo2dkzdgFJkiRJGpGqF4+KiJuBXuDOMvuXRMRjEfHY\n1q1bxzY4jWtli0wBHSd30PmJTvb/ar/34UqSJEk1rpLEdhNwyoDHJxe3VXLMoOdGxAeBK4DFxWXM\nh0kpLUspLUwpLZw9e3YF4UqVKVlkKhtMOXMK+X15Nn1lEw+f9jCrPuB9uJIkSVItqySxfRRYEBGn\nRsQk4Crg3kOOuRe4plgd+S3ArpTSy4OdGxGXAjcCv5NS6h6l1yNVrGSRqW+czoXPXMjCpxYy59o5\nhQP7Xn9e/324kiRJkmrDkMWjUkq9EXE9cD/QAHwzpfRMRFxX3H87cB9wObAG6AauHezc4lPfCmSB\nn0UEwEMppetG88VJQylXZGraOdM4/Zuns/lbm6HEWoLc+hy7H93N9IXTKX7/SpIkSaqSSqoik1K6\nj0LyOnDb7QM+TsDHKz23uP20YUUqVUG2JVtYhlzCExc8wbTzpjF3yVya39fMKz96hbU3ryW3IUe2\nJUvb0jarK0uqKcXVUl+i8MfmO1JKtxyyfzFwExDAHuCjKaWnxjxQSZKGqerFo6RaVuo+3MzkDDMv\nm0njCY3sfXIvnR/r5MHZD3ovrqSaVmFf+heAt6eUzga+ACwb2yglSToyJrbSIErdh9t+Rzvn3ncu\nF226iDO+dwYz3jEDevBeXEm17mBf+pTSAaC/t/xBKaVfpJR2FB8+RKHooyRJNa+ipcjSRFbuPtxM\nNkPzVc00X9XM8szysvfibrp9Eyf+txNpOqGJrju7XK4sqVpK9Za/cJDjP0yZHvMRsQRYAtDS0jJa\n8UmSdMRMbKVRMNi9uJ0f7WTNJ9Yw9eypdD/dTTpQyID7lysDJreSakpEvINCYvu2UvtTSssoLlNe\nuHBhyXZ9kiSNJZciS6Og5L24UzLMvW4uM39rJqkvse+JfQeT2n4uV5Y0hirpS09EnAPcASxKKb0y\nRrFJkjQiJrbSKCh5L+7X22m/rZ1z7z+Xt256a9lzc+tzbL17K337XrtJt+vOLjpaO1ieWU5Ha4dF\nqCSNhiH70kdEC/AD4P0ppeerEKMkSUfEpcjSKCl3Ly5Adk6W7Pzyy5WfufIZMlMyHH/p8TQ1N9H1\n7S7y+/OAS5YljY4K+9J/BjgB+FqxR3dvSmlhtWKWJKlSJrbSGGlb2sbqJavJd+cPbstMznDC755A\nbl2O3Q/tZts920qe279k2cRW0khU0Jf+I8BHxjouSZJGysRWGiP9SWm5qsi5TTm23rOVNTesKXl+\nbn2OHct3cNyvH0emKWOFZUmSJKnIxFYaQ4MuV56X5eTrT2bjn20su2T5qXc8RcOxDUw9fSp7n9xr\nhWVJkiQJi0dJNadUheXIBsdffjxTz5xK3+4+9jyyp3SF5T+2wrIkSZImHmdspRoz1JLl/ev28/Cp\nD5c8N7cxxy9/45fMuGQGMy+ZybEXHsvWv9/qkmVJkiSNaya2Ug0abMnylNYpg1ZY3vXgLnY9uIv1\nn18PTUAfUKxX5ZJlSZIkjUcuRZbqUKnlypmpGRYsW8BZPzyLeTfMY+qZU6GHg0ltv3x3nuc//jzb\nf7qd3t29gH1zJUmSVN+csZXq0FDLlWctmgXA8sxySIef37erjxXvWgEZyJ6S5cCmA6ReC1FJkiSp\nPpnYSnVqsOXK/bItpZcsN0xvYOqZU9n7+N6S+/PdeZ6//nkmzZ3E9IXTaTy28KvCFkOSJEmqRSa2\n0jjWtrSN1UtWk+9+bT1yZmqGN972RpoXN9PX3cd/TPuP0rO6O/t46pKnIGDq6VNpnNXInof2kHqc\n2ZUkSVJt8R5baRxrXtxM+7J2svOzEJCdn6V9WfvBRLRhagPZlmzJcxumNzD9gulEU9C9qpvd/7H7\nYFLbL9+dp/OGTnb++056dvYA3q8rSZKkseeMrTTODbVkeahZ3Xwuz94Ve3nigidKnt+7o5cnL34S\ngMZZjfTu6C1UYqb8rK5LmiVJkjSaTGylCW6oQlSZbIZj33xs2RZDmWkZjjnjGPat3Efvtt7D9ue7\n8zz3oefY8fMdHHPWMRzYcoBNX9lEfn8hkXZJsyRJkkbKxFZSRYWoys3stt9eWNqc783zwKQHSt6v\nmw4kNn9rc9nnznfnWfOHa5h+wXQmnzqZTGPhLglndiVJklQJE1tJFRlyZrcxU7YKc1NzE62fbWXf\n0/t46WsvlXz+nq09PPLGR4imYMppU8hMzbDvqX1DtiEy+ZUkSZKJraSKHen9uqf9+WkHz3vln18p\nvaR5coam2U3kNuboXtVd8vnz3Xmeu/Y5tt6zlSlvmELPKz10/U0XKTd0pWYTYEmSpPHLxFbSqBlq\nVhcGWdJcrNbct6+P7s5uHn/T4yU/R+pJbLt7W9kY8t15Vl+3mlfXvcrk1slMbp3Mnl/uYe1Naw9+\nTmd/JUmSxhcTW0mjaqhZ3aGS34ZjGph+3vSyxaqa5jRx2l+cxv41+1n3mXUlP0d+b54X/tcLg8aZ\n787T+YlOGmc0kj0ly66HdvGrP/zVkMkvmABLkiTVGhNbSWNuJMWqTvuz02i+unDuy994uWTy23h8\nI3M/MpdX173Kq+teZc8je0p+jt7tvay8YmXZGPr79GYmZ5h00iQmzZ3Ezn/fSefHOp39lSRJqiEm\ntpJq0kiWNS/48oLXHdfR2lH6vt5jMhx30XHkXix/X2/vjl6eufKZQWPNd+fpvL6TlE9Map7Ensf3\nsP4L6ytqaWQCLEmSNHImtpJq1kiXNfcre1/vX7UfPLZjfge5DYcnvw3TGphxyQwOvHSA3Es5Dmw6\nUDKW3p29PHfNc2VjzXfnWb1kNbt+sYum2U1Mmj2Jfc/t4+Wvvzxk8atKk1+TZEmSNFFFSiWaTtao\nhQsXpscee6zaYUiqQ0MlfV13dg1a1Kpf2QR4egMnXHECB7oOsPNfd44s2AaYds40mk5oondvL3sf\n23uw7RFAZINTbjyFE997Ik0zm2ic2cjWe7by/JLnh4y/kuR3IiXIEfF4SmlhteOoZ16bJUmj6Uiv\nzSa2klRUadI3VAJcbulz46xGWj/dSs/WHg5sPcDLf/Xy0X1BQGZahnnXzaNxRiP7nt/H1ru2kg68\n9ns/MznDG/7iDcy5dg6ZbIYtf7ulogQfRjdJrlYybWI7cl6bJUmjycRWksbIqM3+lkmAJ500ibN+\neBa923tZcemKsnFMPWMqvTt66dnRc3A580hEU5D6EuQP35c5JsPcD82lYXoDDdMa2Pfc4UlyTA5O\n/cKpNF/dTOaYDNvu3UbnRzsrmkWuZLyORvJrYjtyXpslSaPJxFaSasjRnv3Nzs/y1nVvPfi4o6WD\n3MbSFaJbbmyhd1cvG/5kQ9l4oylIPWNzPYhsMOM3ZpCZmqFhagPb/mkb+X2HZ9ONxzdy2pdOo2FK\nA7se2sWmr2x6XQJfbiZ5WLGY2I6Y12ZJ0mg60mtzRcWjIuJS4EtAA3BHSumWQ/ZHcf/lQDfwwZTS\nE4OdGxHHA38HtALrgPemlHYM9wVIUi2qpKXRSCo/ty1te91ztf3J0BWiu/62a9AkOZ/L8/BpD5N7\nsXSCPP/T8+nb20ffnj42/unGsq9r0txJ9O3ro293X8n9KZfY8fOhf933bu/lufcPXpBr7c1rx+39\nv5IkqXJDJrYR0QB8FXgn8CLwaETcm1J6dsBhlwELiv8uBG4DLhzi3E8C/5JSuiUiPll8fNPovTRJ\nqn2jVfl5NJLkTDZD2y2VtVDa8ndbhpxJLldoq6m5iTO+ewb57jx93X10Xt9J7/bew47LHJNh1qJZ\n5Pfn2XbPtpLjU+r5JUnSxFPJjO0FwJqU0lqAiLgLWAQMTGwXAd9JhXXND0XEjIiYS2E2tty5i4CL\ni+d/G1iOia0kHaaS2d9Kjqsk+R1pC6WBM8lt/6f0Maf9+Wkc/87jX3uyPEO3Yyq3JLslO+iYSJKk\niaGSxHYeMHDN2YsUZmWHOmbeEOc2p5T6S4JuBlxLJklHWaVLpEdjGfVYzjZLkqSJraJ7bI+2lFKK\niJJVSyJiCbAEoKWlZUzjkiSVN1pJciXHVZokS5KkiamSxHYTcMqAxycXt1VyTNMg53ZFxNyU0svF\nZctbSn3ylNIyYBkUKi9WEK8kaRyqNEmWJEkTT6aCYx4FFkTEqRExCbgKuPeQY+4FromCtwC7isuM\nBzv3XuADxY8/APzjCF+LJEkaRERcGhGrI2JNsXDjofsjIr5c3L8iIs6vRpySJA3XkDO2KaXeiLge\nuJ9Cy55vppSeiYjrivtvB+6j0OpnDYV2P9cOdm7xqW8Bvh8RHwbWA+8d1VcmSZIOGkmXg7GOVZKk\n4aroHtuU0n0UkteB224f8HECPl7pucXtrwCXDCdYSZJ0xI64y8GAYo+SJNWkSpYiS5Kk+leug8Fw\nj5EkqebURFXkSj3++OPbImL9EIfNAraNRTxHQT3HDvUdv7FXRz3HDvUdv7EXzB+l55lQBnYsAHIR\n8XQ14xkH6vnnsVY4hiPnGI4Ox3Hk2o/kpLpKbFNKs4c6JiIeSyktHIt4Rls9xw71Hb+xV0c9xw71\nHb+xT0gj6XLwOgM7Fvj1GDnHcOQcw5FzDEeH4zhyEfHYkZznUmRJkiaGkXQ5kCSpptXVjK0kSToy\nI+lyIElSrRuPie2yagcwAvUcO9R3/MZeHfUcO9R3/MY+AY2ky8Eg/HqMnGM4co7hyDmGo8NxHLkj\nGsMoXMMkSZIkSapP3mMrSZIkSapr4yaxjYhLI2J1RKyJiE9WO57hioh1EbEyIp480kpgYyUivhkR\nWwa2d4iI4yPiZxHRWfx/ZjVjHEyZ+D8XEZuK4/9kRFxezRjLiYhTIuLfIuLZiHgmIv6guL3mx3+Q\n2Gt+7CNickQ8EhFPFWP/fHF7PYx7udhrftz7RURDRPwyIn5UfFzz4z4eDXWdLRac+nJx/4qIOL8a\ncdayCsZwcXHsVkbELyLi3GrEWcsqfb8XEW+OiN6IuHIs46sHlYxhRFxcvDY8ExH/PtYx1roKfpaP\ni4h/GnDttV7BIUq9Hz9k/7CvKeNiKXJENADPA++k0Ez+UeDqlNKzVQ1sGCJiHbAwpVTzfa8i4r8C\ne4HvpJTOKm77U2B7SumW4g/4zJTSTdWMs5wy8X8O2JtS+rNqxjaUiJgLzE0pPRER04HHgd8FPkiN\nj/8gsb+XGh/7iAjgmJTS3ohoAh4E/gB4D7U/7uViv5QaH/d+EfE/gYXAsSmlK+rp9814Ucl1tvjH\nkRsoFJ+6EPhSSunCKoRbkyocw4uAVSmlHRFxGfA5x/A1lb7fKx73M+BVCkXS/mGsY61VFX4fzgB+\nAVyaUtoQESemlLZUJeAaVOEYfgo4LqV0U0TMBlYDc1JKB6oRcy0q9X78kP3DvqaMlxnbC4A1KaW1\nxW+Yu4BFVY5p3EopPQBsP2TzIuDbxY+/TSFhqUll4q8LKaWXU0pPFD/eA6wC5lEH4z9I7DUvFewt\nPmwq/kvUx7iXi70uRMTJwG8DdwzYXPPjPg5Vcp1dROENSkopPQTMKP5BSwVDjmFK6RcppR3Fhw9R\n6COs11T6fu8G4G7AZOxwlYzh+4AfpJQ2AJjUHqaSMUzA9OIfl6dReN/ZO7Zh1rYK3o8P+5oyXhLb\necDGAY9fpE7eMA+QgJ9HxOMRsaTawRyB5gG9DjcDzdUM5gjdUFzq8M16WNoYEa3Am4CHqbPxPyR2\nqIOxLy6HfZLCG6WfpZTqZtzLxA51MO7AXwI3AvkB2+pi3MeZSq6z4+FafDQNd3w+DPz4qEZUf4Yc\nw4iYB7wbuG0M46onlXwfvhGYGRHLi+9Lrxmz6OpDJWN4K3AG8BKwEviDlFIeDcewrynjJbEdD96W\nUjoPuAz4eHF6vi4V20XUzYxQ0W1AG3Ae8DLw59UNZ3ARMY3CX6P/R0pp98B9tT7+JWKvi7FPKfUV\nf0ZPBi6IiLMO2V+z414m9pof94i4AtiSUnq83DG1PO7SkYqId1BIbF1iP3x/CdxkEjEijcB/obBa\n5l3ApyPijdUNqe68C3gSOInCdfbWiDi2uiGNf+Mlsd0EnDLg8cnFbXUjpbSp+P8W4B4KyxzqSVf/\n8oDi/3W1bCWl1FV8858Hvk4Nj3/xPsm7gTtTSj8obq6L8S8Vez2NPUBKaSfwbxTuUa2Lce83MPY6\nGfdfB36nWIPgLuA3I+JvqLNxHycquc7W/bX4KKtofCLiHApL7xellF4Zo9jqRSVjuBC4q/h740rg\naxHh7QqvqWQMXwTuTyntK9Z+eQCwkNlrKhnDayks504ppTXAC8DpYxTfeDHsa8p4SWwfBRZExKkR\nMQm4Cri3yjFVLCKOKRbTISKOAX4LKFkhrIbdC3yg+PEHgH+sYizDdsia/XdTo+NfvFfjGxSKi/zF\ngF01P/7lYq+HsY+I2cViGkTEFAoFI56jPsa9ZOz1MO4ppT9OKZ2cUmql8Hv9X1NK/506GPdxqJLr\n7L3ANcVKlm8Bdg1YMq4KxjAiWoAfAO9PKT1fhRhr3ZBjmFI6NaXUWvy98Q/Ax1JKPxz7UGtWJT/L\n/wi8LSIaI2IqhcI9q8Y4zlpWyRhuAC4BiIhmoB1YO6ZR1r9hX1Maxyauoyul1BsR1wP3Aw0UKuA9\nU+WwhqMZuKfwvp9G4G9TSj+pbkjlRcT3gIuBWRHxIvBZ4Bbg+xHxYWA9hUq3NalM/BdHxHkUljSu\nA36/agEO7teB9wMri/dMAnyK+hj/crFfXQdjPxf4drESYgb4fkrpRxHRQe2Pe7nYv1sH415OPXy/\njyvlrrMRcV1x/+3AfRSqV64BuinMWKiowjH8DHAChVlGgN6U0sJqxVxrKhxDDaKSMUwprYqInwAr\nKNQ3uCOlVHN//KyWCr8PvwB8KyJWAkFheXzNdz4ZS2XejzfBkV9TxkW7H0mSJEnSxDVeliJLkiRJ\nkiYoE1tJkiRJUl0zsZUkSZIk1TUTW0mSJElSXTOxlSRJkiTVNRNbSZIkSVJdM7GVJEmSJNU1E1tJ\nkiRJUl37/204AEO4mlpBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17f8a2b44a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_training_progress(plot_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEYCAYAAABsuVKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSSjYiEgSCjYikoSCjYgkoWAjIkn8f+e0E0lbrqeIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17f8cce9860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct class: cat\n",
      "Predictions: ['frog' 'bird' 'dog'] [  9.99975562e-01   1.84183336e-05   2.21035884e-06]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEYCAYAAABsuVKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIJhsiCgIJhsiCoLJhoiCYLIhoiD+D6WU8QAqI1uLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17f8c865320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct class: truck\n",
      "Predictions: ['dog' 'cat' 'bird'] [ 0.4655984   0.23260756  0.18337156]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEYCAYAAABsuVKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17f8a0d1cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct class: automobile\n",
      "Predictions: ['truck' 'horse' 'cat'] [  9.99833584e-01   1.34933565e-04   1.82403510e-05]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEYCAYAAABsuVKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17f8c7caa90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct class: truck\n",
      "Predictions: ['bird' 'frog' 'airplane'] [  9.95070934e-01   3.39995022e-03   5.27196738e-04]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEYCAYAAABsuVKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyleZHJTLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17f8bbb1ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct class: ship\n",
      "Predictions: ['dog' 'horse' 'cat'] [ 0.89723867  0.07807672  0.01602577]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEYCAYAAABsuVKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17f8cd0ca20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct class: ship\n",
      "Predictions: ['automobile' 'truck' 'airplane'] [  9.75293100e-01   2.37451978e-02   9.44912841e-04]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEYCAYAAABsuVKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17f8bbb8710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct class: automobile\n",
      "Predictions: ['dog' 'cat' 'horse'] [ 0.7094292   0.23683818  0.04895716]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEYCAYAAABsuVKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZKFgIyJZ/B8WoyFjLyYD2AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17f8bea9e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct class: ship\n",
      "Predictions: ['airplane' 'bird' 'dog'] [ 0.9796896   0.0096837   0.00739191]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEYCAYAAABsuVKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17f8bcbc550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct class: ship\n",
      "Predictions: ['airplane' 'truck' 'automobile'] [ 0.98922658  0.00726318  0.00324466]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEYCAYAAABsuVKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAH6dJREFUeJzt3WuMpGeVH/D/qXt1VV+mey60x8ZmHGOWtdhxMuslASE2\nLCtjrQLkg7V8QI6EZBQRBNFKCdpIgXxD0cJqP0RIJljrjQgLCiBQhBKBhYSQCMvYGOPLgg2ML+N2\nz7WvVV2Xt04+dFkazJz/UzPT83S7/f9Jo5mpp5+qp956+/Tb9Zw6x9wdIiLXW2m3FyAirw8KNiKS\nhYKNiGShYCMiWSjYiEgWCjYikoWCjYhkoWAjIlko2IhIFpWcD9ZuTfv8/EI4Xq3V47Fqjd53qczj\npsPCMTM+t5y4b4vvGgDP0PbRiI73B30+n2SA12r8mNFlAxiR+y7xJ03XBQCD4TB+3BGfayX+2EUR\nH1NLrLta4d8S9Fxxft+p5zVIvNbFKD5m5VKZzi2V+Dk8IufhcBA/LgA8//yz59z9EP0iXGOwMbO7\nAfwNgDKA/+7un2VfPz+/gP/w7z8djh+68Vg4tnjjUbqW9uw0He8P4xOhWpuic+dmW3ScxEiYDejc\nbmeTjr+09BKf3+2GY8eO3UznWiLc9Pvxyd9IBLL+gD/v5eVz4Vh3i3/Tlev8sdc2NsKxWrlK5x46\ndJiOV8uNcMycnAgANjf4MVl6+Tk6vrZ+Nhybm5mnc5vNJh3vbMTn4YWz8eMCwL/96L/iCx+76l+j\nzKwM4L8BeB+AtwL4kJm99WrvT0T2t2t5z+YuAM+6+6/dvQ/g7wG8f2eWJSL7zbUEm6MAXrjk/y+O\nb/stZna/mZ00s5Mbm/HlrYjsb9d9N8rdH3D3E+5+ot1qX++HE5E96lqCzWkAN13y/xvHt4mI/I5r\nCTY/AXCbmb3JzGoA/hzAt3dmWSKy31z11re7D83s3wH4v9je+n7Q3Z9kcwxAieQasP38F57nF00H\nZvj29K3HbgrHDi3wrdTO5jodH20V4djMAb6uI4fn6PitNxyg4zx94lqrMLKtcZ4flPJ7b1q8pvlX\nK36lxuM8pQQk0wBrq3xyp8LHF9q38vmdeFt++UycSgAAZ15aouOwOE+n1prhcyd0TXk27v4dAN/Z\nkZWIyL6mjyuISBYKNiKShYKNiGShYCMiWSjYiEgWWUtMlKpNTB+NP6vZmJ0Nx2YTW9uNKn8qZy/E\ne5Zbm/zTuPUa//j++nq8Nd7t8s3W+QW+9d1qJ0oisF37VA2JBFYmot/j2+qpkgeVWnxcRkVqyz71\nxOLxwZDfd2rr20kZiZLx+x4V/FxYX+MpFhfXzodj3f4Wnducjr+3AADl+BPrXuHfe5PSlY2IZKFg\nIyJZKNiISBYKNiKShYKNiGShYCMiWSjYiEgWWfNsytUqZm6IuyQcnItzTmaaiVYuiRYdxTCu2D8i\nrT8AYFTlMfnGY3Fl+6lGIt+kwsf7fZ6bsUlyhKoV3kmgWuHPq09beCTmOj+mFdJAoSj4fXc7/JjQ\ndi2JFJ5kGxmSS+Op51zi4zOzPOdqZiHulpI4TbDZ7dHxzlrcXWFlo8PvfEK6shGRLBRsRCQLBRsR\nyULBRkSyULARkSwUbEQkCwUbEckia55NpVLBoYML4XijFOeFjIY8R6FINOkYkloiqYhbMv4Vw36c\ne7HR4zkKnsjxced5OGVyzLzGc4+swXNKanXS3qPOj8n6Oul5AmBtLR5rTjXo3GaivEp/K35exYAf\nk+EgcR4N47ymQZ+/1oNEnR6r8efd78TJSZ0eSVwC0E/U6ak3muHYofoUnzwhXdmISBYKNiKShYKN\niGShYCMiWSjYiEgWCjYikkXWrW8zQ7Uab6eWSPuPcqJ9R6XMx30UbxFvdfjH7/s9Pj7ciLcdi378\n0X0AqNfiFhoAUGvybUdrxj8vSmRbHAB6iTYzPdKuJdVupd3m27iVUvzYZjwdYGqa/4zsbMTnQmeN\nP+etLm/r0+3E7VZGiQ4zBXgaw+aFi3R8ays+l4pEekbqsfvDOBdhtENh4pruxcxOAVgHUAAYuvuJ\nnViUiOw/OxGy/tjdz+3A/YjIPqb3bEQki2sNNg7ge2b2iJndf7kvMLP7zeykmZ1cuXj2Gh9ORF6r\nrjXYvNPdjwN4H4CPmdm7Xv0F7v6Au59w9xNzB+IaqiKyv11TsHH30+O/zwD4JoC7dmJRIrL/XHWw\nMbOWmU2/8m8AfwrgiZ1amIjsL9eyG3UEwDfHbTMqAP6nu/8fNmEwGOD06aVwvEpyaaaqPGekXOaf\noa+VST4KeF5Ht8/LJVTIusuJFjMl5/G+THKPAKBHSh50N3iOT39zi47XpuIcn80N3lpn7gDPH2LP\nq5+qh5D4ETkgrWAGpNQIAFxcW6HjRvJZWnMH6NytIc9NKnX5uHv8em5urNK5Gz1+DvfJcSlIjtqV\nuOpg4+6/BvAHO7IKEdn3tPUtIlko2IhIFgo2IpKFgo2IZKFgIyJZKNiISBZZ69kMBn0sLb0Ujo+6\ncU2NaqLGSbPB8zpmp6fDsVaD116p13hOiZFWF1MkVwXgeRsAUK7xHIepVjy+UfD6KGukNgsAHJlr\nh2PNJj/eG2u8Lky1Eud1LJ9dpnM9cdpOtebCsXKVH+9agx/vUiV+3q3ZuB0KAJQG/JgAvEdNlZ2H\nFZ4fNNrg48Vm/L3X2eJtYialKxsRyULBRkSyULARkSwUbEQkCwUbEclCwUZEssi69e2jEQbdeLu1\nYqS0QImXaqg34m1aAGjNxFUC202+5dio8cNUr8dbkrUaLxFRqyW2vqv8sVlrnLkFXvJgfTVRgqIX\nvx6JDjPodXn7mw4paXD2HC8fe2b5ZTp+5A03x2M33EjnNqbiFAkAYBVDhgNeGqPqfFu93eIpFv1q\nnP7ByoEAQGONf39UL8ZpEqXKBp07KV3ZiEgWCjYikoWCjYhkoWAjIlko2IhIFgo2IpKFgo2IZJE1\nz8bMUKvGH9GfasyGYwsH4rIBADA7zcdbU3GeQS1RxqFe5zG53iQtOEq8PUdni7cWOf88L7ewfoGU\nkRjyshz9grf3aNTi12p1jZen6G2lcnjicgsr67wtycoqz8Mpk3U3Zw/SudbluTI/fuT/hWMvvfg8\nnTvf5qVM3nTsdjo+s/CGcKyUKFVSraRyxeLvgVnjuUeT0pWNiGShYCMiWSjYiEgWCjYikoWCjYhk\noWAjIlko2IhIFsk8GzN7EMCfATjj7neMb5sH8FUAtwA4BeBed+d9QwDUajUcPRrXGjl4MM6BWJjh\n9Tq84PkR3fU47+Pc2TN0bpXkIADA0nKcX/HUEz+jcy88/Y90/FerPJ/lnMc5JQcu/pTOnWrHNX4A\n4Obb/zAcm5/h9VFqpMYPAMwfXAzH1jd4/ZSV1XN0fDiKT+vTy7ylydNP/IiO/+zRfwjHXjjfoXPn\nei/Q8SNveCMdP/4nHw7H/smtb6ZzWw3eZqbaiF+vfsFzxSY1yZXN3wK4+1W3fQrAw+5+G4CHx/8X\nEQklg427/wDAhVfd/H4AD43//RCAD+zwukRkn7na92yOuPvS+N8vAziyQ+sRkX3qmt8gdncHEP5S\nZ2b3m9lJMzu5uvLqCyQReb242mCzbGaLADD+O3yH1d0fcPcT7n5idm7+Kh9ORF7rrjbYfBvAfeN/\n3wfgWzuzHBHZr5LBxsy+AuBHAG43sxfN7CMAPgvgvWb2DIA/Gf9fRCSUzLNx9w8FQ++50gcrlauY\nORDndpRrcS7A+Ys8P2JthY9XEDf8qSTyaABeh2RuJs6PWFxco3OLZ75Lx6c7v6DjnWH8Et5x0yk6\n97FneA7PM9WFcGxxlvfaWl/hz/vQLXG+VT9Rh2fpud/Q8c01Ug+nwV/L3sZ5Ov62+fg8u63Vp3Nr\nLV6vZvGOe+j48Xe8Nxyba/O8p1EiD63Wil/Pc6vqGyUiryEKNiKShYKNiGShYCMiWSjYiEgWCjYi\nkkXWVi7FcIDz5+LWJBtrcZWKYot/fL81e5iOz80eiAeHfMtyNEy0PCnHY3f94V107l3/7G10fPl5\nvvW9RspbjAb85T16nG8x//qFX4VjzTp50gDu/KN30fHbfu+fhmPtKd46ZHWFt3J58um4rMcvfvkU\nnduo8NIY//zt/yIcm5vh7YQGxlMsKlN8fpm8XBubfMt+axC3zgGAtTMvh2PLK/y+J6UrGxHJQsFG\nRLJQsBGRLBRsRCQLBRsRyULBRkSyULARkSyy5tkMhwOcP7cUjg/6cT7LZqK9R6/E8yNGceVS1EY8\nz6ZZictTAMDAinhdq7z0xVSTt9g4dOj36fjRo38UP/YWP2aN53gbmcPzM+FYqRmPAQAqvATFqV8+\nE45tpvI6nJdLaM/GFSH/+N1/RufagJfGaE7FLYXqiTyZuUT+UOG8ZcoqyUMbjeJzEAAqFX6emcV5\nbMMBP96T0pWNiGShYCMiWSjYiEgWCjYikoWCjYhkoWAjIlko2IhIFlnzbEbFCF2SLzMi7VYa0zyv\no1HnLTrgpBhImR+GUSIkj0ZxHsJgxGvhbHUStXIKXoekXI7nu/P7nl0gNX4A3Hpb3HpkWOK1WX71\n3Ck6/vJLcZ7N0lNP0LlTtTodv/PdccuTt9z+Zjq31YrzaABgdTXOdVnf5K1xLmzGcwGg1+f1hfr9\n+Dzr9ngujCdykwpyng36PTp3UrqyEZEsFGxEJAsFGxHJQsFGRLJQsBGRLBRsRCSLrFvfpXIZremF\ncNxL8XIOzMdlAwBgusk/3m9se7rHywp4wbeQi1G8ZcnGAKDe4KUY6tN8vDUVl9aoV/gW8cYqL0Gx\nuhFv5dZnUtvmb6Hjx954czh26vYn6dxaYtv9pt9/azjmZf7zdX0rUU6hGm+Nzx5q06ktfiqgs8VL\nnVy8GJ+nWwW/81EqhaIWH5dGmz+vSSWvbMzsQTM7Y2ZPXHLbZ8zstJk9Nv5zz46sRkT2rUl+jfpb\nAHdf5va/dvfj4z/f2dllich+kww27v4DABcyrEVE9rFreYP442b2+PjXrPAXeDO738xOmtnJ9XVe\nIlNE9q+rDTZfAHAMwHEASwA+F32huz/g7ifc/cT0NH8TV0T2r6sKNu6+7O6Fu48AfBHAXTu7LBHZ\nb64q2JjZ4iX//SAA/jFdEXndS+bZmNlXALwbwEEzexHApwG828yOA3AApwB8dJIHcwD9Im45Ua/G\nOSODLd6qoutxKwoAqJN2LIkOGhgWvJWLleLci3KlTOceWuT5KocP8jwbH8Z5IefO8pYoa5tbdHxE\namusbJ6lc3tdnq+yeXE5HBucj8cAYNDlJQ9ePBfPrx/g+VrtxK/603PxeKXG83+qVf6zvV7n344z\nB+LHXk8ck7UNXt5isxN///T6PP9nUslg4+4fuszNX9qRRxeR1w19XEFEslCwEZEsFGxEJAsFGxHJ\nQsFGRLJQsBGRLPLWs7ESms04b6QgOTgXV1+i931uyOt1jAbxeKXE82hmZ3nuRbs1G47NH+R5NAsH\n+X0Phzx/4uzyuXCsIM8ZAOoN3v7mzNk4N+PMGf56bKyu0vFmNX7sg4cP0bmtcpyPBQCnz8Z5Nqd/\n/RSd22hN0/GFw4vh2OHFm+jcaiIPB13+s98q8doGiW/l5Qs852r1QnzMrMzrIk1KVzYikoWCjYhk\noWAjIlko2IhIFgo2IpKFgo2IZJF169tKJdTq8dZ3pxNvl1qiVUXR421JLl5Yiscu8nIJqbIDRxZv\nCcdKtVvp3FKZl86olBI/Dypxm43OJi8dfeHsi3T84kpcxrXT2aRzFw7GW8QAcHD+cDg2cn5MegUv\nebD4xmPhWO3iDJ17nmwBA8BzLzwbjl1Y42UcKuTcB4CClPQAgBFJDSmcH5NEBxs023GKxpCUMbkS\nurIRkSwUbEQkCwUbEclCwUZEslCwEZEsFGxEJAsFGxHJImuezXAwwPkzcW5HtRr3VGnUeDmEksft\nVACgS0pbzFf4YfAR7/Wy9NKpcOw3zz5N585Mx+UpAKBW4+UUZuaPxIOJFjWtBn/eN9z8lnCsWeMt\nUWrVJh03i/Omelu8LU+jxPNwqvW4FEN19g10bpnkmwDAxZUz4djZczxf64Xnf0rHRwUvJ8IKVMwf\njPOWAOCGG99Mx2cW4tezN9yZVi66shGRLBRsRCQLBRsRyULBRkSyULARkSwUbEQkCwUbEckimWdj\nZjcB+DsAR7CdufGAu/+Nmc0D+CqAWwCcAnCvu9OCHj4aot+Nv8QH8XKKcpeus99P5ChU4nYUwxHP\n2/BEXsdgfSu+70QrlqLP6/CcOb9Ox0ukRsqNN91O51bAn9ewiFvcbBW8TUwpkRc18jjPZqXHa+UU\nzh+70o/HL6zxlibLF3iLmn4vvu9BoubSaMDzVUqJPJt6O84f2uzw82R56Vd0vDW9EI7V2rye06Qm\nubIZAvgLd38rgLcD+JiZvRXApwA87O63AXh4/H8RkctKBht3X3L3R8f/XgfwNICjAN4P4KHxlz0E\n4APXa5Ei8tp3Re/ZmNktAO4E8GMAR9z9lVqbL2P716zLzbnfzE6a2cnNjbVrWKqIvJZNHGzMrA3g\n6wA+6e6/FTXc3RF8EsfdH3D3E+5+otXm9V9FZP+aKNiYWRXbgebL7v6N8c3LZrY4Hl8EEH9CTURe\n95LBxswMwJcAPO3un79k6NsA7hv/+z4A39r55YnIfjFJiYl3APgwgJ+b2WPj2/4SwGcBfM3MPgLg\nOQD3pu6oVCphqhGXHugP4i3kbpdvEcPibVoAcLLV2qzH2+IA0OkkSh404u3nhXleimErsRXbnuIl\nJjZX49YjK01e5qFUKvP73ozfY2uSkh2TKIq4PchoxLeIt7b41ni5HD/v9cQWcS/REqgo4p/PGxd5\n65xERQ/0h7wmiJFhK/jc7nrclgcAyqX4+8OqO5OOlww27v5DANF38nt2ZBUisu8pg1hEslCwEZEs\nFGxEJAsFGxHJQsFGRLJQsBGRLLK2cimVq2jPxa00imGcP9Ht8TyBkfM8g0o5boRh4Pkm9Xrio/+N\n+L6LRE7IoMzjfa/HS2usbcR5ISvLv6Fzp2Z46YABeexh4vWoVPjzYi1qfMhLSHRIbtH2HcSvp6Va\nzAzj/B8AGPXic6EyivPEAGCq3abjlblEe5wGyQczfg4XI17+otmO86aaDV4uZFK6shGRLBRsRCQL\nBRsRyULBRkSyULARkSwUbEQkCwUbEckia56NlUqokNovwy7JZzFec2Z2Lm5FAQAN0vKkVuaHYdDn\nuS6lUpzjM0zU4emR4wEAgyHP3WjNxGtL5R71RzynpD09G45Vq7zODq8uBFTI/E6H5yY1mon2OOSY\nVWr8PGon25bEOUCjPs+TKSfqB6HEf/YX5Npg6HxutTFFx2uNOAeonPj+mJSubEQkCwUbEclCwUZE\nslCwEZEsFGxEJAsFGxHJIuvW92hUYGszLk3Q7cStQ+p1Xhpgpn2IjrMtzdGAb6VuDHhrkVYr3lYs\nElvEpcR2Z7VIlLdosS6jifIViVIO1VpcWqBa4VvIo0RJg5EX4dhU6wCdO93m45ubcXuccplvP1vi\nW2JrK24FwxMNgO4Wbz/N2tsAQLN9mDw2f161alwGBQDaU3Gag6e27CekKxsRyULBRkSyULARkSwU\nbEQkCwUbEclCwUZEslCwEZEssubZ+GiEQS8uH1AjOSntWZ5b0WrxNhklxHkdvV6cOwEA5RIvmFCv\nxvko/YLnm9SqPF8l1RKFlZHwxM+SciJ/qE7KcpQTeTaDAS+N4SNSqiFxzEqNaToOxPkqlUSeTanE\n81GG5Jitb/A8mmEij6adyC+qkufdT7yWwyI+/wFgMIxLlZTIeXAlklc2ZnaTmX3fzJ4ysyfN7BPj\n2z9jZqfN7LHxn3t2ZEUisi9NcmUzBPAX7v6omU0DeMTMvjse+2t3/6vrtzwR2S+SwcbdlwAsjf+9\nbmZPAzh6vRcmIvvLFb1BbGa3ALgTwI/HN33czB43swfN7LK/cJrZ/WZ20sxObqzzlq0isn9NHGzM\nrA3g6wA+6e5rAL4A4BiA49i+8vnc5ea5+wPufsLdT7SnU/VdRWS/mijYmFkV24Hmy+7+DQBw92V3\nL9x9BOCLAO66fssUkde6SXajDMCXADzt7p+/5PbFS77sgwCe2Pnlich+Mclu1DsAfBjAz83ssfFt\nfwngQ2Z2HNtlPE4B+GjqjqxkqDTiujQVkq9Sb/HcCkuEze76xXise4HOnWryX/98lGpcEqsk6owU\nBa+SUiYPbSX+8lqi4YqP4twMIy1NAKBM2tsAQJnkVPX7PB+l1+c1fljNmio5xwCgVuUtTzZqcS5N\nYfx4lhP5KpZ47E3yvLeGPM9ms8vb47DaRpV6h86d1CS7UT/E5dsAfWdHViAirwv6uIKIZKFgIyJZ\nKNiISBYKNiKShYKNiGShYCMiWeStZ+OO3iCumzEiSSP9Pq+PstrnNVD63Tg/opTIj0g9dq9HaoEk\nauEMEj2rSol8lXqNvITOj0m1wnN8nPR2Mud5NvVEv6xyJc53KYr4eAJAf8jzosrkPCon8ppo4hIA\nJz+eWZ4YABSJmjJrm7yuUmdI8o8S53Cqj1eP1B+yjZ3Js9GVjYhkoWAjIlko2IhIFgo2IpKFgo2I\nZKFgIyJZZN36LooBVlbOhuO1ZvwR/H6Pbz9XwLc0S6S9R7nEtwW7qfoVZIuZtacBADO+tT1yXjoA\niFuqGHjbklo1LvcBAOUKXzszHPAyERVS3mKqOUPndhLlErxEtpjLfHt6kNjSLyy+7+ZUosWMJ9Ig\nSJkHAOh34q3xVCuXFLYpXyS2zSelKxsRyULBRkSyULARkSwUbEQkCwUbEclCwUZEslCwEZEs8ubZ\nDIdYWzkXjtdJKYf+Fs+tKKfaaJD2HlOtNp0L0tIEAJyUiWjUeF5Hox7nyQAALJED1I2P2XRrls4t\nl/nLXyrFuUuJw43NDn+9hqM4v2hqap7OnZrhz2tQxDk+RZ+/lt2tDTre68XzK1X+WlZSJT0SLWpK\n/bjUQ9n5a1mQYwIARnLJqlOJ748J6cpGRLJQsBGRLBRsRCQLBRsRyULBRkSyULARkSwUbEQki2Se\njZk1APwA24VTKgD+l7t/2szmAXwVwC0ATgG4190vsvtyOIpRXHeD5TgUBc9BYHk0ANBoxPkuoxHP\njxj0eK2QKsk5Sa27n2hBU6vxmjOsGk6/z9dtxvM+yqT+yiBRP2VYJOrCkLYmlRpvHVJO5KsUHp8L\nPfA8mg5pywMAQ1KHxxI/uweDRL5Wot4NuzZI1Q9KtZEp1eJjVk3kY01qkiubHoB/6e5/AOA4gLvN\n7O0APgXgYXe/DcDD4/+LiFxWMtj4tld+HFTHfxzA+wE8NL79IQAfuC4rFJF9YaL3bMysbGaPATgD\n4Lvu/mMAR9x9afwlLwM4Esy938xOmtnJ7ia/hBWR/WuiYOPuhbsfB3AjgLvM7I5XjTuCtw/c/QF3\nP+HuJ5qpzyCJyL51RbtR7r4C4PsA7gawbGaLADD++8zOL09E9otksDGzQ2Y2N/53E8B7AfwjgG8D\nuG/8ZfcB+Nb1WqSIvPZNsqe1COAhMytjOzh9zd3/t5n9CMDXzOwjAJ4DcG/qjswMFVJSwcjWX9n4\nUstl3nZkqnkgHKuW+H0PRryNTH2K3HfiCJdoEw1g5Hy8WokfoCDtawCg01uj48ONeP7m5iqdW6vz\nJ95qxWUkCk+VQ+Cv9ZCVmEiUCylI6Yvt+47nDwv+nuRwyJ+Xk5ZAADAk6Qb9RHoGEi2DyqU4NaRU\n4Wklk0oGG3d/HMCdl7n9PID37MgqRGTfUwaxiGShYCMiWSjYiEgWCjYikoWCjYhkoWAjIlnY9icN\nMj2Y2Vls5+S84iCAuLfL7tmr6wL27tq0riu3V9d2peu62d0Ppb4oa7D5nQc3O+nuJ3ZtAYG9ui5g\n765N67pye3Vt12td+jVKRLJQsBGRLHY72Dywy48f2avrAvbu2rSuK7dX13Zd1rWr79mIyOvHbl/Z\niMjrhIKNiGSxK8HGzO42s1+Y2bNmtqe6MpjZKTP7uZk9ZmYnd3EdD5rZGTN74pLb5s3su2b2zPjv\nuJBO/rV9xsxOj4/bY2Z2zy6s6yYz+76ZPWVmT5rZJ8a37+pxI+vaC8esYWb/YGY/G6/tv4xv3/Fj\nlv09m3ERrl9iu+LfiwB+AuBD7v5U1oUEzOwUgBPuvqvJVmb2LgAbAP7O3e8Y3/ZfAVxw98+Og/QB\nd/+Pe2RtnwGw4e5/lXs9l6xrEcCiuz9qZtMAHsF2149/g108bmRd92L3j5kBaLn7hm03EvshgE8A\n+NfY4WO2G1c2dwF41t1/7e59AH+P7bYwcgl3/wGAC6+6eU+0zwnWtuvcfcndHx3/ex3A0wCOYpeP\nG1nXrsvZqmk3gs1RAC9c8v8XsUcO/JgD+J6ZPWJm9+/2Yl5lovY5u+jjZvb4+NesXfkV7xVmdgu2\nK0xO3HYoh1etC9gDx+xaWjVdCb1B/LveOW5b8z4AHxv/yrDnsPY5u+QLAI5hu2vqEoDP7dZCzKwN\n4OsAPunuv1VoeTeP22XWtSeO2bW0aroSuxFsTgO46ZL/3zi+bU9w99Pjv88A+Ca2f+3bK/Zs+xx3\nXx6ftCMAX8QuHbfx+w5fB/Bld//G+OZdP26XW9deOWavuN6tmnYj2PwEwG1m9ibbLpP/59huC7Pr\nzKw1fgMPZtYC8KcAnuCzstqz7XNeOTHHPohdOG7jNzu/BOBpd//8JUO7etyide2RY5avVZO7Z/8D\n4B5s70j9CsB/2o01BOs6BuBn4z9P7ubaAHwF25fWA2y/r/URAAsAHgbwDIDvAZjfQ2v7HwB+DuDx\n8Ym6uAvreie2L/cfB/DY+M89u33cyLr2wjF7G4CfjtfwBID/PL59x4+ZPq4gIlnoDWIRyULBRkSy\nULARkSwUbEQkCwUbEclCwUZEslCwEZEs/j/wmlGMIhm+BQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17f8cd14a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct class: cat\n",
      "Predictions: ['airplane' 'ship' 'bird'] [ 0.6248529   0.37288868  0.00127298]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEYCAYAAABsuVKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17f8c460668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct class: bird\n",
      "Predictions: ['dog' 'frog' 'ship'] [ 0.95125693  0.0277505   0.01826219]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEYCAYAAABsuVKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSBL/H3/xCcSgYNFJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17f8bb31eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct class: ship\n",
      "Predictions: ['horse' 'dog' 'cat'] [ 0.60634363  0.15830515  0.10272072]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEYCAYAAABsuVKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17f8bd11a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct class: dog\n",
      "Predictions: ['horse' 'deer' 'automobile'] [  8.12025547e-01   1.87293738e-01   3.79140489e-04]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEYCAYAAABsuVKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+d3PAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17e887b9da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct class: deer\n",
      "Predictions: ['automobile' 'horse' 'truck'] [  9.98405993e-01   1.11370184e-03   2.27105586e-04]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEYCAYAAABsuVKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17e88616438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct class: deer\n",
      "Predictions: ['dog' 'cat' 'frog'] [ 0.98430204  0.00717602  0.00645544]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEYCAYAAABsuVKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17e886fb438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct class: airplane\n",
      "Predictions: ['frog' 'cat' 'bird'] [ 0.84072846  0.07509656  0.03518334]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEYCAYAAABsuVKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17f8a1cf748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct class: airplane\n",
      "Predictions: ['truck' 'automobile' 'airplane'] [  9.83287334e-01   1.65471770e-02   8.93738033e-05]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEYCAYAAABsuVKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJRsRSeL/AWBqbT38o7GQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17e888e47f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct class: deer\n",
      "Predictions: ['bird' 'frog' 'airplane'] [ 0.70057923  0.29675242  0.0011694 ]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEYCAYAAABsuVKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17f8c226b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct class: airplane\n",
      "Predictions: ['horse' 'cat' 'deer'] [ 0.96933293  0.02222641  0.00417694]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARsAAAEYCAYAAABsuVKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmGyIKAgmGyIKgsmGiIJgsiGiIP4/aN56M6AIyJ4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17ee9f14128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct class: horse\n",
      "Predictions: ['cat' 'ship' 'bird'] [ 0.66969067  0.23040417  0.09729752]\n"
     ]
    }
   ],
   "source": [
    "worst = worst_samples(session, test_x, test_y, config)\n",
    "\n",
    "for sample_id, l, predicted in worst:\n",
    "    show_image(test_x[sample_id], data_mean, data_std)\n",
    "    probas = session.run(tf.nn.softmax(logits), feed_dict={X: np.array([test_x[sample_id]])})\n",
    "    probas = probas[0]\n",
    "    predictions  = np.argsort(-probas)\n",
    "\n",
    "    print(\"Correct class:\", class_names[test_y[sample_id]])\n",
    "    print(\"Predictions:\", class_names[predictions[:3]], probas[predictions[:3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ALEX_PATCH_DEPTH_1, ALEX_PATCH_DEPTH_2, ALEX_PATCH_DEPTH_3, ALEX_PATCH_DEPTH_4 = 96, 256, 384, 256\n",
    "ALEX_PATCH_SIZE_1, ALEX_PATCH_SIZE_2, ALEX_PATCH_SIZE_3, ALEX_PATCH_SIZE_4 = 11, 5, 3, 3\n",
    "ALEX_NUM_HIDDEN_1, ALEX_NUM_HIDDEN_2 = 4096, 4096\n",
    " \n",
    "def flatten_tf_array(array, W):\n",
    "    return tf.reshape(array, [-1, W.get_shape().as_list()[0]])\n",
    " \n",
    "def variables_alexnet(patch_size1 = ALEX_PATCH_SIZE_1, patch_size2 = ALEX_PATCH_SIZE_2, \n",
    "                      patch_size3 = ALEX_PATCH_SIZE_3, patch_size4 = ALEX_PATCH_SIZE_4, \n",
    "                      patch_depth1 = ALEX_PATCH_DEPTH_1, patch_depth2 = ALEX_PATCH_DEPTH_2, \n",
    "                      patch_depth3 = ALEX_PATCH_DEPTH_3, patch_depth4 = ALEX_PATCH_DEPTH_4, \n",
    "                      num_hidden1 = ALEX_NUM_HIDDEN_1, num_hidden2 = ALEX_NUM_HIDDEN_2,\n",
    "                      image_width = 32, image_height = 32, image_depth = 3, num_labels = 10):\n",
    " \n",
    "    w1 = tf.Variable(tf.truncated_normal([patch_size1, patch_size1, image_depth, patch_depth1], stddev=0.1))\n",
    "    b1 = tf.Variable(tf.zeros([patch_depth1]))\n",
    "\n",
    "    w2 = tf.Variable(tf.truncated_normal([patch_size2, patch_size2, patch_depth1, patch_depth2], stddev=0.1))\n",
    "    b2 = tf.Variable(tf.constant(1.0, shape=[patch_depth2]))\n",
    "\n",
    "    w3 = tf.Variable(tf.truncated_normal([patch_size3, patch_size3, patch_depth2, patch_depth3], stddev=0.1))\n",
    "    b3 = tf.Variable(tf.zeros([patch_depth3]))\n",
    "\n",
    "    w4 = tf.Variable(tf.truncated_normal([patch_size4, patch_size4, patch_depth3, patch_depth3], stddev=0.1))\n",
    "    b4 = tf.Variable(tf.constant(1.0, shape=[patch_depth3]))\n",
    "\n",
    "    w5 = tf.Variable(tf.truncated_normal([patch_size4, patch_size4, patch_depth3, patch_depth3], stddev=0.1))\n",
    "    b5 = tf.Variable(tf.zeros([patch_depth3]))\n",
    "\n",
    "    pool_reductions = 3\n",
    "    conv_reductions = 2\n",
    "    no_reductions = pool_reductions + conv_reductions\n",
    "    w6 = tf.Variable(tf.truncated_normal([(image_width // 2**no_reductions)*(image_height // 2**no_reductions)*patch_depth3, num_hidden1], stddev=0.1))\n",
    "    b6 = tf.Variable(tf.constant(1.0, shape = [num_hidden1]))\n",
    "\n",
    "    w7 = tf.Variable(tf.truncated_normal([num_hidden1, num_hidden2], stddev=0.1))\n",
    "    b7 = tf.Variable(tf.constant(1.0, shape = [num_hidden2]))\n",
    "\n",
    "    w8 = tf.Variable(tf.truncated_normal([num_hidden2, num_labels], stddev=0.1))\n",
    "    b8 = tf.Variable(tf.constant(1.0, shape = [num_labels]))\n",
    "\n",
    "    variables = {\n",
    "                 'w1': w1, 'w2': w2, 'w3': w3, 'w4': w4, 'w5': w5, 'w6': w6, 'w7': w7, 'w8': w8, \n",
    "                 'b1': b1, 'b2': b2, 'b3': b3, 'b4': b4, 'b5': b5, 'b6': b6, 'b7': b7, 'b8': b8\n",
    "                }\n",
    "    return variables\n",
    " \n",
    "def model_alexnet(data, variables):\n",
    "    layer1_conv = tf.nn.conv2d(data, variables['w1'], [1, 4, 4, 1], padding='SAME')\n",
    "    layer1_relu = tf.nn.relu(layer1_conv + variables['b1'])\n",
    "    layer1_pool = tf.nn.max_pool(layer1_relu, [1, 3, 3, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    layer1_norm = tf.nn.local_response_normalization(layer1_pool)\n",
    " \n",
    "    layer2_conv = tf.nn.conv2d(layer1_norm, variables['w2'], [1, 1, 1, 1], padding='SAME')\n",
    "    layer2_relu = tf.nn.relu(layer2_conv + variables['b2'])\n",
    "    layer2_pool = tf.nn.max_pool(layer2_relu, [1, 3, 3, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    layer2_norm = tf.nn.local_response_normalization(layer2_pool)\n",
    " \n",
    "    layer3_conv = tf.nn.conv2d(layer2_norm, variables['w3'], [1, 1, 1, 1], padding='SAME')\n",
    "    layer3_relu = tf.nn.relu(layer3_conv + variables['b3'])\n",
    " \n",
    "    layer4_conv = tf.nn.conv2d(layer3_relu, variables['w4'], [1, 1, 1, 1], padding='SAME')\n",
    "    layer4_relu = tf.nn.relu(layer4_conv + variables['b4'])\n",
    " \n",
    "    layer5_conv = tf.nn.conv2d(layer4_relu, variables['w5'], [1, 1, 1, 1], padding='SAME')\n",
    "    layer5_relu = tf.nn.relu(layer5_conv + variables['b5'])\n",
    "    layer5_pool = tf.nn.max_pool(layer4_relu, [1, 3, 3, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    layer5_norm = tf.nn.local_response_normalization(layer5_pool)\n",
    " \n",
    "    flat_layer = flatten_tf_array(layer5_norm, variables['w6'])\n",
    "    layer6_fccd = tf.matmul(flat_layer, variables['w6']) + variables['b6']\n",
    "    layer6_tanh = tf.tanh(layer6_fccd)\n",
    "    layer6_drop = tf.nn.dropout(layer6_tanh, 0.5)\n",
    " \n",
    "    layer7_fccd = tf.matmul(layer6_drop, variables['w7']) + variables['b7']\n",
    "    layer7_tanh = tf.tanh(layer7_fccd)\n",
    "    layer7_drop = tf.nn.dropout(layer7_tanh, 0.5)\n",
    " \n",
    "    logits = tf.matmul(layer7_drop, variables['w8']) + variables['b8']\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Graph\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "Y_ = tf.placeholder(tf.int32, [None,])\n",
    "logits = model_alexnet(X, variables=variables_alexnet())\n",
    "\n",
    "# loss\n",
    "regularizers = l2_loss(weights.values())\n",
    "loss_per_sample = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=Y_)\n",
    "data_loss = tf.reduce_mean(loss_per_sample)\n",
    "loss = data_loss + weight_decay*regularizers\n",
    "\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(0.01, global_step, 900, 0.9, staircase=True)\n",
    "train_step = tf.train.AdagradOptimizer(learning_rate).minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, step 50 / 900, loss = 10.85 (0.022 sec/batch)\n",
      "epoch 1, step 100 / 900, loss = 10.50 (0.021 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Korisnik\\Anaconda3\\lib\\site-packages\\skimage\\util\\dtype.py:122: UserWarning: Possible precision loss when converting from float64 to uint8\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, step 150 / 900, loss = 10.73 (0.021 sec/batch)\n",
      "epoch 1, step 200 / 900, loss = 8.76 (0.021 sec/batch)\n",
      "epoch 1, step 250 / 900, loss = 7.39 (0.021 sec/batch)\n",
      "epoch 1, step 300 / 900, loss = 5.87 (0.021 sec/batch)\n",
      "epoch 1, step 350 / 900, loss = 8.13 (0.021 sec/batch)\n",
      "epoch 1, step 400 / 900, loss = 8.25 (0.022 sec/batch)\n",
      "epoch 1, step 450 / 900, loss = 8.09 (0.022 sec/batch)\n",
      "epoch 1, step 500 / 900, loss = 8.52 (0.023 sec/batch)\n",
      "epoch 1, step 550 / 900, loss = 7.14 (0.022 sec/batch)\n",
      "epoch 1, step 600 / 900, loss = 8.57 (0.021 sec/batch)\n",
      "epoch 1, step 650 / 900, loss = 6.54 (0.022 sec/batch)\n",
      "epoch 1, step 700 / 900, loss = 5.30 (0.021 sec/batch)\n",
      "epoch 1, step 750 / 900, loss = 8.09 (0.021 sec/batch)\n",
      "epoch 1, step 800 / 900, loss = 6.92 (0.022 sec/batch)\n",
      "epoch 1, step 850 / 900, loss = 7.56 (0.021 sec/batch)\n",
      "epoch 1, step 900 / 900, loss = 7.55 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 18.42\n",
      " avg loss = 6.87\n",
      "\n",
      "Validation error:\n",
      " accuracy = 18.62\n",
      " avg loss = 6.86\n",
      "\n",
      "Epoch time: 32.309290409088135\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 2, step 50 / 900, loss = 6.82 (0.021 sec/batch)\n",
      "epoch 2, step 100 / 900, loss = 5.44 (0.021 sec/batch)\n",
      "epoch 2, step 150 / 900, loss = 5.70 (0.021 sec/batch)\n",
      "epoch 2, step 200 / 900, loss = 5.11 (0.022 sec/batch)\n",
      "epoch 2, step 250 / 900, loss = 6.12 (0.021 sec/batch)\n",
      "epoch 2, step 300 / 900, loss = 5.16 (0.022 sec/batch)\n",
      "epoch 2, step 350 / 900, loss = 5.56 (0.022 sec/batch)\n",
      "epoch 2, step 400 / 900, loss = 6.72 (0.021 sec/batch)\n",
      "epoch 2, step 450 / 900, loss = 6.21 (0.021 sec/batch)\n",
      "epoch 2, step 500 / 900, loss = 5.24 (0.021 sec/batch)\n",
      "epoch 2, step 550 / 900, loss = 6.45 (0.021 sec/batch)\n",
      "epoch 2, step 600 / 900, loss = 5.91 (0.021 sec/batch)\n",
      "epoch 2, step 650 / 900, loss = 4.87 (0.022 sec/batch)\n",
      "epoch 2, step 700 / 900, loss = 5.23 (0.022 sec/batch)\n",
      "epoch 2, step 750 / 900, loss = 7.05 (0.021 sec/batch)\n",
      "epoch 2, step 800 / 900, loss = 6.30 (0.021 sec/batch)\n",
      "epoch 2, step 850 / 900, loss = 7.12 (0.021 sec/batch)\n",
      "epoch 2, step 900 / 900, loss = 5.86 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 23.78\n",
      " avg loss = 5.53\n",
      "\n",
      "Validation error:\n",
      " accuracy = 23.82\n",
      " avg loss = 5.57\n",
      "\n",
      "Epoch time: 24.660868644714355\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 3, step 50 / 900, loss = 5.53 (0.022 sec/batch)\n",
      "epoch 3, step 100 / 900, loss = 7.17 (0.022 sec/batch)\n",
      "epoch 3, step 150 / 900, loss = 6.28 (0.022 sec/batch)\n",
      "epoch 3, step 200 / 900, loss = 5.24 (0.022 sec/batch)\n",
      "epoch 3, step 250 / 900, loss = 5.05 (0.021 sec/batch)\n",
      "epoch 3, step 300 / 900, loss = 4.88 (0.021 sec/batch)\n",
      "epoch 3, step 350 / 900, loss = 5.34 (0.021 sec/batch)\n",
      "epoch 3, step 400 / 900, loss = 5.59 (0.021 sec/batch)\n",
      "epoch 3, step 450 / 900, loss = 5.44 (0.022 sec/batch)\n",
      "epoch 3, step 500 / 900, loss = 5.70 (0.021 sec/batch)\n",
      "epoch 3, step 550 / 900, loss = 4.83 (0.021 sec/batch)\n",
      "epoch 3, step 600 / 900, loss = 6.04 (0.021 sec/batch)\n",
      "epoch 3, step 650 / 900, loss = 4.18 (0.022 sec/batch)\n",
      "epoch 3, step 700 / 900, loss = 5.36 (0.022 sec/batch)\n",
      "epoch 3, step 750 / 900, loss = 4.19 (0.021 sec/batch)\n",
      "epoch 3, step 800 / 900, loss = 5.02 (0.021 sec/batch)\n",
      "epoch 3, step 850 / 900, loss = 4.56 (0.021 sec/batch)\n",
      "epoch 3, step 900 / 900, loss = 5.23 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 28.77\n",
      " avg loss = 4.91\n",
      "\n",
      "Validation error:\n",
      " accuracy = 27.94\n",
      " avg loss = 5.00\n",
      "\n",
      "Epoch time: 24.669233560562134\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 4, step 50 / 900, loss = 4.73 (0.021 sec/batch)\n",
      "epoch 4, step 100 / 900, loss = 5.27 (0.021 sec/batch)\n",
      "epoch 4, step 150 / 900, loss = 4.68 (0.022 sec/batch)\n",
      "epoch 4, step 200 / 900, loss = 4.84 (0.021 sec/batch)\n",
      "epoch 4, step 250 / 900, loss = 6.59 (0.021 sec/batch)\n",
      "epoch 4, step 300 / 900, loss = 4.87 (0.021 sec/batch)\n",
      "epoch 4, step 350 / 900, loss = 4.37 (0.021 sec/batch)\n",
      "epoch 4, step 400 / 900, loss = 4.19 (0.022 sec/batch)\n",
      "epoch 4, step 450 / 900, loss = 4.45 (0.022 sec/batch)\n",
      "epoch 4, step 500 / 900, loss = 4.06 (0.021 sec/batch)\n",
      "epoch 4, step 550 / 900, loss = 3.67 (0.021 sec/batch)\n",
      "epoch 4, step 600 / 900, loss = 3.70 (0.021 sec/batch)\n",
      "epoch 4, step 650 / 900, loss = 4.88 (0.021 sec/batch)\n",
      "epoch 4, step 700 / 900, loss = 4.25 (0.021 sec/batch)\n",
      "epoch 4, step 750 / 900, loss = 4.60 (0.021 sec/batch)\n",
      "epoch 4, step 800 / 900, loss = 5.59 (0.021 sec/batch)\n",
      "epoch 4, step 850 / 900, loss = 4.67 (0.021 sec/batch)\n",
      "epoch 4, step 900 / 900, loss = 3.54 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 31.78\n",
      " avg loss = 4.34\n",
      "\n",
      "Validation error:\n",
      " accuracy = 31.48\n",
      " avg loss = 4.42\n",
      "\n",
      "Epoch time: 24.683226346969604\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 5, step 50 / 900, loss = 4.73 (0.021 sec/batch)\n",
      "epoch 5, step 100 / 900, loss = 4.29 (0.021 sec/batch)\n",
      "epoch 5, step 150 / 900, loss = 3.91 (0.021 sec/batch)\n",
      "epoch 5, step 200 / 900, loss = 5.04 (0.021 sec/batch)\n",
      "epoch 5, step 250 / 900, loss = 4.67 (0.022 sec/batch)\n",
      "epoch 5, step 300 / 900, loss = 4.10 (0.021 sec/batch)\n",
      "epoch 5, step 350 / 900, loss = 4.97 (0.021 sec/batch)\n",
      "epoch 5, step 400 / 900, loss = 4.05 (0.021 sec/batch)\n",
      "epoch 5, step 450 / 900, loss = 4.51 (0.022 sec/batch)\n",
      "epoch 5, step 500 / 900, loss = 4.79 (0.022 sec/batch)\n",
      "epoch 5, step 550 / 900, loss = 4.94 (0.021 sec/batch)\n",
      "epoch 5, step 600 / 900, loss = 3.48 (0.021 sec/batch)\n",
      "epoch 5, step 650 / 900, loss = 3.55 (0.022 sec/batch)\n",
      "epoch 5, step 700 / 900, loss = 4.88 (0.021 sec/batch)\n",
      "epoch 5, step 750 / 900, loss = 4.62 (0.022 sec/batch)\n",
      "epoch 5, step 800 / 900, loss = 3.72 (0.022 sec/batch)\n",
      "epoch 5, step 850 / 900, loss = 4.46 (0.021 sec/batch)\n",
      "epoch 5, step 900 / 900, loss = 4.16 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 36.36\n",
      " avg loss = 4.00\n",
      "\n",
      "Validation error:\n",
      " accuracy = 35.68\n",
      " avg loss = 4.07\n",
      "\n",
      "Epoch time: 24.6416757106781\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 6, step 50 / 900, loss = 3.44 (0.021 sec/batch)\n",
      "epoch 6, step 100 / 900, loss = 2.79 (0.021 sec/batch)\n",
      "epoch 6, step 150 / 900, loss = 3.79 (0.022 sec/batch)\n",
      "epoch 6, step 200 / 900, loss = 4.80 (0.021 sec/batch)\n",
      "epoch 6, step 250 / 900, loss = 3.71 (0.021 sec/batch)\n",
      "epoch 6, step 300 / 900, loss = 3.51 (0.022 sec/batch)\n",
      "epoch 6, step 350 / 900, loss = 4.08 (0.021 sec/batch)\n",
      "epoch 6, step 400 / 900, loss = 3.56 (0.021 sec/batch)\n",
      "epoch 6, step 450 / 900, loss = 3.50 (0.021 sec/batch)\n",
      "epoch 6, step 500 / 900, loss = 2.44 (0.022 sec/batch)\n",
      "epoch 6, step 550 / 900, loss = 2.86 (0.020 sec/batch)\n",
      "epoch 6, step 600 / 900, loss = 3.70 (0.022 sec/batch)\n",
      "epoch 6, step 650 / 900, loss = 3.30 (0.021 sec/batch)\n",
      "epoch 6, step 700 / 900, loss = 3.82 (0.021 sec/batch)\n",
      "epoch 6, step 750 / 900, loss = 3.99 (0.022 sec/batch)\n",
      "epoch 6, step 800 / 900, loss = 4.16 (0.021 sec/batch)\n",
      "epoch 6, step 850 / 900, loss = 3.73 (0.021 sec/batch)\n",
      "epoch 6, step 900 / 900, loss = 4.86 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 34.97\n",
      " avg loss = 3.97\n",
      "\n",
      "Validation error:\n",
      " accuracy = 34.32\n",
      " avg loss = 4.06\n",
      "\n",
      "Epoch time: 24.665536642074585\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 7, step 50 / 900, loss = 3.45 (0.022 sec/batch)\n",
      "epoch 7, step 100 / 900, loss = 3.51 (0.022 sec/batch)\n",
      "epoch 7, step 150 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 7, step 200 / 900, loss = 3.32 (0.021 sec/batch)\n",
      "epoch 7, step 250 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "epoch 7, step 300 / 900, loss = 4.28 (0.021 sec/batch)\n",
      "epoch 7, step 350 / 900, loss = 4.39 (0.021 sec/batch)\n",
      "epoch 7, step 400 / 900, loss = 2.96 (0.022 sec/batch)\n",
      "epoch 7, step 450 / 900, loss = 4.83 (0.022 sec/batch)\n",
      "epoch 7, step 500 / 900, loss = 2.86 (0.022 sec/batch)\n",
      "epoch 7, step 550 / 900, loss = 4.38 (0.021 sec/batch)\n",
      "epoch 7, step 600 / 900, loss = 4.40 (0.021 sec/batch)\n",
      "epoch 7, step 650 / 900, loss = 3.85 (0.022 sec/batch)\n",
      "epoch 7, step 700 / 900, loss = 4.44 (0.022 sec/batch)\n",
      "epoch 7, step 750 / 900, loss = 3.60 (0.021 sec/batch)\n",
      "epoch 7, step 800 / 900, loss = 3.31 (0.021 sec/batch)\n",
      "epoch 7, step 850 / 900, loss = 4.38 (0.020 sec/batch)\n",
      "epoch 7, step 900 / 900, loss = 3.99 (0.021 sec/batch)\n",
      "Train error:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy = 40.06\n",
      " avg loss = 3.62\n",
      "\n",
      "Validation error:\n",
      " accuracy = 38.52\n",
      " avg loss = 3.87\n",
      "\n",
      "Epoch time: 24.629652976989746\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 8, step 50 / 900, loss = 3.80 (0.021 sec/batch)\n",
      "epoch 8, step 100 / 900, loss = 2.59 (0.021 sec/batch)\n",
      "epoch 8, step 150 / 900, loss = 4.04 (0.021 sec/batch)\n",
      "epoch 8, step 200 / 900, loss = 4.00 (0.022 sec/batch)\n",
      "epoch 8, step 250 / 900, loss = 3.38 (0.021 sec/batch)\n",
      "epoch 8, step 300 / 900, loss = 3.72 (0.022 sec/batch)\n",
      "epoch 8, step 350 / 900, loss = 3.07 (0.021 sec/batch)\n",
      "epoch 8, step 400 / 900, loss = 3.70 (0.021 sec/batch)\n",
      "epoch 8, step 450 / 900, loss = 5.33 (0.021 sec/batch)\n",
      "epoch 8, step 500 / 900, loss = 3.46 (0.022 sec/batch)\n",
      "epoch 8, step 550 / 900, loss = 3.22 (0.021 sec/batch)\n",
      "epoch 8, step 600 / 900, loss = 3.61 (0.021 sec/batch)\n",
      "epoch 8, step 650 / 900, loss = 2.98 (0.022 sec/batch)\n",
      "epoch 8, step 700 / 900, loss = 3.89 (0.022 sec/batch)\n",
      "epoch 8, step 750 / 900, loss = 3.69 (0.021 sec/batch)\n",
      "epoch 8, step 800 / 900, loss = 3.50 (0.022 sec/batch)\n",
      "epoch 8, step 850 / 900, loss = 4.58 (0.021 sec/batch)\n",
      "epoch 8, step 900 / 900, loss = 2.78 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 41.33\n",
      " avg loss = 3.40\n",
      "\n",
      "Validation error:\n",
      " accuracy = 40.56\n",
      " avg loss = 3.61\n",
      "\n",
      "Epoch time: 24.637224197387695\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 9, step 50 / 900, loss = 3.61 (0.021 sec/batch)\n",
      "epoch 9, step 100 / 900, loss = 2.78 (0.021 sec/batch)\n",
      "epoch 9, step 150 / 900, loss = 5.18 (0.021 sec/batch)\n",
      "epoch 9, step 200 / 900, loss = 3.74 (0.021 sec/batch)\n",
      "epoch 9, step 250 / 900, loss = 3.08 (0.022 sec/batch)\n",
      "epoch 9, step 300 / 900, loss = 3.24 (0.021 sec/batch)\n",
      "epoch 9, step 350 / 900, loss = 2.88 (0.021 sec/batch)\n",
      "epoch 9, step 400 / 900, loss = 3.23 (0.022 sec/batch)\n",
      "epoch 9, step 450 / 900, loss = 4.22 (0.021 sec/batch)\n",
      "epoch 9, step 500 / 900, loss = 3.40 (0.021 sec/batch)\n",
      "epoch 9, step 550 / 900, loss = 3.64 (0.021 sec/batch)\n",
      "epoch 9, step 600 / 900, loss = 3.28 (0.021 sec/batch)\n",
      "epoch 9, step 650 / 900, loss = 3.09 (0.022 sec/batch)\n",
      "epoch 9, step 700 / 900, loss = 3.88 (0.022 sec/batch)\n",
      "epoch 9, step 750 / 900, loss = 3.18 (0.021 sec/batch)\n",
      "epoch 9, step 800 / 900, loss = 3.57 (0.021 sec/batch)\n",
      "epoch 9, step 850 / 900, loss = 2.50 (0.022 sec/batch)\n",
      "epoch 9, step 900 / 900, loss = 2.89 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 43.97\n",
      " avg loss = 3.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 42.70\n",
      " avg loss = 3.43\n",
      "\n",
      "Epoch time: 24.67490243911743\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 10, step 50 / 900, loss = 2.49 (0.021 sec/batch)\n",
      "epoch 10, step 100 / 900, loss = 3.75 (0.021 sec/batch)\n",
      "epoch 10, step 150 / 900, loss = 3.96 (0.021 sec/batch)\n",
      "epoch 10, step 200 / 900, loss = 3.58 (0.021 sec/batch)\n",
      "epoch 10, step 250 / 900, loss = 2.68 (0.021 sec/batch)\n",
      "epoch 10, step 300 / 900, loss = 3.14 (0.021 sec/batch)\n",
      "epoch 10, step 350 / 900, loss = 4.16 (0.022 sec/batch)\n",
      "epoch 10, step 400 / 900, loss = 3.26 (0.021 sec/batch)\n",
      "epoch 10, step 450 / 900, loss = 2.33 (0.022 sec/batch)\n",
      "epoch 10, step 500 / 900, loss = 3.95 (0.022 sec/batch)\n",
      "epoch 10, step 550 / 900, loss = 2.96 (0.022 sec/batch)\n",
      "epoch 10, step 600 / 900, loss = 4.29 (0.021 sec/batch)\n",
      "epoch 10, step 650 / 900, loss = 3.27 (0.021 sec/batch)\n",
      "epoch 10, step 700 / 900, loss = 2.56 (0.021 sec/batch)\n",
      "epoch 10, step 750 / 900, loss = 3.01 (0.021 sec/batch)\n",
      "epoch 10, step 800 / 900, loss = 3.63 (0.021 sec/batch)\n",
      "epoch 10, step 850 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 10, step 900 / 900, loss = 2.68 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 45.27\n",
      " avg loss = 3.06\n",
      "\n",
      "Validation error:\n",
      " accuracy = 43.04\n",
      " avg loss = 3.28\n",
      "\n",
      "Epoch time: 24.667642831802368\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 11, step 50 / 900, loss = 3.28 (0.021 sec/batch)\n",
      "epoch 11, step 100 / 900, loss = 2.65 (0.022 sec/batch)\n",
      "epoch 11, step 150 / 900, loss = 3.42 (0.022 sec/batch)\n",
      "epoch 11, step 200 / 900, loss = 3.12 (0.022 sec/batch)\n",
      "epoch 11, step 250 / 900, loss = 3.79 (0.021 sec/batch)\n",
      "epoch 11, step 300 / 900, loss = 3.81 (0.021 sec/batch)\n",
      "epoch 11, step 350 / 900, loss = 3.93 (0.021 sec/batch)\n",
      "epoch 11, step 400 / 900, loss = 2.95 (0.021 sec/batch)\n",
      "epoch 11, step 450 / 900, loss = 2.96 (0.022 sec/batch)\n",
      "epoch 11, step 500 / 900, loss = 3.16 (0.021 sec/batch)\n",
      "epoch 11, step 550 / 900, loss = 2.65 (0.022 sec/batch)\n",
      "epoch 11, step 600 / 900, loss = 3.20 (0.023 sec/batch)\n",
      "epoch 11, step 650 / 900, loss = 3.80 (0.023 sec/batch)\n",
      "epoch 11, step 700 / 900, loss = 3.35 (0.022 sec/batch)\n",
      "epoch 11, step 750 / 900, loss = 3.62 (0.021 sec/batch)\n",
      "epoch 11, step 800 / 900, loss = 3.47 (0.021 sec/batch)\n",
      "epoch 11, step 850 / 900, loss = 3.03 (0.023 sec/batch)\n",
      "epoch 11, step 900 / 900, loss = 2.25 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 46.26\n",
      " avg loss = 2.97\n",
      "\n",
      "Validation error:\n",
      " accuracy = 44.10\n",
      " avg loss = 3.24\n",
      "\n",
      "Epoch time: 24.905050039291382\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 12, step 50 / 900, loss = 2.71 (0.021 sec/batch)\n",
      "epoch 12, step 100 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 12, step 150 / 900, loss = 3.78 (0.023 sec/batch)\n",
      "epoch 12, step 200 / 900, loss = 2.91 (0.021 sec/batch)\n",
      "epoch 12, step 250 / 900, loss = 2.75 (0.022 sec/batch)\n",
      "epoch 12, step 300 / 900, loss = 3.53 (0.021 sec/batch)\n",
      "epoch 12, step 350 / 900, loss = 3.44 (0.022 sec/batch)\n",
      "epoch 12, step 400 / 900, loss = 2.87 (0.022 sec/batch)\n",
      "epoch 12, step 450 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 12, step 500 / 900, loss = 3.68 (0.022 sec/batch)\n",
      "epoch 12, step 550 / 900, loss = 2.90 (0.022 sec/batch)\n",
      "epoch 12, step 600 / 900, loss = 3.65 (0.022 sec/batch)\n",
      "epoch 12, step 650 / 900, loss = 3.14 (0.022 sec/batch)\n",
      "epoch 12, step 700 / 900, loss = 3.53 (0.021 sec/batch)\n",
      "epoch 12, step 750 / 900, loss = 3.08 (0.021 sec/batch)\n",
      "epoch 12, step 800 / 900, loss = 3.49 (0.020 sec/batch)\n",
      "epoch 12, step 850 / 900, loss = 3.06 (0.022 sec/batch)\n",
      "epoch 12, step 900 / 900, loss = 3.93 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 47.66\n",
      " avg loss = 2.91\n",
      "\n",
      "Validation error:\n",
      " accuracy = 45.16\n",
      " avg loss = 3.21\n",
      "\n",
      "Epoch time: 25.156410455703735\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 13, step 50 / 900, loss = 3.40 (0.021 sec/batch)\n",
      "epoch 13, step 100 / 900, loss = 3.07 (0.021 sec/batch)\n",
      "epoch 13, step 150 / 900, loss = 2.89 (0.023 sec/batch)\n",
      "epoch 13, step 200 / 900, loss = 3.53 (0.024 sec/batch)\n",
      "epoch 13, step 250 / 900, loss = 3.47 (0.021 sec/batch)\n",
      "epoch 13, step 300 / 900, loss = 3.28 (0.022 sec/batch)\n",
      "epoch 13, step 350 / 900, loss = 2.01 (0.022 sec/batch)\n",
      "epoch 13, step 400 / 900, loss = 1.82 (0.022 sec/batch)\n",
      "epoch 13, step 450 / 900, loss = 3.94 (0.021 sec/batch)\n",
      "epoch 13, step 500 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "epoch 13, step 550 / 900, loss = 2.96 (0.021 sec/batch)\n",
      "epoch 13, step 600 / 900, loss = 3.62 (0.022 sec/batch)\n",
      "epoch 13, step 650 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "epoch 13, step 700 / 900, loss = 3.46 (0.022 sec/batch)\n",
      "epoch 13, step 750 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 13, step 800 / 900, loss = 2.60 (0.021 sec/batch)\n",
      "epoch 13, step 850 / 900, loss = 2.77 (0.022 sec/batch)\n",
      "epoch 13, step 900 / 900, loss = 3.38 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 48.07\n",
      " avg loss = 2.84\n",
      "\n",
      "Validation error:\n",
      " accuracy = 45.28\n",
      " avg loss = 3.21\n",
      "\n",
      "Epoch time: 25.027442455291748\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 14, step 50 / 900, loss = 2.94 (0.021 sec/batch)\n",
      "epoch 14, step 100 / 900, loss = 2.92 (0.021 sec/batch)\n",
      "epoch 14, step 150 / 900, loss = 2.80 (0.022 sec/batch)\n",
      "epoch 14, step 200 / 900, loss = 2.72 (0.021 sec/batch)\n",
      "epoch 14, step 250 / 900, loss = 2.73 (0.022 sec/batch)\n",
      "epoch 14, step 300 / 900, loss = 2.39 (0.022 sec/batch)\n",
      "epoch 14, step 350 / 900, loss = 3.93 (0.021 sec/batch)\n",
      "epoch 14, step 400 / 900, loss = 2.72 (0.023 sec/batch)\n",
      "epoch 14, step 450 / 900, loss = 2.79 (0.022 sec/batch)\n",
      "epoch 14, step 500 / 900, loss = 2.99 (0.023 sec/batch)\n",
      "epoch 14, step 550 / 900, loss = 3.25 (0.022 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14, step 600 / 900, loss = 2.57 (0.021 sec/batch)\n",
      "epoch 14, step 650 / 900, loss = 2.97 (0.022 sec/batch)\n",
      "epoch 14, step 700 / 900, loss = 2.81 (0.021 sec/batch)\n",
      "epoch 14, step 750 / 900, loss = 2.94 (0.022 sec/batch)\n",
      "epoch 14, step 800 / 900, loss = 2.32 (0.021 sec/batch)\n",
      "epoch 14, step 850 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 14, step 900 / 900, loss = 3.36 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 49.58\n",
      " avg loss = 2.74\n",
      "\n",
      "Validation error:\n",
      " accuracy = 46.54\n",
      " avg loss = 3.04\n",
      "\n",
      "Epoch time: 24.955029249191284\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 15, step 50 / 900, loss = 2.83 (0.021 sec/batch)\n",
      "epoch 15, step 100 / 900, loss = 2.66 (0.021 sec/batch)\n",
      "epoch 15, step 150 / 900, loss = 3.30 (0.021 sec/batch)\n",
      "epoch 15, step 200 / 900, loss = 2.99 (0.021 sec/batch)\n",
      "epoch 15, step 250 / 900, loss = 1.95 (0.022 sec/batch)\n",
      "epoch 15, step 300 / 900, loss = 3.25 (0.022 sec/batch)\n",
      "epoch 15, step 350 / 900, loss = 2.27 (0.022 sec/batch)\n",
      "epoch 15, step 400 / 900, loss = 3.07 (0.021 sec/batch)\n",
      "epoch 15, step 450 / 900, loss = 3.00 (0.021 sec/batch)\n",
      "epoch 15, step 500 / 900, loss = 2.77 (0.023 sec/batch)\n",
      "epoch 15, step 550 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 15, step 600 / 900, loss = 3.53 (0.021 sec/batch)\n",
      "epoch 15, step 650 / 900, loss = 2.66 (0.022 sec/batch)\n",
      "epoch 15, step 700 / 900, loss = 2.76 (0.021 sec/batch)\n",
      "epoch 15, step 750 / 900, loss = 2.64 (0.021 sec/batch)\n",
      "epoch 15, step 800 / 900, loss = 3.38 (0.022 sec/batch)\n",
      "epoch 15, step 850 / 900, loss = 2.46 (0.022 sec/batch)\n",
      "epoch 15, step 900 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 49.96\n",
      " avg loss = 2.67\n",
      "\n",
      "Validation error:\n",
      " accuracy = 45.68\n",
      " avg loss = 3.10\n",
      "\n",
      "Epoch time: 24.797972679138184\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 16, step 50 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "epoch 16, step 100 / 900, loss = 2.73 (0.022 sec/batch)\n",
      "epoch 16, step 150 / 900, loss = 3.46 (0.021 sec/batch)\n",
      "epoch 16, step 200 / 900, loss = 2.76 (0.021 sec/batch)\n",
      "epoch 16, step 250 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 16, step 300 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 16, step 350 / 900, loss = 2.56 (0.022 sec/batch)\n",
      "epoch 16, step 400 / 900, loss = 2.14 (0.022 sec/batch)\n",
      "epoch 16, step 450 / 900, loss = 1.92 (0.021 sec/batch)\n",
      "epoch 16, step 500 / 900, loss = 3.61 (0.021 sec/batch)\n",
      "epoch 16, step 550 / 900, loss = 2.56 (0.021 sec/batch)\n",
      "epoch 16, step 600 / 900, loss = 3.03 (0.021 sec/batch)\n",
      "epoch 16, step 650 / 900, loss = 3.00 (0.021 sec/batch)\n",
      "epoch 16, step 700 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 16, step 750 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 16, step 800 / 900, loss = 2.99 (0.021 sec/batch)\n",
      "epoch 16, step 850 / 900, loss = 3.13 (0.023 sec/batch)\n",
      "epoch 16, step 900 / 900, loss = 2.66 (0.024 sec/batch)\n",
      "Train error:\n",
      " accuracy = 50.50\n",
      " avg loss = 2.66\n",
      "\n",
      "Validation error:\n",
      " accuracy = 46.86\n",
      " avg loss = 3.02\n",
      "\n",
      "Epoch time: 24.766833782196045\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 17, step 50 / 900, loss = 1.77 (0.021 sec/batch)\n",
      "epoch 17, step 100 / 900, loss = 1.71 (0.022 sec/batch)\n",
      "epoch 17, step 150 / 900, loss = 3.24 (0.021 sec/batch)\n",
      "epoch 17, step 200 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "epoch 17, step 250 / 900, loss = 2.47 (0.021 sec/batch)\n",
      "epoch 17, step 300 / 900, loss = 3.24 (0.021 sec/batch)\n",
      "epoch 17, step 350 / 900, loss = 3.49 (0.022 sec/batch)\n",
      "epoch 17, step 400 / 900, loss = 2.67 (0.021 sec/batch)\n",
      "epoch 17, step 450 / 900, loss = 2.01 (0.022 sec/batch)\n",
      "epoch 17, step 500 / 900, loss = 2.75 (0.022 sec/batch)\n",
      "epoch 17, step 550 / 900, loss = 2.70 (0.021 sec/batch)\n",
      "epoch 17, step 600 / 900, loss = 2.49 (0.022 sec/batch)\n",
      "epoch 17, step 650 / 900, loss = 2.13 (0.024 sec/batch)\n",
      "epoch 17, step 700 / 900, loss = 2.69 (0.022 sec/batch)\n",
      "epoch 17, step 750 / 900, loss = 2.26 (0.021 sec/batch)\n",
      "epoch 17, step 800 / 900, loss = 2.57 (0.022 sec/batch)\n",
      "epoch 17, step 850 / 900, loss = 2.49 (0.022 sec/batch)\n",
      "epoch 17, step 900 / 900, loss = 2.57 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 51.24\n",
      " avg loss = 2.60\n",
      "\n",
      "Validation error:\n",
      " accuracy = 47.12\n",
      " avg loss = 3.04\n",
      "\n",
      "Epoch time: 24.92636251449585\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 18, step 50 / 900, loss = 2.81 (0.021 sec/batch)\n",
      "epoch 18, step 100 / 900, loss = 2.70 (0.021 sec/batch)\n",
      "epoch 18, step 150 / 900, loss = 2.55 (0.023 sec/batch)\n",
      "epoch 18, step 200 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 18, step 250 / 900, loss = 2.18 (0.022 sec/batch)\n",
      "epoch 18, step 300 / 900, loss = 3.64 (0.021 sec/batch)\n",
      "epoch 18, step 350 / 900, loss = 3.34 (0.021 sec/batch)\n",
      "epoch 18, step 400 / 900, loss = 3.43 (0.021 sec/batch)\n",
      "epoch 18, step 450 / 900, loss = 1.68 (0.022 sec/batch)\n",
      "epoch 18, step 500 / 900, loss = 2.51 (0.021 sec/batch)\n",
      "epoch 18, step 550 / 900, loss = 1.78 (0.022 sec/batch)\n",
      "epoch 18, step 600 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 18, step 650 / 900, loss = 2.11 (0.022 sec/batch)\n",
      "epoch 18, step 700 / 900, loss = 3.11 (0.024 sec/batch)\n",
      "epoch 18, step 750 / 900, loss = 2.67 (0.021 sec/batch)\n",
      "epoch 18, step 800 / 900, loss = 1.70 (0.021 sec/batch)\n",
      "epoch 18, step 850 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "epoch 18, step 900 / 900, loss = 1.57 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 51.48\n",
      " avg loss = 2.56\n",
      "\n",
      "Validation error:\n",
      " accuracy = 47.82\n",
      " avg loss = 3.03\n",
      "\n",
      "Epoch time: 25.00806975364685\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 19, step 50 / 900, loss = 1.97 (0.022 sec/batch)\n",
      "epoch 19, step 100 / 900, loss = 2.19 (0.022 sec/batch)\n",
      "epoch 19, step 150 / 900, loss = 2.20 (0.022 sec/batch)\n",
      "epoch 19, step 200 / 900, loss = 3.70 (0.021 sec/batch)\n",
      "epoch 19, step 250 / 900, loss = 3.57 (0.021 sec/batch)\n",
      "epoch 19, step 300 / 900, loss = 2.68 (0.023 sec/batch)\n",
      "epoch 19, step 350 / 900, loss = 3.13 (0.021 sec/batch)\n",
      "epoch 19, step 400 / 900, loss = 3.41 (0.021 sec/batch)\n",
      "epoch 19, step 450 / 900, loss = 3.04 (0.021 sec/batch)\n",
      "epoch 19, step 500 / 900, loss = 2.83 (0.021 sec/batch)\n",
      "epoch 19, step 550 / 900, loss = 3.10 (0.022 sec/batch)\n",
      "epoch 19, step 600 / 900, loss = 2.62 (0.021 sec/batch)\n",
      "epoch 19, step 650 / 900, loss = 1.85 (0.021 sec/batch)\n",
      "epoch 19, step 700 / 900, loss = 3.24 (0.021 sec/batch)\n",
      "epoch 19, step 750 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 19, step 800 / 900, loss = 2.47 (0.022 sec/batch)\n",
      "epoch 19, step 850 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "epoch 19, step 900 / 900, loss = 1.73 (0.020 sec/batch)\n",
      "Train error:\n",
      " accuracy = 51.99\n",
      " avg loss = 2.53\n",
      "\n",
      "Validation error:\n",
      " accuracy = 47.00\n",
      " avg loss = 3.01\n",
      "\n",
      "Epoch time: 24.8160457611084\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 20, step 50 / 900, loss = 2.94 (0.022 sec/batch)\n",
      "epoch 20, step 100 / 900, loss = 2.59 (0.022 sec/batch)\n",
      "epoch 20, step 150 / 900, loss = 2.70 (0.021 sec/batch)\n",
      "epoch 20, step 200 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "epoch 20, step 250 / 900, loss = 2.30 (0.022 sec/batch)\n",
      "epoch 20, step 300 / 900, loss = 2.59 (0.022 sec/batch)\n",
      "epoch 20, step 350 / 900, loss = 2.85 (0.022 sec/batch)\n",
      "epoch 20, step 400 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 20, step 450 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 20, step 500 / 900, loss = 2.96 (0.021 sec/batch)\n",
      "epoch 20, step 550 / 900, loss = 2.62 (0.021 sec/batch)\n",
      "epoch 20, step 600 / 900, loss = 1.43 (0.022 sec/batch)\n",
      "epoch 20, step 650 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 20, step 700 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "epoch 20, step 750 / 900, loss = 2.66 (0.022 sec/batch)\n",
      "epoch 20, step 800 / 900, loss = 2.90 (0.021 sec/batch)\n",
      "epoch 20, step 850 / 900, loss = 1.79 (0.021 sec/batch)\n",
      "epoch 20, step 900 / 900, loss = 2.66 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 52.58\n",
      " avg loss = 2.49\n",
      "\n",
      "Validation error:\n",
      " accuracy = 47.52\n",
      " avg loss = 3.04\n",
      "\n",
      "Epoch time: 24.706579446792603\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 21, step 50 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 21, step 100 / 900, loss = 1.99 (0.021 sec/batch)\n",
      "epoch 21, step 150 / 900, loss = 2.05 (0.021 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21, step 200 / 900, loss = 3.04 (0.021 sec/batch)\n",
      "epoch 21, step 250 / 900, loss = 2.51 (0.021 sec/batch)\n",
      "epoch 21, step 300 / 900, loss = 1.95 (0.022 sec/batch)\n",
      "epoch 21, step 350 / 900, loss = 2.71 (0.022 sec/batch)\n",
      "epoch 21, step 400 / 900, loss = 2.82 (0.021 sec/batch)\n",
      "epoch 21, step 450 / 900, loss = 2.45 (0.021 sec/batch)\n",
      "epoch 21, step 500 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 21, step 550 / 900, loss = 1.88 (0.022 sec/batch)\n",
      "epoch 21, step 600 / 900, loss = 2.75 (0.021 sec/batch)\n",
      "epoch 21, step 650 / 900, loss = 2.50 (0.021 sec/batch)\n",
      "epoch 21, step 700 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 21, step 750 / 900, loss = 3.06 (0.021 sec/batch)\n",
      "epoch 21, step 800 / 900, loss = 2.36 (0.021 sec/batch)\n",
      "epoch 21, step 850 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 21, step 900 / 900, loss = 2.75 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 52.55\n",
      " avg loss = 2.48\n",
      "\n",
      "Validation error:\n",
      " accuracy = 47.84\n",
      " avg loss = 2.97\n",
      "\n",
      "Epoch time: 24.647420406341553\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 22, step 50 / 900, loss = 2.60 (0.022 sec/batch)\n",
      "epoch 22, step 100 / 900, loss = 2.66 (0.022 sec/batch)\n",
      "epoch 22, step 150 / 900, loss = 1.85 (0.022 sec/batch)\n",
      "epoch 22, step 200 / 900, loss = 2.68 (0.021 sec/batch)\n",
      "epoch 22, step 250 / 900, loss = 2.70 (0.021 sec/batch)\n",
      "epoch 22, step 300 / 900, loss = 2.23 (0.022 sec/batch)\n",
      "epoch 22, step 350 / 900, loss = 2.79 (0.021 sec/batch)\n",
      "epoch 22, step 400 / 900, loss = 2.26 (0.021 sec/batch)\n",
      "epoch 22, step 450 / 900, loss = 1.94 (0.023 sec/batch)\n",
      "epoch 22, step 500 / 900, loss = 2.04 (0.021 sec/batch)\n",
      "epoch 22, step 550 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "epoch 22, step 600 / 900, loss = 2.70 (0.022 sec/batch)\n",
      "epoch 22, step 650 / 900, loss = 2.41 (0.021 sec/batch)\n",
      "epoch 22, step 700 / 900, loss = 2.88 (0.022 sec/batch)\n",
      "epoch 22, step 750 / 900, loss = 2.06 (0.022 sec/batch)\n",
      "epoch 22, step 800 / 900, loss = 2.36 (0.021 sec/batch)\n",
      "epoch 22, step 850 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 22, step 900 / 900, loss = 2.58 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 53.37\n",
      " avg loss = 2.42\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.32\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.648625135421753\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 23, step 50 / 900, loss = 1.79 (0.022 sec/batch)\n",
      "epoch 23, step 100 / 900, loss = 2.77 (0.022 sec/batch)\n",
      "epoch 23, step 150 / 900, loss = 2.50 (0.022 sec/batch)\n",
      "epoch 23, step 200 / 900, loss = 1.81 (0.022 sec/batch)\n",
      "epoch 23, step 250 / 900, loss = 2.27 (0.022 sec/batch)\n",
      "epoch 23, step 300 / 900, loss = 1.52 (0.021 sec/batch)\n",
      "epoch 23, step 350 / 900, loss = 2.97 (0.021 sec/batch)\n",
      "epoch 23, step 400 / 900, loss = 2.41 (0.022 sec/batch)\n",
      "epoch 23, step 450 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "epoch 23, step 500 / 900, loss = 2.77 (0.021 sec/batch)\n",
      "epoch 23, step 550 / 900, loss = 2.13 (0.023 sec/batch)\n",
      "epoch 23, step 600 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 23, step 650 / 900, loss = 2.56 (0.021 sec/batch)\n",
      "epoch 23, step 700 / 900, loss = 3.18 (0.022 sec/batch)\n",
      "epoch 23, step 750 / 900, loss = 2.50 (0.021 sec/batch)\n",
      "epoch 23, step 800 / 900, loss = 2.54 (0.021 sec/batch)\n",
      "epoch 23, step 850 / 900, loss = 2.37 (0.022 sec/batch)\n",
      "epoch 23, step 900 / 900, loss = 1.77 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 53.76\n",
      " avg loss = 2.41\n",
      "\n",
      "Validation error:\n",
      " accuracy = 48.88\n",
      " avg loss = 2.94\n",
      "\n",
      "Epoch time: 24.669124603271484\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 24, step 50 / 900, loss = 2.39 (0.021 sec/batch)\n",
      "epoch 24, step 100 / 900, loss = 1.83 (0.022 sec/batch)\n",
      "epoch 24, step 150 / 900, loss = 2.57 (0.022 sec/batch)\n",
      "epoch 24, step 200 / 900, loss = 1.54 (0.021 sec/batch)\n",
      "epoch 24, step 250 / 900, loss = 2.32 (0.021 sec/batch)\n",
      "epoch 24, step 300 / 900, loss = 1.63 (0.022 sec/batch)\n",
      "epoch 24, step 350 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 24, step 400 / 900, loss = 2.64 (0.021 sec/batch)\n",
      "epoch 24, step 450 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "epoch 24, step 500 / 900, loss = 2.67 (0.022 sec/batch)\n",
      "epoch 24, step 550 / 900, loss = 2.51 (0.022 sec/batch)\n",
      "epoch 24, step 600 / 900, loss = 2.67 (0.021 sec/batch)\n",
      "epoch 24, step 650 / 900, loss = 1.96 (0.022 sec/batch)\n",
      "epoch 24, step 700 / 900, loss = 2.23 (0.022 sec/batch)\n",
      "epoch 24, step 750 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 24, step 800 / 900, loss = 2.71 (0.021 sec/batch)\n",
      "epoch 24, step 850 / 900, loss = 3.44 (0.021 sec/batch)\n",
      "epoch 24, step 900 / 900, loss = 2.34 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 53.50\n",
      " avg loss = 2.40\n",
      "\n",
      "Validation error:\n",
      " accuracy = 48.90\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.6567862033844\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 25, step 50 / 900, loss = 2.67 (0.021 sec/batch)\n",
      "epoch 25, step 100 / 900, loss = 3.66 (0.021 sec/batch)\n",
      "epoch 25, step 150 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 25, step 200 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 25, step 250 / 900, loss = 3.29 (0.021 sec/batch)\n",
      "epoch 25, step 300 / 900, loss = 2.62 (0.022 sec/batch)\n",
      "epoch 25, step 350 / 900, loss = 2.60 (0.021 sec/batch)\n",
      "epoch 25, step 400 / 900, loss = 2.94 (0.021 sec/batch)\n",
      "epoch 25, step 450 / 900, loss = 2.12 (0.022 sec/batch)\n",
      "epoch 25, step 500 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "epoch 25, step 550 / 900, loss = 3.70 (0.022 sec/batch)\n",
      "epoch 25, step 600 / 900, loss = 2.54 (0.021 sec/batch)\n",
      "epoch 25, step 650 / 900, loss = 3.44 (0.021 sec/batch)\n",
      "epoch 25, step 700 / 900, loss = 1.67 (0.021 sec/batch)\n",
      "epoch 25, step 750 / 900, loss = 1.64 (0.021 sec/batch)\n",
      "epoch 25, step 800 / 900, loss = 2.94 (0.021 sec/batch)\n",
      "epoch 25, step 850 / 900, loss = 3.00 (0.020 sec/batch)\n",
      "epoch 25, step 900 / 900, loss = 2.60 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 53.87\n",
      " avg loss = 2.37\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.10\n",
      " avg loss = 2.93\n",
      "\n",
      "Epoch time: 24.693224668502808\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 26, step 50 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 26, step 100 / 900, loss = 1.76 (0.021 sec/batch)\n",
      "epoch 26, step 150 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 26, step 200 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 26, step 250 / 900, loss = 2.87 (0.022 sec/batch)\n",
      "epoch 26, step 300 / 900, loss = 2.05 (0.022 sec/batch)\n",
      "epoch 26, step 350 / 900, loss = 1.72 (0.022 sec/batch)\n",
      "epoch 26, step 400 / 900, loss = 2.58 (0.021 sec/batch)\n",
      "epoch 26, step 450 / 900, loss = 2.55 (0.021 sec/batch)\n",
      "epoch 26, step 500 / 900, loss = 2.98 (0.022 sec/batch)\n",
      "epoch 26, step 550 / 900, loss = 2.32 (0.021 sec/batch)\n",
      "epoch 26, step 600 / 900, loss = 2.57 (0.021 sec/batch)\n",
      "epoch 26, step 650 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 26, step 700 / 900, loss = 1.09 (0.021 sec/batch)\n",
      "epoch 26, step 750 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 26, step 800 / 900, loss = 1.94 (0.022 sec/batch)\n",
      "epoch 26, step 850 / 900, loss = 1.43 (0.021 sec/batch)\n",
      "epoch 26, step 900 / 900, loss = 2.73 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 54.08\n",
      " avg loss = 2.36\n",
      "\n",
      "Validation error:\n",
      " accuracy = 48.92\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.66315722465515\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 27, step 50 / 900, loss = 2.65 (0.022 sec/batch)\n",
      "epoch 27, step 100 / 900, loss = 2.80 (0.021 sec/batch)\n",
      "epoch 27, step 150 / 900, loss = 2.30 (0.022 sec/batch)\n",
      "epoch 27, step 200 / 900, loss = 2.57 (0.021 sec/batch)\n",
      "epoch 27, step 250 / 900, loss = 2.24 (0.022 sec/batch)\n",
      "epoch 27, step 300 / 900, loss = 2.91 (0.021 sec/batch)\n",
      "epoch 27, step 350 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "epoch 27, step 400 / 900, loss = 2.46 (0.022 sec/batch)\n",
      "epoch 27, step 450 / 900, loss = 2.91 (0.021 sec/batch)\n",
      "epoch 27, step 500 / 900, loss = 1.92 (0.021 sec/batch)\n",
      "epoch 27, step 550 / 900, loss = 2.36 (0.021 sec/batch)\n",
      "epoch 27, step 600 / 900, loss = 2.24 (0.021 sec/batch)\n",
      "epoch 27, step 650 / 900, loss = 2.45 (0.022 sec/batch)\n",
      "epoch 27, step 700 / 900, loss = 2.15 (0.021 sec/batch)\n",
      "epoch 27, step 750 / 900, loss = 2.63 (0.022 sec/batch)\n",
      "epoch 27, step 800 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "epoch 27, step 850 / 900, loss = 2.12 (0.022 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 27, step 900 / 900, loss = 2.39 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 54.45\n",
      " avg loss = 2.34\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.66\n",
      " avg loss = 2.97\n",
      "\n",
      "Epoch time: 24.660907983779907\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 28, step 50 / 900, loss = 2.78 (0.021 sec/batch)\n",
      "epoch 28, step 100 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 28, step 150 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 28, step 200 / 900, loss = 3.50 (0.023 sec/batch)\n",
      "epoch 28, step 250 / 900, loss = 2.86 (0.021 sec/batch)\n",
      "epoch 28, step 300 / 900, loss = 2.30 (0.022 sec/batch)\n",
      "epoch 28, step 350 / 900, loss = 3.56 (0.022 sec/batch)\n",
      "epoch 28, step 400 / 900, loss = 2.56 (0.022 sec/batch)\n",
      "epoch 28, step 450 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "epoch 28, step 500 / 900, loss = 2.77 (0.021 sec/batch)\n",
      "epoch 28, step 550 / 900, loss = 2.58 (0.021 sec/batch)\n",
      "epoch 28, step 600 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 28, step 650 / 900, loss = 1.73 (0.022 sec/batch)\n",
      "epoch 28, step 700 / 900, loss = 2.72 (0.021 sec/batch)\n",
      "epoch 28, step 750 / 900, loss = 2.59 (0.021 sec/batch)\n",
      "epoch 28, step 800 / 900, loss = 2.17 (0.022 sec/batch)\n",
      "epoch 28, step 850 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "epoch 28, step 900 / 900, loss = 2.83 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 54.15\n",
      " avg loss = 2.35\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.30\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.676881790161133\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 29, step 50 / 900, loss = 2.74 (0.022 sec/batch)\n",
      "epoch 29, step 100 / 900, loss = 2.95 (0.022 sec/batch)\n",
      "epoch 29, step 150 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 29, step 200 / 900, loss = 2.62 (0.022 sec/batch)\n",
      "epoch 29, step 250 / 900, loss = 2.95 (0.022 sec/batch)\n",
      "epoch 29, step 300 / 900, loss = 3.17 (0.022 sec/batch)\n",
      "epoch 29, step 350 / 900, loss = 1.96 (0.021 sec/batch)\n",
      "epoch 29, step 400 / 900, loss = 1.92 (0.022 sec/batch)\n",
      "epoch 29, step 450 / 900, loss = 2.51 (0.021 sec/batch)\n",
      "epoch 29, step 500 / 900, loss = 2.15 (0.021 sec/batch)\n",
      "epoch 29, step 550 / 900, loss = 2.78 (0.022 sec/batch)\n",
      "epoch 29, step 600 / 900, loss = 2.68 (0.022 sec/batch)\n",
      "epoch 29, step 650 / 900, loss = 1.88 (0.021 sec/batch)\n",
      "epoch 29, step 700 / 900, loss = 2.31 (0.022 sec/batch)\n",
      "epoch 29, step 750 / 900, loss = 1.75 (0.021 sec/batch)\n",
      "epoch 29, step 800 / 900, loss = 2.18 (0.022 sec/batch)\n",
      "epoch 29, step 850 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 29, step 900 / 900, loss = 2.85 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 54.77\n",
      " avg loss = 2.31\n",
      "\n",
      "Validation error:\n",
      " accuracy = 48.84\n",
      " avg loss = 2.86\n",
      "\n",
      "Epoch time: 24.680814743041992\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 30, step 50 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "epoch 30, step 100 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 30, step 150 / 900, loss = 1.59 (0.022 sec/batch)\n",
      "epoch 30, step 200 / 900, loss = 2.54 (0.021 sec/batch)\n",
      "epoch 30, step 250 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 30, step 300 / 900, loss = 3.23 (0.022 sec/batch)\n",
      "epoch 30, step 350 / 900, loss = 1.51 (0.022 sec/batch)\n",
      "epoch 30, step 400 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 30, step 450 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 30, step 500 / 900, loss = 3.23 (0.022 sec/batch)\n",
      "epoch 30, step 550 / 900, loss = 1.86 (0.022 sec/batch)\n",
      "epoch 30, step 600 / 900, loss = 2.58 (0.021 sec/batch)\n",
      "epoch 30, step 650 / 900, loss = 2.88 (0.021 sec/batch)\n",
      "epoch 30, step 700 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "epoch 30, step 750 / 900, loss = 2.32 (0.021 sec/batch)\n",
      "epoch 30, step 800 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 30, step 850 / 900, loss = 2.49 (0.021 sec/batch)\n",
      "epoch 30, step 900 / 900, loss = 1.87 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 54.64\n",
      " avg loss = 2.35\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.32\n",
      " avg loss = 2.92\n",
      "\n",
      "Epoch time: 24.685240507125854\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 31, step 50 / 900, loss = 1.78 (0.021 sec/batch)\n",
      "epoch 31, step 100 / 900, loss = 2.49 (0.021 sec/batch)\n",
      "epoch 31, step 150 / 900, loss = 2.59 (0.022 sec/batch)\n",
      "epoch 31, step 200 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 31, step 250 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "epoch 31, step 300 / 900, loss = 2.14 (0.023 sec/batch)\n",
      "epoch 31, step 350 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 31, step 400 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 31, step 450 / 900, loss = 2.16 (0.022 sec/batch)\n",
      "epoch 31, step 500 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 31, step 550 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 31, step 600 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 31, step 650 / 900, loss = 3.09 (0.021 sec/batch)\n",
      "epoch 31, step 700 / 900, loss = 2.22 (0.022 sec/batch)\n",
      "epoch 31, step 750 / 900, loss = 2.23 (0.022 sec/batch)\n",
      "epoch 31, step 800 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 31, step 850 / 900, loss = 2.39 (0.022 sec/batch)\n",
      "epoch 31, step 900 / 900, loss = 2.33 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 54.78\n",
      " avg loss = 2.33\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.38\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.678452730178833\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 32, step 50 / 900, loss = 1.39 (0.021 sec/batch)\n",
      "epoch 32, step 100 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 32, step 150 / 900, loss = 1.75 (0.021 sec/batch)\n",
      "epoch 32, step 200 / 900, loss = 2.74 (0.022 sec/batch)\n",
      "epoch 32, step 250 / 900, loss = 2.19 (0.022 sec/batch)\n",
      "epoch 32, step 300 / 900, loss = 3.23 (0.021 sec/batch)\n",
      "epoch 32, step 350 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "epoch 32, step 400 / 900, loss = 2.23 (0.022 sec/batch)\n",
      "epoch 32, step 450 / 900, loss = 1.98 (0.021 sec/batch)\n",
      "epoch 32, step 500 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 32, step 550 / 900, loss = 1.48 (0.022 sec/batch)\n",
      "epoch 32, step 600 / 900, loss = 1.86 (0.021 sec/batch)\n",
      "epoch 32, step 650 / 900, loss = 2.38 (0.022 sec/batch)\n",
      "epoch 32, step 700 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 32, step 750 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 32, step 800 / 900, loss = 2.84 (0.021 sec/batch)\n",
      "epoch 32, step 850 / 900, loss = 3.61 (0.021 sec/batch)\n",
      "epoch 32, step 900 / 900, loss = 2.06 (0.020 sec/batch)\n",
      "Train error:\n",
      " accuracy = 54.97\n",
      " avg loss = 2.29\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.02\n",
      " avg loss = 2.85\n",
      "\n",
      "Epoch time: 24.658416032791138\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 33, step 50 / 900, loss = 2.34 (0.021 sec/batch)\n",
      "epoch 33, step 100 / 900, loss = 2.98 (0.021 sec/batch)\n",
      "epoch 33, step 150 / 900, loss = 2.86 (0.023 sec/batch)\n",
      "epoch 33, step 200 / 900, loss = 2.69 (0.021 sec/batch)\n",
      "epoch 33, step 250 / 900, loss = 2.61 (0.022 sec/batch)\n",
      "epoch 33, step 300 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 33, step 350 / 900, loss = 1.28 (0.022 sec/batch)\n",
      "epoch 33, step 400 / 900, loss = 1.74 (0.022 sec/batch)\n",
      "epoch 33, step 450 / 900, loss = 2.46 (0.021 sec/batch)\n",
      "epoch 33, step 500 / 900, loss = 2.04 (0.021 sec/batch)\n",
      "epoch 33, step 550 / 900, loss = 1.79 (0.022 sec/batch)\n",
      "epoch 33, step 600 / 900, loss = 2.07 (0.022 sec/batch)\n",
      "epoch 33, step 650 / 900, loss = 2.06 (0.021 sec/batch)\n",
      "epoch 33, step 700 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 33, step 750 / 900, loss = 1.70 (0.021 sec/batch)\n",
      "epoch 33, step 800 / 900, loss = 2.07 (0.022 sec/batch)\n",
      "epoch 33, step 850 / 900, loss = 2.54 (0.022 sec/batch)\n",
      "epoch 33, step 900 / 900, loss = 1.93 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 54.92\n",
      " avg loss = 2.31\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.68\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.665374755859375\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 34, step 50 / 900, loss = 2.17 (0.020 sec/batch)\n",
      "epoch 34, step 100 / 900, loss = 2.04 (0.022 sec/batch)\n",
      "epoch 34, step 150 / 900, loss = 1.85 (0.021 sec/batch)\n",
      "epoch 34, step 200 / 900, loss = 2.61 (0.022 sec/batch)\n",
      "epoch 34, step 250 / 900, loss = 2.33 (0.022 sec/batch)\n",
      "epoch 34, step 300 / 900, loss = 1.96 (0.022 sec/batch)\n",
      "epoch 34, step 350 / 900, loss = 2.35 (0.022 sec/batch)\n",
      "epoch 34, step 400 / 900, loss = 2.39 (0.022 sec/batch)\n",
      "epoch 34, step 450 / 900, loss = 2.04 (0.022 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 34, step 500 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 34, step 550 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "epoch 34, step 600 / 900, loss = 1.90 (0.021 sec/batch)\n",
      "epoch 34, step 650 / 900, loss = 1.82 (0.022 sec/batch)\n",
      "epoch 34, step 700 / 900, loss = 1.53 (0.021 sec/batch)\n",
      "epoch 34, step 750 / 900, loss = 2.52 (0.022 sec/batch)\n",
      "epoch 34, step 800 / 900, loss = 2.46 (0.021 sec/batch)\n",
      "epoch 34, step 850 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 34, step 900 / 900, loss = 2.80 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 54.77\n",
      " avg loss = 2.32\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.68\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.70180630683899\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 35, step 50 / 900, loss = 1.80 (0.021 sec/batch)\n",
      "epoch 35, step 100 / 900, loss = 2.26 (0.022 sec/batch)\n",
      "epoch 35, step 150 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 35, step 200 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 35, step 250 / 900, loss = 1.93 (0.022 sec/batch)\n",
      "epoch 35, step 300 / 900, loss = 1.88 (0.022 sec/batch)\n",
      "epoch 35, step 350 / 900, loss = 3.04 (0.021 sec/batch)\n",
      "epoch 35, step 400 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 35, step 450 / 900, loss = 2.26 (0.021 sec/batch)\n",
      "epoch 35, step 500 / 900, loss = 1.76 (0.021 sec/batch)\n",
      "epoch 35, step 550 / 900, loss = 1.77 (0.021 sec/batch)\n",
      "epoch 35, step 600 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "epoch 35, step 650 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "epoch 35, step 700 / 900, loss = 1.70 (0.021 sec/batch)\n",
      "epoch 35, step 750 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 35, step 800 / 900, loss = 2.80 (0.021 sec/batch)\n",
      "epoch 35, step 850 / 900, loss = 3.12 (0.021 sec/batch)\n",
      "epoch 35, step 900 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.02\n",
      " avg loss = 2.30\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.42\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.656322956085205\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 36, step 50 / 900, loss = 2.63 (0.022 sec/batch)\n",
      "epoch 36, step 100 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 36, step 150 / 900, loss = 2.74 (0.021 sec/batch)\n",
      "epoch 36, step 200 / 900, loss = 1.66 (0.022 sec/batch)\n",
      "epoch 36, step 250 / 900, loss = 3.06 (0.021 sec/batch)\n",
      "epoch 36, step 300 / 900, loss = 2.39 (0.022 sec/batch)\n",
      "epoch 36, step 350 / 900, loss = 2.83 (0.022 sec/batch)\n",
      "epoch 36, step 400 / 900, loss = 1.92 (0.021 sec/batch)\n",
      "epoch 36, step 450 / 900, loss = 3.16 (0.021 sec/batch)\n",
      "epoch 36, step 500 / 900, loss = 1.84 (0.021 sec/batch)\n",
      "epoch 36, step 550 / 900, loss = 2.19 (0.022 sec/batch)\n",
      "epoch 36, step 600 / 900, loss = 3.49 (0.021 sec/batch)\n",
      "epoch 36, step 650 / 900, loss = 2.59 (0.021 sec/batch)\n",
      "epoch 36, step 700 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 36, step 750 / 900, loss = 2.55 (0.021 sec/batch)\n",
      "epoch 36, step 800 / 900, loss = 2.58 (0.021 sec/batch)\n",
      "epoch 36, step 850 / 900, loss = 2.82 (0.022 sec/batch)\n",
      "epoch 36, step 900 / 900, loss = 2.75 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.06\n",
      " avg loss = 2.30\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.18\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.659995794296265\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 37, step 50 / 900, loss = 1.90 (0.021 sec/batch)\n",
      "epoch 37, step 100 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "epoch 37, step 150 / 900, loss = 2.85 (0.021 sec/batch)\n",
      "epoch 37, step 200 / 900, loss = 2.41 (0.021 sec/batch)\n",
      "epoch 37, step 250 / 900, loss = 2.16 (0.022 sec/batch)\n",
      "epoch 37, step 300 / 900, loss = 1.39 (0.022 sec/batch)\n",
      "epoch 37, step 350 / 900, loss = 2.98 (0.022 sec/batch)\n",
      "epoch 37, step 400 / 900, loss = 2.87 (0.021 sec/batch)\n",
      "epoch 37, step 450 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 37, step 500 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "epoch 37, step 550 / 900, loss = 1.73 (0.021 sec/batch)\n",
      "epoch 37, step 600 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 37, step 650 / 900, loss = 2.67 (0.021 sec/batch)\n",
      "epoch 37, step 700 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "epoch 37, step 750 / 900, loss = 2.29 (0.020 sec/batch)\n",
      "epoch 37, step 800 / 900, loss = 1.77 (0.021 sec/batch)\n",
      "epoch 37, step 850 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 37, step 900 / 900, loss = 2.59 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.19\n",
      " avg loss = 2.30\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.44\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.663344383239746\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 38, step 50 / 900, loss = 2.04 (0.021 sec/batch)\n",
      "epoch 38, step 100 / 900, loss = 1.81 (0.021 sec/batch)\n",
      "epoch 38, step 150 / 900, loss = 1.87 (0.022 sec/batch)\n",
      "epoch 38, step 200 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 38, step 250 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 38, step 300 / 900, loss = 2.33 (0.022 sec/batch)\n",
      "epoch 38, step 350 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 38, step 400 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "epoch 38, step 450 / 900, loss = 2.12 (0.022 sec/batch)\n",
      "epoch 38, step 500 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "epoch 38, step 550 / 900, loss = 1.40 (0.021 sec/batch)\n",
      "epoch 38, step 600 / 900, loss = 2.79 (0.021 sec/batch)\n",
      "epoch 38, step 650 / 900, loss = 3.12 (0.021 sec/batch)\n",
      "epoch 38, step 700 / 900, loss = 2.62 (0.022 sec/batch)\n",
      "epoch 38, step 750 / 900, loss = 2.17 (0.022 sec/batch)\n",
      "epoch 38, step 800 / 900, loss = 2.42 (0.022 sec/batch)\n",
      "epoch 38, step 850 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 38, step 900 / 900, loss = 1.84 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.50\n",
      " avg loss = 2.28\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.34\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.653724431991577\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 39, step 50 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "epoch 39, step 100 / 900, loss = 2.62 (0.021 sec/batch)\n",
      "epoch 39, step 150 / 900, loss = 2.39 (0.021 sec/batch)\n",
      "epoch 39, step 200 / 900, loss = 2.85 (0.021 sec/batch)\n",
      "epoch 39, step 250 / 900, loss = 3.70 (0.021 sec/batch)\n",
      "epoch 39, step 300 / 900, loss = 3.03 (0.022 sec/batch)\n",
      "epoch 39, step 350 / 900, loss = 2.66 (0.021 sec/batch)\n",
      "epoch 39, step 400 / 900, loss = 2.75 (0.021 sec/batch)\n",
      "epoch 39, step 450 / 900, loss = 2.89 (0.022 sec/batch)\n",
      "epoch 39, step 500 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "epoch 39, step 550 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 39, step 600 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 39, step 650 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "epoch 39, step 700 / 900, loss = 1.68 (0.021 sec/batch)\n",
      "epoch 39, step 750 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 39, step 800 / 900, loss = 2.66 (0.022 sec/batch)\n",
      "epoch 39, step 850 / 900, loss = 3.05 (0.021 sec/batch)\n",
      "epoch 39, step 900 / 900, loss = 1.56 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.17\n",
      " avg loss = 2.29\n",
      "\n",
      "Validation error:\n",
      " accuracy = 48.92\n",
      " avg loss = 2.92\n",
      "\n",
      "Epoch time: 24.675780296325684\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 40, step 50 / 900, loss = 2.73 (0.022 sec/batch)\n",
      "epoch 40, step 100 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 40, step 150 / 900, loss = 2.73 (0.022 sec/batch)\n",
      "epoch 40, step 200 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 40, step 250 / 900, loss = 2.63 (0.022 sec/batch)\n",
      "epoch 40, step 300 / 900, loss = 1.90 (0.022 sec/batch)\n",
      "epoch 40, step 350 / 900, loss = 1.44 (0.021 sec/batch)\n",
      "epoch 40, step 400 / 900, loss = 3.17 (0.021 sec/batch)\n",
      "epoch 40, step 450 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 40, step 500 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 40, step 550 / 900, loss = 2.49 (0.021 sec/batch)\n",
      "epoch 40, step 600 / 900, loss = 2.51 (0.021 sec/batch)\n",
      "epoch 40, step 650 / 900, loss = 2.59 (0.022 sec/batch)\n",
      "epoch 40, step 700 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 40, step 750 / 900, loss = 2.49 (0.022 sec/batch)\n",
      "epoch 40, step 800 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 40, step 850 / 900, loss = 1.15 (0.022 sec/batch)\n",
      "epoch 40, step 900 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.19\n",
      " avg loss = 2.29\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.52\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.658817768096924\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 41, step 50 / 900, loss = 1.54 (0.021 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 41, step 100 / 900, loss = 2.69 (0.022 sec/batch)\n",
      "epoch 41, step 150 / 900, loss = 2.68 (0.021 sec/batch)\n",
      "epoch 41, step 200 / 900, loss = 2.99 (0.021 sec/batch)\n",
      "epoch 41, step 250 / 900, loss = 1.90 (0.021 sec/batch)\n",
      "epoch 41, step 300 / 900, loss = 2.76 (0.021 sec/batch)\n",
      "epoch 41, step 350 / 900, loss = 2.62 (0.022 sec/batch)\n",
      "epoch 41, step 400 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 41, step 450 / 900, loss = 1.81 (0.021 sec/batch)\n",
      "epoch 41, step 500 / 900, loss = 2.31 (0.022 sec/batch)\n",
      "epoch 41, step 550 / 900, loss = 1.53 (0.022 sec/batch)\n",
      "epoch 41, step 600 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 41, step 650 / 900, loss = 1.71 (0.021 sec/batch)\n",
      "epoch 41, step 700 / 900, loss = 1.82 (0.022 sec/batch)\n",
      "epoch 41, step 750 / 900, loss = 2.36 (0.021 sec/batch)\n",
      "epoch 41, step 800 / 900, loss = 2.45 (0.021 sec/batch)\n",
      "epoch 41, step 850 / 900, loss = 2.72 (0.022 sec/batch)\n",
      "epoch 41, step 900 / 900, loss = 1.78 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.42\n",
      " avg loss = 2.28\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.96\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.663236618041992\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 42, step 50 / 900, loss = 1.71 (0.021 sec/batch)\n",
      "epoch 42, step 100 / 900, loss = 1.72 (0.021 sec/batch)\n",
      "epoch 42, step 150 / 900, loss = 2.20 (0.022 sec/batch)\n",
      "epoch 42, step 200 / 900, loss = 2.01 (0.022 sec/batch)\n",
      "epoch 42, step 250 / 900, loss = 2.36 (0.022 sec/batch)\n",
      "epoch 42, step 300 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 42, step 350 / 900, loss = 2.14 (0.022 sec/batch)\n",
      "epoch 42, step 400 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 42, step 450 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 42, step 500 / 900, loss = 2.06 (0.021 sec/batch)\n",
      "epoch 42, step 550 / 900, loss = 2.71 (0.021 sec/batch)\n",
      "epoch 42, step 600 / 900, loss = 2.33 (0.022 sec/batch)\n",
      "epoch 42, step 650 / 900, loss = 2.46 (0.022 sec/batch)\n",
      "epoch 42, step 700 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "epoch 42, step 750 / 900, loss = 2.51 (0.022 sec/batch)\n",
      "epoch 42, step 800 / 900, loss = 1.20 (0.021 sec/batch)\n",
      "epoch 42, step 850 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 42, step 900 / 900, loss = 3.51 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.16\n",
      " avg loss = 2.30\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.76\n",
      " avg loss = 2.85\n",
      "\n",
      "Epoch time: 24.67352318763733\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 43, step 50 / 900, loss = 1.35 (0.021 sec/batch)\n",
      "epoch 43, step 100 / 900, loss = 2.22 (0.022 sec/batch)\n",
      "epoch 43, step 150 / 900, loss = 2.03 (0.022 sec/batch)\n",
      "epoch 43, step 200 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 43, step 250 / 900, loss = 2.34 (0.021 sec/batch)\n",
      "epoch 43, step 300 / 900, loss = 2.93 (0.022 sec/batch)\n",
      "epoch 43, step 350 / 900, loss = 1.76 (0.022 sec/batch)\n",
      "epoch 43, step 400 / 900, loss = 2.55 (0.021 sec/batch)\n",
      "epoch 43, step 450 / 900, loss = 2.34 (0.022 sec/batch)\n",
      "epoch 43, step 500 / 900, loss = 1.85 (0.021 sec/batch)\n",
      "epoch 43, step 550 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 43, step 600 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 43, step 650 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "epoch 43, step 700 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 43, step 750 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "epoch 43, step 800 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 43, step 850 / 900, loss = 2.49 (0.021 sec/batch)\n",
      "epoch 43, step 900 / 900, loss = 2.17 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.40\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.20\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.83696174621582\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 44, step 50 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 44, step 100 / 900, loss = 2.89 (0.021 sec/batch)\n",
      "epoch 44, step 150 / 900, loss = 1.79 (0.021 sec/batch)\n",
      "epoch 44, step 200 / 900, loss = 2.97 (0.021 sec/batch)\n",
      "epoch 44, step 250 / 900, loss = 2.66 (0.021 sec/batch)\n",
      "epoch 44, step 300 / 900, loss = 2.51 (0.021 sec/batch)\n",
      "epoch 44, step 350 / 900, loss = 2.51 (0.022 sec/batch)\n",
      "epoch 44, step 400 / 900, loss = 1.68 (0.021 sec/batch)\n",
      "epoch 44, step 450 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 44, step 500 / 900, loss = 2.16 (0.022 sec/batch)\n",
      "epoch 44, step 550 / 900, loss = 1.79 (0.021 sec/batch)\n",
      "epoch 44, step 600 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "epoch 44, step 650 / 900, loss = 2.09 (0.022 sec/batch)\n",
      "epoch 44, step 700 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 44, step 750 / 900, loss = 1.82 (0.021 sec/batch)\n",
      "epoch 44, step 800 / 900, loss = 1.94 (0.022 sec/batch)\n",
      "epoch 44, step 850 / 900, loss = 2.25 (0.020 sec/batch)\n",
      "epoch 44, step 900 / 900, loss = 2.58 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.45\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.62\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.607937812805176\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 45, step 50 / 900, loss = 2.68 (0.020 sec/batch)\n",
      "epoch 45, step 100 / 900, loss = 2.41 (0.021 sec/batch)\n",
      "epoch 45, step 150 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 45, step 200 / 900, loss = 2.87 (0.022 sec/batch)\n",
      "epoch 45, step 250 / 900, loss = 1.72 (0.022 sec/batch)\n",
      "epoch 45, step 300 / 900, loss = 1.31 (0.022 sec/batch)\n",
      "epoch 45, step 350 / 900, loss = 2.39 (0.021 sec/batch)\n",
      "epoch 45, step 400 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 45, step 450 / 900, loss = 2.57 (0.021 sec/batch)\n",
      "epoch 45, step 500 / 900, loss = 2.40 (0.021 sec/batch)\n",
      "epoch 45, step 550 / 900, loss = 2.78 (0.022 sec/batch)\n",
      "epoch 45, step 600 / 900, loss = 2.40 (0.021 sec/batch)\n",
      "epoch 45, step 650 / 900, loss = 2.15 (0.022 sec/batch)\n",
      "epoch 45, step 700 / 900, loss = 1.82 (0.022 sec/batch)\n",
      "epoch 45, step 750 / 900, loss = 1.98 (0.021 sec/batch)\n",
      "epoch 45, step 800 / 900, loss = 2.75 (0.021 sec/batch)\n",
      "epoch 45, step 850 / 900, loss = 2.14 (0.022 sec/batch)\n",
      "epoch 45, step 900 / 900, loss = 2.68 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.50\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.14\n",
      " avg loss = 2.85\n",
      "\n",
      "Epoch time: 24.58825993537903\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 46, step 50 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "epoch 46, step 100 / 900, loss = 1.57 (0.022 sec/batch)\n",
      "epoch 46, step 150 / 900, loss = 2.44 (0.022 sec/batch)\n",
      "epoch 46, step 200 / 900, loss = 1.84 (0.021 sec/batch)\n",
      "epoch 46, step 250 / 900, loss = 2.91 (0.021 sec/batch)\n",
      "epoch 46, step 300 / 900, loss = 2.06 (0.021 sec/batch)\n",
      "epoch 46, step 350 / 900, loss = 1.54 (0.021 sec/batch)\n",
      "epoch 46, step 400 / 900, loss = 2.32 (0.021 sec/batch)\n",
      "epoch 46, step 450 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 46, step 500 / 900, loss = 2.91 (0.021 sec/batch)\n",
      "epoch 46, step 550 / 900, loss = 1.93 (0.022 sec/batch)\n",
      "epoch 46, step 600 / 900, loss = 1.67 (0.021 sec/batch)\n",
      "epoch 46, step 650 / 900, loss = 1.47 (0.021 sec/batch)\n",
      "epoch 46, step 700 / 900, loss = 2.32 (0.021 sec/batch)\n",
      "epoch 46, step 750 / 900, loss = 2.76 (0.021 sec/batch)\n",
      "epoch 46, step 800 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 46, step 850 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 46, step 900 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.43\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.26\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.58628559112549\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 47, step 50 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 47, step 100 / 900, loss = 2.94 (0.022 sec/batch)\n",
      "epoch 47, step 150 / 900, loss = 2.39 (0.021 sec/batch)\n",
      "epoch 47, step 200 / 900, loss = 2.76 (0.022 sec/batch)\n",
      "epoch 47, step 250 / 900, loss = 1.56 (0.021 sec/batch)\n",
      "epoch 47, step 300 / 900, loss = 1.93 (0.022 sec/batch)\n",
      "epoch 47, step 350 / 900, loss = 3.00 (0.021 sec/batch)\n",
      "epoch 47, step 400 / 900, loss = 1.87 (0.021 sec/batch)\n",
      "epoch 47, step 450 / 900, loss = 1.81 (0.022 sec/batch)\n",
      "epoch 47, step 500 / 900, loss = 3.16 (0.021 sec/batch)\n",
      "epoch 47, step 550 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 47, step 600 / 900, loss = 3.01 (0.022 sec/batch)\n",
      "epoch 47, step 650 / 900, loss = 1.88 (0.021 sec/batch)\n",
      "epoch 47, step 700 / 900, loss = 2.26 (0.021 sec/batch)\n",
      "epoch 47, step 750 / 900, loss = 1.47 (0.022 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 47, step 800 / 900, loss = 2.96 (0.021 sec/batch)\n",
      "epoch 47, step 850 / 900, loss = 2.18 (0.022 sec/batch)\n",
      "epoch 47, step 900 / 900, loss = 2.94 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.20\n",
      " avg loss = 2.28\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.88\n",
      " avg loss = 2.86\n",
      "\n",
      "Epoch time: 24.58887815475464\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 48, step 50 / 900, loss = 2.49 (0.021 sec/batch)\n",
      "epoch 48, step 100 / 900, loss = 2.80 (0.021 sec/batch)\n",
      "epoch 48, step 150 / 900, loss = 1.92 (0.022 sec/batch)\n",
      "epoch 48, step 200 / 900, loss = 2.32 (0.021 sec/batch)\n",
      "epoch 48, step 250 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 48, step 300 / 900, loss = 2.04 (0.022 sec/batch)\n",
      "epoch 48, step 350 / 900, loss = 2.78 (0.020 sec/batch)\n",
      "epoch 48, step 400 / 900, loss = 2.08 (0.022 sec/batch)\n",
      "epoch 48, step 450 / 900, loss = 2.92 (0.021 sec/batch)\n",
      "epoch 48, step 500 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 48, step 550 / 900, loss = 2.73 (0.022 sec/batch)\n",
      "epoch 48, step 600 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 48, step 650 / 900, loss = 1.59 (0.021 sec/batch)\n",
      "epoch 48, step 700 / 900, loss = 2.55 (0.022 sec/batch)\n",
      "epoch 48, step 750 / 900, loss = 2.49 (0.021 sec/batch)\n",
      "epoch 48, step 800 / 900, loss = 2.72 (0.022 sec/batch)\n",
      "epoch 48, step 850 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 48, step 900 / 900, loss = 2.63 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.53\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.42\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.582715272903442\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 49, step 50 / 900, loss = 2.48 (0.022 sec/batch)\n",
      "epoch 49, step 100 / 900, loss = 2.35 (0.022 sec/batch)\n",
      "epoch 49, step 150 / 900, loss = 3.27 (0.021 sec/batch)\n",
      "epoch 49, step 200 / 900, loss = 1.91 (0.021 sec/batch)\n",
      "epoch 49, step 250 / 900, loss = 2.09 (0.022 sec/batch)\n",
      "epoch 49, step 300 / 900, loss = 1.64 (0.022 sec/batch)\n",
      "epoch 49, step 350 / 900, loss = 1.85 (0.022 sec/batch)\n",
      "epoch 49, step 400 / 900, loss = 2.64 (0.021 sec/batch)\n",
      "epoch 49, step 450 / 900, loss = 1.98 (0.021 sec/batch)\n",
      "epoch 49, step 500 / 900, loss = 2.65 (0.022 sec/batch)\n",
      "epoch 49, step 550 / 900, loss = 2.04 (0.022 sec/batch)\n",
      "epoch 49, step 600 / 900, loss = 2.33 (0.022 sec/batch)\n",
      "epoch 49, step 650 / 900, loss = 2.80 (0.021 sec/batch)\n",
      "epoch 49, step 700 / 900, loss = 2.94 (0.022 sec/batch)\n",
      "epoch 49, step 750 / 900, loss = 2.54 (0.021 sec/batch)\n",
      "epoch 49, step 800 / 900, loss = 1.82 (0.022 sec/batch)\n",
      "epoch 49, step 850 / 900, loss = 3.05 (0.021 sec/batch)\n",
      "epoch 49, step 900 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.24\n",
      " avg loss = 2.28\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.30\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.605129718780518\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 50, step 50 / 900, loss = 1.12 (0.021 sec/batch)\n",
      "epoch 50, step 100 / 900, loss = 3.00 (0.022 sec/batch)\n",
      "epoch 50, step 150 / 900, loss = 2.81 (0.021 sec/batch)\n",
      "epoch 50, step 200 / 900, loss = 2.82 (0.021 sec/batch)\n",
      "epoch 50, step 250 / 900, loss = 2.69 (0.021 sec/batch)\n",
      "epoch 50, step 300 / 900, loss = 2.29 (0.022 sec/batch)\n",
      "epoch 50, step 350 / 900, loss = 1.79 (0.021 sec/batch)\n",
      "epoch 50, step 400 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 50, step 450 / 900, loss = 3.72 (0.021 sec/batch)\n",
      "epoch 50, step 500 / 900, loss = 1.46 (0.021 sec/batch)\n",
      "epoch 50, step 550 / 900, loss = 2.41 (0.022 sec/batch)\n",
      "epoch 50, step 600 / 900, loss = 2.53 (0.022 sec/batch)\n",
      "epoch 50, step 650 / 900, loss = 2.32 (0.021 sec/batch)\n",
      "epoch 50, step 700 / 900, loss = 2.72 (0.021 sec/batch)\n",
      "epoch 50, step 750 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 50, step 800 / 900, loss = 2.14 (0.022 sec/batch)\n",
      "epoch 50, step 850 / 900, loss = 2.41 (0.022 sec/batch)\n",
      "epoch 50, step 900 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.37\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.12\n",
      " avg loss = 2.83\n",
      "\n",
      "Epoch time: 24.579780101776123\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 51, step 50 / 900, loss = 2.53 (0.022 sec/batch)\n",
      "epoch 51, step 100 / 900, loss = 2.66 (0.021 sec/batch)\n",
      "epoch 51, step 150 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "epoch 51, step 200 / 900, loss = 1.92 (0.022 sec/batch)\n",
      "epoch 51, step 250 / 900, loss = 3.03 (0.022 sec/batch)\n",
      "epoch 51, step 300 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 51, step 350 / 900, loss = 3.25 (0.021 sec/batch)\n",
      "epoch 51, step 400 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 51, step 450 / 900, loss = 2.78 (0.021 sec/batch)\n",
      "epoch 51, step 500 / 900, loss = 1.90 (0.021 sec/batch)\n",
      "epoch 51, step 550 / 900, loss = 2.17 (0.020 sec/batch)\n",
      "epoch 51, step 600 / 900, loss = 3.07 (0.021 sec/batch)\n",
      "epoch 51, step 650 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 51, step 700 / 900, loss = 2.34 (0.022 sec/batch)\n",
      "epoch 51, step 750 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 51, step 800 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "epoch 51, step 850 / 900, loss = 3.27 (0.021 sec/batch)\n",
      "epoch 51, step 900 / 900, loss = 3.30 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.62\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.78\n",
      " avg loss = 2.85\n",
      "\n",
      "Epoch time: 24.566575288772583\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 52, step 50 / 900, loss = 2.08 (0.022 sec/batch)\n",
      "epoch 52, step 100 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 52, step 150 / 900, loss = 1.91 (0.022 sec/batch)\n",
      "epoch 52, step 200 / 900, loss = 2.05 (0.022 sec/batch)\n",
      "epoch 52, step 250 / 900, loss = 2.50 (0.021 sec/batch)\n",
      "epoch 52, step 300 / 900, loss = 2.32 (0.021 sec/batch)\n",
      "epoch 52, step 350 / 900, loss = 1.80 (0.022 sec/batch)\n",
      "epoch 52, step 400 / 900, loss = 3.26 (0.021 sec/batch)\n",
      "epoch 52, step 450 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 52, step 500 / 900, loss = 3.22 (0.021 sec/batch)\n",
      "epoch 52, step 550 / 900, loss = 1.57 (0.021 sec/batch)\n",
      "epoch 52, step 600 / 900, loss = 2.61 (0.021 sec/batch)\n",
      "epoch 52, step 650 / 900, loss = 2.39 (0.022 sec/batch)\n",
      "epoch 52, step 700 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "epoch 52, step 750 / 900, loss = 1.54 (0.021 sec/batch)\n",
      "epoch 52, step 800 / 900, loss = 2.25 (0.022 sec/batch)\n",
      "epoch 52, step 850 / 900, loss = 3.01 (0.022 sec/batch)\n",
      "epoch 52, step 900 / 900, loss = 2.19 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.44\n",
      " avg loss = 2.28\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.00\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.5869357585907\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 53, step 50 / 900, loss = 1.75 (0.021 sec/batch)\n",
      "epoch 53, step 100 / 900, loss = 2.58 (0.022 sec/batch)\n",
      "epoch 53, step 150 / 900, loss = 2.73 (0.022 sec/batch)\n",
      "epoch 53, step 200 / 900, loss = 3.44 (0.021 sec/batch)\n",
      "epoch 53, step 250 / 900, loss = 3.40 (0.021 sec/batch)\n",
      "epoch 53, step 300 / 900, loss = 2.44 (0.021 sec/batch)\n",
      "epoch 53, step 350 / 900, loss = 2.05 (0.022 sec/batch)\n",
      "epoch 53, step 400 / 900, loss = 2.01 (0.022 sec/batch)\n",
      "epoch 53, step 450 / 900, loss = 2.24 (0.021 sec/batch)\n",
      "epoch 53, step 500 / 900, loss = 2.44 (0.021 sec/batch)\n",
      "epoch 53, step 550 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "epoch 53, step 600 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 53, step 650 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 53, step 700 / 900, loss = 3.58 (0.021 sec/batch)\n",
      "epoch 53, step 750 / 900, loss = 2.35 (0.022 sec/batch)\n",
      "epoch 53, step 800 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 53, step 850 / 900, loss = 1.54 (0.021 sec/batch)\n",
      "epoch 53, step 900 / 900, loss = 1.63 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.36\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.26\n",
      " avg loss = 2.84\n",
      "\n",
      "Epoch time: 24.584325551986694\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 54, step 50 / 900, loss = 2.49 (0.021 sec/batch)\n",
      "epoch 54, step 100 / 900, loss = 2.08 (0.022 sec/batch)\n",
      "epoch 54, step 150 / 900, loss = 1.92 (0.021 sec/batch)\n",
      "epoch 54, step 200 / 900, loss = 2.41 (0.021 sec/batch)\n",
      "epoch 54, step 250 / 900, loss = 2.45 (0.021 sec/batch)\n",
      "epoch 54, step 300 / 900, loss = 2.90 (0.021 sec/batch)\n",
      "epoch 54, step 350 / 900, loss = 2.85 (0.021 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 54, step 400 / 900, loss = 1.83 (0.022 sec/batch)\n",
      "epoch 54, step 450 / 900, loss = 3.00 (0.021 sec/batch)\n",
      "epoch 54, step 500 / 900, loss = 2.24 (0.022 sec/batch)\n",
      "epoch 54, step 550 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 54, step 600 / 900, loss = 2.81 (0.021 sec/batch)\n",
      "epoch 54, step 650 / 900, loss = 1.56 (0.021 sec/batch)\n",
      "epoch 54, step 700 / 900, loss = 2.17 (0.022 sec/batch)\n",
      "epoch 54, step 750 / 900, loss = 2.58 (0.022 sec/batch)\n",
      "epoch 54, step 800 / 900, loss = 2.30 (0.022 sec/batch)\n",
      "epoch 54, step 850 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 54, step 900 / 900, loss = 2.64 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.22\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 48.30\n",
      " avg loss = 2.92\n",
      "\n",
      "Epoch time: 24.606391191482544\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 55, step 50 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "epoch 55, step 100 / 900, loss = 2.57 (0.022 sec/batch)\n",
      "epoch 55, step 150 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 55, step 200 / 900, loss = 2.95 (0.022 sec/batch)\n",
      "epoch 55, step 250 / 900, loss = 2.41 (0.021 sec/batch)\n",
      "epoch 55, step 300 / 900, loss = 2.12 (0.022 sec/batch)\n",
      "epoch 55, step 350 / 900, loss = 2.61 (0.021 sec/batch)\n",
      "epoch 55, step 400 / 900, loss = 1.81 (0.021 sec/batch)\n",
      "epoch 55, step 450 / 900, loss = 2.93 (0.021 sec/batch)\n",
      "epoch 55, step 500 / 900, loss = 2.50 (0.021 sec/batch)\n",
      "epoch 55, step 550 / 900, loss = 1.99 (0.021 sec/batch)\n",
      "epoch 55, step 600 / 900, loss = 1.24 (0.022 sec/batch)\n",
      "epoch 55, step 650 / 900, loss = 1.75 (0.021 sec/batch)\n",
      "epoch 55, step 700 / 900, loss = 1.42 (0.021 sec/batch)\n",
      "epoch 55, step 750 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 55, step 800 / 900, loss = 3.13 (0.021 sec/batch)\n",
      "epoch 55, step 850 / 900, loss = 2.78 (0.021 sec/batch)\n",
      "epoch 55, step 900 / 900, loss = 2.50 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.36\n",
      " avg loss = 2.28\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.36\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.57470679283142\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 56, step 50 / 900, loss = 1.77 (0.021 sec/batch)\n",
      "epoch 56, step 100 / 900, loss = 2.75 (0.022 sec/batch)\n",
      "epoch 56, step 150 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 56, step 200 / 900, loss = 1.23 (0.021 sec/batch)\n",
      "epoch 56, step 250 / 900, loss = 2.80 (0.021 sec/batch)\n",
      "epoch 56, step 300 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 56, step 350 / 900, loss = 3.45 (0.022 sec/batch)\n",
      "epoch 56, step 400 / 900, loss = 1.64 (0.021 sec/batch)\n",
      "epoch 56, step 450 / 900, loss = 1.89 (0.020 sec/batch)\n",
      "epoch 56, step 500 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 56, step 550 / 900, loss = 2.47 (0.022 sec/batch)\n",
      "epoch 56, step 600 / 900, loss = 2.51 (0.021 sec/batch)\n",
      "epoch 56, step 650 / 900, loss = 2.19 (0.022 sec/batch)\n",
      "epoch 56, step 700 / 900, loss = 2.62 (0.021 sec/batch)\n",
      "epoch 56, step 750 / 900, loss = 1.50 (0.022 sec/batch)\n",
      "epoch 56, step 800 / 900, loss = 2.52 (0.022 sec/batch)\n",
      "epoch 56, step 850 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 56, step 900 / 900, loss = 2.49 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.76\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.48\n",
      " avg loss = 2.86\n",
      "\n",
      "Epoch time: 24.58645987510681\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 57, step 50 / 900, loss = 2.56 (0.022 sec/batch)\n",
      "epoch 57, step 100 / 900, loss = 1.55 (0.021 sec/batch)\n",
      "epoch 57, step 150 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 57, step 200 / 900, loss = 2.79 (0.022 sec/batch)\n",
      "epoch 57, step 250 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "epoch 57, step 300 / 900, loss = 2.46 (0.021 sec/batch)\n",
      "epoch 57, step 350 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 57, step 400 / 900, loss = 1.77 (0.022 sec/batch)\n",
      "epoch 57, step 450 / 900, loss = 2.04 (0.021 sec/batch)\n",
      "epoch 57, step 500 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 57, step 550 / 900, loss = 2.46 (0.021 sec/batch)\n",
      "epoch 57, step 600 / 900, loss = 1.61 (0.021 sec/batch)\n",
      "epoch 57, step 650 / 900, loss = 1.80 (0.021 sec/batch)\n",
      "epoch 57, step 700 / 900, loss = 2.67 (0.021 sec/batch)\n",
      "epoch 57, step 750 / 900, loss = 1.87 (0.022 sec/batch)\n",
      "epoch 57, step 800 / 900, loss = 2.31 (0.020 sec/batch)\n",
      "epoch 57, step 850 / 900, loss = 1.32 (0.021 sec/batch)\n",
      "epoch 57, step 900 / 900, loss = 1.39 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.56\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.18\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.58371090888977\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 58, step 50 / 900, loss = 1.58 (0.021 sec/batch)\n",
      "epoch 58, step 100 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "epoch 58, step 150 / 900, loss = 2.78 (0.022 sec/batch)\n",
      "epoch 58, step 200 / 900, loss = 3.01 (0.021 sec/batch)\n",
      "epoch 58, step 250 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 58, step 300 / 900, loss = 1.66 (0.021 sec/batch)\n",
      "epoch 58, step 350 / 900, loss = 2.57 (0.021 sec/batch)\n",
      "epoch 58, step 400 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 58, step 450 / 900, loss = 2.40 (0.021 sec/batch)\n",
      "epoch 58, step 500 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 58, step 550 / 900, loss = 2.59 (0.022 sec/batch)\n",
      "epoch 58, step 600 / 900, loss = 2.67 (0.021 sec/batch)\n",
      "epoch 58, step 650 / 900, loss = 1.65 (0.021 sec/batch)\n",
      "epoch 58, step 700 / 900, loss = 2.64 (0.021 sec/batch)\n",
      "epoch 58, step 750 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 58, step 800 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 58, step 850 / 900, loss = 3.51 (0.022 sec/batch)\n",
      "epoch 58, step 900 / 900, loss = 1.85 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.58\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.82\n",
      " avg loss = 2.95\n",
      "\n",
      "Epoch time: 24.594974756240845\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 59, step 50 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "epoch 59, step 100 / 900, loss = 1.56 (0.021 sec/batch)\n",
      "epoch 59, step 150 / 900, loss = 2.27 (0.022 sec/batch)\n",
      "epoch 59, step 200 / 900, loss = 2.49 (0.021 sec/batch)\n",
      "epoch 59, step 250 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 59, step 300 / 900, loss = 1.57 (0.021 sec/batch)\n",
      "epoch 59, step 350 / 900, loss = 2.55 (0.022 sec/batch)\n",
      "epoch 59, step 400 / 900, loss = 2.32 (0.021 sec/batch)\n",
      "epoch 59, step 450 / 900, loss = 2.79 (0.021 sec/batch)\n",
      "epoch 59, step 500 / 900, loss = 1.72 (0.021 sec/batch)\n",
      "epoch 59, step 550 / 900, loss = 1.87 (0.021 sec/batch)\n",
      "epoch 59, step 600 / 900, loss = 2.34 (0.022 sec/batch)\n",
      "epoch 59, step 650 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 59, step 700 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 59, step 750 / 900, loss = 1.66 (0.022 sec/batch)\n",
      "epoch 59, step 800 / 900, loss = 3.01 (0.021 sec/batch)\n",
      "epoch 59, step 850 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 59, step 900 / 900, loss = 2.99 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.28\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.62\n",
      " avg loss = 2.86\n",
      "\n",
      "Epoch time: 24.579309701919556\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 60, step 50 / 900, loss = 2.19 (0.022 sec/batch)\n",
      "epoch 60, step 100 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "epoch 60, step 150 / 900, loss = 2.72 (0.021 sec/batch)\n",
      "epoch 60, step 200 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 60, step 250 / 900, loss = 1.93 (0.021 sec/batch)\n",
      "epoch 60, step 300 / 900, loss = 2.54 (0.022 sec/batch)\n",
      "epoch 60, step 350 / 900, loss = 2.84 (0.021 sec/batch)\n",
      "epoch 60, step 400 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 60, step 450 / 900, loss = 2.35 (0.022 sec/batch)\n",
      "epoch 60, step 500 / 900, loss = 2.44 (0.021 sec/batch)\n",
      "epoch 60, step 550 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 60, step 600 / 900, loss = 1.75 (0.021 sec/batch)\n",
      "epoch 60, step 650 / 900, loss = 2.75 (0.021 sec/batch)\n",
      "epoch 60, step 700 / 900, loss = 1.69 (0.021 sec/batch)\n",
      "epoch 60, step 750 / 900, loss = 2.92 (0.021 sec/batch)\n",
      "epoch 60, step 800 / 900, loss = 1.71 (0.021 sec/batch)\n",
      "epoch 60, step 850 / 900, loss = 2.01 (0.022 sec/batch)\n",
      "epoch 60, step 900 / 900, loss = 3.27 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.57\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.02\n",
      " avg loss = 2.83\n",
      "\n",
      "Epoch time: 24.58625316619873\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 61, step 50 / 900, loss = 2.60 (0.021 sec/batch)\n",
      "epoch 61, step 100 / 900, loss = 2.48 (0.020 sec/batch)\n",
      "epoch 61, step 150 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 61, step 200 / 900, loss = 1.47 (0.021 sec/batch)\n",
      "epoch 61, step 250 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 61, step 300 / 900, loss = 2.49 (0.022 sec/batch)\n",
      "epoch 61, step 350 / 900, loss = 2.26 (0.021 sec/batch)\n",
      "epoch 61, step 400 / 900, loss = 2.93 (0.022 sec/batch)\n",
      "epoch 61, step 450 / 900, loss = 2.26 (0.021 sec/batch)\n",
      "epoch 61, step 500 / 900, loss = 2.35 (0.022 sec/batch)\n",
      "epoch 61, step 550 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "epoch 61, step 600 / 900, loss = 2.46 (0.022 sec/batch)\n",
      "epoch 61, step 650 / 900, loss = 2.58 (0.021 sec/batch)\n",
      "epoch 61, step 700 / 900, loss = 1.54 (0.021 sec/batch)\n",
      "epoch 61, step 750 / 900, loss = 1.31 (0.021 sec/batch)\n",
      "epoch 61, step 800 / 900, loss = 1.78 (0.021 sec/batch)\n",
      "epoch 61, step 850 / 900, loss = 1.67 (0.022 sec/batch)\n",
      "epoch 61, step 900 / 900, loss = 1.40 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.58\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.04\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.591726541519165\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 62, step 50 / 900, loss = 1.80 (0.021 sec/batch)\n",
      "epoch 62, step 100 / 900, loss = 2.57 (0.021 sec/batch)\n",
      "epoch 62, step 150 / 900, loss = 1.84 (0.021 sec/batch)\n",
      "epoch 62, step 200 / 900, loss = 2.46 (0.022 sec/batch)\n",
      "epoch 62, step 250 / 900, loss = 2.74 (0.021 sec/batch)\n",
      "epoch 62, step 300 / 900, loss = 2.51 (0.021 sec/batch)\n",
      "epoch 62, step 350 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "epoch 62, step 400 / 900, loss = 1.93 (0.021 sec/batch)\n",
      "epoch 62, step 450 / 900, loss = 1.83 (0.022 sec/batch)\n",
      "epoch 62, step 500 / 900, loss = 2.67 (0.022 sec/batch)\n",
      "epoch 62, step 550 / 900, loss = 1.98 (0.021 sec/batch)\n",
      "epoch 62, step 600 / 900, loss = 2.99 (0.022 sec/batch)\n",
      "epoch 62, step 650 / 900, loss = 2.94 (0.021 sec/batch)\n",
      "epoch 62, step 700 / 900, loss = 1.69 (0.021 sec/batch)\n",
      "epoch 62, step 750 / 900, loss = 1.47 (0.021 sec/batch)\n",
      "epoch 62, step 800 / 900, loss = 1.64 (0.022 sec/batch)\n",
      "epoch 62, step 850 / 900, loss = 3.20 (0.022 sec/batch)\n",
      "epoch 62, step 900 / 900, loss = 2.34 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.21\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.72\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.576561212539673\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 63, step 50 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 63, step 100 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "epoch 63, step 150 / 900, loss = 2.49 (0.022 sec/batch)\n",
      "epoch 63, step 200 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 63, step 250 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 63, step 300 / 900, loss = 2.72 (0.021 sec/batch)\n",
      "epoch 63, step 350 / 900, loss = 1.52 (0.021 sec/batch)\n",
      "epoch 63, step 400 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 63, step 450 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 63, step 500 / 900, loss = 2.67 (0.021 sec/batch)\n",
      "epoch 63, step 550 / 900, loss = 1.96 (0.021 sec/batch)\n",
      "epoch 63, step 600 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 63, step 650 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "epoch 63, step 700 / 900, loss = 1.98 (0.021 sec/batch)\n",
      "epoch 63, step 750 / 900, loss = 1.26 (0.021 sec/batch)\n",
      "epoch 63, step 800 / 900, loss = 2.13 (0.022 sec/batch)\n",
      "epoch 63, step 850 / 900, loss = 2.25 (0.022 sec/batch)\n",
      "epoch 63, step 900 / 900, loss = 2.78 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.79\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.42\n",
      " avg loss = 2.83\n",
      "\n",
      "Epoch time: 24.594675540924072\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 64, step 50 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 64, step 100 / 900, loss = 2.35 (0.022 sec/batch)\n",
      "epoch 64, step 150 / 900, loss = 1.49 (0.021 sec/batch)\n",
      "epoch 64, step 200 / 900, loss = 2.72 (0.021 sec/batch)\n",
      "epoch 64, step 250 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "epoch 64, step 300 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "epoch 64, step 350 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 64, step 400 / 900, loss = 2.40 (0.021 sec/batch)\n",
      "epoch 64, step 450 / 900, loss = 2.60 (0.022 sec/batch)\n",
      "epoch 64, step 500 / 900, loss = 2.29 (0.022 sec/batch)\n",
      "epoch 64, step 550 / 900, loss = 1.63 (0.022 sec/batch)\n",
      "epoch 64, step 600 / 900, loss = 2.46 (0.021 sec/batch)\n",
      "epoch 64, step 650 / 900, loss = 2.50 (0.021 sec/batch)\n",
      "epoch 64, step 700 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 64, step 750 / 900, loss = 1.88 (0.021 sec/batch)\n",
      "epoch 64, step 800 / 900, loss = 1.86 (0.021 sec/batch)\n",
      "epoch 64, step 850 / 900, loss = 1.67 (0.021 sec/batch)\n",
      "epoch 64, step 900 / 900, loss = 1.93 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.52\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.46\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.57109570503235\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 65, step 50 / 900, loss = 2.57 (0.022 sec/batch)\n",
      "epoch 65, step 100 / 900, loss = 2.51 (0.021 sec/batch)\n",
      "epoch 65, step 150 / 900, loss = 2.63 (0.021 sec/batch)\n",
      "epoch 65, step 200 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "epoch 65, step 250 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 65, step 300 / 900, loss = 1.98 (0.021 sec/batch)\n",
      "epoch 65, step 350 / 900, loss = 2.73 (0.021 sec/batch)\n",
      "epoch 65, step 400 / 900, loss = 3.06 (0.021 sec/batch)\n",
      "epoch 65, step 450 / 900, loss = 1.93 (0.022 sec/batch)\n",
      "epoch 65, step 500 / 900, loss = 2.98 (0.021 sec/batch)\n",
      "epoch 65, step 550 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 65, step 600 / 900, loss = 2.41 (0.021 sec/batch)\n",
      "epoch 65, step 650 / 900, loss = 2.73 (0.021 sec/batch)\n",
      "epoch 65, step 700 / 900, loss = 2.74 (0.021 sec/batch)\n",
      "epoch 65, step 750 / 900, loss = 3.03 (0.022 sec/batch)\n",
      "epoch 65, step 800 / 900, loss = 1.44 (0.021 sec/batch)\n",
      "epoch 65, step 850 / 900, loss = 1.91 (0.022 sec/batch)\n",
      "epoch 65, step 900 / 900, loss = 2.00 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.52\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 48.98\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.59774136543274\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 66, step 50 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 66, step 100 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "epoch 66, step 150 / 900, loss = 1.79 (0.022 sec/batch)\n",
      "epoch 66, step 200 / 900, loss = 2.41 (0.021 sec/batch)\n",
      "epoch 66, step 250 / 900, loss = 2.93 (0.021 sec/batch)\n",
      "epoch 66, step 300 / 900, loss = 2.12 (0.022 sec/batch)\n",
      "epoch 66, step 350 / 900, loss = 2.31 (0.022 sec/batch)\n",
      "epoch 66, step 400 / 900, loss = 2.30 (0.022 sec/batch)\n",
      "epoch 66, step 450 / 900, loss = 1.22 (0.022 sec/batch)\n",
      "epoch 66, step 500 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "epoch 66, step 550 / 900, loss = 1.91 (0.022 sec/batch)\n",
      "epoch 66, step 600 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "epoch 66, step 650 / 900, loss = 1.85 (0.021 sec/batch)\n",
      "epoch 66, step 700 / 900, loss = 1.19 (0.021 sec/batch)\n",
      "epoch 66, step 750 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "epoch 66, step 800 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 66, step 850 / 900, loss = 2.86 (0.022 sec/batch)\n",
      "epoch 66, step 900 / 900, loss = 1.45 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.52\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.60\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.592371940612793\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 67, step 50 / 900, loss = 3.01 (0.021 sec/batch)\n",
      "epoch 67, step 100 / 900, loss = 2.39 (0.021 sec/batch)\n",
      "epoch 67, step 150 / 900, loss = 1.68 (0.022 sec/batch)\n",
      "epoch 67, step 200 / 900, loss = 2.35 (0.022 sec/batch)\n",
      "epoch 67, step 250 / 900, loss = 3.07 (0.021 sec/batch)\n",
      "epoch 67, step 300 / 900, loss = 2.04 (0.021 sec/batch)\n",
      "epoch 67, step 350 / 900, loss = 2.15 (0.022 sec/batch)\n",
      "epoch 67, step 400 / 900, loss = 3.62 (0.022 sec/batch)\n",
      "epoch 67, step 450 / 900, loss = 3.33 (0.021 sec/batch)\n",
      "epoch 67, step 500 / 900, loss = 2.81 (0.021 sec/batch)\n",
      "epoch 67, step 550 / 900, loss = 2.68 (0.021 sec/batch)\n",
      "epoch 67, step 600 / 900, loss = 2.36 (0.021 sec/batch)\n",
      "epoch 67, step 650 / 900, loss = 3.05 (0.021 sec/batch)\n",
      "epoch 67, step 700 / 900, loss = 2.15 (0.021 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 67, step 750 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 67, step 800 / 900, loss = 3.09 (0.021 sec/batch)\n",
      "epoch 67, step 850 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 67, step 900 / 900, loss = 2.97 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.47\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.12\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.5869300365448\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 68, step 50 / 900, loss = 2.71 (0.021 sec/batch)\n",
      "epoch 68, step 100 / 900, loss = 2.83 (0.021 sec/batch)\n",
      "epoch 68, step 150 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 68, step 200 / 900, loss = 2.50 (0.021 sec/batch)\n",
      "epoch 68, step 250 / 900, loss = 2.52 (0.022 sec/batch)\n",
      "epoch 68, step 300 / 900, loss = 1.95 (0.021 sec/batch)\n",
      "epoch 68, step 350 / 900, loss = 2.38 (0.022 sec/batch)\n",
      "epoch 68, step 400 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 68, step 450 / 900, loss = 2.44 (0.020 sec/batch)\n",
      "epoch 68, step 500 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 68, step 550 / 900, loss = 2.56 (0.022 sec/batch)\n",
      "epoch 68, step 600 / 900, loss = 2.91 (0.021 sec/batch)\n",
      "epoch 68, step 650 / 900, loss = 2.53 (0.022 sec/batch)\n",
      "epoch 68, step 700 / 900, loss = 1.66 (0.021 sec/batch)\n",
      "epoch 68, step 750 / 900, loss = 2.60 (0.021 sec/batch)\n",
      "epoch 68, step 800 / 900, loss = 2.46 (0.021 sec/batch)\n",
      "epoch 68, step 850 / 900, loss = 3.48 (0.022 sec/batch)\n",
      "epoch 68, step 900 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.14\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.40\n",
      " avg loss = 2.92\n",
      "\n",
      "Epoch time: 24.622119903564453\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 69, step 50 / 900, loss = 2.83 (0.021 sec/batch)\n",
      "epoch 69, step 100 / 900, loss = 1.65 (0.021 sec/batch)\n",
      "epoch 69, step 150 / 900, loss = 1.70 (0.021 sec/batch)\n",
      "epoch 69, step 200 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 69, step 250 / 900, loss = 1.96 (0.022 sec/batch)\n",
      "epoch 69, step 300 / 900, loss = 3.23 (0.021 sec/batch)\n",
      "epoch 69, step 350 / 900, loss = 2.04 (0.022 sec/batch)\n",
      "epoch 69, step 400 / 900, loss = 2.67 (0.022 sec/batch)\n",
      "epoch 69, step 450 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 69, step 500 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 69, step 550 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 69, step 600 / 900, loss = 2.61 (0.022 sec/batch)\n",
      "epoch 69, step 650 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 69, step 700 / 900, loss = 2.84 (0.021 sec/batch)\n",
      "epoch 69, step 750 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 69, step 800 / 900, loss = 2.15 (0.021 sec/batch)\n",
      "epoch 69, step 850 / 900, loss = 2.25 (0.022 sec/batch)\n",
      "epoch 69, step 900 / 900, loss = 2.68 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.94\n",
      " avg loss = 2.24\n",
      "\n",
      "Validation error:\n",
      " accuracy = 48.72\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.587834119796753\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 70, step 50 / 900, loss = 2.36 (0.021 sec/batch)\n",
      "epoch 70, step 100 / 900, loss = 1.73 (0.021 sec/batch)\n",
      "epoch 70, step 150 / 900, loss = 3.13 (0.022 sec/batch)\n",
      "epoch 70, step 200 / 900, loss = 1.93 (0.021 sec/batch)\n",
      "epoch 70, step 250 / 900, loss = 2.34 (0.022 sec/batch)\n",
      "epoch 70, step 300 / 900, loss = 2.38 (0.022 sec/batch)\n",
      "epoch 70, step 350 / 900, loss = 2.61 (0.021 sec/batch)\n",
      "epoch 70, step 400 / 900, loss = 3.13 (0.022 sec/batch)\n",
      "epoch 70, step 450 / 900, loss = 3.11 (0.020 sec/batch)\n",
      "epoch 70, step 500 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 70, step 550 / 900, loss = 2.24 (0.022 sec/batch)\n",
      "epoch 70, step 600 / 900, loss = 3.14 (0.022 sec/batch)\n",
      "epoch 70, step 650 / 900, loss = 1.56 (0.021 sec/batch)\n",
      "epoch 70, step 700 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 70, step 750 / 900, loss = 2.36 (0.021 sec/batch)\n",
      "epoch 70, step 800 / 900, loss = 2.15 (0.021 sec/batch)\n",
      "epoch 70, step 850 / 900, loss = 2.47 (0.021 sec/batch)\n",
      "epoch 70, step 900 / 900, loss = 2.33 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.63\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.70\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.592357873916626\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 71, step 50 / 900, loss = 1.59 (0.022 sec/batch)\n",
      "epoch 71, step 100 / 900, loss = 2.64 (0.022 sec/batch)\n",
      "epoch 71, step 150 / 900, loss = 2.39 (0.021 sec/batch)\n",
      "epoch 71, step 200 / 900, loss = 1.70 (0.021 sec/batch)\n",
      "epoch 71, step 250 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 71, step 300 / 900, loss = 2.60 (0.022 sec/batch)\n",
      "epoch 71, step 350 / 900, loss = 2.19 (0.022 sec/batch)\n",
      "epoch 71, step 400 / 900, loss = 3.03 (0.022 sec/batch)\n",
      "epoch 71, step 450 / 900, loss = 2.51 (0.022 sec/batch)\n",
      "epoch 71, step 500 / 900, loss = 3.72 (0.021 sec/batch)\n",
      "epoch 71, step 550 / 900, loss = 1.66 (0.022 sec/batch)\n",
      "epoch 71, step 600 / 900, loss = 2.44 (0.022 sec/batch)\n",
      "epoch 71, step 650 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "epoch 71, step 700 / 900, loss = 2.60 (0.022 sec/batch)\n",
      "epoch 71, step 750 / 900, loss = 2.74 (0.021 sec/batch)\n",
      "epoch 71, step 800 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 71, step 850 / 900, loss = 2.07 (0.022 sec/batch)\n",
      "epoch 71, step 900 / 900, loss = 1.40 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.98\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.80\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.57901120185852\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 72, step 50 / 900, loss = 1.99 (0.021 sec/batch)\n",
      "epoch 72, step 100 / 900, loss = 2.67 (0.022 sec/batch)\n",
      "epoch 72, step 150 / 900, loss = 1.73 (0.021 sec/batch)\n",
      "epoch 72, step 200 / 900, loss = 1.43 (0.022 sec/batch)\n",
      "epoch 72, step 250 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 72, step 300 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 72, step 350 / 900, loss = 2.85 (0.021 sec/batch)\n",
      "epoch 72, step 400 / 900, loss = 1.82 (0.022 sec/batch)\n",
      "epoch 72, step 450 / 900, loss = 2.29 (0.022 sec/batch)\n",
      "epoch 72, step 500 / 900, loss = 2.82 (0.022 sec/batch)\n",
      "epoch 72, step 550 / 900, loss = 2.57 (0.021 sec/batch)\n",
      "epoch 72, step 600 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 72, step 650 / 900, loss = 2.28 (0.022 sec/batch)\n",
      "epoch 72, step 700 / 900, loss = 2.86 (0.021 sec/batch)\n",
      "epoch 72, step 750 / 900, loss = 2.76 (0.020 sec/batch)\n",
      "epoch 72, step 800 / 900, loss = 2.67 (0.022 sec/batch)\n",
      "epoch 72, step 850 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 72, step 900 / 900, loss = 1.91 (0.020 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.47\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.28\n",
      " avg loss = 2.85\n",
      "\n",
      "Epoch time: 24.57241916656494\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 73, step 50 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 73, step 100 / 900, loss = 1.74 (0.021 sec/batch)\n",
      "epoch 73, step 150 / 900, loss = 2.65 (0.022 sec/batch)\n",
      "epoch 73, step 200 / 900, loss = 2.39 (0.021 sec/batch)\n",
      "epoch 73, step 250 / 900, loss = 2.49 (0.021 sec/batch)\n",
      "epoch 73, step 300 / 900, loss = 1.93 (0.021 sec/batch)\n",
      "epoch 73, step 350 / 900, loss = 1.88 (0.022 sec/batch)\n",
      "epoch 73, step 400 / 900, loss = 2.63 (0.022 sec/batch)\n",
      "epoch 73, step 450 / 900, loss = 1.79 (0.021 sec/batch)\n",
      "epoch 73, step 500 / 900, loss = 1.60 (0.022 sec/batch)\n",
      "epoch 73, step 550 / 900, loss = 1.88 (0.022 sec/batch)\n",
      "epoch 73, step 600 / 900, loss = 2.03 (0.022 sec/batch)\n",
      "epoch 73, step 650 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "epoch 73, step 700 / 900, loss = 2.58 (0.021 sec/batch)\n",
      "epoch 73, step 750 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 73, step 800 / 900, loss = 1.28 (0.021 sec/batch)\n",
      "epoch 73, step 850 / 900, loss = 1.63 (0.021 sec/batch)\n",
      "epoch 73, step 900 / 900, loss = 1.67 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.75\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 48.98\n",
      " avg loss = 2.93\n",
      "\n",
      "Epoch time: 24.604268550872803\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 74, step 50 / 900, loss = 2.57 (0.021 sec/batch)\n",
      "epoch 74, step 100 / 900, loss = 2.88 (0.022 sec/batch)\n",
      "epoch 74, step 150 / 900, loss = 2.34 (0.021 sec/batch)\n",
      "epoch 74, step 200 / 900, loss = 1.70 (0.021 sec/batch)\n",
      "epoch 74, step 250 / 900, loss = 3.50 (0.020 sec/batch)\n",
      "epoch 74, step 300 / 900, loss = 1.99 (0.021 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 74, step 350 / 900, loss = 2.39 (0.022 sec/batch)\n",
      "epoch 74, step 400 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 74, step 450 / 900, loss = 1.71 (0.021 sec/batch)\n",
      "epoch 74, step 500 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 74, step 550 / 900, loss = 2.41 (0.022 sec/batch)\n",
      "epoch 74, step 600 / 900, loss = 2.49 (0.021 sec/batch)\n",
      "epoch 74, step 650 / 900, loss = 2.44 (0.022 sec/batch)\n",
      "epoch 74, step 700 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 74, step 750 / 900, loss = 2.40 (0.022 sec/batch)\n",
      "epoch 74, step 800 / 900, loss = 3.36 (0.021 sec/batch)\n",
      "epoch 74, step 850 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 74, step 900 / 900, loss = 2.80 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.39\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.14\n",
      " avg loss = 2.83\n",
      "\n",
      "Epoch time: 24.58602809906006\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 75, step 50 / 900, loss = 3.07 (0.021 sec/batch)\n",
      "epoch 75, step 100 / 900, loss = 2.54 (0.022 sec/batch)\n",
      "epoch 75, step 150 / 900, loss = 3.06 (0.021 sec/batch)\n",
      "epoch 75, step 200 / 900, loss = 1.80 (0.021 sec/batch)\n",
      "epoch 75, step 250 / 900, loss = 1.88 (0.021 sec/batch)\n",
      "epoch 75, step 300 / 900, loss = 1.92 (0.022 sec/batch)\n",
      "epoch 75, step 350 / 900, loss = 1.76 (0.021 sec/batch)\n",
      "epoch 75, step 400 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 75, step 450 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 75, step 500 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "epoch 75, step 550 / 900, loss = 2.15 (0.021 sec/batch)\n",
      "epoch 75, step 600 / 900, loss = 1.58 (0.021 sec/batch)\n",
      "epoch 75, step 650 / 900, loss = 2.00 (0.022 sec/batch)\n",
      "epoch 75, step 700 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 75, step 750 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "epoch 75, step 800 / 900, loss = 1.55 (0.021 sec/batch)\n",
      "epoch 75, step 850 / 900, loss = 1.95 (0.021 sec/batch)\n",
      "epoch 75, step 900 / 900, loss = 2.70 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.91\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.16\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.58393692970276\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 76, step 50 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "epoch 76, step 100 / 900, loss = 2.06 (0.021 sec/batch)\n",
      "epoch 76, step 150 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 76, step 200 / 900, loss = 2.77 (0.021 sec/batch)\n",
      "epoch 76, step 250 / 900, loss = 2.04 (0.022 sec/batch)\n",
      "epoch 76, step 300 / 900, loss = 2.66 (0.021 sec/batch)\n",
      "epoch 76, step 350 / 900, loss = 2.24 (0.021 sec/batch)\n",
      "epoch 76, step 400 / 900, loss = 1.38 (0.021 sec/batch)\n",
      "epoch 76, step 450 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 76, step 500 / 900, loss = 2.50 (0.021 sec/batch)\n",
      "epoch 76, step 550 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 76, step 600 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 76, step 650 / 900, loss = 2.04 (0.021 sec/batch)\n",
      "epoch 76, step 700 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "epoch 76, step 750 / 900, loss = 2.47 (0.021 sec/batch)\n",
      "epoch 76, step 800 / 900, loss = 1.90 (0.022 sec/batch)\n",
      "epoch 76, step 850 / 900, loss = 2.69 (0.021 sec/batch)\n",
      "epoch 76, step 900 / 900, loss = 2.27 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.55\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.72\n",
      " avg loss = 2.83\n",
      "\n",
      "Epoch time: 24.60583233833313\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 77, step 50 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 77, step 100 / 900, loss = 2.79 (0.021 sec/batch)\n",
      "epoch 77, step 150 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 77, step 200 / 900, loss = 1.75 (0.022 sec/batch)\n",
      "epoch 77, step 250 / 900, loss = 2.39 (0.021 sec/batch)\n",
      "epoch 77, step 300 / 900, loss = 2.66 (0.021 sec/batch)\n",
      "epoch 77, step 350 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 77, step 400 / 900, loss = 2.69 (0.021 sec/batch)\n",
      "epoch 77, step 450 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 77, step 500 / 900, loss = 2.70 (0.022 sec/batch)\n",
      "epoch 77, step 550 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "epoch 77, step 600 / 900, loss = 2.59 (0.021 sec/batch)\n",
      "epoch 77, step 650 / 900, loss = 2.27 (0.022 sec/batch)\n",
      "epoch 77, step 700 / 900, loss = 2.46 (0.022 sec/batch)\n",
      "epoch 77, step 750 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 77, step 800 / 900, loss = 2.35 (0.022 sec/batch)\n",
      "epoch 77, step 850 / 900, loss = 2.41 (0.021 sec/batch)\n",
      "epoch 77, step 900 / 900, loss = 2.32 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.50\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 48.98\n",
      " avg loss = 2.92\n",
      "\n",
      "Epoch time: 24.5924129486084\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 78, step 50 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "epoch 78, step 100 / 900, loss = 1.91 (0.021 sec/batch)\n",
      "epoch 78, step 150 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 78, step 200 / 900, loss = 2.44 (0.021 sec/batch)\n",
      "epoch 78, step 250 / 900, loss = 2.63 (0.021 sec/batch)\n",
      "epoch 78, step 300 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "epoch 78, step 350 / 900, loss = 2.43 (0.022 sec/batch)\n",
      "epoch 78, step 400 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 78, step 450 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "epoch 78, step 500 / 900, loss = 1.63 (0.021 sec/batch)\n",
      "epoch 78, step 550 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 78, step 600 / 900, loss = 2.59 (0.021 sec/batch)\n",
      "epoch 78, step 650 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 78, step 700 / 900, loss = 1.93 (0.022 sec/batch)\n",
      "epoch 78, step 750 / 900, loss = 1.93 (0.021 sec/batch)\n",
      "epoch 78, step 800 / 900, loss = 2.41 (0.022 sec/batch)\n",
      "epoch 78, step 850 / 900, loss = 2.67 (0.021 sec/batch)\n",
      "epoch 78, step 900 / 900, loss = 2.68 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.60\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 48.78\n",
      " avg loss = 2.99\n",
      "\n",
      "Epoch time: 24.59820294380188\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 79, step 50 / 900, loss = 1.75 (0.022 sec/batch)\n",
      "epoch 79, step 100 / 900, loss = 2.36 (0.021 sec/batch)\n",
      "epoch 79, step 150 / 900, loss = 1.86 (0.021 sec/batch)\n",
      "epoch 79, step 200 / 900, loss = 1.48 (0.022 sec/batch)\n",
      "epoch 79, step 250 / 900, loss = 1.51 (0.021 sec/batch)\n",
      "epoch 79, step 300 / 900, loss = 2.36 (0.022 sec/batch)\n",
      "epoch 79, step 350 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 79, step 400 / 900, loss = 2.86 (0.022 sec/batch)\n",
      "epoch 79, step 450 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "epoch 79, step 500 / 900, loss = 1.66 (0.021 sec/batch)\n",
      "epoch 79, step 550 / 900, loss = 3.29 (0.022 sec/batch)\n",
      "epoch 79, step 600 / 900, loss = 2.47 (0.020 sec/batch)\n",
      "epoch 79, step 650 / 900, loss = 1.55 (0.022 sec/batch)\n",
      "epoch 79, step 700 / 900, loss = 1.62 (0.022 sec/batch)\n",
      "epoch 79, step 750 / 900, loss = 2.33 (0.022 sec/batch)\n",
      "epoch 79, step 800 / 900, loss = 3.53 (0.021 sec/batch)\n",
      "epoch 79, step 850 / 900, loss = 2.52 (0.022 sec/batch)\n",
      "epoch 79, step 900 / 900, loss = 1.71 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.95\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.62\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.566250801086426\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 80, step 50 / 900, loss = 2.56 (0.021 sec/batch)\n",
      "epoch 80, step 100 / 900, loss = 2.81 (0.021 sec/batch)\n",
      "epoch 80, step 150 / 900, loss = 2.68 (0.021 sec/batch)\n",
      "epoch 80, step 200 / 900, loss = 1.98 (0.022 sec/batch)\n",
      "epoch 80, step 250 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "epoch 80, step 300 / 900, loss = 1.92 (0.022 sec/batch)\n",
      "epoch 80, step 350 / 900, loss = 1.44 (0.021 sec/batch)\n",
      "epoch 80, step 400 / 900, loss = 2.67 (0.021 sec/batch)\n",
      "epoch 80, step 450 / 900, loss = 3.89 (0.021 sec/batch)\n",
      "epoch 80, step 500 / 900, loss = 2.78 (0.021 sec/batch)\n",
      "epoch 80, step 550 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 80, step 600 / 900, loss = 2.77 (0.021 sec/batch)\n",
      "epoch 80, step 650 / 900, loss = 2.51 (0.021 sec/batch)\n",
      "epoch 80, step 700 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 80, step 750 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 80, step 800 / 900, loss = 2.98 (0.022 sec/batch)\n",
      "epoch 80, step 850 / 900, loss = 2.66 (0.022 sec/batch)\n",
      "epoch 80, step 900 / 900, loss = 1.25 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.78\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.50\n",
      " avg loss = 2.84\n",
      "\n",
      "Epoch time: 24.566778898239136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 81, step 50 / 900, loss = 2.86 (0.021 sec/batch)\n",
      "epoch 81, step 100 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 81, step 150 / 900, loss = 1.35 (0.021 sec/batch)\n",
      "epoch 81, step 200 / 900, loss = 1.88 (0.022 sec/batch)\n",
      "epoch 81, step 250 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 81, step 300 / 900, loss = 2.86 (0.021 sec/batch)\n",
      "epoch 81, step 350 / 900, loss = 3.04 (0.021 sec/batch)\n",
      "epoch 81, step 400 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 81, step 450 / 900, loss = 3.00 (0.022 sec/batch)\n",
      "epoch 81, step 500 / 900, loss = 3.41 (0.021 sec/batch)\n",
      "epoch 81, step 550 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 81, step 600 / 900, loss = 2.44 (0.021 sec/batch)\n",
      "epoch 81, step 650 / 900, loss = 1.88 (0.021 sec/batch)\n",
      "epoch 81, step 700 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 81, step 750 / 900, loss = 1.76 (0.021 sec/batch)\n",
      "epoch 81, step 800 / 900, loss = 3.17 (0.021 sec/batch)\n",
      "epoch 81, step 850 / 900, loss = 2.57 (0.022 sec/batch)\n",
      "epoch 81, step 900 / 900, loss = 2.75 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.10\n",
      " avg loss = 2.28\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.96\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.57139492034912\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 82, step 50 / 900, loss = 1.74 (0.021 sec/batch)\n",
      "epoch 82, step 100 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 82, step 150 / 900, loss = 2.45 (0.022 sec/batch)\n",
      "epoch 82, step 200 / 900, loss = 1.95 (0.022 sec/batch)\n",
      "epoch 82, step 250 / 900, loss = 3.65 (0.021 sec/batch)\n",
      "epoch 82, step 300 / 900, loss = 2.59 (0.022 sec/batch)\n",
      "epoch 82, step 350 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 82, step 400 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 82, step 450 / 900, loss = 3.43 (0.022 sec/batch)\n",
      "epoch 82, step 500 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 82, step 550 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 82, step 600 / 900, loss = 1.59 (0.021 sec/batch)\n",
      "epoch 82, step 650 / 900, loss = 1.73 (0.021 sec/batch)\n",
      "epoch 82, step 700 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "epoch 82, step 750 / 900, loss = 2.46 (0.022 sec/batch)\n",
      "epoch 82, step 800 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 82, step 850 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 82, step 900 / 900, loss = 2.34 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.34\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.08\n",
      " avg loss = 2.96\n",
      "\n",
      "Epoch time: 24.60529136657715\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 83, step 50 / 900, loss = 1.74 (0.021 sec/batch)\n",
      "epoch 83, step 100 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 83, step 150 / 900, loss = 2.58 (0.021 sec/batch)\n",
      "epoch 83, step 200 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "epoch 83, step 250 / 900, loss = 2.19 (0.022 sec/batch)\n",
      "epoch 83, step 300 / 900, loss = 2.94 (0.021 sec/batch)\n",
      "epoch 83, step 350 / 900, loss = 2.82 (0.021 sec/batch)\n",
      "epoch 83, step 400 / 900, loss = 2.58 (0.021 sec/batch)\n",
      "epoch 83, step 450 / 900, loss = 1.61 (0.021 sec/batch)\n",
      "epoch 83, step 500 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "epoch 83, step 550 / 900, loss = 1.85 (0.020 sec/batch)\n",
      "epoch 83, step 600 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 83, step 650 / 900, loss = 1.92 (0.021 sec/batch)\n",
      "epoch 83, step 700 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 83, step 750 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 83, step 800 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 83, step 850 / 900, loss = 2.88 (0.021 sec/batch)\n",
      "epoch 83, step 900 / 900, loss = 2.02 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.69\n",
      " avg loss = 2.24\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.58\n",
      " avg loss = 2.83\n",
      "\n",
      "Epoch time: 24.575600147247314\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 84, step 50 / 900, loss = 1.70 (0.021 sec/batch)\n",
      "epoch 84, step 100 / 900, loss = 3.03 (0.021 sec/batch)\n",
      "epoch 84, step 150 / 900, loss = 2.77 (0.021 sec/batch)\n",
      "epoch 84, step 200 / 900, loss = 2.62 (0.021 sec/batch)\n",
      "epoch 84, step 250 / 900, loss = 2.00 (0.022 sec/batch)\n",
      "epoch 84, step 300 / 900, loss = 2.14 (0.022 sec/batch)\n",
      "epoch 84, step 350 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 84, step 400 / 900, loss = 2.55 (0.021 sec/batch)\n",
      "epoch 84, step 450 / 900, loss = 2.05 (0.022 sec/batch)\n",
      "epoch 84, step 500 / 900, loss = 1.61 (0.020 sec/batch)\n",
      "epoch 84, step 550 / 900, loss = 2.39 (0.022 sec/batch)\n",
      "epoch 84, step 600 / 900, loss = 1.97 (0.022 sec/batch)\n",
      "epoch 84, step 650 / 900, loss = 1.66 (0.021 sec/batch)\n",
      "epoch 84, step 700 / 900, loss = 2.20 (0.022 sec/batch)\n",
      "epoch 84, step 750 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 84, step 800 / 900, loss = 2.23 (0.020 sec/batch)\n",
      "epoch 84, step 850 / 900, loss = 2.60 (0.021 sec/batch)\n",
      "epoch 84, step 900 / 900, loss = 2.99 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.40\n",
      " avg loss = 2.28\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.00\n",
      " avg loss = 2.86\n",
      "\n",
      "Epoch time: 24.570683002471924\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 85, step 50 / 900, loss = 2.25 (0.022 sec/batch)\n",
      "epoch 85, step 100 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 85, step 150 / 900, loss = 1.75 (0.021 sec/batch)\n",
      "epoch 85, step 200 / 900, loss = 2.69 (0.021 sec/batch)\n",
      "epoch 85, step 250 / 900, loss = 2.73 (0.021 sec/batch)\n",
      "epoch 85, step 300 / 900, loss = 1.82 (0.021 sec/batch)\n",
      "epoch 85, step 350 / 900, loss = 1.83 (0.022 sec/batch)\n",
      "epoch 85, step 400 / 900, loss = 3.90 (0.022 sec/batch)\n",
      "epoch 85, step 450 / 900, loss = 2.57 (0.022 sec/batch)\n",
      "epoch 85, step 500 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 85, step 550 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 85, step 600 / 900, loss = 2.44 (0.021 sec/batch)\n",
      "epoch 85, step 650 / 900, loss = 2.48 (0.022 sec/batch)\n",
      "epoch 85, step 700 / 900, loss = 2.41 (0.022 sec/batch)\n",
      "epoch 85, step 750 / 900, loss = 2.24 (0.021 sec/batch)\n",
      "epoch 85, step 800 / 900, loss = 2.57 (0.021 sec/batch)\n",
      "epoch 85, step 850 / 900, loss = 2.11 (0.022 sec/batch)\n",
      "epoch 85, step 900 / 900, loss = 1.72 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.78\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 48.94\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.559618949890137\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 86, step 50 / 900, loss = 1.94 (0.022 sec/batch)\n",
      "epoch 86, step 100 / 900, loss = 2.45 (0.021 sec/batch)\n",
      "epoch 86, step 150 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 86, step 200 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 86, step 250 / 900, loss = 1.75 (0.021 sec/batch)\n",
      "epoch 86, step 300 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 86, step 350 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 86, step 400 / 900, loss = 2.78 (0.021 sec/batch)\n",
      "epoch 86, step 450 / 900, loss = 2.58 (0.023 sec/batch)\n",
      "epoch 86, step 500 / 900, loss = 1.94 (0.020 sec/batch)\n",
      "epoch 86, step 550 / 900, loss = 2.69 (0.021 sec/batch)\n",
      "epoch 86, step 600 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "epoch 86, step 650 / 900, loss = 2.50 (0.021 sec/batch)\n",
      "epoch 86, step 700 / 900, loss = 2.12 (0.022 sec/batch)\n",
      "epoch 86, step 750 / 900, loss = 2.51 (0.021 sec/batch)\n",
      "epoch 86, step 800 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 86, step 850 / 900, loss = 1.80 (0.022 sec/batch)\n",
      "epoch 86, step 900 / 900, loss = 1.39 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.64\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.66\n",
      " avg loss = 2.86\n",
      "\n",
      "Epoch time: 24.56414818763733\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 87, step 50 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 87, step 100 / 900, loss = 2.25 (0.022 sec/batch)\n",
      "epoch 87, step 150 / 900, loss = 2.76 (0.021 sec/batch)\n",
      "epoch 87, step 200 / 900, loss = 2.71 (0.021 sec/batch)\n",
      "epoch 87, step 250 / 900, loss = 1.80 (0.021 sec/batch)\n",
      "epoch 87, step 300 / 900, loss = 2.61 (0.021 sec/batch)\n",
      "epoch 87, step 350 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "epoch 87, step 400 / 900, loss = 2.60 (0.021 sec/batch)\n",
      "epoch 87, step 450 / 900, loss = 1.22 (0.021 sec/batch)\n",
      "epoch 87, step 500 / 900, loss = 2.59 (0.020 sec/batch)\n",
      "epoch 87, step 550 / 900, loss = 2.69 (0.022 sec/batch)\n",
      "epoch 87, step 600 / 900, loss = 2.43 (0.021 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 87, step 650 / 900, loss = 2.79 (0.022 sec/batch)\n",
      "epoch 87, step 700 / 900, loss = 2.68 (0.021 sec/batch)\n",
      "epoch 87, step 750 / 900, loss = 1.80 (0.021 sec/batch)\n",
      "epoch 87, step 800 / 900, loss = 2.18 (0.022 sec/batch)\n",
      "epoch 87, step 850 / 900, loss = 2.61 (0.022 sec/batch)\n",
      "epoch 87, step 900 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.39\n",
      " avg loss = 2.28\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.52\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.60157823562622\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 88, step 50 / 900, loss = 2.64 (0.021 sec/batch)\n",
      "epoch 88, step 100 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 88, step 150 / 900, loss = 1.65 (0.021 sec/batch)\n",
      "epoch 88, step 200 / 900, loss = 2.84 (0.021 sec/batch)\n",
      "epoch 88, step 250 / 900, loss = 2.26 (0.021 sec/batch)\n",
      "epoch 88, step 300 / 900, loss = 3.22 (0.021 sec/batch)\n",
      "epoch 88, step 350 / 900, loss = 1.80 (0.022 sec/batch)\n",
      "epoch 88, step 400 / 900, loss = 2.58 (0.022 sec/batch)\n",
      "epoch 88, step 450 / 900, loss = 2.69 (0.022 sec/batch)\n",
      "epoch 88, step 500 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 88, step 550 / 900, loss = 2.82 (0.021 sec/batch)\n",
      "epoch 88, step 600 / 900, loss = 2.20 (0.022 sec/batch)\n",
      "epoch 88, step 650 / 900, loss = 1.97 (0.022 sec/batch)\n",
      "epoch 88, step 700 / 900, loss = 2.17 (0.022 sec/batch)\n",
      "epoch 88, step 750 / 900, loss = 2.46 (0.022 sec/batch)\n",
      "epoch 88, step 800 / 900, loss = 1.15 (0.021 sec/batch)\n",
      "epoch 88, step 850 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 88, step 900 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.71\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.38\n",
      " avg loss = 2.85\n",
      "\n",
      "Epoch time: 24.56174397468567\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 89, step 50 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "epoch 89, step 100 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 89, step 150 / 900, loss = 2.61 (0.021 sec/batch)\n",
      "epoch 89, step 200 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 89, step 250 / 900, loss = 1.51 (0.021 sec/batch)\n",
      "epoch 89, step 300 / 900, loss = 1.64 (0.022 sec/batch)\n",
      "epoch 89, step 350 / 900, loss = 1.43 (0.022 sec/batch)\n",
      "epoch 89, step 400 / 900, loss = 2.50 (0.021 sec/batch)\n",
      "epoch 89, step 450 / 900, loss = 2.77 (0.021 sec/batch)\n",
      "epoch 89, step 500 / 900, loss = 2.47 (0.022 sec/batch)\n",
      "epoch 89, step 550 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 89, step 600 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 89, step 650 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 89, step 700 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 89, step 750 / 900, loss = 2.75 (0.022 sec/batch)\n",
      "epoch 89, step 800 / 900, loss = 2.90 (0.022 sec/batch)\n",
      "epoch 89, step 850 / 900, loss = 1.53 (0.021 sec/batch)\n",
      "epoch 89, step 900 / 900, loss = 2.45 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.62\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.78\n",
      " avg loss = 2.83\n",
      "\n",
      "Epoch time: 24.56847333908081\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 90, step 50 / 900, loss = 3.09 (0.021 sec/batch)\n",
      "epoch 90, step 100 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 90, step 150 / 900, loss = 3.07 (0.021 sec/batch)\n",
      "epoch 90, step 200 / 900, loss = 2.86 (0.021 sec/batch)\n",
      "epoch 90, step 250 / 900, loss = 3.34 (0.022 sec/batch)\n",
      "epoch 90, step 300 / 900, loss = 2.31 (0.022 sec/batch)\n",
      "epoch 90, step 350 / 900, loss = 2.74 (0.022 sec/batch)\n",
      "epoch 90, step 400 / 900, loss = 2.64 (0.022 sec/batch)\n",
      "epoch 90, step 450 / 900, loss = 2.16 (0.022 sec/batch)\n",
      "epoch 90, step 500 / 900, loss = 3.14 (0.021 sec/batch)\n",
      "epoch 90, step 550 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 90, step 600 / 900, loss = 2.40 (0.021 sec/batch)\n",
      "epoch 90, step 650 / 900, loss = 2.34 (0.021 sec/batch)\n",
      "epoch 90, step 700 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "epoch 90, step 750 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 90, step 800 / 900, loss = 2.72 (0.022 sec/batch)\n",
      "epoch 90, step 850 / 900, loss = 1.45 (0.021 sec/batch)\n",
      "epoch 90, step 900 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.51\n",
      " avg loss = 2.28\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.06\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.565598011016846\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 91, step 50 / 900, loss = 1.81 (0.021 sec/batch)\n",
      "epoch 91, step 100 / 900, loss = 1.67 (0.021 sec/batch)\n",
      "epoch 91, step 150 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 91, step 200 / 900, loss = 3.05 (0.021 sec/batch)\n",
      "epoch 91, step 250 / 900, loss = 1.34 (0.023 sec/batch)\n",
      "epoch 91, step 300 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 91, step 350 / 900, loss = 1.85 (0.021 sec/batch)\n",
      "epoch 91, step 400 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "epoch 91, step 450 / 900, loss = 1.72 (0.021 sec/batch)\n",
      "epoch 91, step 500 / 900, loss = 1.72 (0.021 sec/batch)\n",
      "epoch 91, step 550 / 900, loss = 3.27 (0.022 sec/batch)\n",
      "epoch 91, step 600 / 900, loss = 1.66 (0.021 sec/batch)\n",
      "epoch 91, step 650 / 900, loss = 2.77 (0.022 sec/batch)\n",
      "epoch 91, step 700 / 900, loss = 2.85 (0.022 sec/batch)\n",
      "epoch 91, step 750 / 900, loss = 1.87 (0.021 sec/batch)\n",
      "epoch 91, step 800 / 900, loss = 1.82 (0.022 sec/batch)\n",
      "epoch 91, step 850 / 900, loss = 1.99 (0.021 sec/batch)\n",
      "epoch 91, step 900 / 900, loss = 2.29 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.43\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.22\n",
      " avg loss = 2.86\n",
      "\n",
      "Epoch time: 24.580476999282837\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 92, step 50 / 900, loss = 2.06 (0.021 sec/batch)\n",
      "epoch 92, step 100 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 92, step 150 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 92, step 200 / 900, loss = 1.91 (0.021 sec/batch)\n",
      "epoch 92, step 250 / 900, loss = 1.59 (0.021 sec/batch)\n",
      "epoch 92, step 300 / 900, loss = 1.81 (0.021 sec/batch)\n",
      "epoch 92, step 350 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 92, step 400 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 92, step 450 / 900, loss = 2.45 (0.021 sec/batch)\n",
      "epoch 92, step 500 / 900, loss = 1.89 (0.022 sec/batch)\n",
      "epoch 92, step 550 / 900, loss = 2.51 (0.022 sec/batch)\n",
      "epoch 92, step 600 / 900, loss = 2.51 (0.021 sec/batch)\n",
      "epoch 92, step 650 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 92, step 700 / 900, loss = 2.77 (0.022 sec/batch)\n",
      "epoch 92, step 750 / 900, loss = 2.78 (0.021 sec/batch)\n",
      "epoch 92, step 800 / 900, loss = 2.07 (0.022 sec/batch)\n",
      "epoch 92, step 850 / 900, loss = 2.36 (0.021 sec/batch)\n",
      "epoch 92, step 900 / 900, loss = 1.21 (0.020 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.58\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.50\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.626222848892212\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 93, step 50 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 93, step 100 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 93, step 150 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 93, step 200 / 900, loss = 2.51 (0.022 sec/batch)\n",
      "epoch 93, step 250 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 93, step 300 / 900, loss = 2.34 (0.021 sec/batch)\n",
      "epoch 93, step 350 / 900, loss = 1.88 (0.022 sec/batch)\n",
      "epoch 93, step 400 / 900, loss = 1.85 (0.021 sec/batch)\n",
      "epoch 93, step 450 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 93, step 500 / 900, loss = 1.63 (0.021 sec/batch)\n",
      "epoch 93, step 550 / 900, loss = 3.05 (0.021 sec/batch)\n",
      "epoch 93, step 600 / 900, loss = 2.57 (0.022 sec/batch)\n",
      "epoch 93, step 650 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "epoch 93, step 700 / 900, loss = 3.12 (0.021 sec/batch)\n",
      "epoch 93, step 750 / 900, loss = 1.17 (0.022 sec/batch)\n",
      "epoch 93, step 800 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 93, step 850 / 900, loss = 2.55 (0.021 sec/batch)\n",
      "epoch 93, step 900 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.52\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.98\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.576401710510254\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 94, step 50 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "epoch 94, step 100 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 94, step 150 / 900, loss = 1.76 (0.021 sec/batch)\n",
      "epoch 94, step 200 / 900, loss = 2.55 (0.021 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 94, step 250 / 900, loss = 2.40 (0.021 sec/batch)\n",
      "epoch 94, step 300 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 94, step 350 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 94, step 400 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "epoch 94, step 450 / 900, loss = 1.97 (0.022 sec/batch)\n",
      "epoch 94, step 500 / 900, loss = 1.57 (0.022 sec/batch)\n",
      "epoch 94, step 550 / 900, loss = 2.63 (0.021 sec/batch)\n",
      "epoch 94, step 600 / 900, loss = 2.45 (0.022 sec/batch)\n",
      "epoch 94, step 650 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 94, step 700 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 94, step 750 / 900, loss = 2.14 (0.022 sec/batch)\n",
      "epoch 94, step 800 / 900, loss = 2.24 (0.021 sec/batch)\n",
      "epoch 94, step 850 / 900, loss = 2.41 (0.022 sec/batch)\n",
      "epoch 94, step 900 / 900, loss = 1.78 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.49\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.66\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.590320587158203\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 95, step 50 / 900, loss = 2.60 (0.021 sec/batch)\n",
      "epoch 95, step 100 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 95, step 150 / 900, loss = 2.62 (0.022 sec/batch)\n",
      "epoch 95, step 200 / 900, loss = 2.63 (0.022 sec/batch)\n",
      "epoch 95, step 250 / 900, loss = 1.99 (0.022 sec/batch)\n",
      "epoch 95, step 300 / 900, loss = 1.82 (0.022 sec/batch)\n",
      "epoch 95, step 350 / 900, loss = 2.45 (0.021 sec/batch)\n",
      "epoch 95, step 400 / 900, loss = 3.26 (0.022 sec/batch)\n",
      "epoch 95, step 450 / 900, loss = 2.87 (0.022 sec/batch)\n",
      "epoch 95, step 500 / 900, loss = 1.98 (0.021 sec/batch)\n",
      "epoch 95, step 550 / 900, loss = 1.62 (0.021 sec/batch)\n",
      "epoch 95, step 600 / 900, loss = 3.02 (0.021 sec/batch)\n",
      "epoch 95, step 650 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "epoch 95, step 700 / 900, loss = 2.67 (0.021 sec/batch)\n",
      "epoch 95, step 750 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 95, step 800 / 900, loss = 2.74 (0.021 sec/batch)\n",
      "epoch 95, step 850 / 900, loss = 1.59 (0.021 sec/batch)\n",
      "epoch 95, step 900 / 900, loss = 1.82 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.88\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.14\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.58015012741089\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 96, step 50 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 96, step 100 / 900, loss = 3.05 (0.021 sec/batch)\n",
      "epoch 96, step 150 / 900, loss = 2.69 (0.022 sec/batch)\n",
      "epoch 96, step 200 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "epoch 96, step 250 / 900, loss = 1.67 (0.022 sec/batch)\n",
      "epoch 96, step 300 / 900, loss = 3.35 (0.021 sec/batch)\n",
      "epoch 96, step 350 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 96, step 400 / 900, loss = 2.23 (0.022 sec/batch)\n",
      "epoch 96, step 450 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 96, step 500 / 900, loss = 2.02 (0.022 sec/batch)\n",
      "epoch 96, step 550 / 900, loss = 1.98 (0.022 sec/batch)\n",
      "epoch 96, step 600 / 900, loss = 1.81 (0.022 sec/batch)\n",
      "epoch 96, step 650 / 900, loss = 2.20 (0.022 sec/batch)\n",
      "epoch 96, step 700 / 900, loss = 2.15 (0.021 sec/batch)\n",
      "epoch 96, step 750 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 96, step 800 / 900, loss = 2.24 (0.022 sec/batch)\n",
      "epoch 96, step 850 / 900, loss = 2.67 (0.021 sec/batch)\n",
      "epoch 96, step 900 / 900, loss = 2.62 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.44\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.40\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.595916032791138\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 97, step 50 / 900, loss = 1.58 (0.021 sec/batch)\n",
      "epoch 97, step 100 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "epoch 97, step 150 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 97, step 200 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 97, step 250 / 900, loss = 1.95 (0.022 sec/batch)\n",
      "epoch 97, step 300 / 900, loss = 2.59 (0.021 sec/batch)\n",
      "epoch 97, step 350 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 97, step 400 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "epoch 97, step 450 / 900, loss = 2.34 (0.022 sec/batch)\n",
      "epoch 97, step 500 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 97, step 550 / 900, loss = 1.73 (0.021 sec/batch)\n",
      "epoch 97, step 600 / 900, loss = 3.03 (0.021 sec/batch)\n",
      "epoch 97, step 650 / 900, loss = 2.85 (0.022 sec/batch)\n",
      "epoch 97, step 700 / 900, loss = 2.09 (0.022 sec/batch)\n",
      "epoch 97, step 750 / 900, loss = 1.76 (0.021 sec/batch)\n",
      "epoch 97, step 800 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 97, step 850 / 900, loss = 1.63 (0.021 sec/batch)\n",
      "epoch 97, step 900 / 900, loss = 2.46 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.83\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.90\n",
      " avg loss = 2.85\n",
      "\n",
      "Epoch time: 24.602860689163208\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 98, step 50 / 900, loss = 2.51 (0.022 sec/batch)\n",
      "epoch 98, step 100 / 900, loss = 2.60 (0.022 sec/batch)\n",
      "epoch 98, step 150 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "epoch 98, step 200 / 900, loss = 2.93 (0.021 sec/batch)\n",
      "epoch 98, step 250 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 98, step 300 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 98, step 350 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 98, step 400 / 900, loss = 1.99 (0.022 sec/batch)\n",
      "epoch 98, step 450 / 900, loss = 2.82 (0.022 sec/batch)\n",
      "epoch 98, step 500 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 98, step 550 / 900, loss = 1.54 (0.022 sec/batch)\n",
      "epoch 98, step 600 / 900, loss = 3.31 (0.021 sec/batch)\n",
      "epoch 98, step 650 / 900, loss = 1.76 (0.021 sec/batch)\n",
      "epoch 98, step 700 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 98, step 750 / 900, loss = 2.50 (0.021 sec/batch)\n",
      "epoch 98, step 800 / 900, loss = 1.90 (0.022 sec/batch)\n",
      "epoch 98, step 850 / 900, loss = 1.13 (0.021 sec/batch)\n",
      "epoch 98, step 900 / 900, loss = 1.70 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.61\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.48\n",
      " avg loss = 2.93\n",
      "\n",
      "Epoch time: 24.575759410858154\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 99, step 50 / 900, loss = 1.46 (0.021 sec/batch)\n",
      "epoch 99, step 100 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 99, step 150 / 900, loss = 2.85 (0.021 sec/batch)\n",
      "epoch 99, step 200 / 900, loss = 2.00 (0.022 sec/batch)\n",
      "epoch 99, step 250 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "epoch 99, step 300 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 99, step 350 / 900, loss = 2.47 (0.022 sec/batch)\n",
      "epoch 99, step 400 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 99, step 450 / 900, loss = 1.87 (0.022 sec/batch)\n",
      "epoch 99, step 500 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 99, step 550 / 900, loss = 2.64 (0.021 sec/batch)\n",
      "epoch 99, step 600 / 900, loss = 2.38 (0.022 sec/batch)\n",
      "epoch 99, step 650 / 900, loss = 2.55 (0.021 sec/batch)\n",
      "epoch 99, step 700 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "epoch 99, step 750 / 900, loss = 2.46 (0.022 sec/batch)\n",
      "epoch 99, step 800 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "epoch 99, step 850 / 900, loss = 2.59 (0.021 sec/batch)\n",
      "epoch 99, step 900 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.22\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.58\n",
      " avg loss = 2.84\n",
      "\n",
      "Epoch time: 24.57441782951355\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 100, step 50 / 900, loss = 2.82 (0.022 sec/batch)\n",
      "epoch 100, step 100 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "epoch 100, step 150 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "epoch 100, step 200 / 900, loss = 1.77 (0.021 sec/batch)\n",
      "epoch 100, step 250 / 900, loss = 3.15 (0.021 sec/batch)\n",
      "epoch 100, step 300 / 900, loss = 2.80 (0.021 sec/batch)\n",
      "epoch 100, step 350 / 900, loss = 2.15 (0.022 sec/batch)\n",
      "epoch 100, step 400 / 900, loss = 1.33 (0.022 sec/batch)\n",
      "epoch 100, step 450 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 100, step 500 / 900, loss = 2.19 (0.022 sec/batch)\n",
      "epoch 100, step 550 / 900, loss = 2.45 (0.021 sec/batch)\n",
      "epoch 100, step 600 / 900, loss = 2.94 (0.021 sec/batch)\n",
      "epoch 100, step 650 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 100, step 700 / 900, loss = 2.64 (0.021 sec/batch)\n",
      "epoch 100, step 750 / 900, loss = 1.48 (0.021 sec/batch)\n",
      "epoch 100, step 800 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 100, step 850 / 900, loss = 2.60 (0.021 sec/batch)\n",
      "epoch 100, step 900 / 900, loss = 1.48 (0.022 sec/batch)\n",
      "Train error:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy = 55.84\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.18\n",
      " avg loss = 2.93\n",
      "\n",
      "Epoch time: 24.5896315574646\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 101, step 50 / 900, loss = 2.48 (0.022 sec/batch)\n",
      "epoch 101, step 100 / 900, loss = 3.55 (0.021 sec/batch)\n",
      "epoch 101, step 150 / 900, loss = 2.23 (0.022 sec/batch)\n",
      "epoch 101, step 200 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 101, step 250 / 900, loss = 1.68 (0.021 sec/batch)\n",
      "epoch 101, step 300 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "epoch 101, step 350 / 900, loss = 1.87 (0.021 sec/batch)\n",
      "epoch 101, step 400 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 101, step 450 / 900, loss = 2.04 (0.021 sec/batch)\n",
      "epoch 101, step 500 / 900, loss = 1.63 (0.022 sec/batch)\n",
      "epoch 101, step 550 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 101, step 600 / 900, loss = 2.84 (0.020 sec/batch)\n",
      "epoch 101, step 650 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 101, step 700 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 101, step 750 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 101, step 800 / 900, loss = 2.94 (0.021 sec/batch)\n",
      "epoch 101, step 850 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 101, step 900 / 900, loss = 2.54 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.76\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.36\n",
      " avg loss = 2.93\n",
      "\n",
      "Epoch time: 24.59053373336792\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 102, step 50 / 900, loss = 1.84 (0.021 sec/batch)\n",
      "epoch 102, step 100 / 900, loss = 2.34 (0.021 sec/batch)\n",
      "epoch 102, step 150 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 102, step 200 / 900, loss = 2.90 (0.021 sec/batch)\n",
      "epoch 102, step 250 / 900, loss = 1.73 (0.021 sec/batch)\n",
      "epoch 102, step 300 / 900, loss = 3.06 (0.021 sec/batch)\n",
      "epoch 102, step 350 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "epoch 102, step 400 / 900, loss = 1.53 (0.021 sec/batch)\n",
      "epoch 102, step 450 / 900, loss = 2.72 (0.021 sec/batch)\n",
      "epoch 102, step 500 / 900, loss = 1.55 (0.021 sec/batch)\n",
      "epoch 102, step 550 / 900, loss = 1.93 (0.021 sec/batch)\n",
      "epoch 102, step 600 / 900, loss = 2.99 (0.021 sec/batch)\n",
      "epoch 102, step 650 / 900, loss = 2.43 (0.022 sec/batch)\n",
      "epoch 102, step 700 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "epoch 102, step 750 / 900, loss = 1.92 (0.021 sec/batch)\n",
      "epoch 102, step 800 / 900, loss = 2.64 (0.021 sec/batch)\n",
      "epoch 102, step 850 / 900, loss = 2.46 (0.021 sec/batch)\n",
      "epoch 102, step 900 / 900, loss = 2.24 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.96\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.30\n",
      " avg loss = 2.86\n",
      "\n",
      "Epoch time: 24.59998321533203\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 103, step 50 / 900, loss = 2.37 (0.022 sec/batch)\n",
      "epoch 103, step 100 / 900, loss = 2.41 (0.021 sec/batch)\n",
      "epoch 103, step 150 / 900, loss = 1.81 (0.021 sec/batch)\n",
      "epoch 103, step 200 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 103, step 250 / 900, loss = 2.31 (0.022 sec/batch)\n",
      "epoch 103, step 300 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "epoch 103, step 350 / 900, loss = 2.88 (0.021 sec/batch)\n",
      "epoch 103, step 400 / 900, loss = 1.91 (0.021 sec/batch)\n",
      "epoch 103, step 450 / 900, loss = 1.71 (0.022 sec/batch)\n",
      "epoch 103, step 500 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 103, step 550 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "epoch 103, step 600 / 900, loss = 2.04 (0.021 sec/batch)\n",
      "epoch 103, step 650 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 103, step 700 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 103, step 750 / 900, loss = 2.85 (0.022 sec/batch)\n",
      "epoch 103, step 800 / 900, loss = 2.61 (0.021 sec/batch)\n",
      "epoch 103, step 850 / 900, loss = 3.85 (0.021 sec/batch)\n",
      "epoch 103, step 900 / 900, loss = 1.85 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.42\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.40\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.57115912437439\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 104, step 50 / 900, loss = 1.49 (0.021 sec/batch)\n",
      "epoch 104, step 100 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 104, step 150 / 900, loss = 2.77 (0.021 sec/batch)\n",
      "epoch 104, step 200 / 900, loss = 2.83 (0.021 sec/batch)\n",
      "epoch 104, step 250 / 900, loss = 2.98 (0.021 sec/batch)\n",
      "epoch 104, step 300 / 900, loss = 1.55 (0.021 sec/batch)\n",
      "epoch 104, step 350 / 900, loss = 1.58 (0.022 sec/batch)\n",
      "epoch 104, step 400 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 104, step 450 / 900, loss = 1.77 (0.021 sec/batch)\n",
      "epoch 104, step 500 / 900, loss = 3.63 (0.021 sec/batch)\n",
      "epoch 104, step 550 / 900, loss = 3.01 (0.022 sec/batch)\n",
      "epoch 104, step 600 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 104, step 650 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 104, step 700 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "epoch 104, step 750 / 900, loss = 2.40 (0.023 sec/batch)\n",
      "epoch 104, step 800 / 900, loss = 2.83 (0.021 sec/batch)\n",
      "epoch 104, step 850 / 900, loss = 1.82 (0.021 sec/batch)\n",
      "epoch 104, step 900 / 900, loss = 2.16 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.60\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.00\n",
      " avg loss = 2.84\n",
      "\n",
      "Epoch time: 24.597213983535767\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 105, step 50 / 900, loss = 2.32 (0.021 sec/batch)\n",
      "epoch 105, step 100 / 900, loss = 1.67 (0.021 sec/batch)\n",
      "epoch 105, step 150 / 900, loss = 2.95 (0.021 sec/batch)\n",
      "epoch 105, step 200 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 105, step 250 / 900, loss = 3.51 (0.021 sec/batch)\n",
      "epoch 105, step 300 / 900, loss = 2.38 (0.022 sec/batch)\n",
      "epoch 105, step 350 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 105, step 400 / 900, loss = 1.82 (0.021 sec/batch)\n",
      "epoch 105, step 450 / 900, loss = 2.93 (0.021 sec/batch)\n",
      "epoch 105, step 500 / 900, loss = 2.98 (0.021 sec/batch)\n",
      "epoch 105, step 550 / 900, loss = 2.36 (0.021 sec/batch)\n",
      "epoch 105, step 600 / 900, loss = 2.82 (0.021 sec/batch)\n",
      "epoch 105, step 650 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 105, step 700 / 900, loss = 3.06 (0.021 sec/batch)\n",
      "epoch 105, step 750 / 900, loss = 2.03 (0.022 sec/batch)\n",
      "epoch 105, step 800 / 900, loss = 1.95 (0.021 sec/batch)\n",
      "epoch 105, step 850 / 900, loss = 2.35 (0.020 sec/batch)\n",
      "epoch 105, step 900 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.72\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.02\n",
      " avg loss = 2.86\n",
      "\n",
      "Epoch time: 24.57901382446289\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 106, step 50 / 900, loss = 1.90 (0.021 sec/batch)\n",
      "epoch 106, step 100 / 900, loss = 2.78 (0.021 sec/batch)\n",
      "epoch 106, step 150 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 106, step 200 / 900, loss = 2.71 (0.021 sec/batch)\n",
      "epoch 106, step 250 / 900, loss = 2.84 (0.021 sec/batch)\n",
      "epoch 106, step 300 / 900, loss = 3.71 (0.022 sec/batch)\n",
      "epoch 106, step 350 / 900, loss = 2.62 (0.021 sec/batch)\n",
      "epoch 106, step 400 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "epoch 106, step 450 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "epoch 106, step 500 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 106, step 550 / 900, loss = 1.74 (0.021 sec/batch)\n",
      "epoch 106, step 600 / 900, loss = 2.58 (0.021 sec/batch)\n",
      "epoch 106, step 650 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "epoch 106, step 700 / 900, loss = 2.29 (0.022 sec/batch)\n",
      "epoch 106, step 750 / 900, loss = 2.61 (0.021 sec/batch)\n",
      "epoch 106, step 800 / 900, loss = 3.37 (0.022 sec/batch)\n",
      "epoch 106, step 850 / 900, loss = 2.66 (0.020 sec/batch)\n",
      "epoch 106, step 900 / 900, loss = 1.79 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.48\n",
      " avg loss = 2.28\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.36\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.593544960021973\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 107, step 50 / 900, loss = 2.61 (0.022 sec/batch)\n",
      "epoch 107, step 100 / 900, loss = 2.65 (0.022 sec/batch)\n",
      "epoch 107, step 150 / 900, loss = 2.73 (0.022 sec/batch)\n",
      "epoch 107, step 200 / 900, loss = 1.52 (0.021 sec/batch)\n",
      "epoch 107, step 250 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 107, step 300 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "epoch 107, step 350 / 900, loss = 2.34 (0.021 sec/batch)\n",
      "epoch 107, step 400 / 900, loss = 1.94 (0.022 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 107, step 450 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 107, step 500 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "epoch 107, step 550 / 900, loss = 1.90 (0.021 sec/batch)\n",
      "epoch 107, step 600 / 900, loss = 2.76 (0.021 sec/batch)\n",
      "epoch 107, step 650 / 900, loss = 3.41 (0.021 sec/batch)\n",
      "epoch 107, step 700 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 107, step 750 / 900, loss = 1.78 (0.021 sec/batch)\n",
      "epoch 107, step 800 / 900, loss = 2.79 (0.021 sec/batch)\n",
      "epoch 107, step 850 / 900, loss = 2.62 (0.021 sec/batch)\n",
      "epoch 107, step 900 / 900, loss = 1.48 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.42\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.72\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.5863676071167\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 108, step 50 / 900, loss = 2.81 (0.021 sec/batch)\n",
      "epoch 108, step 100 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 108, step 150 / 900, loss = 1.87 (0.021 sec/batch)\n",
      "epoch 108, step 200 / 900, loss = 2.77 (0.021 sec/batch)\n",
      "epoch 108, step 250 / 900, loss = 1.21 (0.021 sec/batch)\n",
      "epoch 108, step 300 / 900, loss = 1.75 (0.021 sec/batch)\n",
      "epoch 108, step 350 / 900, loss = 2.39 (0.022 sec/batch)\n",
      "epoch 108, step 400 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 108, step 450 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 108, step 500 / 900, loss = 2.30 (0.022 sec/batch)\n",
      "epoch 108, step 550 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 108, step 600 / 900, loss = 2.91 (0.021 sec/batch)\n",
      "epoch 108, step 650 / 900, loss = 2.78 (0.021 sec/batch)\n",
      "epoch 108, step 700 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "epoch 108, step 750 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 108, step 800 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "epoch 108, step 850 / 900, loss = 2.60 (0.022 sec/batch)\n",
      "epoch 108, step 900 / 900, loss = 1.90 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.85\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.68\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.569588899612427\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 109, step 50 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "epoch 109, step 100 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 109, step 150 / 900, loss = 2.32 (0.022 sec/batch)\n",
      "epoch 109, step 200 / 900, loss = 1.73 (0.021 sec/batch)\n",
      "epoch 109, step 250 / 900, loss = 3.08 (0.021 sec/batch)\n",
      "epoch 109, step 300 / 900, loss = 1.77 (0.021 sec/batch)\n",
      "epoch 109, step 350 / 900, loss = 2.62 (0.022 sec/batch)\n",
      "epoch 109, step 400 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "epoch 109, step 450 / 900, loss = 2.61 (0.021 sec/batch)\n",
      "epoch 109, step 500 / 900, loss = 2.75 (0.021 sec/batch)\n",
      "epoch 109, step 550 / 900, loss = 2.42 (0.022 sec/batch)\n",
      "epoch 109, step 600 / 900, loss = 2.10 (0.022 sec/batch)\n",
      "epoch 109, step 650 / 900, loss = 1.98 (0.021 sec/batch)\n",
      "epoch 109, step 700 / 900, loss = 2.84 (0.022 sec/batch)\n",
      "epoch 109, step 750 / 900, loss = 2.72 (0.021 sec/batch)\n",
      "epoch 109, step 800 / 900, loss = 2.86 (0.021 sec/batch)\n",
      "epoch 109, step 850 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 109, step 900 / 900, loss = 3.15 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.56\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.90\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.578389883041382\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 110, step 50 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 110, step 100 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 110, step 150 / 900, loss = 1.76 (0.021 sec/batch)\n",
      "epoch 110, step 200 / 900, loss = 3.18 (0.021 sec/batch)\n",
      "epoch 110, step 250 / 900, loss = 1.34 (0.022 sec/batch)\n",
      "epoch 110, step 300 / 900, loss = 2.21 (0.022 sec/batch)\n",
      "epoch 110, step 350 / 900, loss = 2.73 (0.022 sec/batch)\n",
      "epoch 110, step 400 / 900, loss = 2.63 (0.021 sec/batch)\n",
      "epoch 110, step 450 / 900, loss = 2.32 (0.021 sec/batch)\n",
      "epoch 110, step 500 / 900, loss = 1.90 (0.021 sec/batch)\n",
      "epoch 110, step 550 / 900, loss = 3.46 (0.022 sec/batch)\n",
      "epoch 110, step 600 / 900, loss = 2.76 (0.021 sec/batch)\n",
      "epoch 110, step 650 / 900, loss = 2.16 (0.022 sec/batch)\n",
      "epoch 110, step 700 / 900, loss = 1.60 (0.022 sec/batch)\n",
      "epoch 110, step 750 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 110, step 800 / 900, loss = 2.04 (0.022 sec/batch)\n",
      "epoch 110, step 850 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 110, step 900 / 900, loss = 1.85 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.13\n",
      " avg loss = 2.29\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.52\n",
      " avg loss = 2.94\n",
      "\n",
      "Epoch time: 24.588939905166626\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 111, step 50 / 900, loss = 2.73 (0.021 sec/batch)\n",
      "epoch 111, step 100 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 111, step 150 / 900, loss = 1.99 (0.021 sec/batch)\n",
      "epoch 111, step 200 / 900, loss = 2.01 (0.022 sec/batch)\n",
      "epoch 111, step 250 / 900, loss = 2.60 (0.021 sec/batch)\n",
      "epoch 111, step 300 / 900, loss = 2.62 (0.021 sec/batch)\n",
      "epoch 111, step 350 / 900, loss = 1.95 (0.022 sec/batch)\n",
      "epoch 111, step 400 / 900, loss = 1.76 (0.022 sec/batch)\n",
      "epoch 111, step 450 / 900, loss = 1.46 (0.021 sec/batch)\n",
      "epoch 111, step 500 / 900, loss = 2.20 (0.022 sec/batch)\n",
      "epoch 111, step 550 / 900, loss = 2.04 (0.021 sec/batch)\n",
      "epoch 111, step 600 / 900, loss = 3.24 (0.021 sec/batch)\n",
      "epoch 111, step 650 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 111, step 700 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 111, step 750 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 111, step 800 / 900, loss = 2.36 (0.021 sec/batch)\n",
      "epoch 111, step 850 / 900, loss = 1.93 (0.020 sec/batch)\n",
      "epoch 111, step 900 / 900, loss = 1.95 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.64\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.32\n",
      " avg loss = 2.92\n",
      "\n",
      "Epoch time: 24.61372947692871\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 112, step 50 / 900, loss = 2.83 (0.021 sec/batch)\n",
      "epoch 112, step 100 / 900, loss = 3.32 (0.021 sec/batch)\n",
      "epoch 112, step 150 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "epoch 112, step 200 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "epoch 112, step 250 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 112, step 300 / 900, loss = 2.16 (0.022 sec/batch)\n",
      "epoch 112, step 350 / 900, loss = 1.60 (0.021 sec/batch)\n",
      "epoch 112, step 400 / 900, loss = 3.09 (0.021 sec/batch)\n",
      "epoch 112, step 450 / 900, loss = 2.25 (0.022 sec/batch)\n",
      "epoch 112, step 500 / 900, loss = 2.79 (0.021 sec/batch)\n",
      "epoch 112, step 550 / 900, loss = 2.14 (0.022 sec/batch)\n",
      "epoch 112, step 600 / 900, loss = 1.88 (0.021 sec/batch)\n",
      "epoch 112, step 650 / 900, loss = 2.07 (0.022 sec/batch)\n",
      "epoch 112, step 700 / 900, loss = 2.50 (0.022 sec/batch)\n",
      "epoch 112, step 750 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 112, step 800 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "epoch 112, step 850 / 900, loss = 2.05 (0.022 sec/batch)\n",
      "epoch 112, step 900 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.09\n",
      " avg loss = 2.28\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.62\n",
      " avg loss = 2.92\n",
      "\n",
      "Epoch time: 24.57649040222168\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 113, step 50 / 900, loss = 2.27 (0.022 sec/batch)\n",
      "epoch 113, step 100 / 900, loss = 1.88 (0.021 sec/batch)\n",
      "epoch 113, step 150 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 113, step 200 / 900, loss = 2.03 (0.022 sec/batch)\n",
      "epoch 113, step 250 / 900, loss = 1.88 (0.021 sec/batch)\n",
      "epoch 113, step 300 / 900, loss = 1.06 (0.021 sec/batch)\n",
      "epoch 113, step 350 / 900, loss = 3.07 (0.021 sec/batch)\n",
      "epoch 113, step 400 / 900, loss = 2.31 (0.022 sec/batch)\n",
      "epoch 113, step 450 / 900, loss = 1.90 (0.022 sec/batch)\n",
      "epoch 113, step 500 / 900, loss = 1.38 (0.021 sec/batch)\n",
      "epoch 113, step 550 / 900, loss = 3.05 (0.022 sec/batch)\n",
      "epoch 113, step 600 / 900, loss = 2.73 (0.022 sec/batch)\n",
      "epoch 113, step 650 / 900, loss = 2.23 (0.022 sec/batch)\n",
      "epoch 113, step 700 / 900, loss = 3.44 (0.021 sec/batch)\n",
      "epoch 113, step 750 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 113, step 800 / 900, loss = 1.78 (0.021 sec/batch)\n",
      "epoch 113, step 850 / 900, loss = 3.18 (0.021 sec/batch)\n",
      "epoch 113, step 900 / 900, loss = 2.76 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.43\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.50\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.588286638259888\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 114, step 50 / 900, loss = 2.75 (0.022 sec/batch)\n",
      "epoch 114, step 100 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 114, step 150 / 900, loss = 2.98 (0.022 sec/batch)\n",
      "epoch 114, step 200 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "epoch 114, step 250 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "epoch 114, step 300 / 900, loss = 2.92 (0.022 sec/batch)\n",
      "epoch 114, step 350 / 900, loss = 1.66 (0.021 sec/batch)\n",
      "epoch 114, step 400 / 900, loss = 1.68 (0.022 sec/batch)\n",
      "epoch 114, step 450 / 900, loss = 2.85 (0.022 sec/batch)\n",
      "epoch 114, step 500 / 900, loss = 2.86 (0.021 sec/batch)\n",
      "epoch 114, step 550 / 900, loss = 1.67 (0.022 sec/batch)\n",
      "epoch 114, step 600 / 900, loss = 1.79 (0.022 sec/batch)\n",
      "epoch 114, step 650 / 900, loss = 1.98 (0.021 sec/batch)\n",
      "epoch 114, step 700 / 900, loss = 1.64 (0.021 sec/batch)\n",
      "epoch 114, step 750 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 114, step 800 / 900, loss = 2.06 (0.021 sec/batch)\n",
      "epoch 114, step 850 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 114, step 900 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.44\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.28\n",
      " avg loss = 2.83\n",
      "\n",
      "Epoch time: 24.586771965026855\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 115, step 50 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 115, step 100 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 115, step 150 / 900, loss = 2.72 (0.021 sec/batch)\n",
      "epoch 115, step 200 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "epoch 115, step 250 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "epoch 115, step 300 / 900, loss = 2.34 (0.021 sec/batch)\n",
      "epoch 115, step 350 / 900, loss = 2.34 (0.021 sec/batch)\n",
      "epoch 115, step 400 / 900, loss = 2.66 (0.022 sec/batch)\n",
      "epoch 115, step 450 / 900, loss = 2.11 (0.022 sec/batch)\n",
      "epoch 115, step 500 / 900, loss = 2.40 (0.021 sec/batch)\n",
      "epoch 115, step 550 / 900, loss = 2.97 (0.021 sec/batch)\n",
      "epoch 115, step 600 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "epoch 115, step 650 / 900, loss = 2.40 (0.021 sec/batch)\n",
      "epoch 115, step 700 / 900, loss = 1.92 (0.021 sec/batch)\n",
      "epoch 115, step 750 / 900, loss = 1.57 (0.021 sec/batch)\n",
      "epoch 115, step 800 / 900, loss = 2.04 (0.021 sec/batch)\n",
      "epoch 115, step 850 / 900, loss = 2.34 (0.022 sec/batch)\n",
      "epoch 115, step 900 / 900, loss = 2.96 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.34\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.28\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.575841188430786\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 116, step 50 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 116, step 100 / 900, loss = 1.71 (0.023 sec/batch)\n",
      "epoch 116, step 150 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "epoch 116, step 200 / 900, loss = 2.30 (0.022 sec/batch)\n",
      "epoch 116, step 250 / 900, loss = 1.88 (0.021 sec/batch)\n",
      "epoch 116, step 300 / 900, loss = 2.06 (0.021 sec/batch)\n",
      "epoch 116, step 350 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 116, step 400 / 900, loss = 1.84 (0.022 sec/batch)\n",
      "epoch 116, step 450 / 900, loss = 2.73 (0.021 sec/batch)\n",
      "epoch 116, step 500 / 900, loss = 2.29 (0.022 sec/batch)\n",
      "epoch 116, step 550 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "epoch 116, step 600 / 900, loss = 2.61 (0.021 sec/batch)\n",
      "epoch 116, step 650 / 900, loss = 1.99 (0.021 sec/batch)\n",
      "epoch 116, step 700 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 116, step 750 / 900, loss = 3.46 (0.021 sec/batch)\n",
      "epoch 116, step 800 / 900, loss = 2.82 (0.022 sec/batch)\n",
      "epoch 116, step 850 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 116, step 900 / 900, loss = 1.42 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.54\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.52\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.61784553527832\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 117, step 50 / 900, loss = 2.20 (0.022 sec/batch)\n",
      "epoch 117, step 100 / 900, loss = 3.31 (0.022 sec/batch)\n",
      "epoch 117, step 150 / 900, loss = 1.46 (0.022 sec/batch)\n",
      "epoch 117, step 200 / 900, loss = 1.28 (0.021 sec/batch)\n",
      "epoch 117, step 250 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "epoch 117, step 300 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 117, step 350 / 900, loss = 3.19 (0.021 sec/batch)\n",
      "epoch 117, step 400 / 900, loss = 2.72 (0.021 sec/batch)\n",
      "epoch 117, step 450 / 900, loss = 1.63 (0.021 sec/batch)\n",
      "epoch 117, step 500 / 900, loss = 2.92 (0.021 sec/batch)\n",
      "epoch 117, step 550 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "epoch 117, step 600 / 900, loss = 2.87 (0.021 sec/batch)\n",
      "epoch 117, step 650 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 117, step 700 / 900, loss = 3.04 (0.021 sec/batch)\n",
      "epoch 117, step 750 / 900, loss = 1.92 (0.021 sec/batch)\n",
      "epoch 117, step 800 / 900, loss = 2.34 (0.021 sec/batch)\n",
      "epoch 117, step 850 / 900, loss = 2.63 (0.021 sec/batch)\n",
      "epoch 117, step 900 / 900, loss = 2.57 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.42\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 48.28\n",
      " avg loss = 2.95\n",
      "\n",
      "Epoch time: 24.581011056900024\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 118, step 50 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 118, step 100 / 900, loss = 2.07 (0.022 sec/batch)\n",
      "epoch 118, step 150 / 900, loss = 2.25 (0.022 sec/batch)\n",
      "epoch 118, step 200 / 900, loss = 1.91 (0.021 sec/batch)\n",
      "epoch 118, step 250 / 900, loss = 2.24 (0.022 sec/batch)\n",
      "epoch 118, step 300 / 900, loss = 2.67 (0.021 sec/batch)\n",
      "epoch 118, step 350 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 118, step 400 / 900, loss = 2.15 (0.022 sec/batch)\n",
      "epoch 118, step 450 / 900, loss = 3.13 (0.021 sec/batch)\n",
      "epoch 118, step 500 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 118, step 550 / 900, loss = 3.12 (0.022 sec/batch)\n",
      "epoch 118, step 600 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 118, step 650 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 118, step 700 / 900, loss = 3.27 (0.021 sec/batch)\n",
      "epoch 118, step 750 / 900, loss = 3.37 (0.021 sec/batch)\n",
      "epoch 118, step 800 / 900, loss = 3.10 (0.022 sec/batch)\n",
      "epoch 118, step 850 / 900, loss = 1.82 (0.021 sec/batch)\n",
      "epoch 118, step 900 / 900, loss = 2.50 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.49\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.28\n",
      " avg loss = 2.85\n",
      "\n",
      "Epoch time: 24.591952323913574\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 119, step 50 / 900, loss = 1.68 (0.021 sec/batch)\n",
      "epoch 119, step 100 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 119, step 150 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 119, step 200 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 119, step 250 / 900, loss = 1.90 (0.021 sec/batch)\n",
      "epoch 119, step 300 / 900, loss = 3.06 (0.021 sec/batch)\n",
      "epoch 119, step 350 / 900, loss = 2.39 (0.021 sec/batch)\n",
      "epoch 119, step 400 / 900, loss = 2.40 (0.022 sec/batch)\n",
      "epoch 119, step 450 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 119, step 500 / 900, loss = 2.40 (0.021 sec/batch)\n",
      "epoch 119, step 550 / 900, loss = 2.22 (0.022 sec/batch)\n",
      "epoch 119, step 600 / 900, loss = 2.74 (0.021 sec/batch)\n",
      "epoch 119, step 650 / 900, loss = 1.56 (0.021 sec/batch)\n",
      "epoch 119, step 700 / 900, loss = 2.50 (0.021 sec/batch)\n",
      "epoch 119, step 750 / 900, loss = 2.59 (0.021 sec/batch)\n",
      "epoch 119, step 800 / 900, loss = 2.72 (0.021 sec/batch)\n",
      "epoch 119, step 850 / 900, loss = 1.82 (0.021 sec/batch)\n",
      "epoch 119, step 900 / 900, loss = 2.12 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.68\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 48.90\n",
      " avg loss = 2.94\n",
      "\n",
      "Epoch time: 24.59885025024414\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 120, step 50 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 120, step 100 / 900, loss = 2.33 (0.022 sec/batch)\n",
      "epoch 120, step 150 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "epoch 120, step 200 / 900, loss = 3.32 (0.021 sec/batch)\n",
      "epoch 120, step 250 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 120, step 300 / 900, loss = 2.83 (0.021 sec/batch)\n",
      "epoch 120, step 350 / 900, loss = 2.14 (0.022 sec/batch)\n",
      "epoch 120, step 400 / 900, loss = 3.07 (0.021 sec/batch)\n",
      "epoch 120, step 450 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "epoch 120, step 500 / 900, loss = 1.90 (0.021 sec/batch)\n",
      "epoch 120, step 550 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 120, step 600 / 900, loss = 2.71 (0.021 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 120, step 650 / 900, loss = 1.30 (0.021 sec/batch)\n",
      "epoch 120, step 700 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 120, step 750 / 900, loss = 2.77 (0.022 sec/batch)\n",
      "epoch 120, step 800 / 900, loss = 1.68 (0.022 sec/batch)\n",
      "epoch 120, step 850 / 900, loss = 2.26 (0.021 sec/batch)\n",
      "epoch 120, step 900 / 900, loss = 2.76 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.47\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.40\n",
      " avg loss = 2.86\n",
      "\n",
      "Epoch time: 24.583725690841675\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 121, step 50 / 900, loss = 2.12 (0.022 sec/batch)\n",
      "epoch 121, step 100 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 121, step 150 / 900, loss = 2.31 (0.022 sec/batch)\n",
      "epoch 121, step 200 / 900, loss = 3.00 (0.021 sec/batch)\n",
      "epoch 121, step 250 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 121, step 300 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 121, step 350 / 900, loss = 2.14 (0.022 sec/batch)\n",
      "epoch 121, step 400 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 121, step 450 / 900, loss = 1.37 (0.022 sec/batch)\n",
      "epoch 121, step 500 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 121, step 550 / 900, loss = 1.98 (0.021 sec/batch)\n",
      "epoch 121, step 600 / 900, loss = 2.70 (0.021 sec/batch)\n",
      "epoch 121, step 650 / 900, loss = 2.06 (0.021 sec/batch)\n",
      "epoch 121, step 700 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 121, step 750 / 900, loss = 2.68 (0.022 sec/batch)\n",
      "epoch 121, step 800 / 900, loss = 2.43 (0.020 sec/batch)\n",
      "epoch 121, step 850 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 121, step 900 / 900, loss = 2.69 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.41\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.34\n",
      " avg loss = 2.83\n",
      "\n",
      "Epoch time: 24.600705862045288\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 122, step 50 / 900, loss = 1.86 (0.021 sec/batch)\n",
      "epoch 122, step 100 / 900, loss = 2.14 (0.022 sec/batch)\n",
      "epoch 122, step 150 / 900, loss = 2.26 (0.021 sec/batch)\n",
      "epoch 122, step 200 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 122, step 250 / 900, loss = 1.90 (0.021 sec/batch)\n",
      "epoch 122, step 300 / 900, loss = 2.73 (0.022 sec/batch)\n",
      "epoch 122, step 350 / 900, loss = 2.34 (0.022 sec/batch)\n",
      "epoch 122, step 400 / 900, loss = 3.08 (0.021 sec/batch)\n",
      "epoch 122, step 450 / 900, loss = 2.63 (0.022 sec/batch)\n",
      "epoch 122, step 500 / 900, loss = 1.77 (0.022 sec/batch)\n",
      "epoch 122, step 550 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "epoch 122, step 600 / 900, loss = 2.69 (0.021 sec/batch)\n",
      "epoch 122, step 650 / 900, loss = 2.28 (0.022 sec/batch)\n",
      "epoch 122, step 700 / 900, loss = 1.46 (0.022 sec/batch)\n",
      "epoch 122, step 750 / 900, loss = 2.21 (0.022 sec/batch)\n",
      "epoch 122, step 800 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 122, step 850 / 900, loss = 2.80 (0.021 sec/batch)\n",
      "epoch 122, step 900 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.67\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.22\n",
      " avg loss = 2.96\n",
      "\n",
      "Epoch time: 24.578537702560425\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 123, step 50 / 900, loss = 2.38 (0.022 sec/batch)\n",
      "epoch 123, step 100 / 900, loss = 2.64 (0.021 sec/batch)\n",
      "epoch 123, step 150 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 123, step 200 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 123, step 250 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "epoch 123, step 300 / 900, loss = 2.70 (0.022 sec/batch)\n",
      "epoch 123, step 350 / 900, loss = 1.95 (0.021 sec/batch)\n",
      "epoch 123, step 400 / 900, loss = 2.68 (0.022 sec/batch)\n",
      "epoch 123, step 450 / 900, loss = 1.90 (0.021 sec/batch)\n",
      "epoch 123, step 500 / 900, loss = 1.70 (0.021 sec/batch)\n",
      "epoch 123, step 550 / 900, loss = 1.99 (0.020 sec/batch)\n",
      "epoch 123, step 600 / 900, loss = 1.48 (0.021 sec/batch)\n",
      "epoch 123, step 650 / 900, loss = 2.74 (0.021 sec/batch)\n",
      "epoch 123, step 700 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "epoch 123, step 750 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 123, step 800 / 900, loss = 1.95 (0.022 sec/batch)\n",
      "epoch 123, step 850 / 900, loss = 1.96 (0.022 sec/batch)\n",
      "epoch 123, step 900 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.77\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.14\n",
      " avg loss = 2.83\n",
      "\n",
      "Epoch time: 24.569894552230835\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 124, step 50 / 900, loss = 2.96 (0.021 sec/batch)\n",
      "epoch 124, step 100 / 900, loss = 2.15 (0.021 sec/batch)\n",
      "epoch 124, step 150 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 124, step 200 / 900, loss = 1.96 (0.021 sec/batch)\n",
      "epoch 124, step 250 / 900, loss = 2.40 (0.022 sec/batch)\n",
      "epoch 124, step 300 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 124, step 350 / 900, loss = 2.70 (0.022 sec/batch)\n",
      "epoch 124, step 400 / 900, loss = 1.86 (0.022 sec/batch)\n",
      "epoch 124, step 450 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 124, step 500 / 900, loss = 2.40 (0.021 sec/batch)\n",
      "epoch 124, step 550 / 900, loss = 1.52 (0.022 sec/batch)\n",
      "epoch 124, step 600 / 900, loss = 1.74 (0.021 sec/batch)\n",
      "epoch 124, step 650 / 900, loss = 2.32 (0.021 sec/batch)\n",
      "epoch 124, step 700 / 900, loss = 2.54 (0.021 sec/batch)\n",
      "epoch 124, step 750 / 900, loss = 1.90 (0.021 sec/batch)\n",
      "epoch 124, step 800 / 900, loss = 2.16 (0.022 sec/batch)\n",
      "epoch 124, step 850 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 124, step 900 / 900, loss = 1.71 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.52\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.30\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.59508228302002\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 125, step 50 / 900, loss = 1.91 (0.021 sec/batch)\n",
      "epoch 125, step 100 / 900, loss = 1.85 (0.022 sec/batch)\n",
      "epoch 125, step 150 / 900, loss = 2.34 (0.021 sec/batch)\n",
      "epoch 125, step 200 / 900, loss = 2.44 (0.021 sec/batch)\n",
      "epoch 125, step 250 / 900, loss = 2.36 (0.022 sec/batch)\n",
      "epoch 125, step 300 / 900, loss = 2.58 (0.021 sec/batch)\n",
      "epoch 125, step 350 / 900, loss = 2.69 (0.021 sec/batch)\n",
      "epoch 125, step 400 / 900, loss = 2.71 (0.021 sec/batch)\n",
      "epoch 125, step 450 / 900, loss = 3.02 (0.021 sec/batch)\n",
      "epoch 125, step 500 / 900, loss = 2.19 (0.022 sec/batch)\n",
      "epoch 125, step 550 / 900, loss = 2.48 (0.020 sec/batch)\n",
      "epoch 125, step 600 / 900, loss = 1.99 (0.022 sec/batch)\n",
      "epoch 125, step 650 / 900, loss = 1.98 (0.021 sec/batch)\n",
      "epoch 125, step 700 / 900, loss = 2.38 (0.022 sec/batch)\n",
      "epoch 125, step 750 / 900, loss = 1.64 (0.021 sec/batch)\n",
      "epoch 125, step 800 / 900, loss = 3.07 (0.021 sec/batch)\n",
      "epoch 125, step 850 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 125, step 900 / 900, loss = 2.83 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.44\n",
      " avg loss = 2.28\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.84\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.587823152542114\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 126, step 50 / 900, loss = 2.36 (0.021 sec/batch)\n",
      "epoch 126, step 100 / 900, loss = 1.57 (0.022 sec/batch)\n",
      "epoch 126, step 150 / 900, loss = 1.75 (0.022 sec/batch)\n",
      "epoch 126, step 200 / 900, loss = 1.67 (0.021 sec/batch)\n",
      "epoch 126, step 250 / 900, loss = 2.74 (0.021 sec/batch)\n",
      "epoch 126, step 300 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 126, step 350 / 900, loss = 1.78 (0.022 sec/batch)\n",
      "epoch 126, step 400 / 900, loss = 1.58 (0.021 sec/batch)\n",
      "epoch 126, step 450 / 900, loss = 2.05 (0.022 sec/batch)\n",
      "epoch 126, step 500 / 900, loss = 2.50 (0.022 sec/batch)\n",
      "epoch 126, step 550 / 900, loss = 2.75 (0.021 sec/batch)\n",
      "epoch 126, step 600 / 900, loss = 2.64 (0.022 sec/batch)\n",
      "epoch 126, step 650 / 900, loss = 1.74 (0.021 sec/batch)\n",
      "epoch 126, step 700 / 900, loss = 1.66 (0.021 sec/batch)\n",
      "epoch 126, step 750 / 900, loss = 2.18 (0.022 sec/batch)\n",
      "epoch 126, step 800 / 900, loss = 2.52 (0.022 sec/batch)\n",
      "epoch 126, step 850 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 126, step 900 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.75\n",
      " avg loss = 2.24\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.86\n",
      " avg loss = 2.97\n",
      "\n",
      "Epoch time: 24.595332860946655\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 127, step 50 / 900, loss = 3.17 (0.021 sec/batch)\n",
      "epoch 127, step 100 / 900, loss = 1.80 (0.021 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 127, step 150 / 900, loss = 2.77 (0.022 sec/batch)\n",
      "epoch 127, step 200 / 900, loss = 1.63 (0.021 sec/batch)\n",
      "epoch 127, step 250 / 900, loss = 2.45 (0.021 sec/batch)\n",
      "epoch 127, step 300 / 900, loss = 2.97 (0.021 sec/batch)\n",
      "epoch 127, step 350 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 127, step 400 / 900, loss = 2.42 (0.022 sec/batch)\n",
      "epoch 127, step 450 / 900, loss = 2.45 (0.021 sec/batch)\n",
      "epoch 127, step 500 / 900, loss = 2.56 (0.021 sec/batch)\n",
      "epoch 127, step 550 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 127, step 600 / 900, loss = 2.76 (0.021 sec/batch)\n",
      "epoch 127, step 650 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "epoch 127, step 700 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 127, step 750 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 127, step 800 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 127, step 850 / 900, loss = 1.93 (0.021 sec/batch)\n",
      "epoch 127, step 900 / 900, loss = 1.84 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.38\n",
      " avg loss = 2.28\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.26\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.58068537712097\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 128, step 50 / 900, loss = 1.26 (0.021 sec/batch)\n",
      "epoch 128, step 100 / 900, loss = 1.70 (0.021 sec/batch)\n",
      "epoch 128, step 150 / 900, loss = 1.95 (0.021 sec/batch)\n",
      "epoch 128, step 200 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "epoch 128, step 250 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 128, step 300 / 900, loss = 2.47 (0.021 sec/batch)\n",
      "epoch 128, step 350 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 128, step 400 / 900, loss = 2.26 (0.021 sec/batch)\n",
      "epoch 128, step 450 / 900, loss = 1.79 (0.022 sec/batch)\n",
      "epoch 128, step 500 / 900, loss = 1.58 (0.022 sec/batch)\n",
      "epoch 128, step 550 / 900, loss = 3.09 (0.021 sec/batch)\n",
      "epoch 128, step 600 / 900, loss = 1.39 (0.021 sec/batch)\n",
      "epoch 128, step 650 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 128, step 700 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 128, step 750 / 900, loss = 3.02 (0.021 sec/batch)\n",
      "epoch 128, step 800 / 900, loss = 2.17 (0.022 sec/batch)\n",
      "epoch 128, step 850 / 900, loss = 2.07 (0.022 sec/batch)\n",
      "epoch 128, step 900 / 900, loss = 2.67 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.54\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 48.90\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.57477045059204\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 129, step 50 / 900, loss = 2.85 (0.021 sec/batch)\n",
      "epoch 129, step 100 / 900, loss = 2.63 (0.021 sec/batch)\n",
      "epoch 129, step 150 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 129, step 200 / 900, loss = 1.90 (0.022 sec/batch)\n",
      "epoch 129, step 250 / 900, loss = 2.27 (0.022 sec/batch)\n",
      "epoch 129, step 300 / 900, loss = 2.91 (0.021 sec/batch)\n",
      "epoch 129, step 350 / 900, loss = 2.89 (0.022 sec/batch)\n",
      "epoch 129, step 400 / 900, loss = 1.77 (0.022 sec/batch)\n",
      "epoch 129, step 450 / 900, loss = 1.77 (0.021 sec/batch)\n",
      "epoch 129, step 500 / 900, loss = 2.90 (0.021 sec/batch)\n",
      "epoch 129, step 550 / 900, loss = 1.76 (0.021 sec/batch)\n",
      "epoch 129, step 600 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 129, step 650 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 129, step 700 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 129, step 750 / 900, loss = 1.76 (0.021 sec/batch)\n",
      "epoch 129, step 800 / 900, loss = 1.78 (0.021 sec/batch)\n",
      "epoch 129, step 850 / 900, loss = 2.68 (0.022 sec/batch)\n",
      "epoch 129, step 900 / 900, loss = 2.39 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.71\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.90\n",
      " avg loss = 2.94\n",
      "\n",
      "Epoch time: 24.590465784072876\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 130, step 50 / 900, loss = 2.41 (0.021 sec/batch)\n",
      "epoch 130, step 100 / 900, loss = 2.47 (0.021 sec/batch)\n",
      "epoch 130, step 150 / 900, loss = 1.74 (0.021 sec/batch)\n",
      "epoch 130, step 200 / 900, loss = 1.82 (0.022 sec/batch)\n",
      "epoch 130, step 250 / 900, loss = 2.91 (0.022 sec/batch)\n",
      "epoch 130, step 300 / 900, loss = 2.63 (0.021 sec/batch)\n",
      "epoch 130, step 350 / 900, loss = 2.85 (0.021 sec/batch)\n",
      "epoch 130, step 400 / 900, loss = 2.41 (0.021 sec/batch)\n",
      "epoch 130, step 450 / 900, loss = 1.71 (0.021 sec/batch)\n",
      "epoch 130, step 500 / 900, loss = 2.39 (0.021 sec/batch)\n",
      "epoch 130, step 550 / 900, loss = 2.36 (0.022 sec/batch)\n",
      "epoch 130, step 600 / 900, loss = 1.70 (0.021 sec/batch)\n",
      "epoch 130, step 650 / 900, loss = 2.45 (0.021 sec/batch)\n",
      "epoch 130, step 700 / 900, loss = 2.73 (0.021 sec/batch)\n",
      "epoch 130, step 750 / 900, loss = 1.98 (0.021 sec/batch)\n",
      "epoch 130, step 800 / 900, loss = 1.84 (0.022 sec/batch)\n",
      "epoch 130, step 850 / 900, loss = 3.08 (0.022 sec/batch)\n",
      "epoch 130, step 900 / 900, loss = 2.09 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.61\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.54\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.58277130126953\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 131, step 50 / 900, loss = 1.62 (0.021 sec/batch)\n",
      "epoch 131, step 100 / 900, loss = 2.04 (0.021 sec/batch)\n",
      "epoch 131, step 150 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 131, step 200 / 900, loss = 1.25 (0.021 sec/batch)\n",
      "epoch 131, step 250 / 900, loss = 2.15 (0.021 sec/batch)\n",
      "epoch 131, step 300 / 900, loss = 2.68 (0.021 sec/batch)\n",
      "epoch 131, step 350 / 900, loss = 2.82 (0.022 sec/batch)\n",
      "epoch 131, step 400 / 900, loss = 2.83 (0.021 sec/batch)\n",
      "epoch 131, step 450 / 900, loss = 1.91 (0.022 sec/batch)\n",
      "epoch 131, step 500 / 900, loss = 2.71 (0.021 sec/batch)\n",
      "epoch 131, step 550 / 900, loss = 2.75 (0.022 sec/batch)\n",
      "epoch 131, step 600 / 900, loss = 2.34 (0.022 sec/batch)\n",
      "epoch 131, step 650 / 900, loss = 2.58 (0.021 sec/batch)\n",
      "epoch 131, step 700 / 900, loss = 1.93 (0.021 sec/batch)\n",
      "epoch 131, step 750 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 131, step 800 / 900, loss = 1.66 (0.021 sec/batch)\n",
      "epoch 131, step 850 / 900, loss = 1.79 (0.023 sec/batch)\n",
      "epoch 131, step 900 / 900, loss = 2.94 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.77\n",
      " avg loss = 2.24\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.68\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.601818084716797\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 132, step 50 / 900, loss = 1.81 (0.021 sec/batch)\n",
      "epoch 132, step 100 / 900, loss = 2.70 (0.021 sec/batch)\n",
      "epoch 132, step 150 / 900, loss = 2.49 (0.021 sec/batch)\n",
      "epoch 132, step 200 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "epoch 132, step 250 / 900, loss = 2.59 (0.021 sec/batch)\n",
      "epoch 132, step 300 / 900, loss = 2.76 (0.021 sec/batch)\n",
      "epoch 132, step 350 / 900, loss = 2.21 (0.022 sec/batch)\n",
      "epoch 132, step 400 / 900, loss = 2.67 (0.022 sec/batch)\n",
      "epoch 132, step 450 / 900, loss = 2.33 (0.022 sec/batch)\n",
      "epoch 132, step 500 / 900, loss = 1.68 (0.022 sec/batch)\n",
      "epoch 132, step 550 / 900, loss = 2.63 (0.021 sec/batch)\n",
      "epoch 132, step 600 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 132, step 650 / 900, loss = 1.66 (0.022 sec/batch)\n",
      "epoch 132, step 700 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 132, step 750 / 900, loss = 2.20 (0.022 sec/batch)\n",
      "epoch 132, step 800 / 900, loss = 3.01 (0.021 sec/batch)\n",
      "epoch 132, step 850 / 900, loss = 3.25 (0.022 sec/batch)\n",
      "epoch 132, step 900 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.50\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.50\n",
      " avg loss = 2.96\n",
      "\n",
      "Epoch time: 24.60694432258606\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 133, step 50 / 900, loss = 2.04 (0.021 sec/batch)\n",
      "epoch 133, step 100 / 900, loss = 2.97 (0.021 sec/batch)\n",
      "epoch 133, step 150 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "epoch 133, step 200 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 133, step 250 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 133, step 300 / 900, loss = 1.82 (0.021 sec/batch)\n",
      "epoch 133, step 350 / 900, loss = 2.02 (0.022 sec/batch)\n",
      "epoch 133, step 400 / 900, loss = 2.99 (0.021 sec/batch)\n",
      "epoch 133, step 450 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "epoch 133, step 500 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 133, step 550 / 900, loss = 2.31 (0.022 sec/batch)\n",
      "epoch 133, step 600 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 133, step 650 / 900, loss = 1.77 (0.021 sec/batch)\n",
      "epoch 133, step 700 / 900, loss = 2.08 (0.021 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 133, step 750 / 900, loss = 2.72 (0.022 sec/batch)\n",
      "epoch 133, step 800 / 900, loss = 2.69 (0.022 sec/batch)\n",
      "epoch 133, step 850 / 900, loss = 2.37 (0.022 sec/batch)\n",
      "epoch 133, step 900 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.20\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.88\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.580636024475098\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 134, step 50 / 900, loss = 2.92 (0.022 sec/batch)\n",
      "epoch 134, step 100 / 900, loss = 1.62 (0.021 sec/batch)\n",
      "epoch 134, step 150 / 900, loss = 1.52 (0.021 sec/batch)\n",
      "epoch 134, step 200 / 900, loss = 2.93 (0.021 sec/batch)\n",
      "epoch 134, step 250 / 900, loss = 2.30 (0.022 sec/batch)\n",
      "epoch 134, step 300 / 900, loss = 2.69 (0.022 sec/batch)\n",
      "epoch 134, step 350 / 900, loss = 2.64 (0.021 sec/batch)\n",
      "epoch 134, step 400 / 900, loss = 1.78 (0.022 sec/batch)\n",
      "epoch 134, step 450 / 900, loss = 2.12 (0.020 sec/batch)\n",
      "epoch 134, step 500 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 134, step 550 / 900, loss = 3.12 (0.021 sec/batch)\n",
      "epoch 134, step 600 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "epoch 134, step 650 / 900, loss = 1.87 (0.022 sec/batch)\n",
      "epoch 134, step 700 / 900, loss = 2.93 (0.021 sec/batch)\n",
      "epoch 134, step 750 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "epoch 134, step 800 / 900, loss = 2.12 (0.022 sec/batch)\n",
      "epoch 134, step 850 / 900, loss = 2.67 (0.021 sec/batch)\n",
      "epoch 134, step 900 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.85\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.60\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.59131097793579\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 135, step 50 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 135, step 100 / 900, loss = 2.88 (0.021 sec/batch)\n",
      "epoch 135, step 150 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 135, step 200 / 900, loss = 1.87 (0.022 sec/batch)\n",
      "epoch 135, step 250 / 900, loss = 3.32 (0.021 sec/batch)\n",
      "epoch 135, step 300 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 135, step 350 / 900, loss = 2.50 (0.021 sec/batch)\n",
      "epoch 135, step 400 / 900, loss = 2.46 (0.022 sec/batch)\n",
      "epoch 135, step 450 / 900, loss = 2.76 (0.022 sec/batch)\n",
      "epoch 135, step 500 / 900, loss = 2.34 (0.021 sec/batch)\n",
      "epoch 135, step 550 / 900, loss = 2.41 (0.021 sec/batch)\n",
      "epoch 135, step 600 / 900, loss = 2.97 (0.022 sec/batch)\n",
      "epoch 135, step 650 / 900, loss = 3.27 (0.021 sec/batch)\n",
      "epoch 135, step 700 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "epoch 135, step 750 / 900, loss = 1.84 (0.021 sec/batch)\n",
      "epoch 135, step 800 / 900, loss = 2.87 (0.021 sec/batch)\n",
      "epoch 135, step 850 / 900, loss = 2.96 (0.022 sec/batch)\n",
      "epoch 135, step 900 / 900, loss = 2.36 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.84\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.10\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.5826735496521\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 136, step 50 / 900, loss = 3.13 (0.021 sec/batch)\n",
      "epoch 136, step 100 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 136, step 150 / 900, loss = 2.40 (0.021 sec/batch)\n",
      "epoch 136, step 200 / 900, loss = 2.84 (0.021 sec/batch)\n",
      "epoch 136, step 250 / 900, loss = 2.38 (0.022 sec/batch)\n",
      "epoch 136, step 300 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "epoch 136, step 350 / 900, loss = 2.50 (0.021 sec/batch)\n",
      "epoch 136, step 400 / 900, loss = 2.09 (0.022 sec/batch)\n",
      "epoch 136, step 450 / 900, loss = 1.57 (0.021 sec/batch)\n",
      "epoch 136, step 500 / 900, loss = 2.94 (0.022 sec/batch)\n",
      "epoch 136, step 550 / 900, loss = 1.73 (0.021 sec/batch)\n",
      "epoch 136, step 600 / 900, loss = 2.50 (0.022 sec/batch)\n",
      "epoch 136, step 650 / 900, loss = 1.91 (0.020 sec/batch)\n",
      "epoch 136, step 700 / 900, loss = 2.81 (0.021 sec/batch)\n",
      "epoch 136, step 750 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "epoch 136, step 800 / 900, loss = 1.66 (0.022 sec/batch)\n",
      "epoch 136, step 850 / 900, loss = 2.06 (0.021 sec/batch)\n",
      "epoch 136, step 900 / 900, loss = 2.24 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 56.03\n",
      " avg loss = 2.23\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.34\n",
      " avg loss = 2.85\n",
      "\n",
      "Epoch time: 24.59792137145996\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 137, step 50 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 137, step 100 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "epoch 137, step 150 / 900, loss = 2.11 (0.022 sec/batch)\n",
      "epoch 137, step 200 / 900, loss = 1.87 (0.021 sec/batch)\n",
      "epoch 137, step 250 / 900, loss = 2.75 (0.022 sec/batch)\n",
      "epoch 137, step 300 / 900, loss = 2.25 (0.022 sec/batch)\n",
      "epoch 137, step 350 / 900, loss = 2.63 (0.021 sec/batch)\n",
      "epoch 137, step 400 / 900, loss = 2.75 (0.022 sec/batch)\n",
      "epoch 137, step 450 / 900, loss = 1.66 (0.021 sec/batch)\n",
      "epoch 137, step 500 / 900, loss = 1.96 (0.021 sec/batch)\n",
      "epoch 137, step 550 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 137, step 600 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 137, step 650 / 900, loss = 2.58 (0.022 sec/batch)\n",
      "epoch 137, step 700 / 900, loss = 2.58 (0.022 sec/batch)\n",
      "epoch 137, step 750 / 900, loss = 3.54 (0.021 sec/batch)\n",
      "epoch 137, step 800 / 900, loss = 2.14 (0.022 sec/batch)\n",
      "epoch 137, step 850 / 900, loss = 2.32 (0.021 sec/batch)\n",
      "epoch 137, step 900 / 900, loss = 2.55 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.69\n",
      " avg loss = 2.24\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.02\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.602076292037964\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 138, step 50 / 900, loss = 3.33 (0.021 sec/batch)\n",
      "epoch 138, step 100 / 900, loss = 2.79 (0.021 sec/batch)\n",
      "epoch 138, step 150 / 900, loss = 2.09 (0.022 sec/batch)\n",
      "epoch 138, step 200 / 900, loss = 2.47 (0.022 sec/batch)\n",
      "epoch 138, step 250 / 900, loss = 2.09 (0.022 sec/batch)\n",
      "epoch 138, step 300 / 900, loss = 3.36 (0.021 sec/batch)\n",
      "epoch 138, step 350 / 900, loss = 3.05 (0.021 sec/batch)\n",
      "epoch 138, step 400 / 900, loss = 1.79 (0.022 sec/batch)\n",
      "epoch 138, step 450 / 900, loss = 1.47 (0.021 sec/batch)\n",
      "epoch 138, step 500 / 900, loss = 2.54 (0.021 sec/batch)\n",
      "epoch 138, step 550 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 138, step 600 / 900, loss = 2.17 (0.022 sec/batch)\n",
      "epoch 138, step 650 / 900, loss = 1.90 (0.021 sec/batch)\n",
      "epoch 138, step 700 / 900, loss = 2.26 (0.021 sec/batch)\n",
      "epoch 138, step 750 / 900, loss = 2.64 (0.022 sec/batch)\n",
      "epoch 138, step 800 / 900, loss = 2.15 (0.022 sec/batch)\n",
      "epoch 138, step 850 / 900, loss = 1.86 (0.021 sec/batch)\n",
      "epoch 138, step 900 / 900, loss = 3.05 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.40\n",
      " avg loss = 2.28\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.70\n",
      " avg loss = 2.83\n",
      "\n",
      "Epoch time: 24.579657316207886\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 139, step 50 / 900, loss = 1.74 (0.021 sec/batch)\n",
      "epoch 139, step 100 / 900, loss = 1.71 (0.021 sec/batch)\n",
      "epoch 139, step 150 / 900, loss = 2.12 (0.023 sec/batch)\n",
      "epoch 139, step 200 / 900, loss = 2.97 (0.021 sec/batch)\n",
      "epoch 139, step 250 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 139, step 300 / 900, loss = 2.94 (0.022 sec/batch)\n",
      "epoch 139, step 350 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "epoch 139, step 400 / 900, loss = 1.97 (0.022 sec/batch)\n",
      "epoch 139, step 450 / 900, loss = 2.41 (0.021 sec/batch)\n",
      "epoch 139, step 500 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 139, step 550 / 900, loss = 2.10 (0.022 sec/batch)\n",
      "epoch 139, step 600 / 900, loss = 2.50 (0.022 sec/batch)\n",
      "epoch 139, step 650 / 900, loss = 1.35 (0.021 sec/batch)\n",
      "epoch 139, step 700 / 900, loss = 2.68 (0.022 sec/batch)\n",
      "epoch 139, step 750 / 900, loss = 1.77 (0.021 sec/batch)\n",
      "epoch 139, step 800 / 900, loss = 1.79 (0.021 sec/batch)\n",
      "epoch 139, step 850 / 900, loss = 2.50 (0.021 sec/batch)\n",
      "epoch 139, step 900 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.93\n",
      " avg loss = 2.24\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.40\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.601919412612915\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 140, step 50 / 900, loss = 2.40 (0.022 sec/batch)\n",
      "epoch 140, step 100 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 140, step 150 / 900, loss = 1.48 (0.022 sec/batch)\n",
      "epoch 140, step 200 / 900, loss = 2.02 (0.021 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 140, step 250 / 900, loss = 2.59 (0.021 sec/batch)\n",
      "epoch 140, step 300 / 900, loss = 1.92 (0.022 sec/batch)\n",
      "epoch 140, step 350 / 900, loss = 2.84 (0.022 sec/batch)\n",
      "epoch 140, step 400 / 900, loss = 2.00 (0.022 sec/batch)\n",
      "epoch 140, step 450 / 900, loss = 2.39 (0.021 sec/batch)\n",
      "epoch 140, step 500 / 900, loss = 2.44 (0.021 sec/batch)\n",
      "epoch 140, step 550 / 900, loss = 2.95 (0.021 sec/batch)\n",
      "epoch 140, step 600 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 140, step 650 / 900, loss = 2.87 (0.020 sec/batch)\n",
      "epoch 140, step 700 / 900, loss = 1.64 (0.022 sec/batch)\n",
      "epoch 140, step 750 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 140, step 800 / 900, loss = 3.08 (0.021 sec/batch)\n",
      "epoch 140, step 850 / 900, loss = 1.42 (0.021 sec/batch)\n",
      "epoch 140, step 900 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.82\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 48.66\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.6248562335968\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 141, step 50 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 141, step 100 / 900, loss = 1.85 (0.021 sec/batch)\n",
      "epoch 141, step 150 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "epoch 141, step 200 / 900, loss = 1.72 (0.021 sec/batch)\n",
      "epoch 141, step 250 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 141, step 300 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 141, step 350 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 141, step 400 / 900, loss = 2.57 (0.022 sec/batch)\n",
      "epoch 141, step 450 / 900, loss = 2.39 (0.021 sec/batch)\n",
      "epoch 141, step 500 / 900, loss = 2.36 (0.021 sec/batch)\n",
      "epoch 141, step 550 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 141, step 600 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 141, step 650 / 900, loss = 2.93 (0.021 sec/batch)\n",
      "epoch 141, step 700 / 900, loss = 2.78 (0.022 sec/batch)\n",
      "epoch 141, step 750 / 900, loss = 1.61 (0.021 sec/batch)\n",
      "epoch 141, step 800 / 900, loss = 1.93 (0.021 sec/batch)\n",
      "epoch 141, step 850 / 900, loss = 1.53 (0.021 sec/batch)\n",
      "epoch 141, step 900 / 900, loss = 2.05 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.78\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.74\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.586212873458862\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 142, step 50 / 900, loss = 2.55 (0.022 sec/batch)\n",
      "epoch 142, step 100 / 900, loss = 2.49 (0.021 sec/batch)\n",
      "epoch 142, step 150 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 142, step 200 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 142, step 250 / 900, loss = 1.12 (0.021 sec/batch)\n",
      "epoch 142, step 300 / 900, loss = 2.56 (0.021 sec/batch)\n",
      "epoch 142, step 350 / 900, loss = 2.41 (0.021 sec/batch)\n",
      "epoch 142, step 400 / 900, loss = 2.50 (0.022 sec/batch)\n",
      "epoch 142, step 450 / 900, loss = 1.82 (0.021 sec/batch)\n",
      "epoch 142, step 500 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 142, step 550 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 142, step 600 / 900, loss = 2.97 (0.022 sec/batch)\n",
      "epoch 142, step 650 / 900, loss = 1.45 (0.021 sec/batch)\n",
      "epoch 142, step 700 / 900, loss = 3.31 (0.021 sec/batch)\n",
      "epoch 142, step 750 / 900, loss = 2.54 (0.021 sec/batch)\n",
      "epoch 142, step 800 / 900, loss = 2.51 (0.021 sec/batch)\n",
      "epoch 142, step 850 / 900, loss = 2.63 (0.021 sec/batch)\n",
      "epoch 142, step 900 / 900, loss = 1.62 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.44\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.22\n",
      " avg loss = 2.92\n",
      "\n",
      "Epoch time: 24.58095407485962\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 143, step 50 / 900, loss = 1.78 (0.021 sec/batch)\n",
      "epoch 143, step 100 / 900, loss = 2.00 (0.022 sec/batch)\n",
      "epoch 143, step 150 / 900, loss = 2.71 (0.022 sec/batch)\n",
      "epoch 143, step 200 / 900, loss = 1.14 (0.021 sec/batch)\n",
      "epoch 143, step 250 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "epoch 143, step 300 / 900, loss = 3.23 (0.021 sec/batch)\n",
      "epoch 143, step 350 / 900, loss = 2.10 (0.022 sec/batch)\n",
      "epoch 143, step 400 / 900, loss = 1.64 (0.021 sec/batch)\n",
      "epoch 143, step 450 / 900, loss = 2.49 (0.022 sec/batch)\n",
      "epoch 143, step 500 / 900, loss = 2.29 (0.022 sec/batch)\n",
      "epoch 143, step 550 / 900, loss = 1.62 (0.022 sec/batch)\n",
      "epoch 143, step 600 / 900, loss = 3.38 (0.021 sec/batch)\n",
      "epoch 143, step 650 / 900, loss = 2.63 (0.022 sec/batch)\n",
      "epoch 143, step 700 / 900, loss = 1.59 (0.021 sec/batch)\n",
      "epoch 143, step 750 / 900, loss = 1.94 (0.022 sec/batch)\n",
      "epoch 143, step 800 / 900, loss = 2.58 (0.021 sec/batch)\n",
      "epoch 143, step 850 / 900, loss = 1.33 (0.021 sec/batch)\n",
      "epoch 143, step 900 / 900, loss = 1.68 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.64\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.26\n",
      " avg loss = 2.92\n",
      "\n",
      "Epoch time: 24.588531970977783\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 144, step 50 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 144, step 100 / 900, loss = 1.96 (0.021 sec/batch)\n",
      "epoch 144, step 150 / 900, loss = 2.59 (0.021 sec/batch)\n",
      "epoch 144, step 200 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 144, step 250 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 144, step 300 / 900, loss = 2.41 (0.022 sec/batch)\n",
      "epoch 144, step 350 / 900, loss = 2.30 (0.022 sec/batch)\n",
      "epoch 144, step 400 / 900, loss = 3.38 (0.021 sec/batch)\n",
      "epoch 144, step 450 / 900, loss = 1.79 (0.021 sec/batch)\n",
      "epoch 144, step 500 / 900, loss = 1.74 (0.022 sec/batch)\n",
      "epoch 144, step 550 / 900, loss = 2.34 (0.021 sec/batch)\n",
      "epoch 144, step 600 / 900, loss = 1.98 (0.021 sec/batch)\n",
      "epoch 144, step 650 / 900, loss = 2.05 (0.022 sec/batch)\n",
      "epoch 144, step 700 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 144, step 750 / 900, loss = 2.07 (0.022 sec/batch)\n",
      "epoch 144, step 800 / 900, loss = 2.72 (0.021 sec/batch)\n",
      "epoch 144, step 850 / 900, loss = 3.71 (0.022 sec/batch)\n",
      "epoch 144, step 900 / 900, loss = 2.66 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.35\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.80\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.578592777252197\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 145, step 50 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 145, step 100 / 900, loss = 3.16 (0.021 sec/batch)\n",
      "epoch 145, step 150 / 900, loss = 1.81 (0.021 sec/batch)\n",
      "epoch 145, step 200 / 900, loss = 1.73 (0.022 sec/batch)\n",
      "epoch 145, step 250 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 145, step 300 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 145, step 350 / 900, loss = 3.04 (0.021 sec/batch)\n",
      "epoch 145, step 400 / 900, loss = 2.44 (0.022 sec/batch)\n",
      "epoch 145, step 450 / 900, loss = 3.00 (0.022 sec/batch)\n",
      "epoch 145, step 500 / 900, loss = 2.72 (0.022 sec/batch)\n",
      "epoch 145, step 550 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 145, step 600 / 900, loss = 2.73 (0.021 sec/batch)\n",
      "epoch 145, step 650 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 145, step 700 / 900, loss = 1.47 (0.022 sec/batch)\n",
      "epoch 145, step 750 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "epoch 145, step 800 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 145, step 850 / 900, loss = 2.88 (0.021 sec/batch)\n",
      "epoch 145, step 900 / 900, loss = 2.26 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.27\n",
      " avg loss = 2.28\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.40\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.612729787826538\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 146, step 50 / 900, loss = 1.99 (0.021 sec/batch)\n",
      "epoch 146, step 100 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 146, step 150 / 900, loss = 2.73 (0.021 sec/batch)\n",
      "epoch 146, step 200 / 900, loss = 1.94 (0.022 sec/batch)\n",
      "epoch 146, step 250 / 900, loss = 2.06 (0.021 sec/batch)\n",
      "epoch 146, step 300 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 146, step 350 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 146, step 400 / 900, loss = 2.70 (0.021 sec/batch)\n",
      "epoch 146, step 450 / 900, loss = 2.57 (0.022 sec/batch)\n",
      "epoch 146, step 500 / 900, loss = 1.98 (0.021 sec/batch)\n",
      "epoch 146, step 550 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 146, step 600 / 900, loss = 2.22 (0.022 sec/batch)\n",
      "epoch 146, step 650 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 146, step 700 / 900, loss = 1.71 (0.021 sec/batch)\n",
      "epoch 146, step 750 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 146, step 800 / 900, loss = 1.51 (0.021 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 146, step 850 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 146, step 900 / 900, loss = 1.86 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.55\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.16\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.580929279327393\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 147, step 50 / 900, loss = 2.49 (0.022 sec/batch)\n",
      "epoch 147, step 100 / 900, loss = 2.78 (0.021 sec/batch)\n",
      "epoch 147, step 150 / 900, loss = 2.72 (0.022 sec/batch)\n",
      "epoch 147, step 200 / 900, loss = 3.01 (0.021 sec/batch)\n",
      "epoch 147, step 250 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 147, step 300 / 900, loss = 1.59 (0.021 sec/batch)\n",
      "epoch 147, step 350 / 900, loss = 1.43 (0.021 sec/batch)\n",
      "epoch 147, step 400 / 900, loss = 2.06 (0.022 sec/batch)\n",
      "epoch 147, step 450 / 900, loss = 2.54 (0.021 sec/batch)\n",
      "epoch 147, step 500 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 147, step 550 / 900, loss = 2.59 (0.021 sec/batch)\n",
      "epoch 147, step 600 / 900, loss = 1.96 (0.021 sec/batch)\n",
      "epoch 147, step 650 / 900, loss = 2.56 (0.022 sec/batch)\n",
      "epoch 147, step 700 / 900, loss = 2.81 (0.021 sec/batch)\n",
      "epoch 147, step 750 / 900, loss = 2.55 (0.021 sec/batch)\n",
      "epoch 147, step 800 / 900, loss = 2.12 (0.022 sec/batch)\n",
      "epoch 147, step 850 / 900, loss = 2.89 (0.021 sec/batch)\n",
      "epoch 147, step 900 / 900, loss = 1.54 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.72\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 48.92\n",
      " avg loss = 2.95\n",
      "\n",
      "Epoch time: 24.573448181152344\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 148, step 50 / 900, loss = 1.59 (0.022 sec/batch)\n",
      "epoch 148, step 100 / 900, loss = 2.31 (0.022 sec/batch)\n",
      "epoch 148, step 150 / 900, loss = 2.47 (0.021 sec/batch)\n",
      "epoch 148, step 200 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 148, step 250 / 900, loss = 1.81 (0.022 sec/batch)\n",
      "epoch 148, step 300 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 148, step 350 / 900, loss = 1.91 (0.021 sec/batch)\n",
      "epoch 148, step 400 / 900, loss = 2.26 (0.021 sec/batch)\n",
      "epoch 148, step 450 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "epoch 148, step 500 / 900, loss = 2.02 (0.022 sec/batch)\n",
      "epoch 148, step 550 / 900, loss = 1.96 (0.021 sec/batch)\n",
      "epoch 148, step 600 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 148, step 650 / 900, loss = 2.64 (0.023 sec/batch)\n",
      "epoch 148, step 700 / 900, loss = 2.06 (0.021 sec/batch)\n",
      "epoch 148, step 750 / 900, loss = 1.77 (0.021 sec/batch)\n",
      "epoch 148, step 800 / 900, loss = 1.50 (0.021 sec/batch)\n",
      "epoch 148, step 850 / 900, loss = 2.20 (0.022 sec/batch)\n",
      "epoch 148, step 900 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.64\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.24\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.584168910980225\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 149, step 50 / 900, loss = 2.55 (0.022 sec/batch)\n",
      "epoch 149, step 100 / 900, loss = 2.41 (0.021 sec/batch)\n",
      "epoch 149, step 150 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "epoch 149, step 200 / 900, loss = 2.50 (0.022 sec/batch)\n",
      "epoch 149, step 250 / 900, loss = 2.74 (0.021 sec/batch)\n",
      "epoch 149, step 300 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "epoch 149, step 350 / 900, loss = 2.43 (0.022 sec/batch)\n",
      "epoch 149, step 400 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "epoch 149, step 450 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "epoch 149, step 500 / 900, loss = 2.04 (0.021 sec/batch)\n",
      "epoch 149, step 550 / 900, loss = 2.69 (0.021 sec/batch)\n",
      "epoch 149, step 600 / 900, loss = 2.46 (0.021 sec/batch)\n",
      "epoch 149, step 650 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 149, step 700 / 900, loss = 2.62 (0.022 sec/batch)\n",
      "epoch 149, step 750 / 900, loss = 2.03 (0.022 sec/batch)\n",
      "epoch 149, step 800 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "epoch 149, step 850 / 900, loss = 2.24 (0.022 sec/batch)\n",
      "epoch 149, step 900 / 900, loss = 2.44 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.02\n",
      " avg loss = 2.29\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.32\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.592042446136475\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 150, step 50 / 900, loss = 1.65 (0.021 sec/batch)\n",
      "epoch 150, step 100 / 900, loss = 2.81 (0.021 sec/batch)\n",
      "epoch 150, step 150 / 900, loss = 1.88 (0.021 sec/batch)\n",
      "epoch 150, step 200 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "epoch 150, step 250 / 900, loss = 2.51 (0.022 sec/batch)\n",
      "epoch 150, step 300 / 900, loss = 2.47 (0.021 sec/batch)\n",
      "epoch 150, step 350 / 900, loss = 2.15 (0.021 sec/batch)\n",
      "epoch 150, step 400 / 900, loss = 2.78 (0.021 sec/batch)\n",
      "epoch 150, step 450 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 150, step 500 / 900, loss = 2.50 (0.021 sec/batch)\n",
      "epoch 150, step 550 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 150, step 600 / 900, loss = 2.53 (0.022 sec/batch)\n",
      "epoch 150, step 650 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 150, step 700 / 900, loss = 1.61 (0.021 sec/batch)\n",
      "epoch 150, step 750 / 900, loss = 1.59 (0.021 sec/batch)\n",
      "epoch 150, step 800 / 900, loss = 1.46 (0.022 sec/batch)\n",
      "epoch 150, step 850 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 150, step 900 / 900, loss = 1.95 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.78\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.28\n",
      " avg loss = 2.85\n",
      "\n",
      "Epoch time: 24.59611964225769\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 151, step 50 / 900, loss = 2.40 (0.022 sec/batch)\n",
      "epoch 151, step 100 / 900, loss = 2.74 (0.021 sec/batch)\n",
      "epoch 151, step 150 / 900, loss = 2.01 (0.022 sec/batch)\n",
      "epoch 151, step 200 / 900, loss = 1.45 (0.021 sec/batch)\n",
      "epoch 151, step 250 / 900, loss = 3.26 (0.021 sec/batch)\n",
      "epoch 151, step 300 / 900, loss = 2.26 (0.022 sec/batch)\n",
      "epoch 151, step 350 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 151, step 400 / 900, loss = 2.63 (0.021 sec/batch)\n",
      "epoch 151, step 450 / 900, loss = 2.53 (0.022 sec/batch)\n",
      "epoch 151, step 500 / 900, loss = 1.66 (0.021 sec/batch)\n",
      "epoch 151, step 550 / 900, loss = 3.06 (0.021 sec/batch)\n",
      "epoch 151, step 600 / 900, loss = 1.86 (0.022 sec/batch)\n",
      "epoch 151, step 650 / 900, loss = 2.61 (0.022 sec/batch)\n",
      "epoch 151, step 700 / 900, loss = 2.08 (0.022 sec/batch)\n",
      "epoch 151, step 750 / 900, loss = 1.59 (0.021 sec/batch)\n",
      "epoch 151, step 800 / 900, loss = 2.21 (0.022 sec/batch)\n",
      "epoch 151, step 850 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 151, step 900 / 900, loss = 2.40 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.68\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.02\n",
      " avg loss = 2.85\n",
      "\n",
      "Epoch time: 24.5874240398407\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 152, step 50 / 900, loss = 1.76 (0.021 sec/batch)\n",
      "epoch 152, step 100 / 900, loss = 2.61 (0.021 sec/batch)\n",
      "epoch 152, step 150 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "epoch 152, step 200 / 900, loss = 3.50 (0.022 sec/batch)\n",
      "epoch 152, step 250 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 152, step 300 / 900, loss = 2.47 (0.021 sec/batch)\n",
      "epoch 152, step 350 / 900, loss = 2.55 (0.022 sec/batch)\n",
      "epoch 152, step 400 / 900, loss = 2.56 (0.021 sec/batch)\n",
      "epoch 152, step 450 / 900, loss = 2.50 (0.021 sec/batch)\n",
      "epoch 152, step 500 / 900, loss = 2.75 (0.021 sec/batch)\n",
      "epoch 152, step 550 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 152, step 600 / 900, loss = 2.94 (0.022 sec/batch)\n",
      "epoch 152, step 650 / 900, loss = 2.52 (0.022 sec/batch)\n",
      "epoch 152, step 700 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "epoch 152, step 750 / 900, loss = 2.75 (0.022 sec/batch)\n",
      "epoch 152, step 800 / 900, loss = 2.58 (0.022 sec/batch)\n",
      "epoch 152, step 850 / 900, loss = 2.89 (0.021 sec/batch)\n",
      "epoch 152, step 900 / 900, loss = 1.53 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.80\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.76\n",
      " avg loss = 2.92\n",
      "\n",
      "Epoch time: 24.588013887405396\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 153, step 50 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "epoch 153, step 100 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 153, step 150 / 900, loss = 2.77 (0.021 sec/batch)\n",
      "epoch 153, step 200 / 900, loss = 1.71 (0.021 sec/batch)\n",
      "epoch 153, step 250 / 900, loss = 1.41 (0.021 sec/batch)\n",
      "epoch 153, step 300 / 900, loss = 1.92 (0.021 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 153, step 350 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 153, step 400 / 900, loss = 2.56 (0.022 sec/batch)\n",
      "epoch 153, step 450 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "epoch 153, step 500 / 900, loss = 2.30 (0.022 sec/batch)\n",
      "epoch 153, step 550 / 900, loss = 2.99 (0.021 sec/batch)\n",
      "epoch 153, step 600 / 900, loss = 2.23 (0.022 sec/batch)\n",
      "epoch 153, step 650 / 900, loss = 2.47 (0.022 sec/batch)\n",
      "epoch 153, step 700 / 900, loss = 2.44 (0.021 sec/batch)\n",
      "epoch 153, step 750 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 153, step 800 / 900, loss = 1.78 (0.021 sec/batch)\n",
      "epoch 153, step 850 / 900, loss = 1.81 (0.022 sec/batch)\n",
      "epoch 153, step 900 / 900, loss = 2.62 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.39\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.56\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.583227396011353\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 154, step 50 / 900, loss = 2.46 (0.022 sec/batch)\n",
      "epoch 154, step 100 / 900, loss = 1.86 (0.021 sec/batch)\n",
      "epoch 154, step 150 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 154, step 200 / 900, loss = 2.71 (0.022 sec/batch)\n",
      "epoch 154, step 250 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 154, step 300 / 900, loss = 2.56 (0.021 sec/batch)\n",
      "epoch 154, step 350 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 154, step 400 / 900, loss = 1.61 (0.021 sec/batch)\n",
      "epoch 154, step 450 / 900, loss = 2.68 (0.022 sec/batch)\n",
      "epoch 154, step 500 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 154, step 550 / 900, loss = 2.58 (0.021 sec/batch)\n",
      "epoch 154, step 600 / 900, loss = 1.95 (0.022 sec/batch)\n",
      "epoch 154, step 650 / 900, loss = 2.63 (0.021 sec/batch)\n",
      "epoch 154, step 700 / 900, loss = 1.80 (0.021 sec/batch)\n",
      "epoch 154, step 750 / 900, loss = 2.57 (0.021 sec/batch)\n",
      "epoch 154, step 800 / 900, loss = 3.01 (0.021 sec/batch)\n",
      "epoch 154, step 850 / 900, loss = 2.56 (0.021 sec/batch)\n",
      "epoch 154, step 900 / 900, loss = 2.77 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.58\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.72\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.605456829071045\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 155, step 50 / 900, loss = 1.77 (0.021 sec/batch)\n",
      "epoch 155, step 100 / 900, loss = 2.40 (0.021 sec/batch)\n",
      "epoch 155, step 150 / 900, loss = 2.40 (0.021 sec/batch)\n",
      "epoch 155, step 200 / 900, loss = 2.02 (0.022 sec/batch)\n",
      "epoch 155, step 250 / 900, loss = 1.60 (0.021 sec/batch)\n",
      "epoch 155, step 300 / 900, loss = 1.80 (0.021 sec/batch)\n",
      "epoch 155, step 350 / 900, loss = 1.58 (0.022 sec/batch)\n",
      "epoch 155, step 400 / 900, loss = 1.91 (0.021 sec/batch)\n",
      "epoch 155, step 450 / 900, loss = 2.39 (0.022 sec/batch)\n",
      "epoch 155, step 500 / 900, loss = 1.14 (0.021 sec/batch)\n",
      "epoch 155, step 550 / 900, loss = 2.15 (0.021 sec/batch)\n",
      "epoch 155, step 600 / 900, loss = 2.73 (0.022 sec/batch)\n",
      "epoch 155, step 650 / 900, loss = 2.40 (0.021 sec/batch)\n",
      "epoch 155, step 700 / 900, loss = 1.93 (0.021 sec/batch)\n",
      "epoch 155, step 750 / 900, loss = 3.15 (0.022 sec/batch)\n",
      "epoch 155, step 800 / 900, loss = 1.78 (0.021 sec/batch)\n",
      "epoch 155, step 850 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "epoch 155, step 900 / 900, loss = 1.14 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.50\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.90\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.58927845954895\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 156, step 50 / 900, loss = 2.55 (0.021 sec/batch)\n",
      "epoch 156, step 100 / 900, loss = 2.39 (0.020 sec/batch)\n",
      "epoch 156, step 150 / 900, loss = 1.61 (0.021 sec/batch)\n",
      "epoch 156, step 200 / 900, loss = 2.38 (0.022 sec/batch)\n",
      "epoch 156, step 250 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "epoch 156, step 300 / 900, loss = 2.09 (0.022 sec/batch)\n",
      "epoch 156, step 350 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 156, step 400 / 900, loss = 2.79 (0.021 sec/batch)\n",
      "epoch 156, step 450 / 900, loss = 2.04 (0.022 sec/batch)\n",
      "epoch 156, step 500 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 156, step 550 / 900, loss = 2.98 (0.021 sec/batch)\n",
      "epoch 156, step 600 / 900, loss = 2.41 (0.021 sec/batch)\n",
      "epoch 156, step 650 / 900, loss = 2.38 (0.022 sec/batch)\n",
      "epoch 156, step 700 / 900, loss = 1.85 (0.021 sec/batch)\n",
      "epoch 156, step 750 / 900, loss = 2.85 (0.021 sec/batch)\n",
      "epoch 156, step 800 / 900, loss = 2.44 (0.021 sec/batch)\n",
      "epoch 156, step 850 / 900, loss = 3.23 (0.021 sec/batch)\n",
      "epoch 156, step 900 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.29\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.22\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.575400590896606\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 157, step 50 / 900, loss = 1.71 (0.021 sec/batch)\n",
      "epoch 157, step 100 / 900, loss = 2.32 (0.021 sec/batch)\n",
      "epoch 157, step 150 / 900, loss = 1.63 (0.021 sec/batch)\n",
      "epoch 157, step 200 / 900, loss = 1.77 (0.022 sec/batch)\n",
      "epoch 157, step 250 / 900, loss = 1.60 (0.021 sec/batch)\n",
      "epoch 157, step 300 / 900, loss = 2.06 (0.021 sec/batch)\n",
      "epoch 157, step 350 / 900, loss = 2.21 (0.022 sec/batch)\n",
      "epoch 157, step 400 / 900, loss = 2.36 (0.021 sec/batch)\n",
      "epoch 157, step 450 / 900, loss = 1.73 (0.021 sec/batch)\n",
      "epoch 157, step 500 / 900, loss = 2.75 (0.021 sec/batch)\n",
      "epoch 157, step 550 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 157, step 600 / 900, loss = 1.78 (0.022 sec/batch)\n",
      "epoch 157, step 650 / 900, loss = 1.73 (0.021 sec/batch)\n",
      "epoch 157, step 700 / 900, loss = 1.64 (0.022 sec/batch)\n",
      "epoch 157, step 750 / 900, loss = 1.58 (0.021 sec/batch)\n",
      "epoch 157, step 800 / 900, loss = 3.13 (0.021 sec/batch)\n",
      "epoch 157, step 850 / 900, loss = 2.82 (0.021 sec/batch)\n",
      "epoch 157, step 900 / 900, loss = 2.07 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.52\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.58\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.594343185424805\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 158, step 50 / 900, loss = 1.97 (0.022 sec/batch)\n",
      "epoch 158, step 100 / 900, loss = 1.47 (0.022 sec/batch)\n",
      "epoch 158, step 150 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 158, step 200 / 900, loss = 3.30 (0.022 sec/batch)\n",
      "epoch 158, step 250 / 900, loss = 2.16 (0.022 sec/batch)\n",
      "epoch 158, step 300 / 900, loss = 1.79 (0.021 sec/batch)\n",
      "epoch 158, step 350 / 900, loss = 2.20 (0.022 sec/batch)\n",
      "epoch 158, step 400 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 158, step 450 / 900, loss = 1.68 (0.021 sec/batch)\n",
      "epoch 158, step 500 / 900, loss = 2.90 (0.021 sec/batch)\n",
      "epoch 158, step 550 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 158, step 600 / 900, loss = 3.12 (0.021 sec/batch)\n",
      "epoch 158, step 650 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 158, step 700 / 900, loss = 2.58 (0.021 sec/batch)\n",
      "epoch 158, step 750 / 900, loss = 1.99 (0.021 sec/batch)\n",
      "epoch 158, step 800 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 158, step 850 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 158, step 900 / 900, loss = 2.24 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.43\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.52\n",
      " avg loss = 2.86\n",
      "\n",
      "Epoch time: 24.583449840545654\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 159, step 50 / 900, loss = 1.66 (0.021 sec/batch)\n",
      "epoch 159, step 100 / 900, loss = 1.64 (0.021 sec/batch)\n",
      "epoch 159, step 150 / 900, loss = 2.58 (0.021 sec/batch)\n",
      "epoch 159, step 200 / 900, loss = 2.35 (0.022 sec/batch)\n",
      "epoch 159, step 250 / 900, loss = 2.72 (0.021 sec/batch)\n",
      "epoch 159, step 300 / 900, loss = 1.76 (0.021 sec/batch)\n",
      "epoch 159, step 350 / 900, loss = 1.85 (0.022 sec/batch)\n",
      "epoch 159, step 400 / 900, loss = 2.75 (0.021 sec/batch)\n",
      "epoch 159, step 450 / 900, loss = 2.42 (0.022 sec/batch)\n",
      "epoch 159, step 500 / 900, loss = 1.56 (0.022 sec/batch)\n",
      "epoch 159, step 550 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 159, step 600 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 159, step 650 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 159, step 700 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 159, step 750 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 159, step 800 / 900, loss = 2.86 (0.021 sec/batch)\n",
      "epoch 159, step 850 / 900, loss = 1.68 (0.021 sec/batch)\n",
      "epoch 159, step 900 / 900, loss = 1.55 (0.022 sec/batch)\n",
      "Train error:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy = 55.72\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.10\n",
      " avg loss = 2.97\n",
      "\n",
      "Epoch time: 24.641222953796387\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 160, step 50 / 900, loss = 1.81 (0.022 sec/batch)\n",
      "epoch 160, step 100 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 160, step 150 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 160, step 200 / 900, loss = 2.07 (0.022 sec/batch)\n",
      "epoch 160, step 250 / 900, loss = 2.69 (0.021 sec/batch)\n",
      "epoch 160, step 300 / 900, loss = 1.86 (0.022 sec/batch)\n",
      "epoch 160, step 350 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 160, step 400 / 900, loss = 2.96 (0.021 sec/batch)\n",
      "epoch 160, step 450 / 900, loss = 2.03 (0.022 sec/batch)\n",
      "epoch 160, step 500 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 160, step 550 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "epoch 160, step 600 / 900, loss = 2.36 (0.022 sec/batch)\n",
      "epoch 160, step 650 / 900, loss = 2.19 (0.022 sec/batch)\n",
      "epoch 160, step 700 / 900, loss = 1.84 (0.021 sec/batch)\n",
      "epoch 160, step 750 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "epoch 160, step 800 / 900, loss = 2.78 (0.021 sec/batch)\n",
      "epoch 160, step 850 / 900, loss = 2.26 (0.022 sec/batch)\n",
      "epoch 160, step 900 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.66\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.52\n",
      " avg loss = 2.86\n",
      "\n",
      "Epoch time: 24.60433292388916\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 161, step 50 / 900, loss = 2.75 (0.021 sec/batch)\n",
      "epoch 161, step 100 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 161, step 150 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 161, step 200 / 900, loss = 1.92 (0.022 sec/batch)\n",
      "epoch 161, step 250 / 900, loss = 2.87 (0.021 sec/batch)\n",
      "epoch 161, step 300 / 900, loss = 2.50 (0.021 sec/batch)\n",
      "epoch 161, step 350 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 161, step 400 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "epoch 161, step 450 / 900, loss = 3.14 (0.021 sec/batch)\n",
      "epoch 161, step 500 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 161, step 550 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 161, step 600 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 161, step 650 / 900, loss = 2.82 (0.021 sec/batch)\n",
      "epoch 161, step 700 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 161, step 750 / 900, loss = 1.76 (0.021 sec/batch)\n",
      "epoch 161, step 800 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 161, step 850 / 900, loss = 1.64 (0.021 sec/batch)\n",
      "epoch 161, step 900 / 900, loss = 3.44 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.48\n",
      " avg loss = 2.28\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.50\n",
      " avg loss = 2.82\n",
      "\n",
      "Epoch time: 24.581157684326172\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 162, step 50 / 900, loss = 1.62 (0.022 sec/batch)\n",
      "epoch 162, step 100 / 900, loss = 3.25 (0.022 sec/batch)\n",
      "epoch 162, step 150 / 900, loss = 2.81 (0.021 sec/batch)\n",
      "epoch 162, step 200 / 900, loss = 1.89 (0.022 sec/batch)\n",
      "epoch 162, step 250 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 162, step 300 / 900, loss = 2.74 (0.021 sec/batch)\n",
      "epoch 162, step 350 / 900, loss = 1.96 (0.021 sec/batch)\n",
      "epoch 162, step 400 / 900, loss = 2.37 (0.022 sec/batch)\n",
      "epoch 162, step 450 / 900, loss = 1.81 (0.021 sec/batch)\n",
      "epoch 162, step 500 / 900, loss = 1.74 (0.021 sec/batch)\n",
      "epoch 162, step 550 / 900, loss = 1.98 (0.022 sec/batch)\n",
      "epoch 162, step 600 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 162, step 650 / 900, loss = 2.15 (0.021 sec/batch)\n",
      "epoch 162, step 700 / 900, loss = 1.95 (0.021 sec/batch)\n",
      "epoch 162, step 750 / 900, loss = 1.56 (0.021 sec/batch)\n",
      "epoch 162, step 800 / 900, loss = 2.54 (0.021 sec/batch)\n",
      "epoch 162, step 850 / 900, loss = 2.58 (0.021 sec/batch)\n",
      "epoch 162, step 900 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.76\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.32\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.589856386184692\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 163, step 50 / 900, loss = 2.26 (0.021 sec/batch)\n",
      "epoch 163, step 100 / 900, loss = 1.91 (0.021 sec/batch)\n",
      "epoch 163, step 150 / 900, loss = 2.82 (0.022 sec/batch)\n",
      "epoch 163, step 200 / 900, loss = 2.39 (0.022 sec/batch)\n",
      "epoch 163, step 250 / 900, loss = 2.64 (0.021 sec/batch)\n",
      "epoch 163, step 300 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 163, step 350 / 900, loss = 1.71 (0.021 sec/batch)\n",
      "epoch 163, step 400 / 900, loss = 1.50 (0.021 sec/batch)\n",
      "epoch 163, step 450 / 900, loss = 2.51 (0.021 sec/batch)\n",
      "epoch 163, step 500 / 900, loss = 1.95 (0.021 sec/batch)\n",
      "epoch 163, step 550 / 900, loss = 1.68 (0.021 sec/batch)\n",
      "epoch 163, step 600 / 900, loss = 1.99 (0.022 sec/batch)\n",
      "epoch 163, step 650 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 163, step 700 / 900, loss = 1.82 (0.021 sec/batch)\n",
      "epoch 163, step 750 / 900, loss = 2.18 (0.022 sec/batch)\n",
      "epoch 163, step 800 / 900, loss = 1.91 (0.021 sec/batch)\n",
      "epoch 163, step 850 / 900, loss = 3.06 (0.021 sec/batch)\n",
      "epoch 163, step 900 / 900, loss = 1.26 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.71\n",
      " avg loss = 2.23\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.90\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.562727212905884\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 164, step 50 / 900, loss = 1.61 (0.022 sec/batch)\n",
      "epoch 164, step 100 / 900, loss = 2.92 (0.021 sec/batch)\n",
      "epoch 164, step 150 / 900, loss = 2.39 (0.021 sec/batch)\n",
      "epoch 164, step 200 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 164, step 250 / 900, loss = 2.15 (0.021 sec/batch)\n",
      "epoch 164, step 300 / 900, loss = 1.85 (0.021 sec/batch)\n",
      "epoch 164, step 350 / 900, loss = 2.45 (0.022 sec/batch)\n",
      "epoch 164, step 400 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 164, step 450 / 900, loss = 2.83 (0.022 sec/batch)\n",
      "epoch 164, step 500 / 900, loss = 2.87 (0.021 sec/batch)\n",
      "epoch 164, step 550 / 900, loss = 1.64 (0.021 sec/batch)\n",
      "epoch 164, step 600 / 900, loss = 1.73 (0.021 sec/batch)\n",
      "epoch 164, step 650 / 900, loss = 3.34 (0.021 sec/batch)\n",
      "epoch 164, step 700 / 900, loss = 1.99 (0.021 sec/batch)\n",
      "epoch 164, step 750 / 900, loss = 2.88 (0.022 sec/batch)\n",
      "epoch 164, step 800 / 900, loss = 2.21 (0.022 sec/batch)\n",
      "epoch 164, step 850 / 900, loss = 2.39 (0.021 sec/batch)\n",
      "epoch 164, step 900 / 900, loss = 2.67 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.54\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.12\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.636694192886353\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 165, step 50 / 900, loss = 2.20 (0.022 sec/batch)\n",
      "epoch 165, step 100 / 900, loss = 1.74 (0.021 sec/batch)\n",
      "epoch 165, step 150 / 900, loss = 2.67 (0.021 sec/batch)\n",
      "epoch 165, step 200 / 900, loss = 2.44 (0.021 sec/batch)\n",
      "epoch 165, step 250 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "epoch 165, step 300 / 900, loss = 2.26 (0.022 sec/batch)\n",
      "epoch 165, step 350 / 900, loss = 1.57 (0.021 sec/batch)\n",
      "epoch 165, step 400 / 900, loss = 1.70 (0.021 sec/batch)\n",
      "epoch 165, step 450 / 900, loss = 3.25 (0.022 sec/batch)\n",
      "epoch 165, step 500 / 900, loss = 2.62 (0.021 sec/batch)\n",
      "epoch 165, step 550 / 900, loss = 3.28 (0.021 sec/batch)\n",
      "epoch 165, step 600 / 900, loss = 2.35 (0.022 sec/batch)\n",
      "epoch 165, step 650 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "epoch 165, step 700 / 900, loss = 1.64 (0.021 sec/batch)\n",
      "epoch 165, step 750 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 165, step 800 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 165, step 850 / 900, loss = 2.86 (0.022 sec/batch)\n",
      "epoch 165, step 900 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.51\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.96\n",
      " avg loss = 2.78\n",
      "\n",
      "Epoch time: 24.589086771011353\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 166, step 50 / 900, loss = 1.72 (0.021 sec/batch)\n",
      "epoch 166, step 100 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 166, step 150 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "epoch 166, step 200 / 900, loss = 1.87 (0.021 sec/batch)\n",
      "epoch 166, step 250 / 900, loss = 3.44 (0.022 sec/batch)\n",
      "epoch 166, step 300 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 166, step 350 / 900, loss = 2.67 (0.022 sec/batch)\n",
      "epoch 166, step 400 / 900, loss = 2.29 (0.022 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 166, step 450 / 900, loss = 2.41 (0.022 sec/batch)\n",
      "epoch 166, step 500 / 900, loss = 2.88 (0.021 sec/batch)\n",
      "epoch 166, step 550 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "epoch 166, step 600 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 166, step 650 / 900, loss = 2.34 (0.022 sec/batch)\n",
      "epoch 166, step 700 / 900, loss = 2.26 (0.021 sec/batch)\n",
      "epoch 166, step 750 / 900, loss = 1.89 (0.022 sec/batch)\n",
      "epoch 166, step 800 / 900, loss = 1.63 (0.022 sec/batch)\n",
      "epoch 166, step 850 / 900, loss = 3.42 (0.021 sec/batch)\n",
      "epoch 166, step 900 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 56.03\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.70\n",
      " avg loss = 2.85\n",
      "\n",
      "Epoch time: 24.58377194404602\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 167, step 50 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "epoch 167, step 100 / 900, loss = 2.27 (0.022 sec/batch)\n",
      "epoch 167, step 150 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 167, step 200 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 167, step 250 / 900, loss = 2.72 (0.021 sec/batch)\n",
      "epoch 167, step 300 / 900, loss = 2.24 (0.021 sec/batch)\n",
      "epoch 167, step 350 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "epoch 167, step 400 / 900, loss = 1.89 (0.022 sec/batch)\n",
      "epoch 167, step 450 / 900, loss = 1.91 (0.021 sec/batch)\n",
      "epoch 167, step 500 / 900, loss = 2.81 (0.021 sec/batch)\n",
      "epoch 167, step 550 / 900, loss = 2.87 (0.021 sec/batch)\n",
      "epoch 167, step 600 / 900, loss = 2.61 (0.021 sec/batch)\n",
      "epoch 167, step 650 / 900, loss = 1.92 (0.021 sec/batch)\n",
      "epoch 167, step 700 / 900, loss = 2.95 (0.022 sec/batch)\n",
      "epoch 167, step 750 / 900, loss = 2.79 (0.022 sec/batch)\n",
      "epoch 167, step 800 / 900, loss = 2.47 (0.021 sec/batch)\n",
      "epoch 167, step 850 / 900, loss = 2.80 (0.021 sec/batch)\n",
      "epoch 167, step 900 / 900, loss = 1.73 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.43\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.52\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.572911977767944\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 168, step 50 / 900, loss = 2.68 (0.021 sec/batch)\n",
      "epoch 168, step 100 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 168, step 150 / 900, loss = 2.14 (0.022 sec/batch)\n",
      "epoch 168, step 200 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "epoch 168, step 250 / 900, loss = 1.62 (0.021 sec/batch)\n",
      "epoch 168, step 300 / 900, loss = 2.43 (0.022 sec/batch)\n",
      "epoch 168, step 350 / 900, loss = 2.73 (0.022 sec/batch)\n",
      "epoch 168, step 400 / 900, loss = 2.52 (0.022 sec/batch)\n",
      "epoch 168, step 450 / 900, loss = 1.91 (0.021 sec/batch)\n",
      "epoch 168, step 500 / 900, loss = 1.95 (0.021 sec/batch)\n",
      "epoch 168, step 550 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 168, step 600 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "epoch 168, step 650 / 900, loss = 1.47 (0.021 sec/batch)\n",
      "epoch 168, step 700 / 900, loss = 2.15 (0.022 sec/batch)\n",
      "epoch 168, step 750 / 900, loss = 0.98 (0.021 sec/batch)\n",
      "epoch 168, step 800 / 900, loss = 2.89 (0.021 sec/batch)\n",
      "epoch 168, step 850 / 900, loss = 2.56 (0.022 sec/batch)\n",
      "epoch 168, step 900 / 900, loss = 2.41 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.77\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.12\n",
      " avg loss = 2.85\n",
      "\n",
      "Epoch time: 24.569664001464844\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 169, step 50 / 900, loss = 2.50 (0.021 sec/batch)\n",
      "epoch 169, step 100 / 900, loss = 1.97 (0.022 sec/batch)\n",
      "epoch 169, step 150 / 900, loss = 2.76 (0.022 sec/batch)\n",
      "epoch 169, step 200 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 169, step 250 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 169, step 300 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 169, step 350 / 900, loss = 1.73 (0.021 sec/batch)\n",
      "epoch 169, step 400 / 900, loss = 2.52 (0.022 sec/batch)\n",
      "epoch 169, step 450 / 900, loss = 1.83 (0.022 sec/batch)\n",
      "epoch 169, step 500 / 900, loss = 2.06 (0.022 sec/batch)\n",
      "epoch 169, step 550 / 900, loss = 2.62 (0.021 sec/batch)\n",
      "epoch 169, step 600 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 169, step 650 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 169, step 700 / 900, loss = 2.52 (0.022 sec/batch)\n",
      "epoch 169, step 750 / 900, loss = 1.78 (0.022 sec/batch)\n",
      "epoch 169, step 800 / 900, loss = 2.51 (0.021 sec/batch)\n",
      "epoch 169, step 850 / 900, loss = 2.11 (0.022 sec/batch)\n",
      "epoch 169, step 900 / 900, loss = 1.65 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.52\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.62\n",
      " avg loss = 2.86\n",
      "\n",
      "Epoch time: 24.60205626487732\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 170, step 50 / 900, loss = 2.85 (0.021 sec/batch)\n",
      "epoch 170, step 100 / 900, loss = 2.68 (0.021 sec/batch)\n",
      "epoch 170, step 150 / 900, loss = 2.30 (0.022 sec/batch)\n",
      "epoch 170, step 200 / 900, loss = 1.34 (0.021 sec/batch)\n",
      "epoch 170, step 250 / 900, loss = 1.71 (0.021 sec/batch)\n",
      "epoch 170, step 300 / 900, loss = 2.47 (0.022 sec/batch)\n",
      "epoch 170, step 350 / 900, loss = 3.56 (0.022 sec/batch)\n",
      "epoch 170, step 400 / 900, loss = 1.54 (0.021 sec/batch)\n",
      "epoch 170, step 450 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 170, step 500 / 900, loss = 1.75 (0.021 sec/batch)\n",
      "epoch 170, step 550 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 170, step 600 / 900, loss = 1.81 (0.021 sec/batch)\n",
      "epoch 170, step 650 / 900, loss = 1.93 (0.021 sec/batch)\n",
      "epoch 170, step 700 / 900, loss = 2.00 (0.022 sec/batch)\n",
      "epoch 170, step 750 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "epoch 170, step 800 / 900, loss = 1.74 (0.021 sec/batch)\n",
      "epoch 170, step 850 / 900, loss = 2.24 (0.021 sec/batch)\n",
      "epoch 170, step 900 / 900, loss = 2.88 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.46\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.28\n",
      " avg loss = 2.84\n",
      "\n",
      "Epoch time: 24.56497359275818\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 171, step 50 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 171, step 100 / 900, loss = 1.87 (0.021 sec/batch)\n",
      "epoch 171, step 150 / 900, loss = 1.67 (0.021 sec/batch)\n",
      "epoch 171, step 200 / 900, loss = 2.35 (0.022 sec/batch)\n",
      "epoch 171, step 250 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 171, step 300 / 900, loss = 3.48 (0.021 sec/batch)\n",
      "epoch 171, step 350 / 900, loss = 2.43 (0.022 sec/batch)\n",
      "epoch 171, step 400 / 900, loss = 1.64 (0.021 sec/batch)\n",
      "epoch 171, step 450 / 900, loss = 1.93 (0.021 sec/batch)\n",
      "epoch 171, step 500 / 900, loss = 2.71 (0.021 sec/batch)\n",
      "epoch 171, step 550 / 900, loss = 2.70 (0.022 sec/batch)\n",
      "epoch 171, step 600 / 900, loss = 1.75 (0.021 sec/batch)\n",
      "epoch 171, step 650 / 900, loss = 1.98 (0.021 sec/batch)\n",
      "epoch 171, step 700 / 900, loss = 3.14 (0.022 sec/batch)\n",
      "epoch 171, step 750 / 900, loss = 1.64 (0.021 sec/batch)\n",
      "epoch 171, step 800 / 900, loss = 2.90 (0.021 sec/batch)\n",
      "epoch 171, step 850 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "epoch 171, step 900 / 900, loss = 2.71 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.82\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.44\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.571274280548096\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 172, step 50 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 172, step 100 / 900, loss = 2.63 (0.022 sec/batch)\n",
      "epoch 172, step 150 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 172, step 200 / 900, loss = 1.91 (0.022 sec/batch)\n",
      "epoch 172, step 250 / 900, loss = 2.74 (0.021 sec/batch)\n",
      "epoch 172, step 300 / 900, loss = 3.31 (0.021 sec/batch)\n",
      "epoch 172, step 350 / 900, loss = 2.13 (0.022 sec/batch)\n",
      "epoch 172, step 400 / 900, loss = 1.75 (0.021 sec/batch)\n",
      "epoch 172, step 450 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 172, step 500 / 900, loss = 1.67 (0.021 sec/batch)\n",
      "epoch 172, step 550 / 900, loss = 1.74 (0.022 sec/batch)\n",
      "epoch 172, step 600 / 900, loss = 1.36 (0.021 sec/batch)\n",
      "epoch 172, step 650 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 172, step 700 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 172, step 750 / 900, loss = 2.50 (0.022 sec/batch)\n",
      "epoch 172, step 800 / 900, loss = 2.01 (0.022 sec/batch)\n",
      "epoch 172, step 850 / 900, loss = 3.29 (0.021 sec/batch)\n",
      "epoch 172, step 900 / 900, loss = 1.39 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.38\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.64\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.582951068878174\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 173, step 50 / 900, loss = 1.95 (0.021 sec/batch)\n",
      "epoch 173, step 100 / 900, loss = 2.54 (0.021 sec/batch)\n",
      "epoch 173, step 150 / 900, loss = 2.84 (0.021 sec/batch)\n",
      "epoch 173, step 200 / 900, loss = 2.44 (0.022 sec/batch)\n",
      "epoch 173, step 250 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "epoch 173, step 300 / 900, loss = 2.88 (0.021 sec/batch)\n",
      "epoch 173, step 350 / 900, loss = 2.01 (0.022 sec/batch)\n",
      "epoch 173, step 400 / 900, loss = 2.21 (0.022 sec/batch)\n",
      "epoch 173, step 450 / 900, loss = 2.50 (0.021 sec/batch)\n",
      "epoch 173, step 500 / 900, loss = 2.50 (0.021 sec/batch)\n",
      "epoch 173, step 550 / 900, loss = 1.78 (0.022 sec/batch)\n",
      "epoch 173, step 600 / 900, loss = 2.30 (0.022 sec/batch)\n",
      "epoch 173, step 650 / 900, loss = 1.99 (0.022 sec/batch)\n",
      "epoch 173, step 700 / 900, loss = 2.27 (0.022 sec/batch)\n",
      "epoch 173, step 750 / 900, loss = 1.56 (0.021 sec/batch)\n",
      "epoch 173, step 800 / 900, loss = 1.99 (0.021 sec/batch)\n",
      "epoch 173, step 850 / 900, loss = 1.78 (0.021 sec/batch)\n",
      "epoch 173, step 900 / 900, loss = 2.80 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.50\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.76\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.598501443862915\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 174, step 50 / 900, loss = 2.40 (0.022 sec/batch)\n",
      "epoch 174, step 100 / 900, loss = 2.41 (0.022 sec/batch)\n",
      "epoch 174, step 150 / 900, loss = 2.55 (0.021 sec/batch)\n",
      "epoch 174, step 200 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 174, step 250 / 900, loss = 1.97 (0.022 sec/batch)\n",
      "epoch 174, step 300 / 900, loss = 1.87 (0.021 sec/batch)\n",
      "epoch 174, step 350 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 174, step 400 / 900, loss = 1.82 (0.022 sec/batch)\n",
      "epoch 174, step 450 / 900, loss = 2.82 (0.022 sec/batch)\n",
      "epoch 174, step 500 / 900, loss = 2.07 (0.022 sec/batch)\n",
      "epoch 174, step 550 / 900, loss = 1.48 (0.022 sec/batch)\n",
      "epoch 174, step 600 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 174, step 650 / 900, loss = 3.00 (0.022 sec/batch)\n",
      "epoch 174, step 700 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "epoch 174, step 750 / 900, loss = 2.73 (0.021 sec/batch)\n",
      "epoch 174, step 800 / 900, loss = 1.88 (0.021 sec/batch)\n",
      "epoch 174, step 850 / 900, loss = 1.86 (0.022 sec/batch)\n",
      "epoch 174, step 900 / 900, loss = 1.92 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.57\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.02\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.60216784477234\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 175, step 50 / 900, loss = 1.39 (0.021 sec/batch)\n",
      "epoch 175, step 100 / 900, loss = 2.47 (0.021 sec/batch)\n",
      "epoch 175, step 150 / 900, loss = 1.71 (0.021 sec/batch)\n",
      "epoch 175, step 200 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 175, step 250 / 900, loss = 3.46 (0.022 sec/batch)\n",
      "epoch 175, step 300 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 175, step 350 / 900, loss = 1.95 (0.022 sec/batch)\n",
      "epoch 175, step 400 / 900, loss = 1.75 (0.021 sec/batch)\n",
      "epoch 175, step 450 / 900, loss = 2.40 (0.021 sec/batch)\n",
      "epoch 175, step 500 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "epoch 175, step 550 / 900, loss = 1.54 (0.021 sec/batch)\n",
      "epoch 175, step 600 / 900, loss = 2.79 (0.021 sec/batch)\n",
      "epoch 175, step 650 / 900, loss = 2.26 (0.021 sec/batch)\n",
      "epoch 175, step 700 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "epoch 175, step 750 / 900, loss = 3.08 (0.021 sec/batch)\n",
      "epoch 175, step 800 / 900, loss = 2.08 (0.022 sec/batch)\n",
      "epoch 175, step 850 / 900, loss = 1.85 (0.022 sec/batch)\n",
      "epoch 175, step 900 / 900, loss = 1.83 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.45\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.18\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.591996669769287\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 176, step 50 / 900, loss = 1.75 (0.021 sec/batch)\n",
      "epoch 176, step 100 / 900, loss = 1.39 (0.021 sec/batch)\n",
      "epoch 176, step 150 / 900, loss = 2.70 (0.021 sec/batch)\n",
      "epoch 176, step 200 / 900, loss = 2.47 (0.022 sec/batch)\n",
      "epoch 176, step 250 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 176, step 300 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 176, step 350 / 900, loss = 2.40 (0.022 sec/batch)\n",
      "epoch 176, step 400 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "epoch 176, step 450 / 900, loss = 2.28 (0.022 sec/batch)\n",
      "epoch 176, step 500 / 900, loss = 1.75 (0.021 sec/batch)\n",
      "epoch 176, step 550 / 900, loss = 1.96 (0.021 sec/batch)\n",
      "epoch 176, step 600 / 900, loss = 2.47 (0.022 sec/batch)\n",
      "epoch 176, step 650 / 900, loss = 3.09 (0.021 sec/batch)\n",
      "epoch 176, step 700 / 900, loss = 2.69 (0.022 sec/batch)\n",
      "epoch 176, step 750 / 900, loss = 1.40 (0.022 sec/batch)\n",
      "epoch 176, step 800 / 900, loss = 2.06 (0.021 sec/batch)\n",
      "epoch 176, step 850 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 176, step 900 / 900, loss = 2.06 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.49\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.40\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.5686137676239\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 177, step 50 / 900, loss = 2.65 (0.022 sec/batch)\n",
      "epoch 177, step 100 / 900, loss = 1.85 (0.021 sec/batch)\n",
      "epoch 177, step 150 / 900, loss = 1.64 (0.021 sec/batch)\n",
      "epoch 177, step 200 / 900, loss = 2.62 (0.022 sec/batch)\n",
      "epoch 177, step 250 / 900, loss = 1.68 (0.022 sec/batch)\n",
      "epoch 177, step 300 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 177, step 350 / 900, loss = 1.78 (0.021 sec/batch)\n",
      "epoch 177, step 400 / 900, loss = 3.02 (0.022 sec/batch)\n",
      "epoch 177, step 450 / 900, loss = 2.93 (0.022 sec/batch)\n",
      "epoch 177, step 500 / 900, loss = 1.70 (0.021 sec/batch)\n",
      "epoch 177, step 550 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 177, step 600 / 900, loss = 2.20 (0.022 sec/batch)\n",
      "epoch 177, step 650 / 900, loss = 1.88 (0.021 sec/batch)\n",
      "epoch 177, step 700 / 900, loss = 1.64 (0.022 sec/batch)\n",
      "epoch 177, step 750 / 900, loss = 2.73 (0.021 sec/batch)\n",
      "epoch 177, step 800 / 900, loss = 1.95 (0.022 sec/batch)\n",
      "epoch 177, step 850 / 900, loss = 1.58 (0.021 sec/batch)\n",
      "epoch 177, step 900 / 900, loss = 1.59 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.83\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.02\n",
      " avg loss = 2.86\n",
      "\n",
      "Epoch time: 24.57487154006958\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 178, step 50 / 900, loss = 2.20 (0.022 sec/batch)\n",
      "epoch 178, step 100 / 900, loss = 1.53 (0.021 sec/batch)\n",
      "epoch 178, step 150 / 900, loss = 2.55 (0.021 sec/batch)\n",
      "epoch 178, step 200 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "epoch 178, step 250 / 900, loss = 1.55 (0.022 sec/batch)\n",
      "epoch 178, step 300 / 900, loss = 2.24 (0.021 sec/batch)\n",
      "epoch 178, step 350 / 900, loss = 1.27 (0.021 sec/batch)\n",
      "epoch 178, step 400 / 900, loss = 2.90 (0.022 sec/batch)\n",
      "epoch 178, step 450 / 900, loss = 2.39 (0.021 sec/batch)\n",
      "epoch 178, step 500 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 178, step 550 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 178, step 600 / 900, loss = 2.51 (0.022 sec/batch)\n",
      "epoch 178, step 650 / 900, loss = 2.54 (0.021 sec/batch)\n",
      "epoch 178, step 700 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 178, step 750 / 900, loss = 1.71 (0.021 sec/batch)\n",
      "epoch 178, step 800 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 178, step 850 / 900, loss = 2.90 (0.020 sec/batch)\n",
      "epoch 178, step 900 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.43\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.10\n",
      " avg loss = 2.92\n",
      "\n",
      "Epoch time: 24.5680251121521\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 179, step 50 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 179, step 100 / 900, loss = 1.56 (0.021 sec/batch)\n",
      "epoch 179, step 150 / 900, loss = 2.52 (0.022 sec/batch)\n",
      "epoch 179, step 200 / 900, loss = 1.77 (0.021 sec/batch)\n",
      "epoch 179, step 250 / 900, loss = 1.98 (0.022 sec/batch)\n",
      "epoch 179, step 300 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 179, step 350 / 900, loss = 2.26 (0.021 sec/batch)\n",
      "epoch 179, step 400 / 900, loss = 1.68 (0.021 sec/batch)\n",
      "epoch 179, step 450 / 900, loss = 2.43 (0.022 sec/batch)\n",
      "epoch 179, step 500 / 900, loss = 2.59 (0.021 sec/batch)\n",
      "epoch 179, step 550 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 179, step 600 / 900, loss = 3.04 (0.021 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 179, step 650 / 900, loss = 2.68 (0.021 sec/batch)\n",
      "epoch 179, step 700 / 900, loss = 2.91 (0.021 sec/batch)\n",
      "epoch 179, step 750 / 900, loss = 1.36 (0.021 sec/batch)\n",
      "epoch 179, step 800 / 900, loss = 2.30 (0.022 sec/batch)\n",
      "epoch 179, step 850 / 900, loss = 3.00 (0.022 sec/batch)\n",
      "epoch 179, step 900 / 900, loss = 2.41 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.70\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.30\n",
      " avg loss = 2.84\n",
      "\n",
      "Epoch time: 24.61084771156311\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 180, step 50 / 900, loss = 1.32 (0.021 sec/batch)\n",
      "epoch 180, step 100 / 900, loss = 2.32 (0.021 sec/batch)\n",
      "epoch 180, step 150 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 180, step 200 / 900, loss = 2.42 (0.022 sec/batch)\n",
      "epoch 180, step 250 / 900, loss = 2.39 (0.022 sec/batch)\n",
      "epoch 180, step 300 / 900, loss = 2.20 (0.022 sec/batch)\n",
      "epoch 180, step 350 / 900, loss = 2.54 (0.022 sec/batch)\n",
      "epoch 180, step 400 / 900, loss = 1.83 (0.022 sec/batch)\n",
      "epoch 180, step 450 / 900, loss = 3.02 (0.021 sec/batch)\n",
      "epoch 180, step 500 / 900, loss = 2.28 (0.022 sec/batch)\n",
      "epoch 180, step 550 / 900, loss = 2.04 (0.021 sec/batch)\n",
      "epoch 180, step 600 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 180, step 650 / 900, loss = 2.49 (0.022 sec/batch)\n",
      "epoch 180, step 700 / 900, loss = 2.44 (0.022 sec/batch)\n",
      "epoch 180, step 750 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 180, step 800 / 900, loss = 2.54 (0.022 sec/batch)\n",
      "epoch 180, step 850 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 180, step 900 / 900, loss = 1.78 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.63\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.14\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.589460849761963\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 181, step 50 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "epoch 181, step 100 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 181, step 150 / 900, loss = 2.58 (0.022 sec/batch)\n",
      "epoch 181, step 200 / 900, loss = 1.58 (0.022 sec/batch)\n",
      "epoch 181, step 250 / 900, loss = 2.83 (0.022 sec/batch)\n",
      "epoch 181, step 300 / 900, loss = 1.79 (0.021 sec/batch)\n",
      "epoch 181, step 350 / 900, loss = 1.32 (0.022 sec/batch)\n",
      "epoch 181, step 400 / 900, loss = 2.67 (0.021 sec/batch)\n",
      "epoch 181, step 450 / 900, loss = 1.92 (0.021 sec/batch)\n",
      "epoch 181, step 500 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 181, step 550 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "epoch 181, step 600 / 900, loss = 1.62 (0.022 sec/batch)\n",
      "epoch 181, step 650 / 900, loss = 3.19 (0.021 sec/batch)\n",
      "epoch 181, step 700 / 900, loss = 1.64 (0.021 sec/batch)\n",
      "epoch 181, step 750 / 900, loss = 2.70 (0.021 sec/batch)\n",
      "epoch 181, step 800 / 900, loss = 2.26 (0.021 sec/batch)\n",
      "epoch 181, step 850 / 900, loss = 3.27 (0.021 sec/batch)\n",
      "epoch 181, step 900 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.45\n",
      " avg loss = 2.28\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.60\n",
      " avg loss = 2.95\n",
      "\n",
      "Epoch time: 24.58999466896057\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 182, step 50 / 900, loss = 1.87 (0.021 sec/batch)\n",
      "epoch 182, step 100 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "epoch 182, step 150 / 900, loss = 2.85 (0.021 sec/batch)\n",
      "epoch 182, step 200 / 900, loss = 3.49 (0.021 sec/batch)\n",
      "epoch 182, step 250 / 900, loss = 2.71 (0.021 sec/batch)\n",
      "epoch 182, step 300 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 182, step 350 / 900, loss = 1.58 (0.022 sec/batch)\n",
      "epoch 182, step 400 / 900, loss = 1.92 (0.021 sec/batch)\n",
      "epoch 182, step 450 / 900, loss = 1.66 (0.022 sec/batch)\n",
      "epoch 182, step 500 / 900, loss = 2.56 (0.022 sec/batch)\n",
      "epoch 182, step 550 / 900, loss = 2.80 (0.021 sec/batch)\n",
      "epoch 182, step 600 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 182, step 650 / 900, loss = 2.85 (0.021 sec/batch)\n",
      "epoch 182, step 700 / 900, loss = 3.67 (0.021 sec/batch)\n",
      "epoch 182, step 750 / 900, loss = 2.80 (0.021 sec/batch)\n",
      "epoch 182, step 800 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "epoch 182, step 850 / 900, loss = 2.84 (0.021 sec/batch)\n",
      "epoch 182, step 900 / 900, loss = 2.32 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.35\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.02\n",
      " avg loss = 2.92\n",
      "\n",
      "Epoch time: 24.565648317337036\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 183, step 50 / 900, loss = 2.51 (0.021 sec/batch)\n",
      "epoch 183, step 100 / 900, loss = 2.02 (0.022 sec/batch)\n",
      "epoch 183, step 150 / 900, loss = 2.26 (0.021 sec/batch)\n",
      "epoch 183, step 200 / 900, loss = 2.70 (0.020 sec/batch)\n",
      "epoch 183, step 250 / 900, loss = 1.99 (0.022 sec/batch)\n",
      "epoch 183, step 300 / 900, loss = 2.39 (0.022 sec/batch)\n",
      "epoch 183, step 350 / 900, loss = 2.89 (0.022 sec/batch)\n",
      "epoch 183, step 400 / 900, loss = 2.25 (0.022 sec/batch)\n",
      "epoch 183, step 450 / 900, loss = 1.99 (0.022 sec/batch)\n",
      "epoch 183, step 500 / 900, loss = 2.49 (0.021 sec/batch)\n",
      "epoch 183, step 550 / 900, loss = 1.88 (0.021 sec/batch)\n",
      "epoch 183, step 600 / 900, loss = 2.70 (0.022 sec/batch)\n",
      "epoch 183, step 650 / 900, loss = 1.95 (0.021 sec/batch)\n",
      "epoch 183, step 700 / 900, loss = 1.54 (0.021 sec/batch)\n",
      "epoch 183, step 750 / 900, loss = 2.78 (0.021 sec/batch)\n",
      "epoch 183, step 800 / 900, loss = 1.95 (0.021 sec/batch)\n",
      "epoch 183, step 850 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 183, step 900 / 900, loss = 1.99 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.58\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.18\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.579512357711792\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 184, step 50 / 900, loss = 2.81 (0.022 sec/batch)\n",
      "epoch 184, step 100 / 900, loss = 2.89 (0.022 sec/batch)\n",
      "epoch 184, step 150 / 900, loss = 2.79 (0.021 sec/batch)\n",
      "epoch 184, step 200 / 900, loss = 2.26 (0.021 sec/batch)\n",
      "epoch 184, step 250 / 900, loss = 3.00 (0.021 sec/batch)\n",
      "epoch 184, step 300 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 184, step 350 / 900, loss = 2.68 (0.021 sec/batch)\n",
      "epoch 184, step 400 / 900, loss = 1.81 (0.021 sec/batch)\n",
      "epoch 184, step 450 / 900, loss = 1.45 (0.021 sec/batch)\n",
      "epoch 184, step 500 / 900, loss = 1.96 (0.022 sec/batch)\n",
      "epoch 184, step 550 / 900, loss = 3.19 (0.021 sec/batch)\n",
      "epoch 184, step 600 / 900, loss = 1.95 (0.022 sec/batch)\n",
      "epoch 184, step 650 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 184, step 700 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 184, step 750 / 900, loss = 2.72 (0.021 sec/batch)\n",
      "epoch 184, step 800 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 184, step 850 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 184, step 900 / 900, loss = 2.58 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.48\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.30\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.607270002365112\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 185, step 50 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 185, step 100 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "epoch 185, step 150 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 185, step 200 / 900, loss = 1.96 (0.021 sec/batch)\n",
      "epoch 185, step 250 / 900, loss = 1.70 (0.022 sec/batch)\n",
      "epoch 185, step 300 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "epoch 185, step 350 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 185, step 400 / 900, loss = 2.70 (0.021 sec/batch)\n",
      "epoch 185, step 450 / 900, loss = 2.34 (0.022 sec/batch)\n",
      "epoch 185, step 500 / 900, loss = 1.70 (0.021 sec/batch)\n",
      "epoch 185, step 550 / 900, loss = 1.71 (0.022 sec/batch)\n",
      "epoch 185, step 600 / 900, loss = 1.64 (0.021 sec/batch)\n",
      "epoch 185, step 650 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "epoch 185, step 700 / 900, loss = 3.19 (0.021 sec/batch)\n",
      "epoch 185, step 750 / 900, loss = 2.21 (0.022 sec/batch)\n",
      "epoch 185, step 800 / 900, loss = 1.50 (0.022 sec/batch)\n",
      "epoch 185, step 850 / 900, loss = 2.77 (0.023 sec/batch)\n",
      "epoch 185, step 900 / 900, loss = 2.10 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.37\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.44\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.58183979988098\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 186, step 50 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 186, step 100 / 900, loss = 1.95 (0.021 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 186, step 150 / 900, loss = 1.30 (0.021 sec/batch)\n",
      "epoch 186, step 200 / 900, loss = 1.68 (0.021 sec/batch)\n",
      "epoch 186, step 250 / 900, loss = 2.26 (0.021 sec/batch)\n",
      "epoch 186, step 300 / 900, loss = 2.15 (0.021 sec/batch)\n",
      "epoch 186, step 350 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 186, step 400 / 900, loss = 2.22 (0.022 sec/batch)\n",
      "epoch 186, step 450 / 900, loss = 2.63 (0.021 sec/batch)\n",
      "epoch 186, step 500 / 900, loss = 1.61 (0.022 sec/batch)\n",
      "epoch 186, step 550 / 900, loss = 2.30 (0.022 sec/batch)\n",
      "epoch 186, step 600 / 900, loss = 2.61 (0.021 sec/batch)\n",
      "epoch 186, step 650 / 900, loss = 2.46 (0.021 sec/batch)\n",
      "epoch 186, step 700 / 900, loss = 3.15 (0.021 sec/batch)\n",
      "epoch 186, step 750 / 900, loss = 1.87 (0.021 sec/batch)\n",
      "epoch 186, step 800 / 900, loss = 2.90 (0.020 sec/batch)\n",
      "epoch 186, step 850 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 186, step 900 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.45\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.38\n",
      " avg loss = 2.93\n",
      "\n",
      "Epoch time: 24.583714723587036\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 187, step 50 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 187, step 100 / 900, loss = 2.51 (0.021 sec/batch)\n",
      "epoch 187, step 150 / 900, loss = 2.18 (0.022 sec/batch)\n",
      "epoch 187, step 200 / 900, loss = 2.76 (0.021 sec/batch)\n",
      "epoch 187, step 250 / 900, loss = 2.69 (0.021 sec/batch)\n",
      "epoch 187, step 300 / 900, loss = 2.75 (0.021 sec/batch)\n",
      "epoch 187, step 350 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "epoch 187, step 400 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 187, step 450 / 900, loss = 3.34 (0.021 sec/batch)\n",
      "epoch 187, step 500 / 900, loss = 2.62 (0.022 sec/batch)\n",
      "epoch 187, step 550 / 900, loss = 1.95 (0.021 sec/batch)\n",
      "epoch 187, step 600 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 187, step 650 / 900, loss = 2.15 (0.021 sec/batch)\n",
      "epoch 187, step 700 / 900, loss = 2.97 (0.021 sec/batch)\n",
      "epoch 187, step 750 / 900, loss = 1.64 (0.022 sec/batch)\n",
      "epoch 187, step 800 / 900, loss = 2.74 (0.022 sec/batch)\n",
      "epoch 187, step 850 / 900, loss = 2.62 (0.022 sec/batch)\n",
      "epoch 187, step 900 / 900, loss = 1.73 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.49\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.56\n",
      " avg loss = 2.83\n",
      "\n",
      "Epoch time: 24.59387183189392\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 188, step 50 / 900, loss = 2.52 (0.022 sec/batch)\n",
      "epoch 188, step 100 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 188, step 150 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 188, step 200 / 900, loss = 2.75 (0.021 sec/batch)\n",
      "epoch 188, step 250 / 900, loss = 1.38 (0.021 sec/batch)\n",
      "epoch 188, step 300 / 900, loss = 2.86 (0.021 sec/batch)\n",
      "epoch 188, step 350 / 900, loss = 2.61 (0.022 sec/batch)\n",
      "epoch 188, step 400 / 900, loss = 2.83 (0.022 sec/batch)\n",
      "epoch 188, step 450 / 900, loss = 2.53 (0.022 sec/batch)\n",
      "epoch 188, step 500 / 900, loss = 1.93 (0.022 sec/batch)\n",
      "epoch 188, step 550 / 900, loss = 1.82 (0.021 sec/batch)\n",
      "epoch 188, step 600 / 900, loss = 2.62 (0.021 sec/batch)\n",
      "epoch 188, step 650 / 900, loss = 1.65 (0.021 sec/batch)\n",
      "epoch 188, step 700 / 900, loss = 2.64 (0.022 sec/batch)\n",
      "epoch 188, step 750 / 900, loss = 2.15 (0.021 sec/batch)\n",
      "epoch 188, step 800 / 900, loss = 1.25 (0.022 sec/batch)\n",
      "epoch 188, step 850 / 900, loss = 1.99 (0.021 sec/batch)\n",
      "epoch 188, step 900 / 900, loss = 1.87 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.68\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.36\n",
      " avg loss = 2.83\n",
      "\n",
      "Epoch time: 24.614250898361206\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 189, step 50 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 189, step 100 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 189, step 150 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 189, step 200 / 900, loss = 2.07 (0.022 sec/batch)\n",
      "epoch 189, step 250 / 900, loss = 1.51 (0.021 sec/batch)\n",
      "epoch 189, step 300 / 900, loss = 2.32 (0.020 sec/batch)\n",
      "epoch 189, step 350 / 900, loss = 2.47 (0.021 sec/batch)\n",
      "epoch 189, step 400 / 900, loss = 2.37 (0.022 sec/batch)\n",
      "epoch 189, step 450 / 900, loss = 3.01 (0.021 sec/batch)\n",
      "epoch 189, step 500 / 900, loss = 2.11 (0.022 sec/batch)\n",
      "epoch 189, step 550 / 900, loss = 2.87 (0.021 sec/batch)\n",
      "epoch 189, step 600 / 900, loss = 2.39 (0.022 sec/batch)\n",
      "epoch 189, step 650 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 189, step 700 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 189, step 750 / 900, loss = 2.37 (0.022 sec/batch)\n",
      "epoch 189, step 800 / 900, loss = 2.26 (0.021 sec/batch)\n",
      "epoch 189, step 850 / 900, loss = 1.86 (0.021 sec/batch)\n",
      "epoch 189, step 900 / 900, loss = 1.68 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.36\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.82\n",
      " avg loss = 2.85\n",
      "\n",
      "Epoch time: 24.56555151939392\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 190, step 50 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 190, step 100 / 900, loss = 2.10 (0.022 sec/batch)\n",
      "epoch 190, step 150 / 900, loss = 2.86 (0.021 sec/batch)\n",
      "epoch 190, step 200 / 900, loss = 2.41 (0.022 sec/batch)\n",
      "epoch 190, step 250 / 900, loss = 1.86 (0.022 sec/batch)\n",
      "epoch 190, step 300 / 900, loss = 1.88 (0.021 sec/batch)\n",
      "epoch 190, step 350 / 900, loss = 2.78 (0.021 sec/batch)\n",
      "epoch 190, step 400 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 190, step 450 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 190, step 500 / 900, loss = 2.68 (0.021 sec/batch)\n",
      "epoch 190, step 550 / 900, loss = 2.23 (0.022 sec/batch)\n",
      "epoch 190, step 600 / 900, loss = 1.66 (0.021 sec/batch)\n",
      "epoch 190, step 650 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "epoch 190, step 700 / 900, loss = 2.04 (0.021 sec/batch)\n",
      "epoch 190, step 750 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "epoch 190, step 800 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 190, step 850 / 900, loss = 2.58 (0.021 sec/batch)\n",
      "epoch 190, step 900 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.44\n",
      " avg loss = 2.28\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.22\n",
      " avg loss = 2.93\n",
      "\n",
      "Epoch time: 24.587135553359985\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 191, step 50 / 900, loss = 2.74 (0.022 sec/batch)\n",
      "epoch 191, step 100 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 191, step 150 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 191, step 200 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 191, step 250 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 191, step 300 / 900, loss = 3.21 (0.021 sec/batch)\n",
      "epoch 191, step 350 / 900, loss = 2.67 (0.022 sec/batch)\n",
      "epoch 191, step 400 / 900, loss = 2.12 (0.022 sec/batch)\n",
      "epoch 191, step 450 / 900, loss = 1.91 (0.022 sec/batch)\n",
      "epoch 191, step 500 / 900, loss = 1.69 (0.022 sec/batch)\n",
      "epoch 191, step 550 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 191, step 600 / 900, loss = 1.83 (0.022 sec/batch)\n",
      "epoch 191, step 650 / 900, loss = 2.55 (0.021 sec/batch)\n",
      "epoch 191, step 700 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 191, step 750 / 900, loss = 1.67 (0.022 sec/batch)\n",
      "epoch 191, step 800 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 191, step 850 / 900, loss = 1.51 (0.021 sec/batch)\n",
      "epoch 191, step 900 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.94\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.72\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.59510612487793\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 192, step 50 / 900, loss = 2.61 (0.022 sec/batch)\n",
      "epoch 192, step 100 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 192, step 150 / 900, loss = 1.96 (0.021 sec/batch)\n",
      "epoch 192, step 200 / 900, loss = 2.60 (0.022 sec/batch)\n",
      "epoch 192, step 250 / 900, loss = 2.68 (0.021 sec/batch)\n",
      "epoch 192, step 300 / 900, loss = 2.55 (0.022 sec/batch)\n",
      "epoch 192, step 350 / 900, loss = 2.71 (0.021 sec/batch)\n",
      "epoch 192, step 400 / 900, loss = 2.78 (0.021 sec/batch)\n",
      "epoch 192, step 450 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 192, step 500 / 900, loss = 1.77 (0.022 sec/batch)\n",
      "epoch 192, step 550 / 900, loss = 2.39 (0.021 sec/batch)\n",
      "epoch 192, step 600 / 900, loss = 1.87 (0.021 sec/batch)\n",
      "epoch 192, step 650 / 900, loss = 2.47 (0.021 sec/batch)\n",
      "epoch 192, step 700 / 900, loss = 2.37 (0.022 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 192, step 750 / 900, loss = 2.33 (0.022 sec/batch)\n",
      "epoch 192, step 800 / 900, loss = 1.53 (0.021 sec/batch)\n",
      "epoch 192, step 850 / 900, loss = 3.00 (0.021 sec/batch)\n",
      "epoch 192, step 900 / 900, loss = 2.44 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.33\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.18\n",
      " avg loss = 2.95\n",
      "\n",
      "Epoch time: 24.585760831832886\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 193, step 50 / 900, loss = 3.69 (0.022 sec/batch)\n",
      "epoch 193, step 100 / 900, loss = 2.73 (0.021 sec/batch)\n",
      "epoch 193, step 150 / 900, loss = 2.62 (0.021 sec/batch)\n",
      "epoch 193, step 200 / 900, loss = 0.98 (0.021 sec/batch)\n",
      "epoch 193, step 250 / 900, loss = 2.71 (0.021 sec/batch)\n",
      "epoch 193, step 300 / 900, loss = 2.69 (0.021 sec/batch)\n",
      "epoch 193, step 350 / 900, loss = 2.36 (0.021 sec/batch)\n",
      "epoch 193, step 400 / 900, loss = 1.77 (0.021 sec/batch)\n",
      "epoch 193, step 450 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 193, step 500 / 900, loss = 2.28 (0.022 sec/batch)\n",
      "epoch 193, step 550 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 193, step 600 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "epoch 193, step 650 / 900, loss = 2.66 (0.021 sec/batch)\n",
      "epoch 193, step 700 / 900, loss = 2.44 (0.022 sec/batch)\n",
      "epoch 193, step 750 / 900, loss = 2.18 (0.022 sec/batch)\n",
      "epoch 193, step 800 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "epoch 193, step 850 / 900, loss = 2.36 (0.021 sec/batch)\n",
      "epoch 193, step 900 / 900, loss = 3.31 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.76\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.26\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.595219612121582\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 194, step 50 / 900, loss = 2.44 (0.022 sec/batch)\n",
      "epoch 194, step 100 / 900, loss = 1.98 (0.021 sec/batch)\n",
      "epoch 194, step 150 / 900, loss = 2.76 (0.021 sec/batch)\n",
      "epoch 194, step 200 / 900, loss = 2.72 (0.022 sec/batch)\n",
      "epoch 194, step 250 / 900, loss = 3.32 (0.022 sec/batch)\n",
      "epoch 194, step 300 / 900, loss = 1.87 (0.021 sec/batch)\n",
      "epoch 194, step 350 / 900, loss = 2.09 (0.022 sec/batch)\n",
      "epoch 194, step 400 / 900, loss = 2.85 (0.021 sec/batch)\n",
      "epoch 194, step 450 / 900, loss = 1.62 (0.021 sec/batch)\n",
      "epoch 194, step 500 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 194, step 550 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "epoch 194, step 600 / 900, loss = 2.41 (0.021 sec/batch)\n",
      "epoch 194, step 650 / 900, loss = 1.86 (0.021 sec/batch)\n",
      "epoch 194, step 700 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 194, step 750 / 900, loss = 1.64 (0.022 sec/batch)\n",
      "epoch 194, step 800 / 900, loss = 2.63 (0.021 sec/batch)\n",
      "epoch 194, step 850 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 194, step 900 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.35\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.12\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.583399772644043\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 195, step 50 / 900, loss = 1.91 (0.021 sec/batch)\n",
      "epoch 195, step 100 / 900, loss = 2.60 (0.021 sec/batch)\n",
      "epoch 195, step 150 / 900, loss = 1.89 (0.022 sec/batch)\n",
      "epoch 195, step 200 / 900, loss = 2.26 (0.022 sec/batch)\n",
      "epoch 195, step 250 / 900, loss = 1.50 (0.022 sec/batch)\n",
      "epoch 195, step 300 / 900, loss = 1.96 (0.021 sec/batch)\n",
      "epoch 195, step 350 / 900, loss = 1.57 (0.021 sec/batch)\n",
      "epoch 195, step 400 / 900, loss = 2.75 (0.021 sec/batch)\n",
      "epoch 195, step 450 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 195, step 500 / 900, loss = 1.63 (0.022 sec/batch)\n",
      "epoch 195, step 550 / 900, loss = 1.81 (0.021 sec/batch)\n",
      "epoch 195, step 600 / 900, loss = 1.53 (0.021 sec/batch)\n",
      "epoch 195, step 650 / 900, loss = 2.55 (0.021 sec/batch)\n",
      "epoch 195, step 700 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 195, step 750 / 900, loss = 2.86 (0.022 sec/batch)\n",
      "epoch 195, step 800 / 900, loss = 2.76 (0.021 sec/batch)\n",
      "epoch 195, step 850 / 900, loss = 2.09 (0.022 sec/batch)\n",
      "epoch 195, step 900 / 900, loss = 3.41 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 56.16\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.92\n",
      " avg loss = 2.86\n",
      "\n",
      "Epoch time: 24.60777997970581\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 196, step 50 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "epoch 196, step 100 / 900, loss = 0.86 (0.021 sec/batch)\n",
      "epoch 196, step 150 / 900, loss = 2.43 (0.022 sec/batch)\n",
      "epoch 196, step 200 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 196, step 250 / 900, loss = 2.39 (0.022 sec/batch)\n",
      "epoch 196, step 300 / 900, loss = 2.34 (0.021 sec/batch)\n",
      "epoch 196, step 350 / 900, loss = 2.69 (0.021 sec/batch)\n",
      "epoch 196, step 400 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 196, step 450 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 196, step 500 / 900, loss = 2.58 (0.021 sec/batch)\n",
      "epoch 196, step 550 / 900, loss = 3.31 (0.021 sec/batch)\n",
      "epoch 196, step 600 / 900, loss = 2.32 (0.021 sec/batch)\n",
      "epoch 196, step 650 / 900, loss = 1.58 (0.022 sec/batch)\n",
      "epoch 196, step 700 / 900, loss = 2.78 (0.021 sec/batch)\n",
      "epoch 196, step 750 / 900, loss = 1.65 (0.021 sec/batch)\n",
      "epoch 196, step 800 / 900, loss = 2.57 (0.022 sec/batch)\n",
      "epoch 196, step 850 / 900, loss = 1.99 (0.021 sec/batch)\n",
      "epoch 196, step 900 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.68\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.94\n",
      " avg loss = 2.85\n",
      "\n",
      "Epoch time: 24.579261541366577\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 197, step 50 / 900, loss = 1.86 (0.022 sec/batch)\n",
      "epoch 197, step 100 / 900, loss = 2.06 (0.021 sec/batch)\n",
      "epoch 197, step 150 / 900, loss = 1.87 (0.022 sec/batch)\n",
      "epoch 197, step 200 / 900, loss = 2.06 (0.021 sec/batch)\n",
      "epoch 197, step 250 / 900, loss = 2.07 (0.022 sec/batch)\n",
      "epoch 197, step 300 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 197, step 350 / 900, loss = 2.39 (0.022 sec/batch)\n",
      "epoch 197, step 400 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "epoch 197, step 450 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 197, step 500 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 197, step 550 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 197, step 600 / 900, loss = 1.76 (0.021 sec/batch)\n",
      "epoch 197, step 650 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 197, step 700 / 900, loss = 1.95 (0.021 sec/batch)\n",
      "epoch 197, step 750 / 900, loss = 1.96 (0.022 sec/batch)\n",
      "epoch 197, step 800 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 197, step 850 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 197, step 900 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.55\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.98\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.5774245262146\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 198, step 50 / 900, loss = 2.24 (0.022 sec/batch)\n",
      "epoch 198, step 100 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 198, step 150 / 900, loss = 2.47 (0.021 sec/batch)\n",
      "epoch 198, step 200 / 900, loss = 2.78 (0.022 sec/batch)\n",
      "epoch 198, step 250 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 198, step 300 / 900, loss = 1.70 (0.022 sec/batch)\n",
      "epoch 198, step 350 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "epoch 198, step 400 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 198, step 450 / 900, loss = 2.95 (0.022 sec/batch)\n",
      "epoch 198, step 500 / 900, loss = 2.00 (0.022 sec/batch)\n",
      "epoch 198, step 550 / 900, loss = 1.50 (0.021 sec/batch)\n",
      "epoch 198, step 600 / 900, loss = 2.31 (0.022 sec/batch)\n",
      "epoch 198, step 650 / 900, loss = 2.93 (0.021 sec/batch)\n",
      "epoch 198, step 700 / 900, loss = 1.90 (0.021 sec/batch)\n",
      "epoch 198, step 750 / 900, loss = 2.15 (0.022 sec/batch)\n",
      "epoch 198, step 800 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "epoch 198, step 850 / 900, loss = 2.37 (0.022 sec/batch)\n",
      "epoch 198, step 900 / 900, loss = 1.81 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.92\n",
      " avg loss = 2.24\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.42\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.612632751464844\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 199, step 50 / 900, loss = 2.54 (0.022 sec/batch)\n",
      "epoch 199, step 100 / 900, loss = 1.50 (0.022 sec/batch)\n",
      "epoch 199, step 150 / 900, loss = 2.72 (0.021 sec/batch)\n",
      "epoch 199, step 200 / 900, loss = 1.57 (0.022 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 199, step 250 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "epoch 199, step 300 / 900, loss = 2.34 (0.021 sec/batch)\n",
      "epoch 199, step 350 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 199, step 400 / 900, loss = 2.06 (0.021 sec/batch)\n",
      "epoch 199, step 450 / 900, loss = 2.23 (0.022 sec/batch)\n",
      "epoch 199, step 500 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "epoch 199, step 550 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 199, step 600 / 900, loss = 2.88 (0.021 sec/batch)\n",
      "epoch 199, step 650 / 900, loss = 1.40 (0.022 sec/batch)\n",
      "epoch 199, step 700 / 900, loss = 1.92 (0.021 sec/batch)\n",
      "epoch 199, step 750 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 199, step 800 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 199, step 850 / 900, loss = 2.57 (0.022 sec/batch)\n",
      "epoch 199, step 900 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.57\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.96\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.566704988479614\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 200, step 50 / 900, loss = 1.44 (0.022 sec/batch)\n",
      "epoch 200, step 100 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 200, step 150 / 900, loss = 2.06 (0.021 sec/batch)\n",
      "epoch 200, step 200 / 900, loss = 1.82 (0.021 sec/batch)\n",
      "epoch 200, step 250 / 900, loss = 1.81 (0.021 sec/batch)\n",
      "epoch 200, step 300 / 900, loss = 2.62 (0.021 sec/batch)\n",
      "epoch 200, step 350 / 900, loss = 2.67 (0.022 sec/batch)\n",
      "epoch 200, step 400 / 900, loss = 2.54 (0.021 sec/batch)\n",
      "epoch 200, step 450 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 200, step 500 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "epoch 200, step 550 / 900, loss = 1.84 (0.022 sec/batch)\n",
      "epoch 200, step 600 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 200, step 650 / 900, loss = 1.98 (0.021 sec/batch)\n",
      "epoch 200, step 700 / 900, loss = 2.89 (0.021 sec/batch)\n",
      "epoch 200, step 750 / 900, loss = 3.14 (0.022 sec/batch)\n",
      "epoch 200, step 800 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 200, step 850 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 200, step 900 / 900, loss = 3.19 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.53\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.76\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.581995248794556\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 201, step 50 / 900, loss = 1.95 (0.021 sec/batch)\n",
      "epoch 201, step 100 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "epoch 201, step 150 / 900, loss = 1.80 (0.021 sec/batch)\n",
      "epoch 201, step 200 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 201, step 250 / 900, loss = 1.99 (0.021 sec/batch)\n",
      "epoch 201, step 300 / 900, loss = 1.81 (0.022 sec/batch)\n",
      "epoch 201, step 350 / 900, loss = 2.92 (0.022 sec/batch)\n",
      "epoch 201, step 400 / 900, loss = 1.53 (0.021 sec/batch)\n",
      "epoch 201, step 450 / 900, loss = 2.61 (0.021 sec/batch)\n",
      "epoch 201, step 500 / 900, loss = 2.04 (0.021 sec/batch)\n",
      "epoch 201, step 550 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "epoch 201, step 600 / 900, loss = 1.51 (0.022 sec/batch)\n",
      "epoch 201, step 650 / 900, loss = 1.44 (0.021 sec/batch)\n",
      "epoch 201, step 700 / 900, loss = 1.99 (0.022 sec/batch)\n",
      "epoch 201, step 750 / 900, loss = 2.16 (0.022 sec/batch)\n",
      "epoch 201, step 800 / 900, loss = 2.56 (0.021 sec/batch)\n",
      "epoch 201, step 850 / 900, loss = 1.96 (0.021 sec/batch)\n",
      "epoch 201, step 900 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.36\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.16\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.57663893699646\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 202, step 50 / 900, loss = 3.24 (0.022 sec/batch)\n",
      "epoch 202, step 100 / 900, loss = 2.81 (0.021 sec/batch)\n",
      "epoch 202, step 150 / 900, loss = 2.40 (0.021 sec/batch)\n",
      "epoch 202, step 200 / 900, loss = 3.32 (0.021 sec/batch)\n",
      "epoch 202, step 250 / 900, loss = 2.46 (0.022 sec/batch)\n",
      "epoch 202, step 300 / 900, loss = 1.51 (0.021 sec/batch)\n",
      "epoch 202, step 350 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 202, step 400 / 900, loss = 2.49 (0.021 sec/batch)\n",
      "epoch 202, step 450 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 202, step 500 / 900, loss = 2.45 (0.022 sec/batch)\n",
      "epoch 202, step 550 / 900, loss = 1.93 (0.022 sec/batch)\n",
      "epoch 202, step 600 / 900, loss = 2.47 (0.022 sec/batch)\n",
      "epoch 202, step 650 / 900, loss = 2.88 (0.021 sec/batch)\n",
      "epoch 202, step 700 / 900, loss = 1.92 (0.022 sec/batch)\n",
      "epoch 202, step 750 / 900, loss = 1.22 (0.021 sec/batch)\n",
      "epoch 202, step 800 / 900, loss = 2.44 (0.021 sec/batch)\n",
      "epoch 202, step 850 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 202, step 900 / 900, loss = 2.20 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.68\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 48.32\n",
      " avg loss = 2.92\n",
      "\n",
      "Epoch time: 24.580647468566895\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 203, step 50 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 203, step 100 / 900, loss = 2.06 (0.021 sec/batch)\n",
      "epoch 203, step 150 / 900, loss = 2.17 (0.022 sec/batch)\n",
      "epoch 203, step 200 / 900, loss = 1.81 (0.021 sec/batch)\n",
      "epoch 203, step 250 / 900, loss = 2.79 (0.021 sec/batch)\n",
      "epoch 203, step 300 / 900, loss = 2.37 (0.022 sec/batch)\n",
      "epoch 203, step 350 / 900, loss = 2.92 (0.021 sec/batch)\n",
      "epoch 203, step 400 / 900, loss = 2.51 (0.021 sec/batch)\n",
      "epoch 203, step 450 / 900, loss = 2.27 (0.022 sec/batch)\n",
      "epoch 203, step 500 / 900, loss = 2.14 (0.022 sec/batch)\n",
      "epoch 203, step 550 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 203, step 600 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "epoch 203, step 650 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 203, step 700 / 900, loss = 2.36 (0.021 sec/batch)\n",
      "epoch 203, step 750 / 900, loss = 1.45 (0.022 sec/batch)\n",
      "epoch 203, step 800 / 900, loss = 2.56 (0.021 sec/batch)\n",
      "epoch 203, step 850 / 900, loss = 1.95 (0.022 sec/batch)\n",
      "epoch 203, step 900 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.64\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.84\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.611733436584473\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 204, step 50 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 204, step 100 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 204, step 150 / 900, loss = 3.14 (0.022 sec/batch)\n",
      "epoch 204, step 200 / 900, loss = 1.09 (0.021 sec/batch)\n",
      "epoch 204, step 250 / 900, loss = 2.22 (0.022 sec/batch)\n",
      "epoch 204, step 300 / 900, loss = 3.22 (0.022 sec/batch)\n",
      "epoch 204, step 350 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 204, step 400 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 204, step 450 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 204, step 500 / 900, loss = 1.22 (0.021 sec/batch)\n",
      "epoch 204, step 550 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 204, step 600 / 900, loss = 1.71 (0.021 sec/batch)\n",
      "epoch 204, step 650 / 900, loss = 1.75 (0.021 sec/batch)\n",
      "epoch 204, step 700 / 900, loss = 2.53 (0.022 sec/batch)\n",
      "epoch 204, step 750 / 900, loss = 2.75 (0.022 sec/batch)\n",
      "epoch 204, step 800 / 900, loss = 1.68 (0.022 sec/batch)\n",
      "epoch 204, step 850 / 900, loss = 3.19 (0.021 sec/batch)\n",
      "epoch 204, step 900 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.37\n",
      " avg loss = 2.28\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.54\n",
      " avg loss = 2.93\n",
      "\n",
      "Epoch time: 24.578304529190063\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 205, step 50 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "epoch 205, step 100 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 205, step 150 / 900, loss = 1.91 (0.022 sec/batch)\n",
      "epoch 205, step 200 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 205, step 250 / 900, loss = 2.54 (0.021 sec/batch)\n",
      "epoch 205, step 300 / 900, loss = 3.03 (0.022 sec/batch)\n",
      "epoch 205, step 350 / 900, loss = 2.77 (0.021 sec/batch)\n",
      "epoch 205, step 400 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 205, step 450 / 900, loss = 2.00 (0.022 sec/batch)\n",
      "epoch 205, step 500 / 900, loss = 3.54 (0.021 sec/batch)\n",
      "epoch 205, step 550 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "epoch 205, step 600 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 205, step 650 / 900, loss = 2.30 (0.023 sec/batch)\n",
      "epoch 205, step 700 / 900, loss = 1.82 (0.022 sec/batch)\n",
      "epoch 205, step 750 / 900, loss = 2.67 (0.021 sec/batch)\n",
      "epoch 205, step 800 / 900, loss = 2.24 (0.021 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 205, step 850 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 205, step 900 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.81\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.58\n",
      " avg loss = 2.81\n",
      "\n",
      "Epoch time: 24.595459938049316\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 206, step 50 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "epoch 206, step 100 / 900, loss = 2.93 (0.021 sec/batch)\n",
      "epoch 206, step 150 / 900, loss = 3.08 (0.021 sec/batch)\n",
      "epoch 206, step 200 / 900, loss = 1.87 (0.021 sec/batch)\n",
      "epoch 206, step 250 / 900, loss = 2.50 (0.021 sec/batch)\n",
      "epoch 206, step 300 / 900, loss = 1.54 (0.022 sec/batch)\n",
      "epoch 206, step 350 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 206, step 400 / 900, loss = 2.21 (0.023 sec/batch)\n",
      "epoch 206, step 450 / 900, loss = 2.36 (0.021 sec/batch)\n",
      "epoch 206, step 500 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 206, step 550 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 206, step 600 / 900, loss = 2.65 (0.022 sec/batch)\n",
      "epoch 206, step 650 / 900, loss = 2.46 (0.022 sec/batch)\n",
      "epoch 206, step 700 / 900, loss = 2.99 (0.021 sec/batch)\n",
      "epoch 206, step 750 / 900, loss = 1.91 (0.021 sec/batch)\n",
      "epoch 206, step 800 / 900, loss = 3.28 (0.021 sec/batch)\n",
      "epoch 206, step 850 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 206, step 900 / 900, loss = 2.56 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.44\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.32\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.583619832992554\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 207, step 50 / 900, loss = 1.85 (0.021 sec/batch)\n",
      "epoch 207, step 100 / 900, loss = 2.61 (0.021 sec/batch)\n",
      "epoch 207, step 150 / 900, loss = 2.50 (0.022 sec/batch)\n",
      "epoch 207, step 200 / 900, loss = 1.80 (0.021 sec/batch)\n",
      "epoch 207, step 250 / 900, loss = 1.52 (0.022 sec/batch)\n",
      "epoch 207, step 300 / 900, loss = 2.62 (0.022 sec/batch)\n",
      "epoch 207, step 350 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 207, step 400 / 900, loss = 2.65 (0.022 sec/batch)\n",
      "epoch 207, step 450 / 900, loss = 1.75 (0.021 sec/batch)\n",
      "epoch 207, step 500 / 900, loss = 1.81 (0.021 sec/batch)\n",
      "epoch 207, step 550 / 900, loss = 2.33 (0.022 sec/batch)\n",
      "epoch 207, step 600 / 900, loss = 2.57 (0.021 sec/batch)\n",
      "epoch 207, step 650 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 207, step 700 / 900, loss = 1.73 (0.022 sec/batch)\n",
      "epoch 207, step 750 / 900, loss = 3.24 (0.021 sec/batch)\n",
      "epoch 207, step 800 / 900, loss = 2.48 (0.022 sec/batch)\n",
      "epoch 207, step 850 / 900, loss = 1.95 (0.021 sec/batch)\n",
      "epoch 207, step 900 / 900, loss = 2.63 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.70\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 48.96\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.591533422470093\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 208, step 50 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "epoch 208, step 100 / 900, loss = 1.88 (0.021 sec/batch)\n",
      "epoch 208, step 150 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 208, step 200 / 900, loss = 1.42 (0.022 sec/batch)\n",
      "epoch 208, step 250 / 900, loss = 1.69 (0.021 sec/batch)\n",
      "epoch 208, step 300 / 900, loss = 2.43 (0.022 sec/batch)\n",
      "epoch 208, step 350 / 900, loss = 2.78 (0.021 sec/batch)\n",
      "epoch 208, step 400 / 900, loss = 2.64 (0.021 sec/batch)\n",
      "epoch 208, step 450 / 900, loss = 2.41 (0.021 sec/batch)\n",
      "epoch 208, step 500 / 900, loss = 1.84 (0.022 sec/batch)\n",
      "epoch 208, step 550 / 900, loss = 2.57 (0.021 sec/batch)\n",
      "epoch 208, step 600 / 900, loss = 2.64 (0.021 sec/batch)\n",
      "epoch 208, step 650 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 208, step 700 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 208, step 750 / 900, loss = 2.73 (0.021 sec/batch)\n",
      "epoch 208, step 800 / 900, loss = 2.35 (0.022 sec/batch)\n",
      "epoch 208, step 850 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 208, step 900 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.53\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.38\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.600302696228027\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 209, step 50 / 900, loss = 2.57 (0.022 sec/batch)\n",
      "epoch 209, step 100 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 209, step 150 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 209, step 200 / 900, loss = 1.93 (0.021 sec/batch)\n",
      "epoch 209, step 250 / 900, loss = 1.71 (0.021 sec/batch)\n",
      "epoch 209, step 300 / 900, loss = 2.77 (0.022 sec/batch)\n",
      "epoch 209, step 350 / 900, loss = 2.40 (0.021 sec/batch)\n",
      "epoch 209, step 400 / 900, loss = 2.97 (0.021 sec/batch)\n",
      "epoch 209, step 450 / 900, loss = 2.51 (0.022 sec/batch)\n",
      "epoch 209, step 500 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 209, step 550 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 209, step 600 / 900, loss = 3.05 (0.021 sec/batch)\n",
      "epoch 209, step 650 / 900, loss = 1.55 (0.021 sec/batch)\n",
      "epoch 209, step 700 / 900, loss = 2.67 (0.022 sec/batch)\n",
      "epoch 209, step 750 / 900, loss = 1.58 (0.021 sec/batch)\n",
      "epoch 209, step 800 / 900, loss = 3.21 (0.021 sec/batch)\n",
      "epoch 209, step 850 / 900, loss = 2.18 (0.022 sec/batch)\n",
      "epoch 209, step 900 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.56\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.54\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.568116426467896\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 210, step 50 / 900, loss = 1.56 (0.022 sec/batch)\n",
      "epoch 210, step 100 / 900, loss = 1.65 (0.021 sec/batch)\n",
      "epoch 210, step 150 / 900, loss = 2.98 (0.021 sec/batch)\n",
      "epoch 210, step 200 / 900, loss = 2.64 (0.022 sec/batch)\n",
      "epoch 210, step 250 / 900, loss = 2.35 (0.020 sec/batch)\n",
      "epoch 210, step 300 / 900, loss = 2.08 (0.022 sec/batch)\n",
      "epoch 210, step 350 / 900, loss = 2.71 (0.021 sec/batch)\n",
      "epoch 210, step 400 / 900, loss = 1.52 (0.021 sec/batch)\n",
      "epoch 210, step 450 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 210, step 500 / 900, loss = 2.57 (0.021 sec/batch)\n",
      "epoch 210, step 550 / 900, loss = 2.00 (0.022 sec/batch)\n",
      "epoch 210, step 600 / 900, loss = 1.64 (0.022 sec/batch)\n",
      "epoch 210, step 650 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "epoch 210, step 700 / 900, loss = 2.07 (0.022 sec/batch)\n",
      "epoch 210, step 750 / 900, loss = 2.80 (0.021 sec/batch)\n",
      "epoch 210, step 800 / 900, loss = 3.01 (0.021 sec/batch)\n",
      "epoch 210, step 850 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 210, step 900 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.72\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.30\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.593398809432983\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 211, step 50 / 900, loss = 2.62 (0.022 sec/batch)\n",
      "epoch 211, step 100 / 900, loss = 1.38 (0.021 sec/batch)\n",
      "epoch 211, step 150 / 900, loss = 1.54 (0.021 sec/batch)\n",
      "epoch 211, step 200 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 211, step 250 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "epoch 211, step 300 / 900, loss = 2.73 (0.022 sec/batch)\n",
      "epoch 211, step 350 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 211, step 400 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "epoch 211, step 450 / 900, loss = 2.94 (0.021 sec/batch)\n",
      "epoch 211, step 500 / 900, loss = 2.64 (0.021 sec/batch)\n",
      "epoch 211, step 550 / 900, loss = 2.46 (0.021 sec/batch)\n",
      "epoch 211, step 600 / 900, loss = 1.77 (0.022 sec/batch)\n",
      "epoch 211, step 650 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 211, step 700 / 900, loss = 2.60 (0.021 sec/batch)\n",
      "epoch 211, step 750 / 900, loss = 1.97 (0.022 sec/batch)\n",
      "epoch 211, step 800 / 900, loss = 2.24 (0.021 sec/batch)\n",
      "epoch 211, step 850 / 900, loss = 1.32 (0.022 sec/batch)\n",
      "epoch 211, step 900 / 900, loss = 2.85 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.66\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.54\n",
      " avg loss = 2.86\n",
      "\n",
      "Epoch time: 24.59992814064026\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 212, step 50 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "epoch 212, step 100 / 900, loss = 1.92 (0.022 sec/batch)\n",
      "epoch 212, step 150 / 900, loss = 2.79 (0.021 sec/batch)\n",
      "epoch 212, step 200 / 900, loss = 2.58 (0.021 sec/batch)\n",
      "epoch 212, step 250 / 900, loss = 2.04 (0.021 sec/batch)\n",
      "epoch 212, step 300 / 900, loss = 3.13 (0.022 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 212, step 350 / 900, loss = 1.71 (0.022 sec/batch)\n",
      "epoch 212, step 400 / 900, loss = 2.13 (0.022 sec/batch)\n",
      "epoch 212, step 450 / 900, loss = 1.44 (0.021 sec/batch)\n",
      "epoch 212, step 500 / 900, loss = 2.24 (0.021 sec/batch)\n",
      "epoch 212, step 550 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 212, step 600 / 900, loss = 2.63 (0.021 sec/batch)\n",
      "epoch 212, step 650 / 900, loss = 2.93 (0.022 sec/batch)\n",
      "epoch 212, step 700 / 900, loss = 1.80 (0.021 sec/batch)\n",
      "epoch 212, step 750 / 900, loss = 1.50 (0.021 sec/batch)\n",
      "epoch 212, step 800 / 900, loss = 1.35 (0.022 sec/batch)\n",
      "epoch 212, step 850 / 900, loss = 2.44 (0.022 sec/batch)\n",
      "epoch 212, step 900 / 900, loss = 2.87 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.55\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.34\n",
      " avg loss = 2.93\n",
      "\n",
      "Epoch time: 24.618103504180908\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 213, step 50 / 900, loss = 1.73 (0.021 sec/batch)\n",
      "epoch 213, step 100 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 213, step 150 / 900, loss = 1.47 (0.021 sec/batch)\n",
      "epoch 213, step 200 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 213, step 250 / 900, loss = 2.39 (0.021 sec/batch)\n",
      "epoch 213, step 300 / 900, loss = 2.16 (0.022 sec/batch)\n",
      "epoch 213, step 350 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 213, step 400 / 900, loss = 1.84 (0.022 sec/batch)\n",
      "epoch 213, step 450 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 213, step 500 / 900, loss = 3.14 (0.021 sec/batch)\n",
      "epoch 213, step 550 / 900, loss = 2.32 (0.022 sec/batch)\n",
      "epoch 213, step 600 / 900, loss = 2.54 (0.021 sec/batch)\n",
      "epoch 213, step 650 / 900, loss = 2.79 (0.021 sec/batch)\n",
      "epoch 213, step 700 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 213, step 750 / 900, loss = 2.12 (0.022 sec/batch)\n",
      "epoch 213, step 800 / 900, loss = 2.62 (0.022 sec/batch)\n",
      "epoch 213, step 850 / 900, loss = 2.63 (0.022 sec/batch)\n",
      "epoch 213, step 900 / 900, loss = 2.90 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.58\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.72\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.586518049240112\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 214, step 50 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 214, step 100 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "epoch 214, step 150 / 900, loss = 1.73 (0.021 sec/batch)\n",
      "epoch 214, step 200 / 900, loss = 3.23 (0.021 sec/batch)\n",
      "epoch 214, step 250 / 900, loss = 1.70 (0.021 sec/batch)\n",
      "epoch 214, step 300 / 900, loss = 2.51 (0.021 sec/batch)\n",
      "epoch 214, step 350 / 900, loss = 1.87 (0.021 sec/batch)\n",
      "epoch 214, step 400 / 900, loss = 1.97 (0.022 sec/batch)\n",
      "epoch 214, step 450 / 900, loss = 1.71 (0.021 sec/batch)\n",
      "epoch 214, step 500 / 900, loss = 1.60 (0.022 sec/batch)\n",
      "epoch 214, step 550 / 900, loss = 2.23 (0.023 sec/batch)\n",
      "epoch 214, step 600 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "epoch 214, step 650 / 900, loss = 2.45 (0.022 sec/batch)\n",
      "epoch 214, step 700 / 900, loss = 1.42 (0.022 sec/batch)\n",
      "epoch 214, step 750 / 900, loss = 1.88 (0.021 sec/batch)\n",
      "epoch 214, step 800 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "epoch 214, step 850 / 900, loss = 2.20 (0.022 sec/batch)\n",
      "epoch 214, step 900 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 56.04\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.20\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.604100465774536\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 215, step 50 / 900, loss = 1.92 (0.021 sec/batch)\n",
      "epoch 215, step 100 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "epoch 215, step 150 / 900, loss = 2.70 (0.022 sec/batch)\n",
      "epoch 215, step 200 / 900, loss = 1.85 (0.021 sec/batch)\n",
      "epoch 215, step 250 / 900, loss = 2.29 (0.022 sec/batch)\n",
      "epoch 215, step 300 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 215, step 350 / 900, loss = 2.41 (0.021 sec/batch)\n",
      "epoch 215, step 400 / 900, loss = 2.75 (0.022 sec/batch)\n",
      "epoch 215, step 450 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "epoch 215, step 500 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 215, step 550 / 900, loss = 2.94 (0.022 sec/batch)\n",
      "epoch 215, step 600 / 900, loss = 2.75 (0.021 sec/batch)\n",
      "epoch 215, step 650 / 900, loss = 1.66 (0.021 sec/batch)\n",
      "epoch 215, step 700 / 900, loss = 2.64 (0.022 sec/batch)\n",
      "epoch 215, step 750 / 900, loss = 1.88 (0.021 sec/batch)\n",
      "epoch 215, step 800 / 900, loss = 3.01 (0.021 sec/batch)\n",
      "epoch 215, step 850 / 900, loss = 1.37 (0.021 sec/batch)\n",
      "epoch 215, step 900 / 900, loss = 2.28 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.62\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.74\n",
      " avg loss = 2.86\n",
      "\n",
      "Epoch time: 24.586331844329834\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 216, step 50 / 900, loss = 2.59 (0.021 sec/batch)\n",
      "epoch 216, step 100 / 900, loss = 2.03 (0.022 sec/batch)\n",
      "epoch 216, step 150 / 900, loss = 2.69 (0.021 sec/batch)\n",
      "epoch 216, step 200 / 900, loss = 1.91 (0.021 sec/batch)\n",
      "epoch 216, step 250 / 900, loss = 2.39 (0.021 sec/batch)\n",
      "epoch 216, step 300 / 900, loss = 1.63 (0.021 sec/batch)\n",
      "epoch 216, step 350 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 216, step 400 / 900, loss = 1.70 (0.021 sec/batch)\n",
      "epoch 216, step 450 / 900, loss = 2.39 (0.021 sec/batch)\n",
      "epoch 216, step 500 / 900, loss = 1.76 (0.022 sec/batch)\n",
      "epoch 216, step 550 / 900, loss = 2.39 (0.022 sec/batch)\n",
      "epoch 216, step 600 / 900, loss = 1.85 (0.021 sec/batch)\n",
      "epoch 216, step 650 / 900, loss = 2.55 (0.022 sec/batch)\n",
      "epoch 216, step 700 / 900, loss = 2.90 (0.021 sec/batch)\n",
      "epoch 216, step 750 / 900, loss = 2.45 (0.021 sec/batch)\n",
      "epoch 216, step 800 / 900, loss = 2.59 (0.022 sec/batch)\n",
      "epoch 216, step 850 / 900, loss = 2.34 (0.021 sec/batch)\n",
      "epoch 216, step 900 / 900, loss = 2.47 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.50\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.78\n",
      " avg loss = 2.93\n",
      "\n",
      "Epoch time: 24.58540630340576\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 217, step 50 / 900, loss = 2.39 (0.021 sec/batch)\n",
      "epoch 217, step 100 / 900, loss = 2.03 (0.022 sec/batch)\n",
      "epoch 217, step 150 / 900, loss = 2.83 (0.021 sec/batch)\n",
      "epoch 217, step 200 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 217, step 250 / 900, loss = 1.84 (0.021 sec/batch)\n",
      "epoch 217, step 300 / 900, loss = 1.46 (0.021 sec/batch)\n",
      "epoch 217, step 350 / 900, loss = 2.75 (0.022 sec/batch)\n",
      "epoch 217, step 400 / 900, loss = 2.41 (0.021 sec/batch)\n",
      "epoch 217, step 450 / 900, loss = 2.57 (0.021 sec/batch)\n",
      "epoch 217, step 500 / 900, loss = 2.05 (0.020 sec/batch)\n",
      "epoch 217, step 550 / 900, loss = 1.62 (0.022 sec/batch)\n",
      "epoch 217, step 600 / 900, loss = 2.78 (0.021 sec/batch)\n",
      "epoch 217, step 650 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 217, step 700 / 900, loss = 1.90 (0.021 sec/batch)\n",
      "epoch 217, step 750 / 900, loss = 1.92 (0.021 sec/batch)\n",
      "epoch 217, step 800 / 900, loss = 1.88 (0.021 sec/batch)\n",
      "epoch 217, step 850 / 900, loss = 2.64 (0.021 sec/batch)\n",
      "epoch 217, step 900 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.67\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.20\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.609013080596924\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 218, step 50 / 900, loss = 1.38 (0.022 sec/batch)\n",
      "epoch 218, step 100 / 900, loss = 2.58 (0.021 sec/batch)\n",
      "epoch 218, step 150 / 900, loss = 2.56 (0.021 sec/batch)\n",
      "epoch 218, step 200 / 900, loss = 2.15 (0.022 sec/batch)\n",
      "epoch 218, step 250 / 900, loss = 2.87 (0.021 sec/batch)\n",
      "epoch 218, step 300 / 900, loss = 2.38 (0.022 sec/batch)\n",
      "epoch 218, step 350 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "epoch 218, step 400 / 900, loss = 2.15 (0.021 sec/batch)\n",
      "epoch 218, step 450 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 218, step 500 / 900, loss = 1.52 (0.021 sec/batch)\n",
      "epoch 218, step 550 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "epoch 218, step 600 / 900, loss = 1.99 (0.022 sec/batch)\n",
      "epoch 218, step 650 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "epoch 218, step 700 / 900, loss = 2.45 (0.021 sec/batch)\n",
      "epoch 218, step 750 / 900, loss = 3.38 (0.021 sec/batch)\n",
      "epoch 218, step 800 / 900, loss = 2.21 (0.022 sec/batch)\n",
      "epoch 218, step 850 / 900, loss = 2.79 (0.021 sec/batch)\n",
      "epoch 218, step 900 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "Train error:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy = 55.66\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.04\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.58046317100525\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 219, step 50 / 900, loss = 1.88 (0.021 sec/batch)\n",
      "epoch 219, step 100 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 219, step 150 / 900, loss = 2.06 (0.021 sec/batch)\n",
      "epoch 219, step 200 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 219, step 250 / 900, loss = 2.47 (0.022 sec/batch)\n",
      "epoch 219, step 300 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 219, step 350 / 900, loss = 3.59 (0.021 sec/batch)\n",
      "epoch 219, step 400 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 219, step 450 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 219, step 500 / 900, loss = 2.19 (0.022 sec/batch)\n",
      "epoch 219, step 550 / 900, loss = 1.65 (0.022 sec/batch)\n",
      "epoch 219, step 600 / 900, loss = 1.83 (0.022 sec/batch)\n",
      "epoch 219, step 650 / 900, loss = 2.00 (0.022 sec/batch)\n",
      "epoch 219, step 700 / 900, loss = 2.39 (0.021 sec/batch)\n",
      "epoch 219, step 750 / 900, loss = 2.06 (0.021 sec/batch)\n",
      "epoch 219, step 800 / 900, loss = 1.88 (0.021 sec/batch)\n",
      "epoch 219, step 850 / 900, loss = 1.77 (0.022 sec/batch)\n",
      "epoch 219, step 900 / 900, loss = 2.61 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.49\n",
      " avg loss = 2.28\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.56\n",
      " avg loss = 2.85\n",
      "\n",
      "Epoch time: 24.594799518585205\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 220, step 50 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "epoch 220, step 100 / 900, loss = 2.77 (0.022 sec/batch)\n",
      "epoch 220, step 150 / 900, loss = 2.17 (0.023 sec/batch)\n",
      "epoch 220, step 200 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "epoch 220, step 250 / 900, loss = 1.82 (0.021 sec/batch)\n",
      "epoch 220, step 300 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 220, step 350 / 900, loss = 2.61 (0.022 sec/batch)\n",
      "epoch 220, step 400 / 900, loss = 1.54 (0.021 sec/batch)\n",
      "epoch 220, step 450 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 220, step 500 / 900, loss = 1.82 (0.021 sec/batch)\n",
      "epoch 220, step 550 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 220, step 600 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 220, step 650 / 900, loss = 2.68 (0.021 sec/batch)\n",
      "epoch 220, step 700 / 900, loss = 2.44 (0.023 sec/batch)\n",
      "epoch 220, step 750 / 900, loss = 2.06 (0.022 sec/batch)\n",
      "epoch 220, step 800 / 900, loss = 2.92 (0.022 sec/batch)\n",
      "epoch 220, step 850 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 220, step 900 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.87\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.98\n",
      " avg loss = 2.92\n",
      "\n",
      "Epoch time: 24.592379570007324\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 221, step 50 / 900, loss = 1.63 (0.022 sec/batch)\n",
      "epoch 221, step 100 / 900, loss = 1.96 (0.021 sec/batch)\n",
      "epoch 221, step 150 / 900, loss = 3.53 (0.021 sec/batch)\n",
      "epoch 221, step 200 / 900, loss = 2.67 (0.021 sec/batch)\n",
      "epoch 221, step 250 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 221, step 300 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "epoch 221, step 350 / 900, loss = 2.25 (0.022 sec/batch)\n",
      "epoch 221, step 400 / 900, loss = 2.49 (0.021 sec/batch)\n",
      "epoch 221, step 450 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 221, step 500 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 221, step 550 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 221, step 600 / 900, loss = 2.57 (0.021 sec/batch)\n",
      "epoch 221, step 650 / 900, loss = 1.98 (0.021 sec/batch)\n",
      "epoch 221, step 700 / 900, loss = 2.68 (0.021 sec/batch)\n",
      "epoch 221, step 750 / 900, loss = 2.28 (0.022 sec/batch)\n",
      "epoch 221, step 800 / 900, loss = 2.81 (0.021 sec/batch)\n",
      "epoch 221, step 850 / 900, loss = 2.67 (0.021 sec/batch)\n",
      "epoch 221, step 900 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.53\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.68\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.575690269470215\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 222, step 50 / 900, loss = 3.49 (0.022 sec/batch)\n",
      "epoch 222, step 100 / 900, loss = 1.84 (0.022 sec/batch)\n",
      "epoch 222, step 150 / 900, loss = 1.91 (0.021 sec/batch)\n",
      "epoch 222, step 200 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "epoch 222, step 250 / 900, loss = 2.36 (0.021 sec/batch)\n",
      "epoch 222, step 300 / 900, loss = 1.65 (0.022 sec/batch)\n",
      "epoch 222, step 350 / 900, loss = 1.88 (0.022 sec/batch)\n",
      "epoch 222, step 400 / 900, loss = 1.96 (0.021 sec/batch)\n",
      "epoch 222, step 450 / 900, loss = 2.45 (0.021 sec/batch)\n",
      "epoch 222, step 500 / 900, loss = 2.39 (0.021 sec/batch)\n",
      "epoch 222, step 550 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 222, step 600 / 900, loss = 2.33 (0.022 sec/batch)\n",
      "epoch 222, step 650 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 222, step 700 / 900, loss = 2.67 (0.021 sec/batch)\n",
      "epoch 222, step 750 / 900, loss = 2.24 (0.022 sec/batch)\n",
      "epoch 222, step 800 / 900, loss = 2.05 (0.022 sec/batch)\n",
      "epoch 222, step 850 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 222, step 900 / 900, loss = 2.64 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.46\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.08\n",
      " avg loss = 2.92\n",
      "\n",
      "Epoch time: 24.60002899169922\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 223, step 50 / 900, loss = 2.77 (0.021 sec/batch)\n",
      "epoch 223, step 100 / 900, loss = 1.74 (0.021 sec/batch)\n",
      "epoch 223, step 150 / 900, loss = 1.53 (0.020 sec/batch)\n",
      "epoch 223, step 200 / 900, loss = 2.71 (0.021 sec/batch)\n",
      "epoch 223, step 250 / 900, loss = 2.06 (0.022 sec/batch)\n",
      "epoch 223, step 300 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 223, step 350 / 900, loss = 1.40 (0.023 sec/batch)\n",
      "epoch 223, step 400 / 900, loss = 2.62 (0.021 sec/batch)\n",
      "epoch 223, step 450 / 900, loss = 2.87 (0.022 sec/batch)\n",
      "epoch 223, step 500 / 900, loss = 1.71 (0.022 sec/batch)\n",
      "epoch 223, step 550 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 223, step 600 / 900, loss = 3.10 (0.021 sec/batch)\n",
      "epoch 223, step 650 / 900, loss = 1.92 (0.022 sec/batch)\n",
      "epoch 223, step 700 / 900, loss = 2.88 (0.021 sec/batch)\n",
      "epoch 223, step 750 / 900, loss = 2.92 (0.022 sec/batch)\n",
      "epoch 223, step 800 / 900, loss = 1.67 (0.022 sec/batch)\n",
      "epoch 223, step 850 / 900, loss = 2.06 (0.021 sec/batch)\n",
      "epoch 223, step 900 / 900, loss = 2.93 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.37\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.82\n",
      " avg loss = 2.95\n",
      "\n",
      "Epoch time: 24.584885835647583\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 224, step 50 / 900, loss = 2.49 (0.021 sec/batch)\n",
      "epoch 224, step 100 / 900, loss = 1.37 (0.022 sec/batch)\n",
      "epoch 224, step 150 / 900, loss = 2.61 (0.022 sec/batch)\n",
      "epoch 224, step 200 / 900, loss = 1.83 (0.022 sec/batch)\n",
      "epoch 224, step 250 / 900, loss = 2.04 (0.021 sec/batch)\n",
      "epoch 224, step 300 / 900, loss = 1.81 (0.022 sec/batch)\n",
      "epoch 224, step 350 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 224, step 400 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "epoch 224, step 450 / 900, loss = 2.40 (0.021 sec/batch)\n",
      "epoch 224, step 500 / 900, loss = 1.95 (0.022 sec/batch)\n",
      "epoch 224, step 550 / 900, loss = 2.80 (0.021 sec/batch)\n",
      "epoch 224, step 600 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 224, step 650 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 224, step 700 / 900, loss = 2.40 (0.021 sec/batch)\n",
      "epoch 224, step 750 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 224, step 800 / 900, loss = 2.40 (0.021 sec/batch)\n",
      "epoch 224, step 850 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 224, step 900 / 900, loss = 2.75 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.66\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.82\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.596535444259644\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 225, step 50 / 900, loss = 2.65 (0.020 sec/batch)\n",
      "epoch 225, step 100 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 225, step 150 / 900, loss = 2.60 (0.022 sec/batch)\n",
      "epoch 225, step 200 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "epoch 225, step 250 / 900, loss = 1.85 (0.021 sec/batch)\n",
      "epoch 225, step 300 / 900, loss = 3.02 (0.021 sec/batch)\n",
      "epoch 225, step 350 / 900, loss = 2.95 (0.022 sec/batch)\n",
      "epoch 225, step 400 / 900, loss = 2.31 (0.022 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 225, step 450 / 900, loss = 1.96 (0.021 sec/batch)\n",
      "epoch 225, step 500 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 225, step 550 / 900, loss = 2.74 (0.021 sec/batch)\n",
      "epoch 225, step 600 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 225, step 650 / 900, loss = 2.32 (0.021 sec/batch)\n",
      "epoch 225, step 700 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 225, step 750 / 900, loss = 2.26 (0.022 sec/batch)\n",
      "epoch 225, step 800 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 225, step 850 / 900, loss = 1.61 (0.020 sec/batch)\n",
      "epoch 225, step 900 / 900, loss = 1.63 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.81\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.48\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.58785581588745\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 226, step 50 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 226, step 100 / 900, loss = 2.69 (0.021 sec/batch)\n",
      "epoch 226, step 150 / 900, loss = 1.70 (0.021 sec/batch)\n",
      "epoch 226, step 200 / 900, loss = 1.51 (0.022 sec/batch)\n",
      "epoch 226, step 250 / 900, loss = 3.22 (0.021 sec/batch)\n",
      "epoch 226, step 300 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 226, step 350 / 900, loss = 2.28 (0.022 sec/batch)\n",
      "epoch 226, step 400 / 900, loss = 2.77 (0.021 sec/batch)\n",
      "epoch 226, step 450 / 900, loss = 1.95 (0.022 sec/batch)\n",
      "epoch 226, step 500 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 226, step 550 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 226, step 600 / 900, loss = 1.82 (0.021 sec/batch)\n",
      "epoch 226, step 650 / 900, loss = 2.08 (0.022 sec/batch)\n",
      "epoch 226, step 700 / 900, loss = 1.56 (0.021 sec/batch)\n",
      "epoch 226, step 750 / 900, loss = 2.53 (0.022 sec/batch)\n",
      "epoch 226, step 800 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 226, step 850 / 900, loss = 1.72 (0.021 sec/batch)\n",
      "epoch 226, step 900 / 900, loss = 2.64 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.70\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.30\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.58115530014038\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 227, step 50 / 900, loss = 1.49 (0.021 sec/batch)\n",
      "epoch 227, step 100 / 900, loss = 2.31 (0.022 sec/batch)\n",
      "epoch 227, step 150 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 227, step 200 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "epoch 227, step 250 / 900, loss = 2.98 (0.021 sec/batch)\n",
      "epoch 227, step 300 / 900, loss = 1.95 (0.021 sec/batch)\n",
      "epoch 227, step 350 / 900, loss = 1.77 (0.021 sec/batch)\n",
      "epoch 227, step 400 / 900, loss = 1.73 (0.021 sec/batch)\n",
      "epoch 227, step 450 / 900, loss = 1.66 (0.021 sec/batch)\n",
      "epoch 227, step 500 / 900, loss = 3.01 (0.021 sec/batch)\n",
      "epoch 227, step 550 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 227, step 600 / 900, loss = 1.78 (0.022 sec/batch)\n",
      "epoch 227, step 650 / 900, loss = 1.92 (0.021 sec/batch)\n",
      "epoch 227, step 700 / 900, loss = 2.40 (0.022 sec/batch)\n",
      "epoch 227, step 750 / 900, loss = 2.15 (0.021 sec/batch)\n",
      "epoch 227, step 800 / 900, loss = 3.36 (0.021 sec/batch)\n",
      "epoch 227, step 850 / 900, loss = 1.77 (0.021 sec/batch)\n",
      "epoch 227, step 900 / 900, loss = 3.47 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.73\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.64\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.593230724334717\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 228, step 50 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 228, step 100 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 228, step 150 / 900, loss = 1.84 (0.021 sec/batch)\n",
      "epoch 228, step 200 / 900, loss = 2.24 (0.021 sec/batch)\n",
      "epoch 228, step 250 / 900, loss = 2.60 (0.022 sec/batch)\n",
      "epoch 228, step 300 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "epoch 228, step 350 / 900, loss = 2.05 (0.022 sec/batch)\n",
      "epoch 228, step 400 / 900, loss = 2.32 (0.022 sec/batch)\n",
      "epoch 228, step 450 / 900, loss = 2.79 (0.021 sec/batch)\n",
      "epoch 228, step 500 / 900, loss = 2.47 (0.022 sec/batch)\n",
      "epoch 228, step 550 / 900, loss = 2.39 (0.022 sec/batch)\n",
      "epoch 228, step 600 / 900, loss = 2.99 (0.022 sec/batch)\n",
      "epoch 228, step 650 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 228, step 700 / 900, loss = 1.53 (0.021 sec/batch)\n",
      "epoch 228, step 750 / 900, loss = 1.96 (0.021 sec/batch)\n",
      "epoch 228, step 800 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 228, step 850 / 900, loss = 2.45 (0.022 sec/batch)\n",
      "epoch 228, step 900 / 900, loss = 2.54 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.63\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.70\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.58955955505371\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 229, step 50 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 229, step 100 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "epoch 229, step 150 / 900, loss = 2.41 (0.022 sec/batch)\n",
      "epoch 229, step 200 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "epoch 229, step 250 / 900, loss = 2.85 (0.022 sec/batch)\n",
      "epoch 229, step 300 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 229, step 350 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "epoch 229, step 400 / 900, loss = 1.35 (0.021 sec/batch)\n",
      "epoch 229, step 450 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "epoch 229, step 500 / 900, loss = 2.45 (0.021 sec/batch)\n",
      "epoch 229, step 550 / 900, loss = 2.08 (0.022 sec/batch)\n",
      "epoch 229, step 600 / 900, loss = 2.50 (0.021 sec/batch)\n",
      "epoch 229, step 650 / 900, loss = 1.87 (0.021 sec/batch)\n",
      "epoch 229, step 700 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 229, step 750 / 900, loss = 1.69 (0.021 sec/batch)\n",
      "epoch 229, step 800 / 900, loss = 2.79 (0.021 sec/batch)\n",
      "epoch 229, step 850 / 900, loss = 1.18 (0.021 sec/batch)\n",
      "epoch 229, step 900 / 900, loss = 1.44 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.36\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.44\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.593066215515137\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 230, step 50 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "epoch 230, step 100 / 900, loss = 1.78 (0.021 sec/batch)\n",
      "epoch 230, step 150 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 230, step 200 / 900, loss = 2.69 (0.022 sec/batch)\n",
      "epoch 230, step 250 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 230, step 300 / 900, loss = 1.85 (0.021 sec/batch)\n",
      "epoch 230, step 350 / 900, loss = 2.05 (0.022 sec/batch)\n",
      "epoch 230, step 400 / 900, loss = 2.72 (0.022 sec/batch)\n",
      "epoch 230, step 450 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "epoch 230, step 500 / 900, loss = 2.95 (0.021 sec/batch)\n",
      "epoch 230, step 550 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "epoch 230, step 600 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 230, step 650 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 230, step 700 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 230, step 750 / 900, loss = 1.71 (0.021 sec/batch)\n",
      "epoch 230, step 800 / 900, loss = 2.92 (0.021 sec/batch)\n",
      "epoch 230, step 850 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "epoch 230, step 900 / 900, loss = 2.04 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.85\n",
      " avg loss = 2.23\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.60\n",
      " avg loss = 2.86\n",
      "\n",
      "Epoch time: 24.57543182373047\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 231, step 50 / 900, loss = 2.04 (0.022 sec/batch)\n",
      "epoch 231, step 100 / 900, loss = 1.66 (0.021 sec/batch)\n",
      "epoch 231, step 150 / 900, loss = 2.68 (0.021 sec/batch)\n",
      "epoch 231, step 200 / 900, loss = 2.54 (0.021 sec/batch)\n",
      "epoch 231, step 250 / 900, loss = 2.62 (0.021 sec/batch)\n",
      "epoch 231, step 300 / 900, loss = 3.32 (0.021 sec/batch)\n",
      "epoch 231, step 350 / 900, loss = 2.88 (0.022 sec/batch)\n",
      "epoch 231, step 400 / 900, loss = 3.00 (0.021 sec/batch)\n",
      "epoch 231, step 450 / 900, loss = 2.83 (0.022 sec/batch)\n",
      "epoch 231, step 500 / 900, loss = 2.24 (0.021 sec/batch)\n",
      "epoch 231, step 550 / 900, loss = 2.63 (0.022 sec/batch)\n",
      "epoch 231, step 600 / 900, loss = 2.72 (0.021 sec/batch)\n",
      "epoch 231, step 650 / 900, loss = 2.56 (0.021 sec/batch)\n",
      "epoch 231, step 700 / 900, loss = 2.78 (0.021 sec/batch)\n",
      "epoch 231, step 750 / 900, loss = 2.96 (0.022 sec/batch)\n",
      "epoch 231, step 800 / 900, loss = 2.84 (0.021 sec/batch)\n",
      "epoch 231, step 850 / 900, loss = 1.71 (0.021 sec/batch)\n",
      "epoch 231, step 900 / 900, loss = 1.87 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.70\n",
      " avg loss = 2.24\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.80\n",
      " avg loss = 2.85\n",
      "\n",
      "Epoch time: 24.591529846191406\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 232, step 50 / 900, loss = 2.96 (0.021 sec/batch)\n",
      "epoch 232, step 100 / 900, loss = 2.36 (0.021 sec/batch)\n",
      "epoch 232, step 150 / 900, loss = 1.63 (0.022 sec/batch)\n",
      "epoch 232, step 200 / 900, loss = 2.44 (0.023 sec/batch)\n",
      "epoch 232, step 250 / 900, loss = 1.93 (0.021 sec/batch)\n",
      "epoch 232, step 300 / 900, loss = 1.79 (0.022 sec/batch)\n",
      "epoch 232, step 350 / 900, loss = 1.99 (0.021 sec/batch)\n",
      "epoch 232, step 400 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 232, step 450 / 900, loss = 2.01 (0.022 sec/batch)\n",
      "epoch 232, step 500 / 900, loss = 1.91 (0.021 sec/batch)\n",
      "epoch 232, step 550 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 232, step 600 / 900, loss = 2.33 (0.022 sec/batch)\n",
      "epoch 232, step 650 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 232, step 700 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 232, step 750 / 900, loss = 2.70 (0.021 sec/batch)\n",
      "epoch 232, step 800 / 900, loss = 2.62 (0.021 sec/batch)\n",
      "epoch 232, step 850 / 900, loss = 2.66 (0.022 sec/batch)\n",
      "epoch 232, step 900 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.30\n",
      " avg loss = 2.28\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.12\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.575910091400146\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 233, step 50 / 900, loss = 2.76 (0.021 sec/batch)\n",
      "epoch 233, step 100 / 900, loss = 2.94 (0.021 sec/batch)\n",
      "epoch 233, step 150 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 233, step 200 / 900, loss = 1.70 (0.022 sec/batch)\n",
      "epoch 233, step 250 / 900, loss = 1.72 (0.022 sec/batch)\n",
      "epoch 233, step 300 / 900, loss = 1.45 (0.021 sec/batch)\n",
      "epoch 233, step 350 / 900, loss = 2.18 (0.022 sec/batch)\n",
      "epoch 233, step 400 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "epoch 233, step 450 / 900, loss = 1.87 (0.021 sec/batch)\n",
      "epoch 233, step 500 / 900, loss = 1.98 (0.021 sec/batch)\n",
      "epoch 233, step 550 / 900, loss = 1.85 (0.021 sec/batch)\n",
      "epoch 233, step 600 / 900, loss = 1.95 (0.022 sec/batch)\n",
      "epoch 233, step 650 / 900, loss = 1.88 (0.021 sec/batch)\n",
      "epoch 233, step 700 / 900, loss = 2.15 (0.021 sec/batch)\n",
      "epoch 233, step 750 / 900, loss = 1.57 (0.022 sec/batch)\n",
      "epoch 233, step 800 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "epoch 233, step 850 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 233, step 900 / 900, loss = 2.72 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.31\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.30\n",
      " avg loss = 2.92\n",
      "\n",
      "Epoch time: 24.59317636489868\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 234, step 50 / 900, loss = 2.36 (0.021 sec/batch)\n",
      "epoch 234, step 100 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "epoch 234, step 150 / 900, loss = 1.88 (0.021 sec/batch)\n",
      "epoch 234, step 200 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 234, step 250 / 900, loss = 2.76 (0.022 sec/batch)\n",
      "epoch 234, step 300 / 900, loss = 2.34 (0.021 sec/batch)\n",
      "epoch 234, step 350 / 900, loss = 2.58 (0.021 sec/batch)\n",
      "epoch 234, step 400 / 900, loss = 1.57 (0.022 sec/batch)\n",
      "epoch 234, step 450 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "epoch 234, step 500 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 234, step 550 / 900, loss = 1.95 (0.021 sec/batch)\n",
      "epoch 234, step 600 / 900, loss = 2.14 (0.022 sec/batch)\n",
      "epoch 234, step 650 / 900, loss = 2.73 (0.021 sec/batch)\n",
      "epoch 234, step 700 / 900, loss = 1.56 (0.021 sec/batch)\n",
      "epoch 234, step 750 / 900, loss = 1.63 (0.021 sec/batch)\n",
      "epoch 234, step 800 / 900, loss = 1.80 (0.021 sec/batch)\n",
      "epoch 234, step 850 / 900, loss = 1.77 (0.022 sec/batch)\n",
      "epoch 234, step 900 / 900, loss = 2.44 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.31\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.08\n",
      " avg loss = 2.83\n",
      "\n",
      "Epoch time: 24.579878091812134\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 235, step 50 / 900, loss = 2.02 (0.022 sec/batch)\n",
      "epoch 235, step 100 / 900, loss = 1.98 (0.021 sec/batch)\n",
      "epoch 235, step 150 / 900, loss = 3.13 (0.021 sec/batch)\n",
      "epoch 235, step 200 / 900, loss = 2.58 (0.022 sec/batch)\n",
      "epoch 235, step 250 / 900, loss = 1.96 (0.021 sec/batch)\n",
      "epoch 235, step 300 / 900, loss = 1.73 (0.021 sec/batch)\n",
      "epoch 235, step 350 / 900, loss = 2.04 (0.021 sec/batch)\n",
      "epoch 235, step 400 / 900, loss = 2.73 (0.021 sec/batch)\n",
      "epoch 235, step 450 / 900, loss = 2.50 (0.021 sec/batch)\n",
      "epoch 235, step 500 / 900, loss = 3.31 (0.021 sec/batch)\n",
      "epoch 235, step 550 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 235, step 600 / 900, loss = 2.41 (0.022 sec/batch)\n",
      "epoch 235, step 650 / 900, loss = 1.88 (0.021 sec/batch)\n",
      "epoch 235, step 700 / 900, loss = 2.90 (0.022 sec/batch)\n",
      "epoch 235, step 750 / 900, loss = 1.42 (0.020 sec/batch)\n",
      "epoch 235, step 800 / 900, loss = 2.62 (0.021 sec/batch)\n",
      "epoch 235, step 850 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 235, step 900 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.66\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.02\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.577892780303955\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 236, step 50 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 236, step 100 / 900, loss = 2.37 (0.022 sec/batch)\n",
      "epoch 236, step 150 / 900, loss = 3.12 (0.021 sec/batch)\n",
      "epoch 236, step 200 / 900, loss = 2.82 (0.022 sec/batch)\n",
      "epoch 236, step 250 / 900, loss = 2.13 (0.022 sec/batch)\n",
      "epoch 236, step 300 / 900, loss = 1.95 (0.020 sec/batch)\n",
      "epoch 236, step 350 / 900, loss = 2.13 (0.022 sec/batch)\n",
      "epoch 236, step 400 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 236, step 450 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 236, step 500 / 900, loss = 1.73 (0.022 sec/batch)\n",
      "epoch 236, step 550 / 900, loss = 1.96 (0.021 sec/batch)\n",
      "epoch 236, step 600 / 900, loss = 2.13 (0.022 sec/batch)\n",
      "epoch 236, step 650 / 900, loss = 2.34 (0.021 sec/batch)\n",
      "epoch 236, step 700 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 236, step 750 / 900, loss = 1.77 (0.022 sec/batch)\n",
      "epoch 236, step 800 / 900, loss = 2.01 (0.022 sec/batch)\n",
      "epoch 236, step 850 / 900, loss = 2.25 (0.022 sec/batch)\n",
      "epoch 236, step 900 / 900, loss = 2.39 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.42\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.22\n",
      " avg loss = 2.92\n",
      "\n",
      "Epoch time: 24.625789880752563\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 237, step 50 / 900, loss = 1.87 (0.021 sec/batch)\n",
      "epoch 237, step 100 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 237, step 150 / 900, loss = 2.88 (0.021 sec/batch)\n",
      "epoch 237, step 200 / 900, loss = 1.96 (0.021 sec/batch)\n",
      "epoch 237, step 250 / 900, loss = 2.47 (0.021 sec/batch)\n",
      "epoch 237, step 300 / 900, loss = 2.62 (0.022 sec/batch)\n",
      "epoch 237, step 350 / 900, loss = 1.44 (0.021 sec/batch)\n",
      "epoch 237, step 400 / 900, loss = 4.12 (0.021 sec/batch)\n",
      "epoch 237, step 450 / 900, loss = 2.54 (0.021 sec/batch)\n",
      "epoch 237, step 500 / 900, loss = 2.57 (0.021 sec/batch)\n",
      "epoch 237, step 550 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 237, step 600 / 900, loss = 2.39 (0.021 sec/batch)\n",
      "epoch 237, step 650 / 900, loss = 1.67 (0.021 sec/batch)\n",
      "epoch 237, step 700 / 900, loss = 3.08 (0.021 sec/batch)\n",
      "epoch 237, step 750 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 237, step 800 / 900, loss = 2.45 (0.021 sec/batch)\n",
      "epoch 237, step 850 / 900, loss = 1.53 (0.021 sec/batch)\n",
      "epoch 237, step 900 / 900, loss = 2.55 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.54\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.50\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.590351581573486\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 238, step 50 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 238, step 100 / 900, loss = 1.98 (0.021 sec/batch)\n",
      "epoch 238, step 150 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "epoch 238, step 200 / 900, loss = 2.25 (0.022 sec/batch)\n",
      "epoch 238, step 250 / 900, loss = 1.95 (0.021 sec/batch)\n",
      "epoch 238, step 300 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 238, step 350 / 900, loss = 2.45 (0.021 sec/batch)\n",
      "epoch 238, step 400 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 238, step 450 / 900, loss = 1.99 (0.021 sec/batch)\n",
      "epoch 238, step 500 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 238, step 550 / 900, loss = 1.37 (0.021 sec/batch)\n",
      "epoch 238, step 600 / 900, loss = 1.75 (0.022 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 238, step 650 / 900, loss = 3.24 (0.021 sec/batch)\n",
      "epoch 238, step 700 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 238, step 750 / 900, loss = 2.92 (0.021 sec/batch)\n",
      "epoch 238, step 800 / 900, loss = 1.64 (0.021 sec/batch)\n",
      "epoch 238, step 850 / 900, loss = 2.34 (0.022 sec/batch)\n",
      "epoch 238, step 900 / 900, loss = 1.79 (0.020 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.51\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.00\n",
      " avg loss = 2.93\n",
      "\n",
      "Epoch time: 24.610833168029785\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 239, step 50 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 239, step 100 / 900, loss = 2.24 (0.022 sec/batch)\n",
      "epoch 239, step 150 / 900, loss = 1.27 (0.021 sec/batch)\n",
      "epoch 239, step 200 / 900, loss = 3.20 (0.021 sec/batch)\n",
      "epoch 239, step 250 / 900, loss = 2.15 (0.021 sec/batch)\n",
      "epoch 239, step 300 / 900, loss = 1.86 (0.021 sec/batch)\n",
      "epoch 239, step 350 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 239, step 400 / 900, loss = 2.40 (0.021 sec/batch)\n",
      "epoch 239, step 450 / 900, loss = 1.80 (0.021 sec/batch)\n",
      "epoch 239, step 500 / 900, loss = 2.87 (0.022 sec/batch)\n",
      "epoch 239, step 550 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 239, step 600 / 900, loss = 2.50 (0.021 sec/batch)\n",
      "epoch 239, step 650 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 239, step 700 / 900, loss = 1.82 (0.021 sec/batch)\n",
      "epoch 239, step 750 / 900, loss = 2.41 (0.021 sec/batch)\n",
      "epoch 239, step 800 / 900, loss = 2.35 (0.022 sec/batch)\n",
      "epoch 239, step 850 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "epoch 239, step 900 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.71\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.08\n",
      " avg loss = 2.92\n",
      "\n",
      "Epoch time: 24.88515329360962\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 240, step 50 / 900, loss = 2.36 (0.021 sec/batch)\n",
      "epoch 240, step 100 / 900, loss = 2.50 (0.022 sec/batch)\n",
      "epoch 240, step 150 / 900, loss = 1.70 (0.021 sec/batch)\n",
      "epoch 240, step 200 / 900, loss = 1.74 (0.021 sec/batch)\n",
      "epoch 240, step 250 / 900, loss = 1.95 (0.022 sec/batch)\n",
      "epoch 240, step 300 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 240, step 350 / 900, loss = 2.10 (0.022 sec/batch)\n",
      "epoch 240, step 400 / 900, loss = 2.40 (0.022 sec/batch)\n",
      "epoch 240, step 450 / 900, loss = 1.94 (0.022 sec/batch)\n",
      "epoch 240, step 500 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 240, step 550 / 900, loss = 1.86 (0.022 sec/batch)\n",
      "epoch 240, step 600 / 900, loss = 1.85 (0.021 sec/batch)\n",
      "epoch 240, step 650 / 900, loss = 1.21 (0.021 sec/batch)\n",
      "epoch 240, step 700 / 900, loss = 2.67 (0.022 sec/batch)\n",
      "epoch 240, step 750 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "epoch 240, step 800 / 900, loss = 2.24 (0.021 sec/batch)\n",
      "epoch 240, step 850 / 900, loss = 2.77 (0.021 sec/batch)\n",
      "epoch 240, step 900 / 900, loss = 2.39 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.69\n",
      " avg loss = 2.28\n",
      "\n",
      "Validation error:\n",
      " accuracy = 48.86\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.675362586975098\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 241, step 50 / 900, loss = 3.30 (0.021 sec/batch)\n",
      "epoch 241, step 100 / 900, loss = 1.61 (0.021 sec/batch)\n",
      "epoch 241, step 150 / 900, loss = 2.15 (0.022 sec/batch)\n",
      "epoch 241, step 200 / 900, loss = 2.15 (0.021 sec/batch)\n",
      "epoch 241, step 250 / 900, loss = 2.65 (0.022 sec/batch)\n",
      "epoch 241, step 300 / 900, loss = 1.69 (0.021 sec/batch)\n",
      "epoch 241, step 350 / 900, loss = 2.83 (0.021 sec/batch)\n",
      "epoch 241, step 400 / 900, loss = 3.10 (0.022 sec/batch)\n",
      "epoch 241, step 450 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "epoch 241, step 500 / 900, loss = 2.07 (0.022 sec/batch)\n",
      "epoch 241, step 550 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "epoch 241, step 600 / 900, loss = 1.80 (0.022 sec/batch)\n",
      "epoch 241, step 650 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "epoch 241, step 700 / 900, loss = 2.98 (0.021 sec/batch)\n",
      "epoch 241, step 750 / 900, loss = 2.39 (0.022 sec/batch)\n",
      "epoch 241, step 800 / 900, loss = 2.57 (0.021 sec/batch)\n",
      "epoch 241, step 850 / 900, loss = 2.06 (0.022 sec/batch)\n",
      "epoch 241, step 900 / 900, loss = 1.45 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.66\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.94\n",
      " avg loss = 2.82\n",
      "\n",
      "Epoch time: 24.69204878807068\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 242, step 50 / 900, loss = 2.13 (0.022 sec/batch)\n",
      "epoch 242, step 100 / 900, loss = 1.27 (0.022 sec/batch)\n",
      "epoch 242, step 150 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "epoch 242, step 200 / 900, loss = 2.47 (0.021 sec/batch)\n",
      "epoch 242, step 250 / 900, loss = 1.86 (0.021 sec/batch)\n",
      "epoch 242, step 300 / 900, loss = 2.50 (0.021 sec/batch)\n",
      "epoch 242, step 350 / 900, loss = 2.15 (0.022 sec/batch)\n",
      "epoch 242, step 400 / 900, loss = 3.33 (0.022 sec/batch)\n",
      "epoch 242, step 450 / 900, loss = 2.53 (0.022 sec/batch)\n",
      "epoch 242, step 500 / 900, loss = 2.71 (0.021 sec/batch)\n",
      "epoch 242, step 550 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 242, step 600 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 242, step 650 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 242, step 700 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "epoch 242, step 750 / 900, loss = 2.25 (0.022 sec/batch)\n",
      "epoch 242, step 800 / 900, loss = 1.78 (0.021 sec/batch)\n",
      "epoch 242, step 850 / 900, loss = 3.23 (0.021 sec/batch)\n",
      "epoch 242, step 900 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.63\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.32\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.656073808670044\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 243, step 50 / 900, loss = 1.74 (0.021 sec/batch)\n",
      "epoch 243, step 100 / 900, loss = 3.14 (0.021 sec/batch)\n",
      "epoch 243, step 150 / 900, loss = 2.26 (0.021 sec/batch)\n",
      "epoch 243, step 200 / 900, loss = 2.48 (0.022 sec/batch)\n",
      "epoch 243, step 250 / 900, loss = 3.39 (0.021 sec/batch)\n",
      "epoch 243, step 300 / 900, loss = 2.46 (0.021 sec/batch)\n",
      "epoch 243, step 350 / 900, loss = 3.48 (0.022 sec/batch)\n",
      "epoch 243, step 400 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 243, step 450 / 900, loss = 2.47 (0.021 sec/batch)\n",
      "epoch 243, step 500 / 900, loss = 3.25 (0.021 sec/batch)\n",
      "epoch 243, step 550 / 900, loss = 2.56 (0.021 sec/batch)\n",
      "epoch 243, step 600 / 900, loss = 2.26 (0.021 sec/batch)\n",
      "epoch 243, step 650 / 900, loss = 1.79 (0.022 sec/batch)\n",
      "epoch 243, step 700 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 243, step 750 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 243, step 800 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 243, step 850 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "epoch 243, step 900 / 900, loss = 2.62 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.66\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.88\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.664952754974365\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 244, step 50 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 244, step 100 / 900, loss = 2.49 (0.021 sec/batch)\n",
      "epoch 244, step 150 / 900, loss = 1.42 (0.022 sec/batch)\n",
      "epoch 244, step 200 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 244, step 250 / 900, loss = 2.66 (0.022 sec/batch)\n",
      "epoch 244, step 300 / 900, loss = 3.12 (0.022 sec/batch)\n",
      "epoch 244, step 350 / 900, loss = 2.30 (0.022 sec/batch)\n",
      "epoch 244, step 400 / 900, loss = 2.73 (0.022 sec/batch)\n",
      "epoch 244, step 450 / 900, loss = 1.59 (0.022 sec/batch)\n",
      "epoch 244, step 500 / 900, loss = 1.86 (0.021 sec/batch)\n",
      "epoch 244, step 550 / 900, loss = 3.16 (0.021 sec/batch)\n",
      "epoch 244, step 600 / 900, loss = 1.68 (0.022 sec/batch)\n",
      "epoch 244, step 650 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 244, step 700 / 900, loss = 1.45 (0.021 sec/batch)\n",
      "epoch 244, step 750 / 900, loss = 2.30 (0.022 sec/batch)\n",
      "epoch 244, step 800 / 900, loss = 2.03 (0.022 sec/batch)\n",
      "epoch 244, step 850 / 900, loss = 1.78 (0.021 sec/batch)\n",
      "epoch 244, step 900 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.81\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.32\n",
      " avg loss = 2.84\n",
      "\n",
      "Epoch time: 24.668058156967163\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 245, step 50 / 900, loss = 2.93 (0.022 sec/batch)\n",
      "epoch 245, step 100 / 900, loss = 2.90 (0.021 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 245, step 150 / 900, loss = 1.99 (0.021 sec/batch)\n",
      "epoch 245, step 200 / 900, loss = 1.88 (0.021 sec/batch)\n",
      "epoch 245, step 250 / 900, loss = 2.26 (0.021 sec/batch)\n",
      "epoch 245, step 300 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 245, step 350 / 900, loss = 2.38 (0.022 sec/batch)\n",
      "epoch 245, step 400 / 900, loss = 1.75 (0.021 sec/batch)\n",
      "epoch 245, step 450 / 900, loss = 1.94 (0.022 sec/batch)\n",
      "epoch 245, step 500 / 900, loss = 1.63 (0.021 sec/batch)\n",
      "epoch 245, step 550 / 900, loss = 2.71 (0.021 sec/batch)\n",
      "epoch 245, step 600 / 900, loss = 2.36 (0.021 sec/batch)\n",
      "epoch 245, step 650 / 900, loss = 1.56 (0.022 sec/batch)\n",
      "epoch 245, step 700 / 900, loss = 1.96 (0.022 sec/batch)\n",
      "epoch 245, step 750 / 900, loss = 2.73 (0.021 sec/batch)\n",
      "epoch 245, step 800 / 900, loss = 2.96 (0.022 sec/batch)\n",
      "epoch 245, step 850 / 900, loss = 2.50 (0.022 sec/batch)\n",
      "epoch 245, step 900 / 900, loss = 3.69 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.42\n",
      " avg loss = 2.28\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.12\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.66593623161316\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 246, step 50 / 900, loss = 2.40 (0.021 sec/batch)\n",
      "epoch 246, step 100 / 900, loss = 1.69 (0.021 sec/batch)\n",
      "epoch 246, step 150 / 900, loss = 1.93 (0.022 sec/batch)\n",
      "epoch 246, step 200 / 900, loss = 2.62 (0.021 sec/batch)\n",
      "epoch 246, step 250 / 900, loss = 2.51 (0.021 sec/batch)\n",
      "epoch 246, step 300 / 900, loss = 1.75 (0.021 sec/batch)\n",
      "epoch 246, step 350 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 246, step 400 / 900, loss = 1.80 (0.021 sec/batch)\n",
      "epoch 246, step 450 / 900, loss = 2.82 (0.022 sec/batch)\n",
      "epoch 246, step 500 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "epoch 246, step 550 / 900, loss = 1.82 (0.022 sec/batch)\n",
      "epoch 246, step 600 / 900, loss = 2.46 (0.021 sec/batch)\n",
      "epoch 246, step 650 / 900, loss = 1.69 (0.021 sec/batch)\n",
      "epoch 246, step 700 / 900, loss = 1.29 (0.021 sec/batch)\n",
      "epoch 246, step 750 / 900, loss = 2.74 (0.021 sec/batch)\n",
      "epoch 246, step 800 / 900, loss = 3.00 (0.021 sec/batch)\n",
      "epoch 246, step 850 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 246, step 900 / 900, loss = 1.99 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.64\n",
      " avg loss = 2.24\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.12\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.684524297714233\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 247, step 50 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 247, step 100 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 247, step 150 / 900, loss = 1.63 (0.022 sec/batch)\n",
      "epoch 247, step 200 / 900, loss = 1.48 (0.021 sec/batch)\n",
      "epoch 247, step 250 / 900, loss = 2.89 (0.023 sec/batch)\n",
      "epoch 247, step 300 / 900, loss = 1.98 (0.021 sec/batch)\n",
      "epoch 247, step 350 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 247, step 400 / 900, loss = 1.63 (0.021 sec/batch)\n",
      "epoch 247, step 450 / 900, loss = 3.05 (0.022 sec/batch)\n",
      "epoch 247, step 500 / 900, loss = 3.37 (0.021 sec/batch)\n",
      "epoch 247, step 550 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 247, step 600 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "epoch 247, step 650 / 900, loss = 2.69 (0.022 sec/batch)\n",
      "epoch 247, step 700 / 900, loss = 2.44 (0.022 sec/batch)\n",
      "epoch 247, step 750 / 900, loss = 2.47 (0.022 sec/batch)\n",
      "epoch 247, step 800 / 900, loss = 2.54 (0.022 sec/batch)\n",
      "epoch 247, step 850 / 900, loss = 2.30 (0.022 sec/batch)\n",
      "epoch 247, step 900 / 900, loss = 3.12 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.73\n",
      " avg loss = 2.23\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.26\n",
      " avg loss = 2.86\n",
      "\n",
      "Epoch time: 24.682100534439087\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 248, step 50 / 900, loss = 1.59 (0.021 sec/batch)\n",
      "epoch 248, step 100 / 900, loss = 2.45 (0.022 sec/batch)\n",
      "epoch 248, step 150 / 900, loss = 2.26 (0.021 sec/batch)\n",
      "epoch 248, step 200 / 900, loss = 1.64 (0.021 sec/batch)\n",
      "epoch 248, step 250 / 900, loss = 1.36 (0.021 sec/batch)\n",
      "epoch 248, step 300 / 900, loss = 1.56 (0.021 sec/batch)\n",
      "epoch 248, step 350 / 900, loss = 2.41 (0.022 sec/batch)\n",
      "epoch 248, step 400 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "epoch 248, step 450 / 900, loss = 3.27 (0.021 sec/batch)\n",
      "epoch 248, step 500 / 900, loss = 2.80 (0.021 sec/batch)\n",
      "epoch 248, step 550 / 900, loss = 2.22 (0.022 sec/batch)\n",
      "epoch 248, step 600 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 248, step 650 / 900, loss = 1.62 (0.021 sec/batch)\n",
      "epoch 248, step 700 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 248, step 750 / 900, loss = 1.63 (0.022 sec/batch)\n",
      "epoch 248, step 800 / 900, loss = 2.06 (0.021 sec/batch)\n",
      "epoch 248, step 850 / 900, loss = 2.81 (0.021 sec/batch)\n",
      "epoch 248, step 900 / 900, loss = 2.88 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.95\n",
      " avg loss = 2.24\n",
      "\n",
      "Validation error:\n",
      " accuracy = 51.02\n",
      " avg loss = 2.85\n",
      "\n",
      "Epoch time: 24.65145182609558\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 249, step 50 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 249, step 100 / 900, loss = 2.77 (0.021 sec/batch)\n",
      "epoch 249, step 150 / 900, loss = 1.57 (0.021 sec/batch)\n",
      "epoch 249, step 200 / 900, loss = 1.51 (0.021 sec/batch)\n",
      "epoch 249, step 250 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 249, step 300 / 900, loss = 2.72 (0.022 sec/batch)\n",
      "epoch 249, step 350 / 900, loss = 2.01 (0.022 sec/batch)\n",
      "epoch 249, step 400 / 900, loss = 2.04 (0.022 sec/batch)\n",
      "epoch 249, step 450 / 900, loss = 2.60 (0.021 sec/batch)\n",
      "epoch 249, step 500 / 900, loss = 2.47 (0.021 sec/batch)\n",
      "epoch 249, step 550 / 900, loss = 2.15 (0.022 sec/batch)\n",
      "epoch 249, step 600 / 900, loss = 1.88 (0.022 sec/batch)\n",
      "epoch 249, step 650 / 900, loss = 1.33 (0.021 sec/batch)\n",
      "epoch 249, step 700 / 900, loss = 1.88 (0.021 sec/batch)\n",
      "epoch 249, step 750 / 900, loss = 1.44 (0.021 sec/batch)\n",
      "epoch 249, step 800 / 900, loss = 1.73 (0.021 sec/batch)\n",
      "epoch 249, step 850 / 900, loss = 1.87 (0.022 sec/batch)\n",
      "epoch 249, step 900 / 900, loss = 1.86 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.74\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.94\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.649195432662964\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 250, step 50 / 900, loss = 2.85 (0.021 sec/batch)\n",
      "epoch 250, step 100 / 900, loss = 2.11 (0.022 sec/batch)\n",
      "epoch 250, step 150 / 900, loss = 1.58 (0.021 sec/batch)\n",
      "epoch 250, step 200 / 900, loss = 2.70 (0.022 sec/batch)\n",
      "epoch 250, step 250 / 900, loss = 1.67 (0.022 sec/batch)\n",
      "epoch 250, step 300 / 900, loss = 1.69 (0.021 sec/batch)\n",
      "epoch 250, step 350 / 900, loss = 2.52 (0.022 sec/batch)\n",
      "epoch 250, step 400 / 900, loss = 2.06 (0.022 sec/batch)\n",
      "epoch 250, step 450 / 900, loss = 1.66 (0.022 sec/batch)\n",
      "epoch 250, step 500 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 250, step 550 / 900, loss = 1.82 (0.022 sec/batch)\n",
      "epoch 250, step 600 / 900, loss = 2.34 (0.022 sec/batch)\n",
      "epoch 250, step 650 / 900, loss = 1.59 (0.021 sec/batch)\n",
      "epoch 250, step 700 / 900, loss = 2.69 (0.021 sec/batch)\n",
      "epoch 250, step 750 / 900, loss = 2.67 (0.022 sec/batch)\n",
      "epoch 250, step 800 / 900, loss = 2.95 (0.022 sec/batch)\n",
      "epoch 250, step 850 / 900, loss = 3.04 (0.021 sec/batch)\n",
      "epoch 250, step 900 / 900, loss = 2.92 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.19\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.40\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.748939037322998\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 251, step 50 / 900, loss = 3.16 (0.021 sec/batch)\n",
      "epoch 251, step 100 / 900, loss = 1.97 (0.022 sec/batch)\n",
      "epoch 251, step 150 / 900, loss = 1.79 (0.021 sec/batch)\n",
      "epoch 251, step 200 / 900, loss = 2.92 (0.020 sec/batch)\n",
      "epoch 251, step 250 / 900, loss = 2.31 (0.022 sec/batch)\n",
      "epoch 251, step 300 / 900, loss = 2.47 (0.022 sec/batch)\n",
      "epoch 251, step 350 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 251, step 400 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 251, step 450 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 251, step 500 / 900, loss = 2.68 (0.021 sec/batch)\n",
      "epoch 251, step 550 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 251, step 600 / 900, loss = 2.87 (0.022 sec/batch)\n",
      "epoch 251, step 650 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 251, step 700 / 900, loss = 2.60 (0.021 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 251, step 750 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 251, step 800 / 900, loss = 2.60 (0.022 sec/batch)\n",
      "epoch 251, step 850 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 251, step 900 / 900, loss = 2.15 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.43\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.16\n",
      " avg loss = 2.85\n",
      "\n",
      "Epoch time: 24.71819567680359\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 252, step 50 / 900, loss = 1.46 (0.021 sec/batch)\n",
      "epoch 252, step 100 / 900, loss = 2.17 (0.023 sec/batch)\n",
      "epoch 252, step 150 / 900, loss = 2.36 (0.021 sec/batch)\n",
      "epoch 252, step 200 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 252, step 250 / 900, loss = 2.72 (0.021 sec/batch)\n",
      "epoch 252, step 300 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 252, step 350 / 900, loss = 2.04 (0.021 sec/batch)\n",
      "epoch 252, step 400 / 900, loss = 2.81 (0.022 sec/batch)\n",
      "epoch 252, step 450 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 252, step 500 / 900, loss = 1.85 (0.021 sec/batch)\n",
      "epoch 252, step 550 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 252, step 600 / 900, loss = 2.21 (0.022 sec/batch)\n",
      "epoch 252, step 650 / 900, loss = 2.95 (0.022 sec/batch)\n",
      "epoch 252, step 700 / 900, loss = 1.82 (0.021 sec/batch)\n",
      "epoch 252, step 750 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 252, step 800 / 900, loss = 3.18 (0.022 sec/batch)\n",
      "epoch 252, step 850 / 900, loss = 2.26 (0.021 sec/batch)\n",
      "epoch 252, step 900 / 900, loss = 1.66 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.65\n",
      " avg loss = 2.28\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.36\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.66507887840271\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 253, step 50 / 900, loss = 2.34 (0.022 sec/batch)\n",
      "epoch 253, step 100 / 900, loss = 3.19 (0.022 sec/batch)\n",
      "epoch 253, step 150 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 253, step 200 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 253, step 250 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "epoch 253, step 300 / 900, loss = 1.80 (0.021 sec/batch)\n",
      "epoch 253, step 350 / 900, loss = 2.24 (0.021 sec/batch)\n",
      "epoch 253, step 400 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 253, step 450 / 900, loss = 2.04 (0.021 sec/batch)\n",
      "epoch 253, step 500 / 900, loss = 1.73 (0.022 sec/batch)\n",
      "epoch 253, step 550 / 900, loss = 1.84 (0.022 sec/batch)\n",
      "epoch 253, step 600 / 900, loss = 2.65 (0.022 sec/batch)\n",
      "epoch 253, step 650 / 900, loss = 2.49 (0.021 sec/batch)\n",
      "epoch 253, step 700 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "epoch 253, step 750 / 900, loss = 2.46 (0.021 sec/batch)\n",
      "epoch 253, step 800 / 900, loss = 2.12 (0.022 sec/batch)\n",
      "epoch 253, step 850 / 900, loss = 3.41 (0.021 sec/batch)\n",
      "epoch 253, step 900 / 900, loss = 2.46 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.54\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.90\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.684630155563354\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 254, step 50 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 254, step 100 / 900, loss = 2.37 (0.022 sec/batch)\n",
      "epoch 254, step 150 / 900, loss = 2.50 (0.022 sec/batch)\n",
      "epoch 254, step 200 / 900, loss = 2.62 (0.022 sec/batch)\n",
      "epoch 254, step 250 / 900, loss = 1.66 (0.022 sec/batch)\n",
      "epoch 254, step 300 / 900, loss = 3.08 (0.021 sec/batch)\n",
      "epoch 254, step 350 / 900, loss = 2.45 (0.021 sec/batch)\n",
      "epoch 254, step 400 / 900, loss = 2.54 (0.022 sec/batch)\n",
      "epoch 254, step 450 / 900, loss = 2.36 (0.021 sec/batch)\n",
      "epoch 254, step 500 / 900, loss = 2.69 (0.021 sec/batch)\n",
      "epoch 254, step 550 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 254, step 600 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 254, step 650 / 900, loss = 2.68 (0.021 sec/batch)\n",
      "epoch 254, step 700 / 900, loss = 2.32 (0.022 sec/batch)\n",
      "epoch 254, step 750 / 900, loss = 2.62 (0.021 sec/batch)\n",
      "epoch 254, step 800 / 900, loss = 1.92 (0.022 sec/batch)\n",
      "epoch 254, step 850 / 900, loss = 2.16 (0.022 sec/batch)\n",
      "epoch 254, step 900 / 900, loss = 2.37 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.39\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.18\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.696946382522583\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 255, step 50 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "epoch 255, step 100 / 900, loss = 2.96 (0.022 sec/batch)\n",
      "epoch 255, step 150 / 900, loss = 1.88 (0.022 sec/batch)\n",
      "epoch 255, step 200 / 900, loss = 2.06 (0.021 sec/batch)\n",
      "epoch 255, step 250 / 900, loss = 3.16 (0.023 sec/batch)\n",
      "epoch 255, step 300 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 255, step 350 / 900, loss = 2.40 (0.022 sec/batch)\n",
      "epoch 255, step 400 / 900, loss = 1.35 (0.022 sec/batch)\n",
      "epoch 255, step 450 / 900, loss = 2.29 (0.022 sec/batch)\n",
      "epoch 255, step 500 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 255, step 550 / 900, loss = 2.60 (0.022 sec/batch)\n",
      "epoch 255, step 600 / 900, loss = 2.41 (0.021 sec/batch)\n",
      "epoch 255, step 650 / 900, loss = 2.07 (0.022 sec/batch)\n",
      "epoch 255, step 700 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 255, step 750 / 900, loss = 2.17 (0.022 sec/batch)\n",
      "epoch 255, step 800 / 900, loss = 3.16 (0.022 sec/batch)\n",
      "epoch 255, step 850 / 900, loss = 2.26 (0.021 sec/batch)\n",
      "epoch 255, step 900 / 900, loss = 2.65 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.76\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 48.86\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.710779428482056\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 256, step 50 / 900, loss = 1.47 (0.021 sec/batch)\n",
      "epoch 256, step 100 / 900, loss = 1.90 (0.021 sec/batch)\n",
      "epoch 256, step 150 / 900, loss = 1.41 (0.021 sec/batch)\n",
      "epoch 256, step 200 / 900, loss = 1.56 (0.021 sec/batch)\n",
      "epoch 256, step 250 / 900, loss = 2.17 (0.022 sec/batch)\n",
      "epoch 256, step 300 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 256, step 350 / 900, loss = 2.59 (0.021 sec/batch)\n",
      "epoch 256, step 400 / 900, loss = 2.06 (0.022 sec/batch)\n",
      "epoch 256, step 450 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 256, step 500 / 900, loss = 3.08 (0.021 sec/batch)\n",
      "epoch 256, step 550 / 900, loss = 2.58 (0.022 sec/batch)\n",
      "epoch 256, step 600 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 256, step 650 / 900, loss = 1.88 (0.021 sec/batch)\n",
      "epoch 256, step 700 / 900, loss = 1.92 (0.021 sec/batch)\n",
      "epoch 256, step 750 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 256, step 800 / 900, loss = 1.72 (0.021 sec/batch)\n",
      "epoch 256, step 850 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "epoch 256, step 900 / 900, loss = 2.18 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.30\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.80\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.700342893600464\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 257, step 50 / 900, loss = 3.00 (0.021 sec/batch)\n",
      "epoch 257, step 100 / 900, loss = 1.63 (0.021 sec/batch)\n",
      "epoch 257, step 150 / 900, loss = 2.74 (0.021 sec/batch)\n",
      "epoch 257, step 200 / 900, loss = 2.25 (0.022 sec/batch)\n",
      "epoch 257, step 250 / 900, loss = 2.84 (0.021 sec/batch)\n",
      "epoch 257, step 300 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 257, step 350 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 257, step 400 / 900, loss = 2.62 (0.021 sec/batch)\n",
      "epoch 257, step 450 / 900, loss = 1.29 (0.021 sec/batch)\n",
      "epoch 257, step 500 / 900, loss = 2.82 (0.022 sec/batch)\n",
      "epoch 257, step 550 / 900, loss = 2.70 (0.021 sec/batch)\n",
      "epoch 257, step 600 / 900, loss = 1.69 (0.021 sec/batch)\n",
      "epoch 257, step 650 / 900, loss = 2.33 (0.022 sec/batch)\n",
      "epoch 257, step 700 / 900, loss = 1.96 (0.021 sec/batch)\n",
      "epoch 257, step 750 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 257, step 800 / 900, loss = 1.94 (0.022 sec/batch)\n",
      "epoch 257, step 850 / 900, loss = 1.84 (0.022 sec/batch)\n",
      "epoch 257, step 900 / 900, loss = 1.78 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.31\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 48.42\n",
      " avg loss = 2.97\n",
      "\n",
      "Epoch time: 24.65478491783142\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 258, step 50 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 258, step 100 / 900, loss = 3.19 (0.022 sec/batch)\n",
      "epoch 258, step 150 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 258, step 200 / 900, loss = 3.27 (0.021 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 258, step 250 / 900, loss = 1.76 (0.021 sec/batch)\n",
      "epoch 258, step 300 / 900, loss = 1.79 (0.021 sec/batch)\n",
      "epoch 258, step 350 / 900, loss = 2.19 (0.022 sec/batch)\n",
      "epoch 258, step 400 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 258, step 450 / 900, loss = 2.49 (0.022 sec/batch)\n",
      "epoch 258, step 500 / 900, loss = 2.40 (0.021 sec/batch)\n",
      "epoch 258, step 550 / 900, loss = 2.28 (0.022 sec/batch)\n",
      "epoch 258, step 600 / 900, loss = 3.14 (0.022 sec/batch)\n",
      "epoch 258, step 650 / 900, loss = 3.74 (0.021 sec/batch)\n",
      "epoch 258, step 700 / 900, loss = 2.66 (0.021 sec/batch)\n",
      "epoch 258, step 750 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 258, step 800 / 900, loss = 2.91 (0.022 sec/batch)\n",
      "epoch 258, step 850 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 258, step 900 / 900, loss = 2.56 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.79\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 48.80\n",
      " avg loss = 2.97\n",
      "\n",
      "Epoch time: 24.65909767150879\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 259, step 50 / 900, loss = 2.06 (0.022 sec/batch)\n",
      "epoch 259, step 100 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "epoch 259, step 150 / 900, loss = 3.87 (0.021 sec/batch)\n",
      "epoch 259, step 200 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 259, step 250 / 900, loss = 2.12 (0.022 sec/batch)\n",
      "epoch 259, step 300 / 900, loss = 2.71 (0.021 sec/batch)\n",
      "epoch 259, step 350 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "epoch 259, step 400 / 900, loss = 2.15 (0.021 sec/batch)\n",
      "epoch 259, step 450 / 900, loss = 2.22 (0.022 sec/batch)\n",
      "epoch 259, step 500 / 900, loss = 2.79 (0.021 sec/batch)\n",
      "epoch 259, step 550 / 900, loss = 2.38 (0.022 sec/batch)\n",
      "epoch 259, step 600 / 900, loss = 1.73 (0.021 sec/batch)\n",
      "epoch 259, step 650 / 900, loss = 1.93 (0.021 sec/batch)\n",
      "epoch 259, step 700 / 900, loss = 3.02 (0.022 sec/batch)\n",
      "epoch 259, step 750 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 259, step 800 / 900, loss = 1.84 (0.021 sec/batch)\n",
      "epoch 259, step 850 / 900, loss = 2.02 (0.022 sec/batch)\n",
      "epoch 259, step 900 / 900, loss = 2.66 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.70\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.92\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.676465272903442\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 260, step 50 / 900, loss = 1.48 (0.021 sec/batch)\n",
      "epoch 260, step 100 / 900, loss = 2.78 (0.022 sec/batch)\n",
      "epoch 260, step 150 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 260, step 200 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 260, step 250 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "epoch 260, step 300 / 900, loss = 2.44 (0.021 sec/batch)\n",
      "epoch 260, step 350 / 900, loss = 1.68 (0.021 sec/batch)\n",
      "epoch 260, step 400 / 900, loss = 3.48 (0.021 sec/batch)\n",
      "epoch 260, step 450 / 900, loss = 1.53 (0.023 sec/batch)\n",
      "epoch 260, step 500 / 900, loss = 1.65 (0.021 sec/batch)\n",
      "epoch 260, step 550 / 900, loss = 1.50 (0.022 sec/batch)\n",
      "epoch 260, step 600 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 260, step 650 / 900, loss = 2.27 (0.022 sec/batch)\n",
      "epoch 260, step 700 / 900, loss = 1.98 (0.021 sec/batch)\n",
      "epoch 260, step 750 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 260, step 800 / 900, loss = 2.35 (0.022 sec/batch)\n",
      "epoch 260, step 850 / 900, loss = 1.93 (0.022 sec/batch)\n",
      "epoch 260, step 900 / 900, loss = 1.80 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.54\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.72\n",
      " avg loss = 2.86\n",
      "\n",
      "Epoch time: 24.70731520652771\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 261, step 50 / 900, loss = 2.47 (0.021 sec/batch)\n",
      "epoch 261, step 100 / 900, loss = 2.98 (0.021 sec/batch)\n",
      "epoch 261, step 150 / 900, loss = 1.91 (0.021 sec/batch)\n",
      "epoch 261, step 200 / 900, loss = 1.76 (0.021 sec/batch)\n",
      "epoch 261, step 250 / 900, loss = 2.05 (0.022 sec/batch)\n",
      "epoch 261, step 300 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 261, step 350 / 900, loss = 2.62 (0.022 sec/batch)\n",
      "epoch 261, step 400 / 900, loss = 1.98 (0.021 sec/batch)\n",
      "epoch 261, step 450 / 900, loss = 2.43 (0.020 sec/batch)\n",
      "epoch 261, step 500 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "epoch 261, step 550 / 900, loss = 2.85 (0.022 sec/batch)\n",
      "epoch 261, step 600 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 261, step 650 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 261, step 700 / 900, loss = 2.54 (0.022 sec/batch)\n",
      "epoch 261, step 750 / 900, loss = 2.24 (0.022 sec/batch)\n",
      "epoch 261, step 800 / 900, loss = 2.69 (0.022 sec/batch)\n",
      "epoch 261, step 850 / 900, loss = 2.04 (0.021 sec/batch)\n",
      "epoch 261, step 900 / 900, loss = 2.60 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.52\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.98\n",
      " avg loss = 2.94\n",
      "\n",
      "Epoch time: 24.66594648361206\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 262, step 50 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 262, step 100 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 262, step 150 / 900, loss = 2.57 (0.021 sec/batch)\n",
      "epoch 262, step 200 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 262, step 250 / 900, loss = 2.91 (0.022 sec/batch)\n",
      "epoch 262, step 300 / 900, loss = 2.43 (0.022 sec/batch)\n",
      "epoch 262, step 350 / 900, loss = 2.62 (0.021 sec/batch)\n",
      "epoch 262, step 400 / 900, loss = 1.34 (0.021 sec/batch)\n",
      "epoch 262, step 450 / 900, loss = 2.05 (0.022 sec/batch)\n",
      "epoch 262, step 500 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 262, step 550 / 900, loss = 2.32 (0.021 sec/batch)\n",
      "epoch 262, step 600 / 900, loss = 1.65 (0.020 sec/batch)\n",
      "epoch 262, step 650 / 900, loss = 1.68 (0.021 sec/batch)\n",
      "epoch 262, step 700 / 900, loss = 2.36 (0.021 sec/batch)\n",
      "epoch 262, step 750 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 262, step 800 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 262, step 850 / 900, loss = 2.39 (0.021 sec/batch)\n",
      "epoch 262, step 900 / 900, loss = 1.85 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.65\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.12\n",
      " avg loss = 2.85\n",
      "\n",
      "Epoch time: 24.67057728767395\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 263, step 50 / 900, loss = 1.85 (0.021 sec/batch)\n",
      "epoch 263, step 100 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 263, step 150 / 900, loss = 1.99 (0.022 sec/batch)\n",
      "epoch 263, step 200 / 900, loss = 2.79 (0.021 sec/batch)\n",
      "epoch 263, step 250 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 263, step 300 / 900, loss = 3.26 (0.022 sec/batch)\n",
      "epoch 263, step 350 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "epoch 263, step 400 / 900, loss = 1.95 (0.023 sec/batch)\n",
      "epoch 263, step 450 / 900, loss = 1.67 (0.021 sec/batch)\n",
      "epoch 263, step 500 / 900, loss = 2.79 (0.021 sec/batch)\n",
      "epoch 263, step 550 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 263, step 600 / 900, loss = 1.64 (0.021 sec/batch)\n",
      "epoch 263, step 650 / 900, loss = 2.71 (0.022 sec/batch)\n",
      "epoch 263, step 700 / 900, loss = 2.79 (0.021 sec/batch)\n",
      "epoch 263, step 750 / 900, loss = 2.19 (0.020 sec/batch)\n",
      "epoch 263, step 800 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 263, step 850 / 900, loss = 2.28 (0.022 sec/batch)\n",
      "epoch 263, step 900 / 900, loss = 2.42 (0.023 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.25\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.02\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.832878828048706\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 264, step 50 / 900, loss = 2.51 (0.021 sec/batch)\n",
      "epoch 264, step 100 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 264, step 150 / 900, loss = 2.38 (0.022 sec/batch)\n",
      "epoch 264, step 200 / 900, loss = 2.14 (0.022 sec/batch)\n",
      "epoch 264, step 250 / 900, loss = 2.22 (0.022 sec/batch)\n",
      "epoch 264, step 300 / 900, loss = 2.59 (0.022 sec/batch)\n",
      "epoch 264, step 350 / 900, loss = 2.45 (0.022 sec/batch)\n",
      "epoch 264, step 400 / 900, loss = 2.67 (0.021 sec/batch)\n",
      "epoch 264, step 450 / 900, loss = 3.34 (0.021 sec/batch)\n",
      "epoch 264, step 500 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 264, step 550 / 900, loss = 2.34 (0.022 sec/batch)\n",
      "epoch 264, step 600 / 900, loss = 2.83 (0.021 sec/batch)\n",
      "epoch 264, step 650 / 900, loss = 1.81 (0.022 sec/batch)\n",
      "epoch 264, step 700 / 900, loss = 2.05 (0.022 sec/batch)\n",
      "epoch 264, step 750 / 900, loss = 2.24 (0.021 sec/batch)\n",
      "epoch 264, step 800 / 900, loss = 1.99 (0.021 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 264, step 850 / 900, loss = 3.12 (0.021 sec/batch)\n",
      "epoch 264, step 900 / 900, loss = 3.51 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.77\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.12\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.588590383529663\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 265, step 50 / 900, loss = 3.10 (0.021 sec/batch)\n",
      "epoch 265, step 100 / 900, loss = 3.62 (0.021 sec/batch)\n",
      "epoch 265, step 150 / 900, loss = 1.82 (0.021 sec/batch)\n",
      "epoch 265, step 200 / 900, loss = 2.04 (0.021 sec/batch)\n",
      "epoch 265, step 250 / 900, loss = 1.91 (0.021 sec/batch)\n",
      "epoch 265, step 300 / 900, loss = 2.40 (0.021 sec/batch)\n",
      "epoch 265, step 350 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 265, step 400 / 900, loss = 2.37 (0.022 sec/batch)\n",
      "epoch 265, step 450 / 900, loss = 2.15 (0.021 sec/batch)\n",
      "epoch 265, step 500 / 900, loss = 1.64 (0.021 sec/batch)\n",
      "epoch 265, step 550 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 265, step 600 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 265, step 650 / 900, loss = 2.34 (0.022 sec/batch)\n",
      "epoch 265, step 700 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 265, step 750 / 900, loss = 2.53 (0.022 sec/batch)\n",
      "epoch 265, step 800 / 900, loss = 1.87 (0.022 sec/batch)\n",
      "epoch 265, step 850 / 900, loss = 1.96 (0.021 sec/batch)\n",
      "epoch 265, step 900 / 900, loss = 1.56 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.37\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.76\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.616772890090942\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 266, step 50 / 900, loss = 1.40 (0.022 sec/batch)\n",
      "epoch 266, step 100 / 900, loss = 1.85 (0.022 sec/batch)\n",
      "epoch 266, step 150 / 900, loss = 1.74 (0.022 sec/batch)\n",
      "epoch 266, step 200 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "epoch 266, step 250 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 266, step 300 / 900, loss = 2.62 (0.022 sec/batch)\n",
      "epoch 266, step 350 / 900, loss = 4.19 (0.021 sec/batch)\n",
      "epoch 266, step 400 / 900, loss = 1.58 (0.022 sec/batch)\n",
      "epoch 266, step 450 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 266, step 500 / 900, loss = 2.54 (0.022 sec/batch)\n",
      "epoch 266, step 550 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 266, step 600 / 900, loss = 2.18 (0.022 sec/batch)\n",
      "epoch 266, step 650 / 900, loss = 1.44 (0.021 sec/batch)\n",
      "epoch 266, step 700 / 900, loss = 1.37 (0.021 sec/batch)\n",
      "epoch 266, step 750 / 900, loss = 1.82 (0.021 sec/batch)\n",
      "epoch 266, step 800 / 900, loss = 3.16 (0.021 sec/batch)\n",
      "epoch 266, step 850 / 900, loss = 2.60 (0.021 sec/batch)\n",
      "epoch 266, step 900 / 900, loss = 1.77 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.73\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.96\n",
      " avg loss = 2.92\n",
      "\n",
      "Epoch time: 24.56991720199585\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 267, step 50 / 900, loss = 1.30 (0.021 sec/batch)\n",
      "epoch 267, step 100 / 900, loss = 2.70 (0.022 sec/batch)\n",
      "epoch 267, step 150 / 900, loss = 1.85 (0.022 sec/batch)\n",
      "epoch 267, step 200 / 900, loss = 1.41 (0.021 sec/batch)\n",
      "epoch 267, step 250 / 900, loss = 2.60 (0.021 sec/batch)\n",
      "epoch 267, step 300 / 900, loss = 2.51 (0.021 sec/batch)\n",
      "epoch 267, step 350 / 900, loss = 1.59 (0.021 sec/batch)\n",
      "epoch 267, step 400 / 900, loss = 1.99 (0.021 sec/batch)\n",
      "epoch 267, step 450 / 900, loss = 2.19 (0.022 sec/batch)\n",
      "epoch 267, step 500 / 900, loss = 1.67 (0.021 sec/batch)\n",
      "epoch 267, step 550 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "epoch 267, step 600 / 900, loss = 1.23 (0.022 sec/batch)\n",
      "epoch 267, step 650 / 900, loss = 2.27 (0.022 sec/batch)\n",
      "epoch 267, step 700 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 267, step 750 / 900, loss = 2.56 (0.021 sec/batch)\n",
      "epoch 267, step 800 / 900, loss = 2.04 (0.021 sec/batch)\n",
      "epoch 267, step 850 / 900, loss = 2.33 (0.022 sec/batch)\n",
      "epoch 267, step 900 / 900, loss = 2.93 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.63\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.38\n",
      " avg loss = 2.82\n",
      "\n",
      "Epoch time: 24.57781410217285\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 268, step 50 / 900, loss = 2.82 (0.022 sec/batch)\n",
      "epoch 268, step 100 / 900, loss = 1.70 (0.022 sec/batch)\n",
      "epoch 268, step 150 / 900, loss = 2.97 (0.021 sec/batch)\n",
      "epoch 268, step 200 / 900, loss = 1.82 (0.021 sec/batch)\n",
      "epoch 268, step 250 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "epoch 268, step 300 / 900, loss = 2.57 (0.022 sec/batch)\n",
      "epoch 268, step 350 / 900, loss = 1.51 (0.021 sec/batch)\n",
      "epoch 268, step 400 / 900, loss = 2.49 (0.021 sec/batch)\n",
      "epoch 268, step 450 / 900, loss = 2.53 (0.022 sec/batch)\n",
      "epoch 268, step 500 / 900, loss = 2.32 (0.021 sec/batch)\n",
      "epoch 268, step 550 / 900, loss = 2.01 (0.022 sec/batch)\n",
      "epoch 268, step 600 / 900, loss = 1.81 (0.021 sec/batch)\n",
      "epoch 268, step 650 / 900, loss = 3.20 (0.021 sec/batch)\n",
      "epoch 268, step 700 / 900, loss = 2.03 (0.022 sec/batch)\n",
      "epoch 268, step 750 / 900, loss = 1.68 (0.021 sec/batch)\n",
      "epoch 268, step 800 / 900, loss = 1.43 (0.021 sec/batch)\n",
      "epoch 268, step 850 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 268, step 900 / 900, loss = 4.10 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.64\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.50\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.60650944709778\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 269, step 50 / 900, loss = 2.87 (0.022 sec/batch)\n",
      "epoch 269, step 100 / 900, loss = 1.88 (0.022 sec/batch)\n",
      "epoch 269, step 150 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "epoch 269, step 200 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 269, step 250 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 269, step 300 / 900, loss = 2.63 (0.021 sec/batch)\n",
      "epoch 269, step 350 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 269, step 400 / 900, loss = 2.90 (0.021 sec/batch)\n",
      "epoch 269, step 450 / 900, loss = 2.67 (0.021 sec/batch)\n",
      "epoch 269, step 500 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 269, step 550 / 900, loss = 1.70 (0.022 sec/batch)\n",
      "epoch 269, step 600 / 900, loss = 1.37 (0.021 sec/batch)\n",
      "epoch 269, step 650 / 900, loss = 1.55 (0.021 sec/batch)\n",
      "epoch 269, step 700 / 900, loss = 1.91 (0.021 sec/batch)\n",
      "epoch 269, step 750 / 900, loss = 2.15 (0.021 sec/batch)\n",
      "epoch 269, step 800 / 900, loss = 1.24 (0.021 sec/batch)\n",
      "epoch 269, step 850 / 900, loss = 1.31 (0.021 sec/batch)\n",
      "epoch 269, step 900 / 900, loss = 1.76 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.66\n",
      " avg loss = 2.24\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.02\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.581418752670288\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 270, step 50 / 900, loss = 1.65 (0.022 sec/batch)\n",
      "epoch 270, step 100 / 900, loss = 2.37 (0.022 sec/batch)\n",
      "epoch 270, step 150 / 900, loss = 2.88 (0.022 sec/batch)\n",
      "epoch 270, step 200 / 900, loss = 1.75 (0.021 sec/batch)\n",
      "epoch 270, step 250 / 900, loss = 1.90 (0.021 sec/batch)\n",
      "epoch 270, step 300 / 900, loss = 1.82 (0.021 sec/batch)\n",
      "epoch 270, step 350 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 270, step 400 / 900, loss = 1.79 (0.021 sec/batch)\n",
      "epoch 270, step 450 / 900, loss = 3.01 (0.021 sec/batch)\n",
      "epoch 270, step 500 / 900, loss = 1.44 (0.022 sec/batch)\n",
      "epoch 270, step 550 / 900, loss = 1.56 (0.021 sec/batch)\n",
      "epoch 270, step 600 / 900, loss = 2.04 (0.021 sec/batch)\n",
      "epoch 270, step 650 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 270, step 700 / 900, loss = 3.58 (0.021 sec/batch)\n",
      "epoch 270, step 750 / 900, loss = 2.98 (0.021 sec/batch)\n",
      "epoch 270, step 800 / 900, loss = 2.69 (0.022 sec/batch)\n",
      "epoch 270, step 850 / 900, loss = 2.65 (0.020 sec/batch)\n",
      "epoch 270, step 900 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.83\n",
      " avg loss = 2.24\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.42\n",
      " avg loss = 2.86\n",
      "\n",
      "Epoch time: 24.59893226623535\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 271, step 50 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 271, step 100 / 900, loss = 1.78 (0.021 sec/batch)\n",
      "epoch 271, step 150 / 900, loss = 1.96 (0.022 sec/batch)\n",
      "epoch 271, step 200 / 900, loss = 1.54 (0.021 sec/batch)\n",
      "epoch 271, step 250 / 900, loss = 2.43 (0.020 sec/batch)\n",
      "epoch 271, step 300 / 900, loss = 2.02 (0.020 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 271, step 350 / 900, loss = 1.16 (0.021 sec/batch)\n",
      "epoch 271, step 400 / 900, loss = 2.45 (0.021 sec/batch)\n",
      "epoch 271, step 450 / 900, loss = 2.47 (0.021 sec/batch)\n",
      "epoch 271, step 500 / 900, loss = 2.26 (0.022 sec/batch)\n",
      "epoch 271, step 550 / 900, loss = 1.99 (0.022 sec/batch)\n",
      "epoch 271, step 600 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "epoch 271, step 650 / 900, loss = 1.98 (0.022 sec/batch)\n",
      "epoch 271, step 700 / 900, loss = 2.26 (0.021 sec/batch)\n",
      "epoch 271, step 750 / 900, loss = 1.90 (0.021 sec/batch)\n",
      "epoch 271, step 800 / 900, loss = 2.20 (0.022 sec/batch)\n",
      "epoch 271, step 850 / 900, loss = 3.32 (0.021 sec/batch)\n",
      "epoch 271, step 900 / 900, loss = 1.45 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.80\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 48.68\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.58565330505371\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 272, step 50 / 900, loss = 1.95 (0.021 sec/batch)\n",
      "epoch 272, step 100 / 900, loss = 2.59 (0.021 sec/batch)\n",
      "epoch 272, step 150 / 900, loss = 1.66 (0.021 sec/batch)\n",
      "epoch 272, step 200 / 900, loss = 1.82 (0.021 sec/batch)\n",
      "epoch 272, step 250 / 900, loss = 2.17 (0.022 sec/batch)\n",
      "epoch 272, step 300 / 900, loss = 1.71 (0.021 sec/batch)\n",
      "epoch 272, step 350 / 900, loss = 1.82 (0.021 sec/batch)\n",
      "epoch 272, step 400 / 900, loss = 2.57 (0.021 sec/batch)\n",
      "epoch 272, step 450 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 272, step 500 / 900, loss = 3.22 (0.021 sec/batch)\n",
      "epoch 272, step 550 / 900, loss = 1.94 (0.022 sec/batch)\n",
      "epoch 272, step 600 / 900, loss = 1.40 (0.022 sec/batch)\n",
      "epoch 272, step 650 / 900, loss = 1.71 (0.020 sec/batch)\n",
      "epoch 272, step 700 / 900, loss = 2.79 (0.021 sec/batch)\n",
      "epoch 272, step 750 / 900, loss = 2.19 (0.022 sec/batch)\n",
      "epoch 272, step 800 / 900, loss = 2.43 (0.022 sec/batch)\n",
      "epoch 272, step 850 / 900, loss = 2.89 (0.021 sec/batch)\n",
      "epoch 272, step 900 / 900, loss = 1.85 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.88\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.88\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.582273483276367\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 273, step 50 / 900, loss = 2.51 (0.021 sec/batch)\n",
      "epoch 273, step 100 / 900, loss = 2.72 (0.021 sec/batch)\n",
      "epoch 273, step 150 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 273, step 200 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 273, step 250 / 900, loss = 2.15 (0.021 sec/batch)\n",
      "epoch 273, step 300 / 900, loss = 1.90 (0.021 sec/batch)\n",
      "epoch 273, step 350 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 273, step 400 / 900, loss = 2.24 (0.021 sec/batch)\n",
      "epoch 273, step 450 / 900, loss = 2.34 (0.022 sec/batch)\n",
      "epoch 273, step 500 / 900, loss = 1.69 (0.020 sec/batch)\n",
      "epoch 273, step 550 / 900, loss = 2.45 (0.022 sec/batch)\n",
      "epoch 273, step 600 / 900, loss = 2.68 (0.021 sec/batch)\n",
      "epoch 273, step 650 / 900, loss = 2.93 (0.021 sec/batch)\n",
      "epoch 273, step 700 / 900, loss = 1.79 (0.021 sec/batch)\n",
      "epoch 273, step 750 / 900, loss = 3.18 (0.022 sec/batch)\n",
      "epoch 273, step 800 / 900, loss = 2.15 (0.022 sec/batch)\n",
      "epoch 273, step 850 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "epoch 273, step 900 / 900, loss = 2.63 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.68\n",
      " avg loss = 2.24\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.76\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.57725191116333\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 274, step 50 / 900, loss = 1.80 (0.021 sec/batch)\n",
      "epoch 274, step 100 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "epoch 274, step 150 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "epoch 274, step 200 / 900, loss = 2.63 (0.021 sec/batch)\n",
      "epoch 274, step 250 / 900, loss = 1.67 (0.021 sec/batch)\n",
      "epoch 274, step 300 / 900, loss = 2.20 (0.022 sec/batch)\n",
      "epoch 274, step 350 / 900, loss = 2.66 (0.021 sec/batch)\n",
      "epoch 274, step 400 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "epoch 274, step 450 / 900, loss = 2.02 (0.022 sec/batch)\n",
      "epoch 274, step 500 / 900, loss = 2.25 (0.022 sec/batch)\n",
      "epoch 274, step 550 / 900, loss = 2.75 (0.021 sec/batch)\n",
      "epoch 274, step 600 / 900, loss = 2.76 (0.022 sec/batch)\n",
      "epoch 274, step 650 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 274, step 700 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 274, step 750 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "epoch 274, step 800 / 900, loss = 1.48 (0.021 sec/batch)\n",
      "epoch 274, step 850 / 900, loss = 1.91 (0.022 sec/batch)\n",
      "epoch 274, step 900 / 900, loss = 3.15 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.44\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.04\n",
      " avg loss = 2.92\n",
      "\n",
      "Epoch time: 24.58859348297119\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 275, step 50 / 900, loss = 1.67 (0.021 sec/batch)\n",
      "epoch 275, step 100 / 900, loss = 1.91 (0.022 sec/batch)\n",
      "epoch 275, step 150 / 900, loss = 1.88 (0.021 sec/batch)\n",
      "epoch 275, step 200 / 900, loss = 1.67 (0.021 sec/batch)\n",
      "epoch 275, step 250 / 900, loss = 1.98 (0.021 sec/batch)\n",
      "epoch 275, step 300 / 900, loss = 1.64 (0.024 sec/batch)\n",
      "epoch 275, step 350 / 900, loss = 2.64 (0.020 sec/batch)\n",
      "epoch 275, step 400 / 900, loss = 2.18 (0.022 sec/batch)\n",
      "epoch 275, step 450 / 900, loss = 2.40 (0.021 sec/batch)\n",
      "epoch 275, step 500 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 275, step 550 / 900, loss = 2.14 (0.022 sec/batch)\n",
      "epoch 275, step 600 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 275, step 650 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "epoch 275, step 700 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "epoch 275, step 750 / 900, loss = 3.03 (0.021 sec/batch)\n",
      "epoch 275, step 800 / 900, loss = 1.68 (0.021 sec/batch)\n",
      "epoch 275, step 850 / 900, loss = 1.99 (0.021 sec/batch)\n",
      "epoch 275, step 900 / 900, loss = 3.31 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.82\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.54\n",
      " avg loss = 2.84\n",
      "\n",
      "Epoch time: 24.591407537460327\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 276, step 50 / 900, loss = 2.59 (0.021 sec/batch)\n",
      "epoch 276, step 100 / 900, loss = 2.57 (0.021 sec/batch)\n",
      "epoch 276, step 150 / 900, loss = 1.05 (0.021 sec/batch)\n",
      "epoch 276, step 200 / 900, loss = 2.76 (0.022 sec/batch)\n",
      "epoch 276, step 250 / 900, loss = 2.04 (0.022 sec/batch)\n",
      "epoch 276, step 300 / 900, loss = 1.70 (0.021 sec/batch)\n",
      "epoch 276, step 350 / 900, loss = 2.18 (0.022 sec/batch)\n",
      "epoch 276, step 400 / 900, loss = 1.92 (0.021 sec/batch)\n",
      "epoch 276, step 450 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 276, step 500 / 900, loss = 1.65 (0.021 sec/batch)\n",
      "epoch 276, step 550 / 900, loss = 2.50 (0.022 sec/batch)\n",
      "epoch 276, step 600 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 276, step 650 / 900, loss = 2.17 (0.022 sec/batch)\n",
      "epoch 276, step 700 / 900, loss = 2.85 (0.022 sec/batch)\n",
      "epoch 276, step 750 / 900, loss = 2.78 (0.021 sec/batch)\n",
      "epoch 276, step 800 / 900, loss = 1.96 (0.022 sec/batch)\n",
      "epoch 276, step 850 / 900, loss = 2.57 (0.021 sec/batch)\n",
      "epoch 276, step 900 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.39\n",
      " avg loss = 2.28\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.58\n",
      " avg loss = 2.82\n",
      "\n",
      "Epoch time: 24.58504605293274\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 277, step 50 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 277, step 100 / 900, loss = 2.42 (0.022 sec/batch)\n",
      "epoch 277, step 150 / 900, loss = 2.99 (0.021 sec/batch)\n",
      "epoch 277, step 200 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 277, step 250 / 900, loss = 1.58 (0.021 sec/batch)\n",
      "epoch 277, step 300 / 900, loss = 1.75 (0.021 sec/batch)\n",
      "epoch 277, step 350 / 900, loss = 2.41 (0.022 sec/batch)\n",
      "epoch 277, step 400 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "epoch 277, step 450 / 900, loss = 2.37 (0.022 sec/batch)\n",
      "epoch 277, step 500 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 277, step 550 / 900, loss = 2.67 (0.021 sec/batch)\n",
      "epoch 277, step 600 / 900, loss = 2.72 (0.020 sec/batch)\n",
      "epoch 277, step 650 / 900, loss = 1.36 (0.021 sec/batch)\n",
      "epoch 277, step 700 / 900, loss = 2.57 (0.021 sec/batch)\n",
      "epoch 277, step 750 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 277, step 800 / 900, loss = 2.13 (0.022 sec/batch)\n",
      "epoch 277, step 850 / 900, loss = 2.20 (0.020 sec/batch)\n",
      "epoch 277, step 900 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "Train error:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy = 55.52\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.90\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.592732429504395\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 278, step 50 / 900, loss = 2.06 (0.021 sec/batch)\n",
      "epoch 278, step 100 / 900, loss = 1.28 (0.022 sec/batch)\n",
      "epoch 278, step 150 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 278, step 200 / 900, loss = 2.82 (0.021 sec/batch)\n",
      "epoch 278, step 250 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 278, step 300 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 278, step 350 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 278, step 400 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 278, step 450 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 278, step 500 / 900, loss = 2.33 (0.022 sec/batch)\n",
      "epoch 278, step 550 / 900, loss = 2.26 (0.022 sec/batch)\n",
      "epoch 278, step 600 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 278, step 650 / 900, loss = 1.75 (0.021 sec/batch)\n",
      "epoch 278, step 700 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "epoch 278, step 750 / 900, loss = 2.51 (0.022 sec/batch)\n",
      "epoch 278, step 800 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 278, step 850 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 278, step 900 / 900, loss = 1.65 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.82\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.44\n",
      " avg loss = 2.92\n",
      "\n",
      "Epoch time: 24.57335591316223\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 279, step 50 / 900, loss = 2.91 (0.021 sec/batch)\n",
      "epoch 279, step 100 / 900, loss = 2.84 (0.022 sec/batch)\n",
      "epoch 279, step 150 / 900, loss = 1.96 (0.021 sec/batch)\n",
      "epoch 279, step 200 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "epoch 279, step 250 / 900, loss = 2.26 (0.022 sec/batch)\n",
      "epoch 279, step 300 / 900, loss = 2.10 (0.022 sec/batch)\n",
      "epoch 279, step 350 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 279, step 400 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 279, step 450 / 900, loss = 2.49 (0.021 sec/batch)\n",
      "epoch 279, step 500 / 900, loss = 2.61 (0.021 sec/batch)\n",
      "epoch 279, step 550 / 900, loss = 2.62 (0.021 sec/batch)\n",
      "epoch 279, step 600 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "epoch 279, step 650 / 900, loss = 1.74 (0.021 sec/batch)\n",
      "epoch 279, step 700 / 900, loss = 1.76 (0.021 sec/batch)\n",
      "epoch 279, step 750 / 900, loss = 2.02 (0.022 sec/batch)\n",
      "epoch 279, step 800 / 900, loss = 1.63 (0.021 sec/batch)\n",
      "epoch 279, step 850 / 900, loss = 2.69 (0.021 sec/batch)\n",
      "epoch 279, step 900 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.51\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.22\n",
      " avg loss = 2.85\n",
      "\n",
      "Epoch time: 24.588234901428223\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 280, step 50 / 900, loss = 1.10 (0.022 sec/batch)\n",
      "epoch 280, step 100 / 900, loss = 2.98 (0.021 sec/batch)\n",
      "epoch 280, step 150 / 900, loss = 1.72 (0.021 sec/batch)\n",
      "epoch 280, step 200 / 900, loss = 3.78 (0.021 sec/batch)\n",
      "epoch 280, step 250 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 280, step 300 / 900, loss = 2.13 (0.022 sec/batch)\n",
      "epoch 280, step 350 / 900, loss = 2.61 (0.022 sec/batch)\n",
      "epoch 280, step 400 / 900, loss = 2.97 (0.022 sec/batch)\n",
      "epoch 280, step 450 / 900, loss = 2.06 (0.021 sec/batch)\n",
      "epoch 280, step 500 / 900, loss = 2.36 (0.021 sec/batch)\n",
      "epoch 280, step 550 / 900, loss = 1.75 (0.021 sec/batch)\n",
      "epoch 280, step 600 / 900, loss = 2.59 (0.021 sec/batch)\n",
      "epoch 280, step 650 / 900, loss = 2.67 (0.021 sec/batch)\n",
      "epoch 280, step 700 / 900, loss = 2.19 (0.023 sec/batch)\n",
      "epoch 280, step 750 / 900, loss = 2.72 (0.022 sec/batch)\n",
      "epoch 280, step 800 / 900, loss = 2.50 (0.022 sec/batch)\n",
      "epoch 280, step 850 / 900, loss = 1.42 (0.021 sec/batch)\n",
      "epoch 280, step 900 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.64\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.70\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.603816986083984\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 281, step 50 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 281, step 100 / 900, loss = 2.13 (0.022 sec/batch)\n",
      "epoch 281, step 150 / 900, loss = 1.26 (0.021 sec/batch)\n",
      "epoch 281, step 200 / 900, loss = 1.77 (0.021 sec/batch)\n",
      "epoch 281, step 250 / 900, loss = 2.05 (0.022 sec/batch)\n",
      "epoch 281, step 300 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "epoch 281, step 350 / 900, loss = 2.56 (0.022 sec/batch)\n",
      "epoch 281, step 400 / 900, loss = 2.28 (0.022 sec/batch)\n",
      "epoch 281, step 450 / 900, loss = 1.67 (0.021 sec/batch)\n",
      "epoch 281, step 500 / 900, loss = 1.67 (0.021 sec/batch)\n",
      "epoch 281, step 550 / 900, loss = 2.93 (0.021 sec/batch)\n",
      "epoch 281, step 600 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 281, step 650 / 900, loss = 1.73 (0.021 sec/batch)\n",
      "epoch 281, step 700 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 281, step 750 / 900, loss = 2.49 (0.021 sec/batch)\n",
      "epoch 281, step 800 / 900, loss = 2.33 (0.022 sec/batch)\n",
      "epoch 281, step 850 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 281, step 900 / 900, loss = 2.73 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.60\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.24\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.58798575401306\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 282, step 50 / 900, loss = 3.15 (0.021 sec/batch)\n",
      "epoch 282, step 100 / 900, loss = 2.08 (0.022 sec/batch)\n",
      "epoch 282, step 150 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 282, step 200 / 900, loss = 2.44 (0.022 sec/batch)\n",
      "epoch 282, step 250 / 900, loss = 2.71 (0.022 sec/batch)\n",
      "epoch 282, step 300 / 900, loss = 2.55 (0.022 sec/batch)\n",
      "epoch 282, step 350 / 900, loss = 3.11 (0.021 sec/batch)\n",
      "epoch 282, step 400 / 900, loss = 1.61 (0.021 sec/batch)\n",
      "epoch 282, step 450 / 900, loss = 1.86 (0.022 sec/batch)\n",
      "epoch 282, step 500 / 900, loss = 2.84 (0.021 sec/batch)\n",
      "epoch 282, step 550 / 900, loss = 1.73 (0.021 sec/batch)\n",
      "epoch 282, step 600 / 900, loss = 2.61 (0.021 sec/batch)\n",
      "epoch 282, step 650 / 900, loss = 1.40 (0.022 sec/batch)\n",
      "epoch 282, step 700 / 900, loss = 1.44 (0.023 sec/batch)\n",
      "epoch 282, step 750 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 282, step 800 / 900, loss = 2.44 (0.022 sec/batch)\n",
      "epoch 282, step 850 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 282, step 900 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.75\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.90\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.582037448883057\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 283, step 50 / 900, loss = 2.47 (0.022 sec/batch)\n",
      "epoch 283, step 100 / 900, loss = 2.53 (0.022 sec/batch)\n",
      "epoch 283, step 150 / 900, loss = 3.17 (0.021 sec/batch)\n",
      "epoch 283, step 200 / 900, loss = 2.99 (0.022 sec/batch)\n",
      "epoch 283, step 250 / 900, loss = 1.42 (0.021 sec/batch)\n",
      "epoch 283, step 300 / 900, loss = 1.56 (0.021 sec/batch)\n",
      "epoch 283, step 350 / 900, loss = 2.16 (0.022 sec/batch)\n",
      "epoch 283, step 400 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 283, step 450 / 900, loss = 1.95 (0.021 sec/batch)\n",
      "epoch 283, step 500 / 900, loss = 2.66 (0.021 sec/batch)\n",
      "epoch 283, step 550 / 900, loss = 2.88 (0.021 sec/batch)\n",
      "epoch 283, step 600 / 900, loss = 1.46 (0.022 sec/batch)\n",
      "epoch 283, step 650 / 900, loss = 2.64 (0.021 sec/batch)\n",
      "epoch 283, step 700 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "epoch 283, step 750 / 900, loss = 1.92 (0.022 sec/batch)\n",
      "epoch 283, step 800 / 900, loss = 1.83 (0.022 sec/batch)\n",
      "epoch 283, step 850 / 900, loss = 2.83 (0.021 sec/batch)\n",
      "epoch 283, step 900 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.56\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.40\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.57834219932556\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 284, step 50 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 284, step 100 / 900, loss = 2.72 (0.021 sec/batch)\n",
      "epoch 284, step 150 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "epoch 284, step 200 / 900, loss = 2.47 (0.022 sec/batch)\n",
      "epoch 284, step 250 / 900, loss = 2.39 (0.021 sec/batch)\n",
      "epoch 284, step 300 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 284, step 350 / 900, loss = 1.65 (0.021 sec/batch)\n",
      "epoch 284, step 400 / 900, loss = 2.46 (0.021 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 284, step 450 / 900, loss = 1.84 (0.021 sec/batch)\n",
      "epoch 284, step 500 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "epoch 284, step 550 / 900, loss = 2.53 (0.022 sec/batch)\n",
      "epoch 284, step 600 / 900, loss = 2.16 (0.022 sec/batch)\n",
      "epoch 284, step 650 / 900, loss = 2.41 (0.021 sec/batch)\n",
      "epoch 284, step 700 / 900, loss = 2.47 (0.021 sec/batch)\n",
      "epoch 284, step 750 / 900, loss = 1.69 (0.021 sec/batch)\n",
      "epoch 284, step 800 / 900, loss = 2.14 (0.022 sec/batch)\n",
      "epoch 284, step 850 / 900, loss = 2.42 (0.022 sec/batch)\n",
      "epoch 284, step 900 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.64\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.72\n",
      " avg loss = 2.86\n",
      "\n",
      "Epoch time: 24.61301565170288\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 285, step 50 / 900, loss = 2.49 (0.021 sec/batch)\n",
      "epoch 285, step 100 / 900, loss = 1.88 (0.021 sec/batch)\n",
      "epoch 285, step 150 / 900, loss = 2.81 (0.022 sec/batch)\n",
      "epoch 285, step 200 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 285, step 250 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 285, step 300 / 900, loss = 2.55 (0.021 sec/batch)\n",
      "epoch 285, step 350 / 900, loss = 1.69 (0.022 sec/batch)\n",
      "epoch 285, step 400 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 285, step 450 / 900, loss = 3.09 (0.023 sec/batch)\n",
      "epoch 285, step 500 / 900, loss = 1.87 (0.022 sec/batch)\n",
      "epoch 285, step 550 / 900, loss = 2.53 (0.022 sec/batch)\n",
      "epoch 285, step 600 / 900, loss = 1.79 (0.021 sec/batch)\n",
      "epoch 285, step 650 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 285, step 700 / 900, loss = 2.56 (0.022 sec/batch)\n",
      "epoch 285, step 750 / 900, loss = 1.54 (0.021 sec/batch)\n",
      "epoch 285, step 800 / 900, loss = 2.38 (0.022 sec/batch)\n",
      "epoch 285, step 850 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 285, step 900 / 900, loss = 2.14 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.68\n",
      " avg loss = 2.28\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.44\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.601996898651123\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 286, step 50 / 900, loss = 2.21 (0.022 sec/batch)\n",
      "epoch 286, step 100 / 900, loss = 2.67 (0.022 sec/batch)\n",
      "epoch 286, step 150 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "epoch 286, step 200 / 900, loss = 2.27 (0.022 sec/batch)\n",
      "epoch 286, step 250 / 900, loss = 2.97 (0.021 sec/batch)\n",
      "epoch 286, step 300 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 286, step 350 / 900, loss = 2.44 (0.022 sec/batch)\n",
      "epoch 286, step 400 / 900, loss = 2.34 (0.021 sec/batch)\n",
      "epoch 286, step 450 / 900, loss = 1.98 (0.021 sec/batch)\n",
      "epoch 286, step 500 / 900, loss = 3.37 (0.022 sec/batch)\n",
      "epoch 286, step 550 / 900, loss = 1.43 (0.022 sec/batch)\n",
      "epoch 286, step 600 / 900, loss = 2.95 (0.021 sec/batch)\n",
      "epoch 286, step 650 / 900, loss = 2.59 (0.021 sec/batch)\n",
      "epoch 286, step 700 / 900, loss = 3.52 (0.021 sec/batch)\n",
      "epoch 286, step 750 / 900, loss = 2.54 (0.022 sec/batch)\n",
      "epoch 286, step 800 / 900, loss = 2.21 (0.022 sec/batch)\n",
      "epoch 286, step 850 / 900, loss = 2.61 (0.021 sec/batch)\n",
      "epoch 286, step 900 / 900, loss = 1.86 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.44\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.62\n",
      " avg loss = 2.82\n",
      "\n",
      "Epoch time: 24.59947633743286\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 287, step 50 / 900, loss = 2.15 (0.021 sec/batch)\n",
      "epoch 287, step 100 / 900, loss = 1.87 (0.021 sec/batch)\n",
      "epoch 287, step 150 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "epoch 287, step 200 / 900, loss = 3.35 (0.022 sec/batch)\n",
      "epoch 287, step 250 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 287, step 300 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "epoch 287, step 350 / 900, loss = 2.17 (0.022 sec/batch)\n",
      "epoch 287, step 400 / 900, loss = 3.12 (0.021 sec/batch)\n",
      "epoch 287, step 450 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 287, step 500 / 900, loss = 2.78 (0.021 sec/batch)\n",
      "epoch 287, step 550 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 287, step 600 / 900, loss = 1.78 (0.022 sec/batch)\n",
      "epoch 287, step 650 / 900, loss = 1.62 (0.021 sec/batch)\n",
      "epoch 287, step 700 / 900, loss = 1.51 (0.021 sec/batch)\n",
      "epoch 287, step 750 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 287, step 800 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 287, step 850 / 900, loss = 1.91 (0.021 sec/batch)\n",
      "epoch 287, step 900 / 900, loss = 1.66 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.56\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.76\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.576125383377075\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 288, step 50 / 900, loss = 2.65 (0.022 sec/batch)\n",
      "epoch 288, step 100 / 900, loss = 1.84 (0.022 sec/batch)\n",
      "epoch 288, step 150 / 900, loss = 2.32 (0.021 sec/batch)\n",
      "epoch 288, step 200 / 900, loss = 1.61 (0.021 sec/batch)\n",
      "epoch 288, step 250 / 900, loss = 2.75 (0.021 sec/batch)\n",
      "epoch 288, step 300 / 900, loss = 2.06 (0.021 sec/batch)\n",
      "epoch 288, step 350 / 900, loss = 1.76 (0.022 sec/batch)\n",
      "epoch 288, step 400 / 900, loss = 1.49 (0.021 sec/batch)\n",
      "epoch 288, step 450 / 900, loss = 2.45 (0.021 sec/batch)\n",
      "epoch 288, step 500 / 900, loss = 2.41 (0.022 sec/batch)\n",
      "epoch 288, step 550 / 900, loss = 2.39 (0.022 sec/batch)\n",
      "epoch 288, step 600 / 900, loss = 2.47 (0.021 sec/batch)\n",
      "epoch 288, step 650 / 900, loss = 1.50 (0.021 sec/batch)\n",
      "epoch 288, step 700 / 900, loss = 2.66 (0.021 sec/batch)\n",
      "epoch 288, step 750 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "epoch 288, step 800 / 900, loss = 2.40 (0.021 sec/batch)\n",
      "epoch 288, step 850 / 900, loss = 2.13 (0.022 sec/batch)\n",
      "epoch 288, step 900 / 900, loss = 2.34 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.37\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.22\n",
      " avg loss = 2.92\n",
      "\n",
      "Epoch time: 24.59639024734497\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 289, step 50 / 900, loss = 2.42 (0.022 sec/batch)\n",
      "epoch 289, step 100 / 900, loss = 1.75 (0.023 sec/batch)\n",
      "epoch 289, step 150 / 900, loss = 1.34 (0.022 sec/batch)\n",
      "epoch 289, step 200 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "epoch 289, step 250 / 900, loss = 1.91 (0.021 sec/batch)\n",
      "epoch 289, step 300 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 289, step 350 / 900, loss = 1.66 (0.021 sec/batch)\n",
      "epoch 289, step 400 / 900, loss = 2.93 (0.021 sec/batch)\n",
      "epoch 289, step 450 / 900, loss = 1.73 (0.021 sec/batch)\n",
      "epoch 289, step 500 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "epoch 289, step 550 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "epoch 289, step 600 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 289, step 650 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 289, step 700 / 900, loss = 1.93 (0.022 sec/batch)\n",
      "epoch 289, step 750 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "epoch 289, step 800 / 900, loss = 2.25 (0.022 sec/batch)\n",
      "epoch 289, step 850 / 900, loss = 2.83 (0.021 sec/batch)\n",
      "epoch 289, step 900 / 900, loss = 1.69 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.69\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.62\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.62825322151184\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 290, step 50 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 290, step 100 / 900, loss = 2.03 (0.022 sec/batch)\n",
      "epoch 290, step 150 / 900, loss = 2.82 (0.021 sec/batch)\n",
      "epoch 290, step 200 / 900, loss = 2.60 (0.021 sec/batch)\n",
      "epoch 290, step 250 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "epoch 290, step 300 / 900, loss = 1.85 (0.021 sec/batch)\n",
      "epoch 290, step 350 / 900, loss = 1.60 (0.022 sec/batch)\n",
      "epoch 290, step 400 / 900, loss = 2.54 (0.021 sec/batch)\n",
      "epoch 290, step 450 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "epoch 290, step 500 / 900, loss = 2.21 (0.022 sec/batch)\n",
      "epoch 290, step 550 / 900, loss = 2.74 (0.021 sec/batch)\n",
      "epoch 290, step 600 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "epoch 290, step 650 / 900, loss = 2.62 (0.021 sec/batch)\n",
      "epoch 290, step 700 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 290, step 750 / 900, loss = 1.82 (0.022 sec/batch)\n",
      "epoch 290, step 800 / 900, loss = 2.75 (0.021 sec/batch)\n",
      "epoch 290, step 850 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "epoch 290, step 900 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.32\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.74\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.598275661468506\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 291, step 50 / 900, loss = 3.35 (0.022 sec/batch)\n",
      "epoch 291, step 100 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 291, step 150 / 900, loss = 1.93 (0.021 sec/batch)\n",
      "epoch 291, step 200 / 900, loss = 2.87 (0.021 sec/batch)\n",
      "epoch 291, step 250 / 900, loss = 1.82 (0.021 sec/batch)\n",
      "epoch 291, step 300 / 900, loss = 1.55 (0.022 sec/batch)\n",
      "epoch 291, step 350 / 900, loss = 2.46 (0.022 sec/batch)\n",
      "epoch 291, step 400 / 900, loss = 2.41 (0.020 sec/batch)\n",
      "epoch 291, step 450 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 291, step 500 / 900, loss = 2.68 (0.021 sec/batch)\n",
      "epoch 291, step 550 / 900, loss = 1.82 (0.021 sec/batch)\n",
      "epoch 291, step 600 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 291, step 650 / 900, loss = 1.78 (0.021 sec/batch)\n",
      "epoch 291, step 700 / 900, loss = 2.69 (0.021 sec/batch)\n",
      "epoch 291, step 750 / 900, loss = 2.83 (0.021 sec/batch)\n",
      "epoch 291, step 800 / 900, loss = 1.66 (0.021 sec/batch)\n",
      "epoch 291, step 850 / 900, loss = 2.10 (0.022 sec/batch)\n",
      "epoch 291, step 900 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.68\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.34\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.59247636795044\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 292, step 50 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "epoch 292, step 100 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 292, step 150 / 900, loss = 2.47 (0.021 sec/batch)\n",
      "epoch 292, step 200 / 900, loss = 2.09 (0.022 sec/batch)\n",
      "epoch 292, step 250 / 900, loss = 1.07 (0.021 sec/batch)\n",
      "epoch 292, step 300 / 900, loss = 2.59 (0.021 sec/batch)\n",
      "epoch 292, step 350 / 900, loss = 2.64 (0.022 sec/batch)\n",
      "epoch 292, step 400 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 292, step 450 / 900, loss = 2.81 (0.022 sec/batch)\n",
      "epoch 292, step 500 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 292, step 550 / 900, loss = 1.80 (0.022 sec/batch)\n",
      "epoch 292, step 600 / 900, loss = 1.25 (0.022 sec/batch)\n",
      "epoch 292, step 650 / 900, loss = 1.99 (0.021 sec/batch)\n",
      "epoch 292, step 700 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 292, step 750 / 900, loss = 2.45 (0.022 sec/batch)\n",
      "epoch 292, step 800 / 900, loss = 1.91 (0.021 sec/batch)\n",
      "epoch 292, step 850 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 292, step 900 / 900, loss = 2.57 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.89\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.34\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.59858465194702\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 293, step 50 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 293, step 100 / 900, loss = 1.35 (0.021 sec/batch)\n",
      "epoch 293, step 150 / 900, loss = 2.14 (0.022 sec/batch)\n",
      "epoch 293, step 200 / 900, loss = 1.62 (0.021 sec/batch)\n",
      "epoch 293, step 250 / 900, loss = 0.97 (0.021 sec/batch)\n",
      "epoch 293, step 300 / 900, loss = 2.05 (0.022 sec/batch)\n",
      "epoch 293, step 350 / 900, loss = 2.55 (0.022 sec/batch)\n",
      "epoch 293, step 400 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 293, step 450 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 293, step 500 / 900, loss = 2.04 (0.021 sec/batch)\n",
      "epoch 293, step 550 / 900, loss = 2.09 (0.022 sec/batch)\n",
      "epoch 293, step 600 / 900, loss = 3.44 (0.021 sec/batch)\n",
      "epoch 293, step 650 / 900, loss = 2.71 (0.021 sec/batch)\n",
      "epoch 293, step 700 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 293, step 750 / 900, loss = 2.10 (0.022 sec/batch)\n",
      "epoch 293, step 800 / 900, loss = 1.63 (0.021 sec/batch)\n",
      "epoch 293, step 850 / 900, loss = 3.10 (0.022 sec/batch)\n",
      "epoch 293, step 900 / 900, loss = 2.08 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.67\n",
      " avg loss = 2.24\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.66\n",
      " avg loss = 2.93\n",
      "\n",
      "Epoch time: 24.588454246520996\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 294, step 50 / 900, loss = 2.39 (0.021 sec/batch)\n",
      "epoch 294, step 100 / 900, loss = 2.44 (0.021 sec/batch)\n",
      "epoch 294, step 150 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 294, step 200 / 900, loss = 1.44 (0.022 sec/batch)\n",
      "epoch 294, step 250 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "epoch 294, step 300 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 294, step 350 / 900, loss = 2.54 (0.022 sec/batch)\n",
      "epoch 294, step 400 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 294, step 450 / 900, loss = 1.56 (0.021 sec/batch)\n",
      "epoch 294, step 500 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 294, step 550 / 900, loss = 2.41 (0.021 sec/batch)\n",
      "epoch 294, step 600 / 900, loss = 2.26 (0.021 sec/batch)\n",
      "epoch 294, step 650 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 294, step 700 / 900, loss = 1.78 (0.021 sec/batch)\n",
      "epoch 294, step 750 / 900, loss = 2.34 (0.022 sec/batch)\n",
      "epoch 294, step 800 / 900, loss = 1.85 (0.022 sec/batch)\n",
      "epoch 294, step 850 / 900, loss = 1.43 (0.021 sec/batch)\n",
      "epoch 294, step 900 / 900, loss = 1.96 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.78\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.10\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.60690212249756\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 295, step 50 / 900, loss = 1.79 (0.021 sec/batch)\n",
      "epoch 295, step 100 / 900, loss = 1.38 (0.021 sec/batch)\n",
      "epoch 295, step 150 / 900, loss = 1.44 (0.021 sec/batch)\n",
      "epoch 295, step 200 / 900, loss = 1.35 (0.021 sec/batch)\n",
      "epoch 295, step 250 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "epoch 295, step 300 / 900, loss = 3.33 (0.021 sec/batch)\n",
      "epoch 295, step 350 / 900, loss = 2.08 (0.022 sec/batch)\n",
      "epoch 295, step 400 / 900, loss = 3.02 (0.022 sec/batch)\n",
      "epoch 295, step 450 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "epoch 295, step 500 / 900, loss = 2.36 (0.021 sec/batch)\n",
      "epoch 295, step 550 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 295, step 600 / 900, loss = 2.96 (0.022 sec/batch)\n",
      "epoch 295, step 650 / 900, loss = 2.50 (0.022 sec/batch)\n",
      "epoch 295, step 700 / 900, loss = 2.54 (0.021 sec/batch)\n",
      "epoch 295, step 750 / 900, loss = 2.52 (0.022 sec/batch)\n",
      "epoch 295, step 800 / 900, loss = 1.87 (0.022 sec/batch)\n",
      "epoch 295, step 850 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 295, step 900 / 900, loss = 2.97 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.78\n",
      " avg loss = 2.28\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.60\n",
      " avg loss = 2.86\n",
      "\n",
      "Epoch time: 24.58879041671753\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 296, step 50 / 900, loss = 2.69 (0.021 sec/batch)\n",
      "epoch 296, step 100 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "epoch 296, step 150 / 900, loss = 2.58 (0.022 sec/batch)\n",
      "epoch 296, step 200 / 900, loss = 1.95 (0.021 sec/batch)\n",
      "epoch 296, step 250 / 900, loss = 2.10 (0.022 sec/batch)\n",
      "epoch 296, step 300 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 296, step 350 / 900, loss = 1.72 (0.021 sec/batch)\n",
      "epoch 296, step 400 / 900, loss = 3.00 (0.021 sec/batch)\n",
      "epoch 296, step 450 / 900, loss = 2.71 (0.022 sec/batch)\n",
      "epoch 296, step 500 / 900, loss = 2.46 (0.021 sec/batch)\n",
      "epoch 296, step 550 / 900, loss = 1.80 (0.022 sec/batch)\n",
      "epoch 296, step 600 / 900, loss = 1.59 (0.021 sec/batch)\n",
      "epoch 296, step 650 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "epoch 296, step 700 / 900, loss = 3.07 (0.021 sec/batch)\n",
      "epoch 296, step 750 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 296, step 800 / 900, loss = 2.35 (0.022 sec/batch)\n",
      "epoch 296, step 850 / 900, loss = 3.11 (0.021 sec/batch)\n",
      "epoch 296, step 900 / 900, loss = 2.51 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.50\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.96\n",
      " avg loss = 2.85\n",
      "\n",
      "Epoch time: 24.583905696868896\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 297, step 50 / 900, loss = 2.36 (0.021 sec/batch)\n",
      "epoch 297, step 100 / 900, loss = 2.78 (0.021 sec/batch)\n",
      "epoch 297, step 150 / 900, loss = 2.92 (0.021 sec/batch)\n",
      "epoch 297, step 200 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 297, step 250 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 297, step 300 / 900, loss = 3.65 (0.022 sec/batch)\n",
      "epoch 297, step 350 / 900, loss = 3.02 (0.022 sec/batch)\n",
      "epoch 297, step 400 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "epoch 297, step 450 / 900, loss = 1.99 (0.021 sec/batch)\n",
      "epoch 297, step 500 / 900, loss = 2.49 (0.021 sec/batch)\n",
      "epoch 297, step 550 / 900, loss = 2.56 (0.022 sec/batch)\n",
      "epoch 297, step 600 / 900, loss = 1.39 (0.022 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 297, step 650 / 900, loss = 1.86 (0.022 sec/batch)\n",
      "epoch 297, step 700 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 297, step 750 / 900, loss = 2.13 (0.022 sec/batch)\n",
      "epoch 297, step 800 / 900, loss = 2.94 (0.022 sec/batch)\n",
      "epoch 297, step 850 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "epoch 297, step 900 / 900, loss = 2.64 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.45\n",
      " avg loss = 2.28\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.20\n",
      " avg loss = 2.84\n",
      "\n",
      "Epoch time: 24.576401948928833\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 298, step 50 / 900, loss = 2.01 (0.022 sec/batch)\n",
      "epoch 298, step 100 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 298, step 150 / 900, loss = 1.87 (0.021 sec/batch)\n",
      "epoch 298, step 200 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "epoch 298, step 250 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "epoch 298, step 300 / 900, loss = 3.20 (0.021 sec/batch)\n",
      "epoch 298, step 350 / 900, loss = 1.68 (0.021 sec/batch)\n",
      "epoch 298, step 400 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 298, step 450 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 298, step 500 / 900, loss = 1.77 (0.021 sec/batch)\n",
      "epoch 298, step 550 / 900, loss = 1.62 (0.020 sec/batch)\n",
      "epoch 298, step 600 / 900, loss = 1.84 (0.022 sec/batch)\n",
      "epoch 298, step 650 / 900, loss = 2.31 (0.022 sec/batch)\n",
      "epoch 298, step 700 / 900, loss = 2.47 (0.022 sec/batch)\n",
      "epoch 298, step 750 / 900, loss = 1.38 (0.021 sec/batch)\n",
      "epoch 298, step 800 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 298, step 850 / 900, loss = 2.84 (0.021 sec/batch)\n",
      "epoch 298, step 900 / 900, loss = 1.79 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.55\n",
      " avg loss = 2.28\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.64\n",
      " avg loss = 2.83\n",
      "\n",
      "Epoch time: 24.59357213973999\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 299, step 50 / 900, loss = 1.96 (0.021 sec/batch)\n",
      "epoch 299, step 100 / 900, loss = 2.21 (0.022 sec/batch)\n",
      "epoch 299, step 150 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 299, step 200 / 900, loss = 1.63 (0.021 sec/batch)\n",
      "epoch 299, step 250 / 900, loss = 2.01 (0.022 sec/batch)\n",
      "epoch 299, step 300 / 900, loss = 1.73 (0.021 sec/batch)\n",
      "epoch 299, step 350 / 900, loss = 2.13 (0.022 sec/batch)\n",
      "epoch 299, step 400 / 900, loss = 2.56 (0.021 sec/batch)\n",
      "epoch 299, step 450 / 900, loss = 3.36 (0.022 sec/batch)\n",
      "epoch 299, step 500 / 900, loss = 1.72 (0.021 sec/batch)\n",
      "epoch 299, step 550 / 900, loss = 2.74 (0.022 sec/batch)\n",
      "epoch 299, step 600 / 900, loss = 1.32 (0.022 sec/batch)\n",
      "epoch 299, step 650 / 900, loss = 3.14 (0.021 sec/batch)\n",
      "epoch 299, step 700 / 900, loss = 2.89 (0.021 sec/batch)\n",
      "epoch 299, step 750 / 900, loss = 2.24 (0.021 sec/batch)\n",
      "epoch 299, step 800 / 900, loss = 2.97 (0.022 sec/batch)\n",
      "epoch 299, step 850 / 900, loss = 2.38 (0.022 sec/batch)\n",
      "epoch 299, step 900 / 900, loss = 1.22 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.73\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.44\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.60391330718994\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 300, step 50 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 300, step 100 / 900, loss = 1.72 (0.021 sec/batch)\n",
      "epoch 300, step 150 / 900, loss = 2.62 (0.021 sec/batch)\n",
      "epoch 300, step 200 / 900, loss = 2.73 (0.021 sec/batch)\n",
      "epoch 300, step 250 / 900, loss = 2.71 (0.021 sec/batch)\n",
      "epoch 300, step 300 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 300, step 350 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "epoch 300, step 400 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 300, step 450 / 900, loss = 2.82 (0.022 sec/batch)\n",
      "epoch 300, step 500 / 900, loss = 1.85 (0.021 sec/batch)\n",
      "epoch 300, step 550 / 900, loss = 2.57 (0.021 sec/batch)\n",
      "epoch 300, step 600 / 900, loss = 2.32 (0.022 sec/batch)\n",
      "epoch 300, step 650 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 300, step 700 / 900, loss = 1.54 (0.021 sec/batch)\n",
      "epoch 300, step 750 / 900, loss = 2.71 (0.022 sec/batch)\n",
      "epoch 300, step 800 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 300, step 850 / 900, loss = 1.77 (0.021 sec/batch)\n",
      "epoch 300, step 900 / 900, loss = 1.79 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.46\n",
      " avg loss = 2.29\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.30\n",
      " avg loss = 2.85\n",
      "\n",
      "Epoch time: 24.572275400161743\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 301, step 50 / 900, loss = 1.10 (0.022 sec/batch)\n",
      "epoch 301, step 100 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 301, step 150 / 900, loss = 1.00 (0.021 sec/batch)\n",
      "epoch 301, step 200 / 900, loss = 2.35 (0.022 sec/batch)\n",
      "epoch 301, step 250 / 900, loss = 1.79 (0.021 sec/batch)\n",
      "epoch 301, step 300 / 900, loss = 1.70 (0.022 sec/batch)\n",
      "epoch 301, step 350 / 900, loss = 2.24 (0.022 sec/batch)\n",
      "epoch 301, step 400 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 301, step 450 / 900, loss = 1.78 (0.022 sec/batch)\n",
      "epoch 301, step 500 / 900, loss = 2.76 (0.021 sec/batch)\n",
      "epoch 301, step 550 / 900, loss = 2.51 (0.021 sec/batch)\n",
      "epoch 301, step 600 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 301, step 650 / 900, loss = 1.60 (0.021 sec/batch)\n",
      "epoch 301, step 700 / 900, loss = 2.27 (0.022 sec/batch)\n",
      "epoch 301, step 750 / 900, loss = 2.55 (0.021 sec/batch)\n",
      "epoch 301, step 800 / 900, loss = 2.32 (0.022 sec/batch)\n",
      "epoch 301, step 850 / 900, loss = 1.71 (0.021 sec/batch)\n",
      "epoch 301, step 900 / 900, loss = 1.82 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.44\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.76\n",
      " avg loss = 2.86\n",
      "\n",
      "Epoch time: 24.590002059936523\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 302, step 50 / 900, loss = 3.01 (0.021 sec/batch)\n",
      "epoch 302, step 100 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 302, step 150 / 900, loss = 1.95 (0.021 sec/batch)\n",
      "epoch 302, step 200 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 302, step 250 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "epoch 302, step 300 / 900, loss = 2.63 (0.021 sec/batch)\n",
      "epoch 302, step 350 / 900, loss = 2.08 (0.022 sec/batch)\n",
      "epoch 302, step 400 / 900, loss = 1.60 (0.021 sec/batch)\n",
      "epoch 302, step 450 / 900, loss = 2.71 (0.021 sec/batch)\n",
      "epoch 302, step 500 / 900, loss = 1.80 (0.021 sec/batch)\n",
      "epoch 302, step 550 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 302, step 600 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 302, step 650 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 302, step 700 / 900, loss = 1.67 (0.022 sec/batch)\n",
      "epoch 302, step 750 / 900, loss = 3.22 (0.022 sec/batch)\n",
      "epoch 302, step 800 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 302, step 850 / 900, loss = 2.36 (0.022 sec/batch)\n",
      "epoch 302, step 900 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.14\n",
      " avg loss = 2.28\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.34\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.586795330047607\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 303, step 50 / 900, loss = 2.19 (0.022 sec/batch)\n",
      "epoch 303, step 100 / 900, loss = 2.31 (0.022 sec/batch)\n",
      "epoch 303, step 150 / 900, loss = 1.54 (0.021 sec/batch)\n",
      "epoch 303, step 200 / 900, loss = 1.69 (0.021 sec/batch)\n",
      "epoch 303, step 250 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 303, step 300 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 303, step 350 / 900, loss = 2.48 (0.022 sec/batch)\n",
      "epoch 303, step 400 / 900, loss = 2.26 (0.021 sec/batch)\n",
      "epoch 303, step 450 / 900, loss = 2.58 (0.021 sec/batch)\n",
      "epoch 303, step 500 / 900, loss = 2.32 (0.022 sec/batch)\n",
      "epoch 303, step 550 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "epoch 303, step 600 / 900, loss = 1.36 (0.022 sec/batch)\n",
      "epoch 303, step 650 / 900, loss = 2.65 (0.022 sec/batch)\n",
      "epoch 303, step 700 / 900, loss = 2.81 (0.021 sec/batch)\n",
      "epoch 303, step 750 / 900, loss = 3.04 (0.022 sec/batch)\n",
      "epoch 303, step 800 / 900, loss = 2.24 (0.021 sec/batch)\n",
      "epoch 303, step 850 / 900, loss = 1.77 (0.021 sec/batch)\n",
      "epoch 303, step 900 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.70\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.68\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.602556943893433\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 304, step 50 / 900, loss = 1.56 (0.021 sec/batch)\n",
      "epoch 304, step 100 / 900, loss = 2.23 (0.021 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 304, step 150 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 304, step 200 / 900, loss = 2.27 (0.022 sec/batch)\n",
      "epoch 304, step 250 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 304, step 300 / 900, loss = 2.80 (0.021 sec/batch)\n",
      "epoch 304, step 350 / 900, loss = 2.61 (0.022 sec/batch)\n",
      "epoch 304, step 400 / 900, loss = 2.05 (0.022 sec/batch)\n",
      "epoch 304, step 450 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 304, step 500 / 900, loss = 2.66 (0.022 sec/batch)\n",
      "epoch 304, step 550 / 900, loss = 2.64 (0.022 sec/batch)\n",
      "epoch 304, step 600 / 900, loss = 1.72 (0.021 sec/batch)\n",
      "epoch 304, step 650 / 900, loss = 2.48 (0.022 sec/batch)\n",
      "epoch 304, step 700 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 304, step 750 / 900, loss = 2.64 (0.021 sec/batch)\n",
      "epoch 304, step 800 / 900, loss = 2.24 (0.022 sec/batch)\n",
      "epoch 304, step 850 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "epoch 304, step 900 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.67\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.80\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.594197988510132\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 305, step 50 / 900, loss = 1.76 (0.021 sec/batch)\n",
      "epoch 305, step 100 / 900, loss = 1.67 (0.021 sec/batch)\n",
      "epoch 305, step 150 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "epoch 305, step 200 / 900, loss = 2.70 (0.021 sec/batch)\n",
      "epoch 305, step 250 / 900, loss = 1.94 (0.022 sec/batch)\n",
      "epoch 305, step 300 / 900, loss = 2.82 (0.021 sec/batch)\n",
      "epoch 305, step 350 / 900, loss = 3.13 (0.021 sec/batch)\n",
      "epoch 305, step 400 / 900, loss = 3.02 (0.022 sec/batch)\n",
      "epoch 305, step 450 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 305, step 500 / 900, loss = 2.40 (0.022 sec/batch)\n",
      "epoch 305, step 550 / 900, loss = 1.76 (0.021 sec/batch)\n",
      "epoch 305, step 600 / 900, loss = 2.24 (0.021 sec/batch)\n",
      "epoch 305, step 650 / 900, loss = 2.91 (0.022 sec/batch)\n",
      "epoch 305, step 700 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "epoch 305, step 750 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 305, step 800 / 900, loss = 2.75 (0.022 sec/batch)\n",
      "epoch 305, step 850 / 900, loss = 1.94 (0.022 sec/batch)\n",
      "epoch 305, step 900 / 900, loss = 1.39 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.66\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.06\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.59218192100525\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 306, step 50 / 900, loss = 1.92 (0.021 sec/batch)\n",
      "epoch 306, step 100 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 306, step 150 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 306, step 200 / 900, loss = 1.84 (0.022 sec/batch)\n",
      "epoch 306, step 250 / 900, loss = 2.54 (0.021 sec/batch)\n",
      "epoch 306, step 300 / 900, loss = 2.23 (0.022 sec/batch)\n",
      "epoch 306, step 350 / 900, loss = 2.86 (0.022 sec/batch)\n",
      "epoch 306, step 400 / 900, loss = 2.39 (0.021 sec/batch)\n",
      "epoch 306, step 450 / 900, loss = 2.78 (0.022 sec/batch)\n",
      "epoch 306, step 500 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "epoch 306, step 550 / 900, loss = 1.58 (0.021 sec/batch)\n",
      "epoch 306, step 600 / 900, loss = 2.24 (0.022 sec/batch)\n",
      "epoch 306, step 650 / 900, loss = 2.60 (0.021 sec/batch)\n",
      "epoch 306, step 700 / 900, loss = 2.18 (0.022 sec/batch)\n",
      "epoch 306, step 750 / 900, loss = 2.93 (0.022 sec/batch)\n",
      "epoch 306, step 800 / 900, loss = 2.47 (0.021 sec/batch)\n",
      "epoch 306, step 850 / 900, loss = 3.31 (0.021 sec/batch)\n",
      "epoch 306, step 900 / 900, loss = 2.49 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.89\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.66\n",
      " avg loss = 2.92\n",
      "\n",
      "Epoch time: 24.640071630477905\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 307, step 50 / 900, loss = 2.24 (0.022 sec/batch)\n",
      "epoch 307, step 100 / 900, loss = 1.81 (0.021 sec/batch)\n",
      "epoch 307, step 150 / 900, loss = 1.41 (0.021 sec/batch)\n",
      "epoch 307, step 200 / 900, loss = 3.20 (0.021 sec/batch)\n",
      "epoch 307, step 250 / 900, loss = 1.82 (0.021 sec/batch)\n",
      "epoch 307, step 300 / 900, loss = 3.10 (0.021 sec/batch)\n",
      "epoch 307, step 350 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 307, step 400 / 900, loss = 2.62 (0.021 sec/batch)\n",
      "epoch 307, step 450 / 900, loss = 2.50 (0.021 sec/batch)\n",
      "epoch 307, step 500 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 307, step 550 / 900, loss = 1.70 (0.021 sec/batch)\n",
      "epoch 307, step 600 / 900, loss = 2.23 (0.022 sec/batch)\n",
      "epoch 307, step 650 / 900, loss = 2.55 (0.022 sec/batch)\n",
      "epoch 307, step 700 / 900, loss = 1.98 (0.021 sec/batch)\n",
      "epoch 307, step 750 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 307, step 800 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 307, step 850 / 900, loss = 2.80 (0.021 sec/batch)\n",
      "epoch 307, step 900 / 900, loss = 2.53 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.79\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.14\n",
      " avg loss = 2.83\n",
      "\n",
      "Epoch time: 24.569308757781982\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 308, step 50 / 900, loss = 1.60 (0.021 sec/batch)\n",
      "epoch 308, step 100 / 900, loss = 2.44 (0.021 sec/batch)\n",
      "epoch 308, step 150 / 900, loss = 2.59 (0.021 sec/batch)\n",
      "epoch 308, step 200 / 900, loss = 2.45 (0.021 sec/batch)\n",
      "epoch 308, step 250 / 900, loss = 2.45 (0.022 sec/batch)\n",
      "epoch 308, step 300 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 308, step 350 / 900, loss = 1.59 (0.022 sec/batch)\n",
      "epoch 308, step 400 / 900, loss = 3.49 (0.020 sec/batch)\n",
      "epoch 308, step 450 / 900, loss = 1.81 (0.021 sec/batch)\n",
      "epoch 308, step 500 / 900, loss = 1.83 (0.023 sec/batch)\n",
      "epoch 308, step 550 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "epoch 308, step 600 / 900, loss = 2.16 (0.022 sec/batch)\n",
      "epoch 308, step 650 / 900, loss = 1.90 (0.021 sec/batch)\n",
      "epoch 308, step 700 / 900, loss = 1.96 (0.021 sec/batch)\n",
      "epoch 308, step 750 / 900, loss = 2.88 (0.022 sec/batch)\n",
      "epoch 308, step 800 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 308, step 850 / 900, loss = 2.90 (0.022 sec/batch)\n",
      "epoch 308, step 900 / 900, loss = 1.78 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.35\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.70\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.647982358932495\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 309, step 50 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "epoch 309, step 100 / 900, loss = 2.08 (0.022 sec/batch)\n",
      "epoch 309, step 150 / 900, loss = 2.80 (0.021 sec/batch)\n",
      "epoch 309, step 200 / 900, loss = 1.95 (0.021 sec/batch)\n",
      "epoch 309, step 250 / 900, loss = 1.90 (0.021 sec/batch)\n",
      "epoch 309, step 300 / 900, loss = 2.65 (0.022 sec/batch)\n",
      "epoch 309, step 350 / 900, loss = 2.54 (0.021 sec/batch)\n",
      "epoch 309, step 400 / 900, loss = 1.71 (0.021 sec/batch)\n",
      "epoch 309, step 450 / 900, loss = 3.32 (0.021 sec/batch)\n",
      "epoch 309, step 500 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 309, step 550 / 900, loss = 3.13 (0.021 sec/batch)\n",
      "epoch 309, step 600 / 900, loss = 2.85 (0.022 sec/batch)\n",
      "epoch 309, step 650 / 900, loss = 2.32 (0.021 sec/batch)\n",
      "epoch 309, step 700 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 309, step 750 / 900, loss = 1.71 (0.022 sec/batch)\n",
      "epoch 309, step 800 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 309, step 850 / 900, loss = 2.84 (0.022 sec/batch)\n",
      "epoch 309, step 900 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.79\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.76\n",
      " avg loss = 2.82\n",
      "\n",
      "Epoch time: 24.589276790618896\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 310, step 50 / 900, loss = 3.42 (0.022 sec/batch)\n",
      "epoch 310, step 100 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 310, step 150 / 900, loss = 2.78 (0.021 sec/batch)\n",
      "epoch 310, step 200 / 900, loss = 1.85 (0.021 sec/batch)\n",
      "epoch 310, step 250 / 900, loss = 2.51 (0.021 sec/batch)\n",
      "epoch 310, step 300 / 900, loss = 1.75 (0.021 sec/batch)\n",
      "epoch 310, step 350 / 900, loss = 1.25 (0.021 sec/batch)\n",
      "epoch 310, step 400 / 900, loss = 1.76 (0.021 sec/batch)\n",
      "epoch 310, step 450 / 900, loss = 1.74 (0.021 sec/batch)\n",
      "epoch 310, step 500 / 900, loss = 1.67 (0.021 sec/batch)\n",
      "epoch 310, step 550 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 310, step 600 / 900, loss = 1.55 (0.022 sec/batch)\n",
      "epoch 310, step 650 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 310, step 700 / 900, loss = 1.89 (0.021 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 310, step 750 / 900, loss = 1.84 (0.021 sec/batch)\n",
      "epoch 310, step 800 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "epoch 310, step 850 / 900, loss = 2.50 (0.022 sec/batch)\n",
      "epoch 310, step 900 / 900, loss = 2.70 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.79\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.14\n",
      " avg loss = 2.86\n",
      "\n",
      "Epoch time: 24.594358444213867\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 311, step 50 / 900, loss = 2.41 (0.022 sec/batch)\n",
      "epoch 311, step 100 / 900, loss = 2.63 (0.022 sec/batch)\n",
      "epoch 311, step 150 / 900, loss = 2.62 (0.022 sec/batch)\n",
      "epoch 311, step 200 / 900, loss = 1.99 (0.021 sec/batch)\n",
      "epoch 311, step 250 / 900, loss = 2.73 (0.020 sec/batch)\n",
      "epoch 311, step 300 / 900, loss = 1.55 (0.021 sec/batch)\n",
      "epoch 311, step 350 / 900, loss = 1.93 (0.021 sec/batch)\n",
      "epoch 311, step 400 / 900, loss = 1.61 (0.021 sec/batch)\n",
      "epoch 311, step 450 / 900, loss = 2.39 (0.021 sec/batch)\n",
      "epoch 311, step 500 / 900, loss = 1.91 (0.021 sec/batch)\n",
      "epoch 311, step 550 / 900, loss = 2.50 (0.021 sec/batch)\n",
      "epoch 311, step 600 / 900, loss = 2.22 (0.022 sec/batch)\n",
      "epoch 311, step 650 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 311, step 700 / 900, loss = 2.70 (0.021 sec/batch)\n",
      "epoch 311, step 750 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "epoch 311, step 800 / 900, loss = 1.98 (0.022 sec/batch)\n",
      "epoch 311, step 850 / 900, loss = 2.52 (0.022 sec/batch)\n",
      "epoch 311, step 900 / 900, loss = 1.84 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.66\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.86\n",
      " avg loss = 2.84\n",
      "\n",
      "Epoch time: 24.58104109764099\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 312, step 50 / 900, loss = 1.58 (0.021 sec/batch)\n",
      "epoch 312, step 100 / 900, loss = 2.41 (0.021 sec/batch)\n",
      "epoch 312, step 150 / 900, loss = 1.75 (0.022 sec/batch)\n",
      "epoch 312, step 200 / 900, loss = 2.18 (0.022 sec/batch)\n",
      "epoch 312, step 250 / 900, loss = 1.97 (0.022 sec/batch)\n",
      "epoch 312, step 300 / 900, loss = 2.04 (0.021 sec/batch)\n",
      "epoch 312, step 350 / 900, loss = 1.76 (0.021 sec/batch)\n",
      "epoch 312, step 400 / 900, loss = 2.73 (0.021 sec/batch)\n",
      "epoch 312, step 450 / 900, loss = 2.06 (0.021 sec/batch)\n",
      "epoch 312, step 500 / 900, loss = 2.24 (0.021 sec/batch)\n",
      "epoch 312, step 550 / 900, loss = 1.95 (0.021 sec/batch)\n",
      "epoch 312, step 600 / 900, loss = 1.77 (0.022 sec/batch)\n",
      "epoch 312, step 650 / 900, loss = 3.83 (0.021 sec/batch)\n",
      "epoch 312, step 700 / 900, loss = 1.86 (0.021 sec/batch)\n",
      "epoch 312, step 750 / 900, loss = 1.89 (0.022 sec/batch)\n",
      "epoch 312, step 800 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 312, step 850 / 900, loss = 1.95 (0.022 sec/batch)\n",
      "epoch 312, step 900 / 900, loss = 1.65 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.72\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.66\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.573495626449585\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 313, step 50 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 313, step 100 / 900, loss = 1.79 (0.021 sec/batch)\n",
      "epoch 313, step 150 / 900, loss = 2.90 (0.022 sec/batch)\n",
      "epoch 313, step 200 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 313, step 250 / 900, loss = 2.54 (0.021 sec/batch)\n",
      "epoch 313, step 300 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 313, step 350 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 313, step 400 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 313, step 450 / 900, loss = 2.73 (0.021 sec/batch)\n",
      "epoch 313, step 500 / 900, loss = 2.78 (0.021 sec/batch)\n",
      "epoch 313, step 550 / 900, loss = 3.16 (0.021 sec/batch)\n",
      "epoch 313, step 600 / 900, loss = 2.79 (0.022 sec/batch)\n",
      "epoch 313, step 650 / 900, loss = 1.49 (0.021 sec/batch)\n",
      "epoch 313, step 700 / 900, loss = 3.02 (0.021 sec/batch)\n",
      "epoch 313, step 750 / 900, loss = 2.87 (0.022 sec/batch)\n",
      "epoch 313, step 800 / 900, loss = 1.99 (0.021 sec/batch)\n",
      "epoch 313, step 850 / 900, loss = 2.62 (0.022 sec/batch)\n",
      "epoch 313, step 900 / 900, loss = 2.20 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.64\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.28\n",
      " avg loss = 2.84\n",
      "\n",
      "Epoch time: 24.603613138198853\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 314, step 50 / 900, loss = 1.72 (0.021 sec/batch)\n",
      "epoch 314, step 100 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "epoch 314, step 150 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 314, step 200 / 900, loss = 2.19 (0.022 sec/batch)\n",
      "epoch 314, step 250 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 314, step 300 / 900, loss = 1.45 (0.022 sec/batch)\n",
      "epoch 314, step 350 / 900, loss = 2.49 (0.021 sec/batch)\n",
      "epoch 314, step 400 / 900, loss = 2.15 (0.021 sec/batch)\n",
      "epoch 314, step 450 / 900, loss = 2.01 (0.020 sec/batch)\n",
      "epoch 314, step 500 / 900, loss = 1.52 (0.022 sec/batch)\n",
      "epoch 314, step 550 / 900, loss = 3.38 (0.021 sec/batch)\n",
      "epoch 314, step 600 / 900, loss = 3.20 (0.022 sec/batch)\n",
      "epoch 314, step 650 / 900, loss = 2.12 (0.022 sec/batch)\n",
      "epoch 314, step 700 / 900, loss = 3.21 (0.021 sec/batch)\n",
      "epoch 314, step 750 / 900, loss = 1.74 (0.021 sec/batch)\n",
      "epoch 314, step 800 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 314, step 850 / 900, loss = 2.46 (0.022 sec/batch)\n",
      "epoch 314, step 900 / 900, loss = 1.86 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.26\n",
      " avg loss = 2.28\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.80\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.58039903640747\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 315, step 50 / 900, loss = 2.37 (0.022 sec/batch)\n",
      "epoch 315, step 100 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "epoch 315, step 150 / 900, loss = 1.65 (0.022 sec/batch)\n",
      "epoch 315, step 200 / 900, loss = 2.70 (0.022 sec/batch)\n",
      "epoch 315, step 250 / 900, loss = 2.51 (0.022 sec/batch)\n",
      "epoch 315, step 300 / 900, loss = 2.46 (0.021 sec/batch)\n",
      "epoch 315, step 350 / 900, loss = 1.73 (0.021 sec/batch)\n",
      "epoch 315, step 400 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 315, step 450 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "epoch 315, step 500 / 900, loss = 1.99 (0.022 sec/batch)\n",
      "epoch 315, step 550 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 315, step 600 / 900, loss = 2.85 (0.022 sec/batch)\n",
      "epoch 315, step 650 / 900, loss = 1.62 (0.021 sec/batch)\n",
      "epoch 315, step 700 / 900, loss = 2.47 (0.022 sec/batch)\n",
      "epoch 315, step 750 / 900, loss = 2.04 (0.021 sec/batch)\n",
      "epoch 315, step 800 / 900, loss = 2.85 (0.021 sec/batch)\n",
      "epoch 315, step 850 / 900, loss = 1.74 (0.022 sec/batch)\n",
      "epoch 315, step 900 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.22\n",
      " avg loss = 2.29\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.14\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.575767993927002\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 316, step 50 / 900, loss = 2.49 (0.022 sec/batch)\n",
      "epoch 316, step 100 / 900, loss = 1.54 (0.022 sec/batch)\n",
      "epoch 316, step 150 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 316, step 200 / 900, loss = 3.12 (0.021 sec/batch)\n",
      "epoch 316, step 250 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 316, step 300 / 900, loss = 1.85 (0.021 sec/batch)\n",
      "epoch 316, step 350 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 316, step 400 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "epoch 316, step 450 / 900, loss = 2.70 (0.021 sec/batch)\n",
      "epoch 316, step 500 / 900, loss = 2.57 (0.022 sec/batch)\n",
      "epoch 316, step 550 / 900, loss = 1.87 (0.022 sec/batch)\n",
      "epoch 316, step 600 / 900, loss = 2.46 (0.021 sec/batch)\n",
      "epoch 316, step 650 / 900, loss = 2.93 (0.021 sec/batch)\n",
      "epoch 316, step 700 / 900, loss = 2.85 (0.021 sec/batch)\n",
      "epoch 316, step 750 / 900, loss = 3.15 (0.021 sec/batch)\n",
      "epoch 316, step 800 / 900, loss = 2.77 (0.021 sec/batch)\n",
      "epoch 316, step 850 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 316, step 900 / 900, loss = 2.08 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.71\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.34\n",
      " avg loss = 2.83\n",
      "\n",
      "Epoch time: 24.606495141983032\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 317, step 50 / 900, loss = 2.82 (0.021 sec/batch)\n",
      "epoch 317, step 100 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 317, step 150 / 900, loss = 2.15 (0.021 sec/batch)\n",
      "epoch 317, step 200 / 900, loss = 2.39 (0.021 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 317, step 250 / 900, loss = 1.96 (0.021 sec/batch)\n",
      "epoch 317, step 300 / 900, loss = 1.66 (0.022 sec/batch)\n",
      "epoch 317, step 350 / 900, loss = 2.55 (0.021 sec/batch)\n",
      "epoch 317, step 400 / 900, loss = 1.78 (0.021 sec/batch)\n",
      "epoch 317, step 450 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 317, step 500 / 900, loss = 2.73 (0.021 sec/batch)\n",
      "epoch 317, step 550 / 900, loss = 1.42 (0.021 sec/batch)\n",
      "epoch 317, step 600 / 900, loss = 1.56 (0.021 sec/batch)\n",
      "epoch 317, step 650 / 900, loss = 1.87 (0.022 sec/batch)\n",
      "epoch 317, step 700 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 317, step 750 / 900, loss = 1.71 (0.021 sec/batch)\n",
      "epoch 317, step 800 / 900, loss = 1.57 (0.022 sec/batch)\n",
      "epoch 317, step 850 / 900, loss = 1.46 (0.022 sec/batch)\n",
      "epoch 317, step 900 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.53\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.28\n",
      " avg loss = 2.84\n",
      "\n",
      "Epoch time: 24.581920385360718\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 318, step 50 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 318, step 100 / 900, loss = 2.74 (0.021 sec/batch)\n",
      "epoch 318, step 150 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 318, step 200 / 900, loss = 2.11 (0.022 sec/batch)\n",
      "epoch 318, step 250 / 900, loss = 2.18 (0.022 sec/batch)\n",
      "epoch 318, step 300 / 900, loss = 2.92 (0.022 sec/batch)\n",
      "epoch 318, step 350 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 318, step 400 / 900, loss = 1.57 (0.021 sec/batch)\n",
      "epoch 318, step 450 / 900, loss = 3.14 (0.021 sec/batch)\n",
      "epoch 318, step 500 / 900, loss = 2.60 (0.021 sec/batch)\n",
      "epoch 318, step 550 / 900, loss = 2.98 (0.021 sec/batch)\n",
      "epoch 318, step 600 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "epoch 318, step 650 / 900, loss = 1.84 (0.021 sec/batch)\n",
      "epoch 318, step 700 / 900, loss = 1.67 (0.021 sec/batch)\n",
      "epoch 318, step 750 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 318, step 800 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 318, step 850 / 900, loss = 2.25 (0.022 sec/batch)\n",
      "epoch 318, step 900 / 900, loss = 2.33 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.67\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.14\n",
      " avg loss = 2.86\n",
      "\n",
      "Epoch time: 24.609806537628174\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 319, step 50 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 319, step 100 / 900, loss = 2.98 (0.021 sec/batch)\n",
      "epoch 319, step 150 / 900, loss = 1.59 (0.021 sec/batch)\n",
      "epoch 319, step 200 / 900, loss = 2.96 (0.022 sec/batch)\n",
      "epoch 319, step 250 / 900, loss = 1.01 (0.021 sec/batch)\n",
      "epoch 319, step 300 / 900, loss = 2.29 (0.022 sec/batch)\n",
      "epoch 319, step 350 / 900, loss = 1.98 (0.021 sec/batch)\n",
      "epoch 319, step 400 / 900, loss = 2.04 (0.022 sec/batch)\n",
      "epoch 319, step 450 / 900, loss = 2.46 (0.021 sec/batch)\n",
      "epoch 319, step 500 / 900, loss = 1.77 (0.021 sec/batch)\n",
      "epoch 319, step 550 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 319, step 600 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 319, step 650 / 900, loss = 2.05 (0.022 sec/batch)\n",
      "epoch 319, step 700 / 900, loss = 2.34 (0.021 sec/batch)\n",
      "epoch 319, step 750 / 900, loss = 3.06 (0.021 sec/batch)\n",
      "epoch 319, step 800 / 900, loss = 2.77 (0.021 sec/batch)\n",
      "epoch 319, step 850 / 900, loss = 2.35 (0.022 sec/batch)\n",
      "epoch 319, step 900 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.53\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.82\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.589515924453735\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 320, step 50 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 320, step 100 / 900, loss = 1.92 (0.021 sec/batch)\n",
      "epoch 320, step 150 / 900, loss = 2.44 (0.021 sec/batch)\n",
      "epoch 320, step 200 / 900, loss = 1.44 (0.021 sec/batch)\n",
      "epoch 320, step 250 / 900, loss = 1.98 (0.022 sec/batch)\n",
      "epoch 320, step 300 / 900, loss = 2.62 (0.021 sec/batch)\n",
      "epoch 320, step 350 / 900, loss = 2.63 (0.021 sec/batch)\n",
      "epoch 320, step 400 / 900, loss = 2.37 (0.022 sec/batch)\n",
      "epoch 320, step 450 / 900, loss = 2.75 (0.021 sec/batch)\n",
      "epoch 320, step 500 / 900, loss = 1.57 (0.021 sec/batch)\n",
      "epoch 320, step 550 / 900, loss = 3.29 (0.022 sec/batch)\n",
      "epoch 320, step 600 / 900, loss = 1.93 (0.021 sec/batch)\n",
      "epoch 320, step 650 / 900, loss = 2.96 (0.021 sec/batch)\n",
      "epoch 320, step 700 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 320, step 750 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 320, step 800 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 320, step 850 / 900, loss = 2.43 (0.022 sec/batch)\n",
      "epoch 320, step 900 / 900, loss = 2.58 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.67\n",
      " avg loss = 2.24\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.22\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.57611632347107\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 321, step 50 / 900, loss = 2.44 (0.022 sec/batch)\n",
      "epoch 321, step 100 / 900, loss = 2.58 (0.021 sec/batch)\n",
      "epoch 321, step 150 / 900, loss = 2.56 (0.022 sec/batch)\n",
      "epoch 321, step 200 / 900, loss = 2.36 (0.021 sec/batch)\n",
      "epoch 321, step 250 / 900, loss = 1.81 (0.021 sec/batch)\n",
      "epoch 321, step 300 / 900, loss = 1.67 (0.021 sec/batch)\n",
      "epoch 321, step 350 / 900, loss = 1.80 (0.022 sec/batch)\n",
      "epoch 321, step 400 / 900, loss = 1.34 (0.022 sec/batch)\n",
      "epoch 321, step 450 / 900, loss = 1.78 (0.021 sec/batch)\n",
      "epoch 321, step 500 / 900, loss = 2.93 (0.021 sec/batch)\n",
      "epoch 321, step 550 / 900, loss = 3.13 (0.021 sec/batch)\n",
      "epoch 321, step 600 / 900, loss = 2.44 (0.023 sec/batch)\n",
      "epoch 321, step 650 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 321, step 700 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 321, step 750 / 900, loss = 1.65 (0.021 sec/batch)\n",
      "epoch 321, step 800 / 900, loss = 2.82 (0.021 sec/batch)\n",
      "epoch 321, step 850 / 900, loss = 2.41 (0.022 sec/batch)\n",
      "epoch 321, step 900 / 900, loss = 2.16 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.49\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.66\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.601441860198975\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 322, step 50 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 322, step 100 / 900, loss = 1.92 (0.022 sec/batch)\n",
      "epoch 322, step 150 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 322, step 200 / 900, loss = 2.36 (0.021 sec/batch)\n",
      "epoch 322, step 250 / 900, loss = 2.76 (0.021 sec/batch)\n",
      "epoch 322, step 300 / 900, loss = 1.91 (0.021 sec/batch)\n",
      "epoch 322, step 350 / 900, loss = 1.86 (0.021 sec/batch)\n",
      "epoch 322, step 400 / 900, loss = 2.47 (0.021 sec/batch)\n",
      "epoch 322, step 450 / 900, loss = 2.09 (0.022 sec/batch)\n",
      "epoch 322, step 500 / 900, loss = 1.76 (0.021 sec/batch)\n",
      "epoch 322, step 550 / 900, loss = 2.20 (0.022 sec/batch)\n",
      "epoch 322, step 600 / 900, loss = 2.63 (0.022 sec/batch)\n",
      "epoch 322, step 650 / 900, loss = 1.60 (0.021 sec/batch)\n",
      "epoch 322, step 700 / 900, loss = 2.77 (0.021 sec/batch)\n",
      "epoch 322, step 750 / 900, loss = 1.68 (0.022 sec/batch)\n",
      "epoch 322, step 800 / 900, loss = 1.84 (0.021 sec/batch)\n",
      "epoch 322, step 850 / 900, loss = 2.36 (0.022 sec/batch)\n",
      "epoch 322, step 900 / 900, loss = 2.18 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.67\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.10\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.585972547531128\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 323, step 50 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 323, step 100 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 323, step 150 / 900, loss = 2.13 (0.022 sec/batch)\n",
      "epoch 323, step 200 / 900, loss = 2.39 (0.021 sec/batch)\n",
      "epoch 323, step 250 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 323, step 300 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "epoch 323, step 350 / 900, loss = 3.00 (0.022 sec/batch)\n",
      "epoch 323, step 400 / 900, loss = 1.80 (0.022 sec/batch)\n",
      "epoch 323, step 450 / 900, loss = 1.80 (0.021 sec/batch)\n",
      "epoch 323, step 500 / 900, loss = 2.32 (0.021 sec/batch)\n",
      "epoch 323, step 550 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 323, step 600 / 900, loss = 2.66 (0.021 sec/batch)\n",
      "epoch 323, step 650 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 323, step 700 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 323, step 750 / 900, loss = 2.74 (0.022 sec/batch)\n",
      "epoch 323, step 800 / 900, loss = 1.80 (0.022 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 323, step 850 / 900, loss = 2.04 (0.022 sec/batch)\n",
      "epoch 323, step 900 / 900, loss = 2.66 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.19\n",
      " avg loss = 2.28\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.18\n",
      " avg loss = 2.86\n",
      "\n",
      "Epoch time: 24.627753973007202\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 324, step 50 / 900, loss = 1.40 (0.021 sec/batch)\n",
      "epoch 324, step 100 / 900, loss = 2.62 (0.021 sec/batch)\n",
      "epoch 324, step 150 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 324, step 200 / 900, loss = 2.58 (0.022 sec/batch)\n",
      "epoch 324, step 250 / 900, loss = 2.85 (0.021 sec/batch)\n",
      "epoch 324, step 300 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 324, step 350 / 900, loss = 3.01 (0.021 sec/batch)\n",
      "epoch 324, step 400 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 324, step 450 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "epoch 324, step 500 / 900, loss = 3.98 (0.021 sec/batch)\n",
      "epoch 324, step 550 / 900, loss = 1.95 (0.021 sec/batch)\n",
      "epoch 324, step 600 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 324, step 650 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 324, step 700 / 900, loss = 2.86 (0.021 sec/batch)\n",
      "epoch 324, step 750 / 900, loss = 3.53 (0.022 sec/batch)\n",
      "epoch 324, step 800 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 324, step 850 / 900, loss = 2.43 (0.022 sec/batch)\n",
      "epoch 324, step 900 / 900, loss = 2.97 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.47\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.90\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.58079195022583\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 325, step 50 / 900, loss = 3.27 (0.021 sec/batch)\n",
      "epoch 325, step 100 / 900, loss = 2.85 (0.022 sec/batch)\n",
      "epoch 325, step 150 / 900, loss = 2.50 (0.022 sec/batch)\n",
      "epoch 325, step 200 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 325, step 250 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 325, step 300 / 900, loss = 1.95 (0.021 sec/batch)\n",
      "epoch 325, step 350 / 900, loss = 2.79 (0.021 sec/batch)\n",
      "epoch 325, step 400 / 900, loss = 1.87 (0.022 sec/batch)\n",
      "epoch 325, step 450 / 900, loss = 2.76 (0.023 sec/batch)\n",
      "epoch 325, step 500 / 900, loss = 3.19 (0.021 sec/batch)\n",
      "epoch 325, step 550 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "epoch 325, step 600 / 900, loss = 1.82 (0.021 sec/batch)\n",
      "epoch 325, step 650 / 900, loss = 1.60 (0.021 sec/batch)\n",
      "epoch 325, step 700 / 900, loss = 2.32 (0.021 sec/batch)\n",
      "epoch 325, step 750 / 900, loss = 1.82 (0.021 sec/batch)\n",
      "epoch 325, step 800 / 900, loss = 3.04 (0.021 sec/batch)\n",
      "epoch 325, step 850 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 325, step 900 / 900, loss = 2.45 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.82\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.80\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.568599939346313\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 326, step 50 / 900, loss = 1.49 (0.021 sec/batch)\n",
      "epoch 326, step 100 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 326, step 150 / 900, loss = 2.98 (0.022 sec/batch)\n",
      "epoch 326, step 200 / 900, loss = 2.50 (0.022 sec/batch)\n",
      "epoch 326, step 250 / 900, loss = 1.13 (0.021 sec/batch)\n",
      "epoch 326, step 300 / 900, loss = 1.59 (0.021 sec/batch)\n",
      "epoch 326, step 350 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 326, step 400 / 900, loss = 2.64 (0.021 sec/batch)\n",
      "epoch 326, step 450 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 326, step 500 / 900, loss = 2.63 (0.022 sec/batch)\n",
      "epoch 326, step 550 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 326, step 600 / 900, loss = 1.59 (0.021 sec/batch)\n",
      "epoch 326, step 650 / 900, loss = 1.98 (0.021 sec/batch)\n",
      "epoch 326, step 700 / 900, loss = 2.19 (0.022 sec/batch)\n",
      "epoch 326, step 750 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 326, step 800 / 900, loss = 2.98 (0.021 sec/batch)\n",
      "epoch 326, step 850 / 900, loss = 2.03 (0.022 sec/batch)\n",
      "epoch 326, step 900 / 900, loss = 2.71 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 56.00\n",
      " avg loss = 2.24\n",
      "\n",
      "Validation error:\n",
      " accuracy = 48.64\n",
      " avg loss = 2.93\n",
      "\n",
      "Epoch time: 24.589956760406494\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 327, step 50 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 327, step 100 / 900, loss = 1.70 (0.021 sec/batch)\n",
      "epoch 327, step 150 / 900, loss = 1.31 (0.022 sec/batch)\n",
      "epoch 327, step 200 / 900, loss = 2.62 (0.022 sec/batch)\n",
      "epoch 327, step 250 / 900, loss = 2.08 (0.020 sec/batch)\n",
      "epoch 327, step 300 / 900, loss = 2.39 (0.021 sec/batch)\n",
      "epoch 327, step 350 / 900, loss = 1.61 (0.021 sec/batch)\n",
      "epoch 327, step 400 / 900, loss = 2.88 (0.022 sec/batch)\n",
      "epoch 327, step 450 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "epoch 327, step 500 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "epoch 327, step 550 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 327, step 600 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 327, step 650 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 327, step 700 / 900, loss = 2.41 (0.021 sec/batch)\n",
      "epoch 327, step 750 / 900, loss = 1.90 (0.022 sec/batch)\n",
      "epoch 327, step 800 / 900, loss = 2.61 (0.021 sec/batch)\n",
      "epoch 327, step 850 / 900, loss = 1.96 (0.021 sec/batch)\n",
      "epoch 327, step 900 / 900, loss = 2.24 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.40\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.38\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.588169813156128\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 328, step 50 / 900, loss = 2.23 (0.022 sec/batch)\n",
      "epoch 328, step 100 / 900, loss = 2.22 (0.022 sec/batch)\n",
      "epoch 328, step 150 / 900, loss = 1.50 (0.021 sec/batch)\n",
      "epoch 328, step 200 / 900, loss = 2.94 (0.022 sec/batch)\n",
      "epoch 328, step 250 / 900, loss = 2.56 (0.021 sec/batch)\n",
      "epoch 328, step 300 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 328, step 350 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 328, step 400 / 900, loss = 2.33 (0.022 sec/batch)\n",
      "epoch 328, step 450 / 900, loss = 2.51 (0.021 sec/batch)\n",
      "epoch 328, step 500 / 900, loss = 1.99 (0.022 sec/batch)\n",
      "epoch 328, step 550 / 900, loss = 2.19 (0.022 sec/batch)\n",
      "epoch 328, step 600 / 900, loss = 1.91 (0.022 sec/batch)\n",
      "epoch 328, step 650 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 328, step 700 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 328, step 750 / 900, loss = 2.25 (0.022 sec/batch)\n",
      "epoch 328, step 800 / 900, loss = 1.71 (0.021 sec/batch)\n",
      "epoch 328, step 850 / 900, loss = 2.34 (0.022 sec/batch)\n",
      "epoch 328, step 900 / 900, loss = 2.31 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.78\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.38\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.610971450805664\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 329, step 50 / 900, loss = 1.92 (0.022 sec/batch)\n",
      "epoch 329, step 100 / 900, loss = 1.73 (0.021 sec/batch)\n",
      "epoch 329, step 150 / 900, loss = 1.67 (0.021 sec/batch)\n",
      "epoch 329, step 200 / 900, loss = 2.28 (0.022 sec/batch)\n",
      "epoch 329, step 250 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 329, step 300 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 329, step 350 / 900, loss = 1.97 (0.022 sec/batch)\n",
      "epoch 329, step 400 / 900, loss = 3.35 (0.022 sec/batch)\n",
      "epoch 329, step 450 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 329, step 500 / 900, loss = 2.39 (0.021 sec/batch)\n",
      "epoch 329, step 550 / 900, loss = 2.71 (0.021 sec/batch)\n",
      "epoch 329, step 600 / 900, loss = 3.09 (0.023 sec/batch)\n",
      "epoch 329, step 650 / 900, loss = 2.63 (0.022 sec/batch)\n",
      "epoch 329, step 700 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 329, step 750 / 900, loss = 3.02 (0.021 sec/batch)\n",
      "epoch 329, step 800 / 900, loss = 1.93 (0.021 sec/batch)\n",
      "epoch 329, step 850 / 900, loss = 2.83 (0.022 sec/batch)\n",
      "epoch 329, step 900 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.55\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 48.80\n",
      " avg loss = 2.93\n",
      "\n",
      "Epoch time: 24.580334186553955\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 330, step 50 / 900, loss = 1.90 (0.021 sec/batch)\n",
      "epoch 330, step 100 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "epoch 330, step 150 / 900, loss = 4.09 (0.022 sec/batch)\n",
      "epoch 330, step 200 / 900, loss = 2.24 (0.021 sec/batch)\n",
      "epoch 330, step 250 / 900, loss = 2.06 (0.022 sec/batch)\n",
      "epoch 330, step 300 / 900, loss = 2.53 (0.021 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 330, step 350 / 900, loss = 3.20 (0.021 sec/batch)\n",
      "epoch 330, step 400 / 900, loss = 2.72 (0.021 sec/batch)\n",
      "epoch 330, step 450 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 330, step 500 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 330, step 550 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 330, step 600 / 900, loss = 2.31 (0.022 sec/batch)\n",
      "epoch 330, step 650 / 900, loss = 2.16 (0.022 sec/batch)\n",
      "epoch 330, step 700 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 330, step 750 / 900, loss = 3.98 (0.021 sec/batch)\n",
      "epoch 330, step 800 / 900, loss = 2.54 (0.021 sec/batch)\n",
      "epoch 330, step 850 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "epoch 330, step 900 / 900, loss = 1.87 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.63\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.58\n",
      " avg loss = 2.86\n",
      "\n",
      "Epoch time: 24.59681224822998\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 331, step 50 / 900, loss = 2.96 (0.021 sec/batch)\n",
      "epoch 331, step 100 / 900, loss = 3.44 (0.021 sec/batch)\n",
      "epoch 331, step 150 / 900, loss = 2.08 (0.022 sec/batch)\n",
      "epoch 331, step 200 / 900, loss = 1.58 (0.021 sec/batch)\n",
      "epoch 331, step 250 / 900, loss = 1.73 (0.021 sec/batch)\n",
      "epoch 331, step 300 / 900, loss = 1.91 (0.021 sec/batch)\n",
      "epoch 331, step 350 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 331, step 400 / 900, loss = 2.23 (0.022 sec/batch)\n",
      "epoch 331, step 450 / 900, loss = 2.10 (0.022 sec/batch)\n",
      "epoch 331, step 500 / 900, loss = 1.99 (0.021 sec/batch)\n",
      "epoch 331, step 550 / 900, loss = 3.13 (0.022 sec/batch)\n",
      "epoch 331, step 600 / 900, loss = 2.76 (0.022 sec/batch)\n",
      "epoch 331, step 650 / 900, loss = 2.68 (0.022 sec/batch)\n",
      "epoch 331, step 700 / 900, loss = 2.02 (0.022 sec/batch)\n",
      "epoch 331, step 750 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 331, step 800 / 900, loss = 1.61 (0.021 sec/batch)\n",
      "epoch 331, step 850 / 900, loss = 1.51 (0.021 sec/batch)\n",
      "epoch 331, step 900 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.70\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.14\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.58184814453125\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 332, step 50 / 900, loss = 3.14 (0.021 sec/batch)\n",
      "epoch 332, step 100 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 332, step 150 / 900, loss = 2.90 (0.021 sec/batch)\n",
      "epoch 332, step 200 / 900, loss = 2.92 (0.021 sec/batch)\n",
      "epoch 332, step 250 / 900, loss = 2.12 (0.022 sec/batch)\n",
      "epoch 332, step 300 / 900, loss = 1.61 (0.021 sec/batch)\n",
      "epoch 332, step 350 / 900, loss = 1.61 (0.022 sec/batch)\n",
      "epoch 332, step 400 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 332, step 450 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 332, step 500 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 332, step 550 / 900, loss = 2.20 (0.023 sec/batch)\n",
      "epoch 332, step 600 / 900, loss = 2.67 (0.021 sec/batch)\n",
      "epoch 332, step 650 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 332, step 700 / 900, loss = 2.15 (0.021 sec/batch)\n",
      "epoch 332, step 750 / 900, loss = 1.96 (0.022 sec/batch)\n",
      "epoch 332, step 800 / 900, loss = 3.07 (0.021 sec/batch)\n",
      "epoch 332, step 850 / 900, loss = 1.93 (0.021 sec/batch)\n",
      "epoch 332, step 900 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.55\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.48\n",
      " avg loss = 2.93\n",
      "\n",
      "Epoch time: 24.614757299423218\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 333, step 50 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "epoch 333, step 100 / 900, loss = 2.69 (0.021 sec/batch)\n",
      "epoch 333, step 150 / 900, loss = 1.53 (0.021 sec/batch)\n",
      "epoch 333, step 200 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "epoch 333, step 250 / 900, loss = 2.36 (0.021 sec/batch)\n",
      "epoch 333, step 300 / 900, loss = 1.90 (0.021 sec/batch)\n",
      "epoch 333, step 350 / 900, loss = 1.66 (0.021 sec/batch)\n",
      "epoch 333, step 400 / 900, loss = 2.08 (0.022 sec/batch)\n",
      "epoch 333, step 450 / 900, loss = 1.80 (0.021 sec/batch)\n",
      "epoch 333, step 500 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 333, step 550 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "epoch 333, step 600 / 900, loss = 1.60 (0.021 sec/batch)\n",
      "epoch 333, step 650 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 333, step 700 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 333, step 750 / 900, loss = 2.01 (0.022 sec/batch)\n",
      "epoch 333, step 800 / 900, loss = 2.14 (0.022 sec/batch)\n",
      "epoch 333, step 850 / 900, loss = 2.27 (0.022 sec/batch)\n",
      "epoch 333, step 900 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.71\n",
      " avg loss = 2.24\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.46\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.587743997573853\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 334, step 50 / 900, loss = 2.47 (0.022 sec/batch)\n",
      "epoch 334, step 100 / 900, loss = 1.95 (0.022 sec/batch)\n",
      "epoch 334, step 150 / 900, loss = 1.76 (0.021 sec/batch)\n",
      "epoch 334, step 200 / 900, loss = 2.89 (0.021 sec/batch)\n",
      "epoch 334, step 250 / 900, loss = 3.16 (0.021 sec/batch)\n",
      "epoch 334, step 300 / 900, loss = 2.83 (0.022 sec/batch)\n",
      "epoch 334, step 350 / 900, loss = 1.56 (0.022 sec/batch)\n",
      "epoch 334, step 400 / 900, loss = 1.84 (0.022 sec/batch)\n",
      "epoch 334, step 450 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "epoch 334, step 500 / 900, loss = 2.14 (0.022 sec/batch)\n",
      "epoch 334, step 550 / 900, loss = 2.41 (0.020 sec/batch)\n",
      "epoch 334, step 600 / 900, loss = 1.90 (0.021 sec/batch)\n",
      "epoch 334, step 650 / 900, loss = 1.95 (0.021 sec/batch)\n",
      "epoch 334, step 700 / 900, loss = 1.47 (0.021 sec/batch)\n",
      "epoch 334, step 750 / 900, loss = 2.30 (0.022 sec/batch)\n",
      "epoch 334, step 800 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 334, step 850 / 900, loss = 2.41 (0.021 sec/batch)\n",
      "epoch 334, step 900 / 900, loss = 2.45 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.76\n",
      " avg loss = 2.24\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.92\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.589336395263672\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 335, step 50 / 900, loss = 2.38 (0.022 sec/batch)\n",
      "epoch 335, step 100 / 900, loss = 3.36 (0.021 sec/batch)\n",
      "epoch 335, step 150 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 335, step 200 / 900, loss = 3.05 (0.021 sec/batch)\n",
      "epoch 335, step 250 / 900, loss = 2.54 (0.021 sec/batch)\n",
      "epoch 335, step 300 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 335, step 350 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 335, step 400 / 900, loss = 2.92 (0.022 sec/batch)\n",
      "epoch 335, step 450 / 900, loss = 2.41 (0.021 sec/batch)\n",
      "epoch 335, step 500 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 335, step 550 / 900, loss = 2.42 (0.022 sec/batch)\n",
      "epoch 335, step 600 / 900, loss = 2.19 (0.022 sec/batch)\n",
      "epoch 335, step 650 / 900, loss = 3.06 (0.022 sec/batch)\n",
      "epoch 335, step 700 / 900, loss = 2.75 (0.022 sec/batch)\n",
      "epoch 335, step 750 / 900, loss = 2.40 (0.022 sec/batch)\n",
      "epoch 335, step 800 / 900, loss = 2.81 (0.021 sec/batch)\n",
      "epoch 335, step 850 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "epoch 335, step 900 / 900, loss = 2.94 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.61\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.20\n",
      " avg loss = 2.86\n",
      "\n",
      "Epoch time: 24.58791708946228\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 336, step 50 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "epoch 336, step 100 / 900, loss = 2.66 (0.021 sec/batch)\n",
      "epoch 336, step 150 / 900, loss = 3.31 (0.022 sec/batch)\n",
      "epoch 336, step 200 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 336, step 250 / 900, loss = 2.32 (0.022 sec/batch)\n",
      "epoch 336, step 300 / 900, loss = 1.77 (0.022 sec/batch)\n",
      "epoch 336, step 350 / 900, loss = 2.54 (0.021 sec/batch)\n",
      "epoch 336, step 400 / 900, loss = 3.09 (0.022 sec/batch)\n",
      "epoch 336, step 450 / 900, loss = 2.82 (0.021 sec/batch)\n",
      "epoch 336, step 500 / 900, loss = 2.70 (0.021 sec/batch)\n",
      "epoch 336, step 550 / 900, loss = 2.83 (0.022 sec/batch)\n",
      "epoch 336, step 600 / 900, loss = 1.93 (0.020 sec/batch)\n",
      "epoch 336, step 650 / 900, loss = 2.66 (0.021 sec/batch)\n",
      "epoch 336, step 700 / 900, loss = 1.40 (0.021 sec/batch)\n",
      "epoch 336, step 750 / 900, loss = 2.44 (0.021 sec/batch)\n",
      "epoch 336, step 800 / 900, loss = 2.32 (0.022 sec/batch)\n",
      "epoch 336, step 850 / 900, loss = 3.14 (0.021 sec/batch)\n",
      "epoch 336, step 900 / 900, loss = 1.96 (0.021 sec/batch)\n",
      "Train error:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy = 55.52\n",
      " avg loss = 2.28\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.32\n",
      " avg loss = 2.82\n",
      "\n",
      "Epoch time: 24.584561586380005\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 337, step 50 / 900, loss = 1.57 (0.021 sec/batch)\n",
      "epoch 337, step 100 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 337, step 150 / 900, loss = 1.65 (0.022 sec/batch)\n",
      "epoch 337, step 200 / 900, loss = 1.80 (0.021 sec/batch)\n",
      "epoch 337, step 250 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 337, step 300 / 900, loss = 1.91 (0.022 sec/batch)\n",
      "epoch 337, step 350 / 900, loss = 1.96 (0.021 sec/batch)\n",
      "epoch 337, step 400 / 900, loss = 2.20 (0.022 sec/batch)\n",
      "epoch 337, step 450 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "epoch 337, step 500 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 337, step 550 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 337, step 600 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "epoch 337, step 650 / 900, loss = 2.80 (0.021 sec/batch)\n",
      "epoch 337, step 700 / 900, loss = 1.60 (0.021 sec/batch)\n",
      "epoch 337, step 750 / 900, loss = 2.21 (0.022 sec/batch)\n",
      "epoch 337, step 800 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "epoch 337, step 850 / 900, loss = 2.64 (0.021 sec/batch)\n",
      "epoch 337, step 900 / 900, loss = 1.75 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.41\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.56\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.612247467041016\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 338, step 50 / 900, loss = 3.30 (0.022 sec/batch)\n",
      "epoch 338, step 100 / 900, loss = 2.43 (0.022 sec/batch)\n",
      "epoch 338, step 150 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 338, step 200 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 338, step 250 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 338, step 300 / 900, loss = 1.99 (0.021 sec/batch)\n",
      "epoch 338, step 350 / 900, loss = 2.25 (0.022 sec/batch)\n",
      "epoch 338, step 400 / 900, loss = 2.33 (0.022 sec/batch)\n",
      "epoch 338, step 450 / 900, loss = 1.66 (0.021 sec/batch)\n",
      "epoch 338, step 500 / 900, loss = 2.83 (0.021 sec/batch)\n",
      "epoch 338, step 550 / 900, loss = 2.63 (0.021 sec/batch)\n",
      "epoch 338, step 600 / 900, loss = 2.68 (0.021 sec/batch)\n",
      "epoch 338, step 650 / 900, loss = 2.64 (0.022 sec/batch)\n",
      "epoch 338, step 700 / 900, loss = 1.76 (0.021 sec/batch)\n",
      "epoch 338, step 750 / 900, loss = 3.05 (0.022 sec/batch)\n",
      "epoch 338, step 800 / 900, loss = 1.91 (0.021 sec/batch)\n",
      "epoch 338, step 850 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 338, step 900 / 900, loss = 1.78 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.61\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.30\n",
      " avg loss = 2.82\n",
      "\n",
      "Epoch time: 24.58853316307068\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 339, step 50 / 900, loss = 2.61 (0.021 sec/batch)\n",
      "epoch 339, step 100 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 339, step 150 / 900, loss = 2.40 (0.021 sec/batch)\n",
      "epoch 339, step 200 / 900, loss = 1.76 (0.021 sec/batch)\n",
      "epoch 339, step 250 / 900, loss = 1.67 (0.021 sec/batch)\n",
      "epoch 339, step 300 / 900, loss = 2.63 (0.021 sec/batch)\n",
      "epoch 339, step 350 / 900, loss = 1.87 (0.022 sec/batch)\n",
      "epoch 339, step 400 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 339, step 450 / 900, loss = 1.79 (0.021 sec/batch)\n",
      "epoch 339, step 500 / 900, loss = 1.78 (0.021 sec/batch)\n",
      "epoch 339, step 550 / 900, loss = 1.64 (0.021 sec/batch)\n",
      "epoch 339, step 600 / 900, loss = 2.47 (0.021 sec/batch)\n",
      "epoch 339, step 650 / 900, loss = 1.73 (0.021 sec/batch)\n",
      "epoch 339, step 700 / 900, loss = 2.60 (0.021 sec/batch)\n",
      "epoch 339, step 750 / 900, loss = 3.05 (0.022 sec/batch)\n",
      "epoch 339, step 800 / 900, loss = 2.24 (0.021 sec/batch)\n",
      "epoch 339, step 850 / 900, loss = 1.68 (0.021 sec/batch)\n",
      "epoch 339, step 900 / 900, loss = 1.82 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.80\n",
      " avg loss = 2.24\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.64\n",
      " avg loss = 2.99\n",
      "\n",
      "Epoch time: 24.579262495040894\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 340, step 50 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 340, step 100 / 900, loss = 1.55 (0.021 sec/batch)\n",
      "epoch 340, step 150 / 900, loss = 2.50 (0.022 sec/batch)\n",
      "epoch 340, step 200 / 900, loss = 3.28 (0.022 sec/batch)\n",
      "epoch 340, step 250 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 340, step 300 / 900, loss = 2.94 (0.020 sec/batch)\n",
      "epoch 340, step 350 / 900, loss = 2.31 (0.022 sec/batch)\n",
      "epoch 340, step 400 / 900, loss = 1.70 (0.022 sec/batch)\n",
      "epoch 340, step 450 / 900, loss = 3.57 (0.021 sec/batch)\n",
      "epoch 340, step 500 / 900, loss = 2.99 (0.021 sec/batch)\n",
      "epoch 340, step 550 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 340, step 600 / 900, loss = 1.35 (0.021 sec/batch)\n",
      "epoch 340, step 650 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 340, step 700 / 900, loss = 2.06 (0.021 sec/batch)\n",
      "epoch 340, step 750 / 900, loss = 2.24 (0.022 sec/batch)\n",
      "epoch 340, step 800 / 900, loss = 2.56 (0.022 sec/batch)\n",
      "epoch 340, step 850 / 900, loss = 2.45 (0.021 sec/batch)\n",
      "epoch 340, step 900 / 900, loss = 1.79 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.57\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.88\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.608420372009277\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 341, step 50 / 900, loss = 2.36 (0.021 sec/batch)\n",
      "epoch 341, step 100 / 900, loss = 1.35 (0.022 sec/batch)\n",
      "epoch 341, step 150 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 341, step 200 / 900, loss = 2.26 (0.022 sec/batch)\n",
      "epoch 341, step 250 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "epoch 341, step 300 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "epoch 341, step 350 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "epoch 341, step 400 / 900, loss = 1.69 (0.022 sec/batch)\n",
      "epoch 341, step 450 / 900, loss = 1.93 (0.021 sec/batch)\n",
      "epoch 341, step 500 / 900, loss = 1.94 (0.022 sec/batch)\n",
      "epoch 341, step 550 / 900, loss = 1.87 (0.021 sec/batch)\n",
      "epoch 341, step 600 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 341, step 650 / 900, loss = 1.92 (0.022 sec/batch)\n",
      "epoch 341, step 700 / 900, loss = 2.80 (0.021 sec/batch)\n",
      "epoch 341, step 750 / 900, loss = 2.59 (0.021 sec/batch)\n",
      "epoch 341, step 800 / 900, loss = 1.66 (0.021 sec/batch)\n",
      "epoch 341, step 850 / 900, loss = 2.55 (0.022 sec/batch)\n",
      "epoch 341, step 900 / 900, loss = 1.64 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.44\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.72\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.581579446792603\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 342, step 50 / 900, loss = 1.87 (0.021 sec/batch)\n",
      "epoch 342, step 100 / 900, loss = 1.53 (0.021 sec/batch)\n",
      "epoch 342, step 150 / 900, loss = 2.32 (0.021 sec/batch)\n",
      "epoch 342, step 200 / 900, loss = 1.77 (0.022 sec/batch)\n",
      "epoch 342, step 250 / 900, loss = 2.68 (0.021 sec/batch)\n",
      "epoch 342, step 300 / 900, loss = 1.81 (0.021 sec/batch)\n",
      "epoch 342, step 350 / 900, loss = 1.86 (0.021 sec/batch)\n",
      "epoch 342, step 400 / 900, loss = 2.47 (0.021 sec/batch)\n",
      "epoch 342, step 450 / 900, loss = 2.77 (0.021 sec/batch)\n",
      "epoch 342, step 500 / 900, loss = 2.44 (0.022 sec/batch)\n",
      "epoch 342, step 550 / 900, loss = 3.10 (0.021 sec/batch)\n",
      "epoch 342, step 600 / 900, loss = 2.67 (0.022 sec/batch)\n",
      "epoch 342, step 650 / 900, loss = 2.11 (0.022 sec/batch)\n",
      "epoch 342, step 700 / 900, loss = 2.04 (0.022 sec/batch)\n",
      "epoch 342, step 750 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 342, step 800 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 342, step 850 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 342, step 900 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.62\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 48.78\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.622663259506226\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 343, step 50 / 900, loss = 1.90 (0.022 sec/batch)\n",
      "epoch 343, step 100 / 900, loss = 2.67 (0.021 sec/batch)\n",
      "epoch 343, step 150 / 900, loss = 2.23 (0.022 sec/batch)\n",
      "epoch 343, step 200 / 900, loss = 0.94 (0.021 sec/batch)\n",
      "epoch 343, step 250 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 343, step 300 / 900, loss = 2.68 (0.021 sec/batch)\n",
      "epoch 343, step 350 / 900, loss = 2.48 (0.022 sec/batch)\n",
      "epoch 343, step 400 / 900, loss = 1.95 (0.022 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 343, step 450 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "epoch 343, step 500 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "epoch 343, step 550 / 900, loss = 1.67 (0.021 sec/batch)\n",
      "epoch 343, step 600 / 900, loss = 1.83 (0.022 sec/batch)\n",
      "epoch 343, step 650 / 900, loss = 2.97 (0.021 sec/batch)\n",
      "epoch 343, step 700 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 343, step 750 / 900, loss = 2.57 (0.021 sec/batch)\n",
      "epoch 343, step 800 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 343, step 850 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 343, step 900 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.77\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.26\n",
      " avg loss = 2.86\n",
      "\n",
      "Epoch time: 24.572341442108154\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 344, step 50 / 900, loss = 2.83 (0.021 sec/batch)\n",
      "epoch 344, step 100 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 344, step 150 / 900, loss = 2.26 (0.021 sec/batch)\n",
      "epoch 344, step 200 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 344, step 250 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "epoch 344, step 300 / 900, loss = 1.81 (0.021 sec/batch)\n",
      "epoch 344, step 350 / 900, loss = 1.84 (0.021 sec/batch)\n",
      "epoch 344, step 400 / 900, loss = 1.81 (0.021 sec/batch)\n",
      "epoch 344, step 450 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 344, step 500 / 900, loss = 1.66 (0.021 sec/batch)\n",
      "epoch 344, step 550 / 900, loss = 3.04 (0.021 sec/batch)\n",
      "epoch 344, step 600 / 900, loss = 2.55 (0.021 sec/batch)\n",
      "epoch 344, step 650 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 344, step 700 / 900, loss = 2.98 (0.021 sec/batch)\n",
      "epoch 344, step 750 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 344, step 800 / 900, loss = 2.57 (0.022 sec/batch)\n",
      "epoch 344, step 850 / 900, loss = 1.82 (0.022 sec/batch)\n",
      "epoch 344, step 900 / 900, loss = 1.85 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.90\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.50\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.595351457595825\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 345, step 50 / 900, loss = 1.95 (0.021 sec/batch)\n",
      "epoch 345, step 100 / 900, loss = 2.82 (0.021 sec/batch)\n",
      "epoch 345, step 150 / 900, loss = 3.15 (0.021 sec/batch)\n",
      "epoch 345, step 200 / 900, loss = 2.41 (0.021 sec/batch)\n",
      "epoch 345, step 250 / 900, loss = 2.90 (0.021 sec/batch)\n",
      "epoch 345, step 300 / 900, loss = 1.62 (0.021 sec/batch)\n",
      "epoch 345, step 350 / 900, loss = 2.51 (0.021 sec/batch)\n",
      "epoch 345, step 400 / 900, loss = 1.87 (0.022 sec/batch)\n",
      "epoch 345, step 450 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 345, step 500 / 900, loss = 1.70 (0.022 sec/batch)\n",
      "epoch 345, step 550 / 900, loss = 2.98 (0.021 sec/batch)\n",
      "epoch 345, step 600 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 345, step 650 / 900, loss = 2.11 (0.022 sec/batch)\n",
      "epoch 345, step 700 / 900, loss = 2.86 (0.022 sec/batch)\n",
      "epoch 345, step 750 / 900, loss = 2.30 (0.022 sec/batch)\n",
      "epoch 345, step 800 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "epoch 345, step 850 / 900, loss = 2.59 (0.021 sec/batch)\n",
      "epoch 345, step 900 / 900, loss = 2.50 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.39\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.66\n",
      " avg loss = 2.86\n",
      "\n",
      "Epoch time: 24.586875438690186\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 346, step 50 / 900, loss = 3.72 (0.020 sec/batch)\n",
      "epoch 346, step 100 / 900, loss = 1.88 (0.021 sec/batch)\n",
      "epoch 346, step 150 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "epoch 346, step 200 / 900, loss = 1.04 (0.021 sec/batch)\n",
      "epoch 346, step 250 / 900, loss = 3.82 (0.021 sec/batch)\n",
      "epoch 346, step 300 / 900, loss = 2.31 (0.022 sec/batch)\n",
      "epoch 346, step 350 / 900, loss = 1.96 (0.021 sec/batch)\n",
      "epoch 346, step 400 / 900, loss = 1.29 (0.021 sec/batch)\n",
      "epoch 346, step 450 / 900, loss = 1.55 (0.021 sec/batch)\n",
      "epoch 346, step 500 / 900, loss = 2.89 (0.021 sec/batch)\n",
      "epoch 346, step 550 / 900, loss = 2.20 (0.022 sec/batch)\n",
      "epoch 346, step 600 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 346, step 650 / 900, loss = 1.30 (0.021 sec/batch)\n",
      "epoch 346, step 700 / 900, loss = 2.40 (0.022 sec/batch)\n",
      "epoch 346, step 750 / 900, loss = 2.99 (0.022 sec/batch)\n",
      "epoch 346, step 800 / 900, loss = 1.93 (0.021 sec/batch)\n",
      "epoch 346, step 850 / 900, loss = 1.41 (0.022 sec/batch)\n",
      "epoch 346, step 900 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.33\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.08\n",
      " avg loss = 2.96\n",
      "\n",
      "Epoch time: 24.584688425064087\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 347, step 50 / 900, loss = 2.85 (0.021 sec/batch)\n",
      "epoch 347, step 100 / 900, loss = 1.82 (0.021 sec/batch)\n",
      "epoch 347, step 150 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "epoch 347, step 200 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 347, step 250 / 900, loss = 3.02 (0.021 sec/batch)\n",
      "epoch 347, step 300 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 347, step 350 / 900, loss = 2.84 (0.021 sec/batch)\n",
      "epoch 347, step 400 / 900, loss = 1.78 (0.022 sec/batch)\n",
      "epoch 347, step 450 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 347, step 500 / 900, loss = 2.18 (0.022 sec/batch)\n",
      "epoch 347, step 550 / 900, loss = 2.04 (0.021 sec/batch)\n",
      "epoch 347, step 600 / 900, loss = 2.98 (0.021 sec/batch)\n",
      "epoch 347, step 650 / 900, loss = 2.75 (0.022 sec/batch)\n",
      "epoch 347, step 700 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 347, step 750 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 347, step 800 / 900, loss = 2.33 (0.022 sec/batch)\n",
      "epoch 347, step 850 / 900, loss = 1.56 (0.021 sec/batch)\n",
      "epoch 347, step 900 / 900, loss = 4.16 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.39\n",
      " avg loss = 2.28\n",
      "\n",
      "Validation error:\n",
      " accuracy = 48.88\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.602083206176758\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 348, step 50 / 900, loss = 2.24 (0.021 sec/batch)\n",
      "epoch 348, step 100 / 900, loss = 1.88 (0.021 sec/batch)\n",
      "epoch 348, step 150 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 348, step 200 / 900, loss = 1.42 (0.021 sec/batch)\n",
      "epoch 348, step 250 / 900, loss = 2.99 (0.021 sec/batch)\n",
      "epoch 348, step 300 / 900, loss = 3.22 (0.021 sec/batch)\n",
      "epoch 348, step 350 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 348, step 400 / 900, loss = 2.30 (0.022 sec/batch)\n",
      "epoch 348, step 450 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "epoch 348, step 500 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 348, step 550 / 900, loss = 2.99 (0.022 sec/batch)\n",
      "epoch 348, step 600 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 348, step 650 / 900, loss = 1.84 (0.022 sec/batch)\n",
      "epoch 348, step 700 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 348, step 750 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 348, step 800 / 900, loss = 2.46 (0.022 sec/batch)\n",
      "epoch 348, step 850 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 348, step 900 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.86\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.68\n",
      " avg loss = 2.86\n",
      "\n",
      "Epoch time: 24.59081196784973\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 349, step 50 / 900, loss = 1.80 (0.021 sec/batch)\n",
      "epoch 349, step 100 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 349, step 150 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "epoch 349, step 200 / 900, loss = 1.80 (0.021 sec/batch)\n",
      "epoch 349, step 250 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 349, step 300 / 900, loss = 1.12 (0.023 sec/batch)\n",
      "epoch 349, step 350 / 900, loss = 1.81 (0.021 sec/batch)\n",
      "epoch 349, step 400 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "epoch 349, step 450 / 900, loss = 2.39 (0.022 sec/batch)\n",
      "epoch 349, step 500 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 349, step 550 / 900, loss = 1.99 (0.022 sec/batch)\n",
      "epoch 349, step 600 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 349, step 650 / 900, loss = 3.02 (0.021 sec/batch)\n",
      "epoch 349, step 700 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "epoch 349, step 750 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 349, step 800 / 900, loss = 1.67 (0.021 sec/batch)\n",
      "epoch 349, step 850 / 900, loss = 1.96 (0.021 sec/batch)\n",
      "epoch 349, step 900 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.45\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.58\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.60157346725464\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 350, step 50 / 900, loss = 3.76 (0.020 sec/batch)\n",
      "epoch 350, step 100 / 900, loss = 1.72 (0.022 sec/batch)\n",
      "epoch 350, step 150 / 900, loss = 3.08 (0.021 sec/batch)\n",
      "epoch 350, step 200 / 900, loss = 2.46 (0.020 sec/batch)\n",
      "epoch 350, step 250 / 900, loss = 2.74 (0.021 sec/batch)\n",
      "epoch 350, step 300 / 900, loss = 2.56 (0.021 sec/batch)\n",
      "epoch 350, step 350 / 900, loss = 2.45 (0.022 sec/batch)\n",
      "epoch 350, step 400 / 900, loss = 2.56 (0.021 sec/batch)\n",
      "epoch 350, step 450 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 350, step 500 / 900, loss = 1.99 (0.021 sec/batch)\n",
      "epoch 350, step 550 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 350, step 600 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 350, step 650 / 900, loss = 2.72 (0.022 sec/batch)\n",
      "epoch 350, step 700 / 900, loss = 2.36 (0.021 sec/batch)\n",
      "epoch 350, step 750 / 900, loss = 2.10 (0.022 sec/batch)\n",
      "epoch 350, step 800 / 900, loss = 2.80 (0.021 sec/batch)\n",
      "epoch 350, step 850 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 350, step 900 / 900, loss = 1.78 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.44\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.74\n",
      " avg loss = 2.82\n",
      "\n",
      "Epoch time: 24.60012173652649\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 351, step 50 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 351, step 100 / 900, loss = 2.49 (0.021 sec/batch)\n",
      "epoch 351, step 150 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "epoch 351, step 200 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "epoch 351, step 250 / 900, loss = 1.93 (0.022 sec/batch)\n",
      "epoch 351, step 300 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 351, step 350 / 900, loss = 1.56 (0.021 sec/batch)\n",
      "epoch 351, step 400 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "epoch 351, step 450 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 351, step 500 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 351, step 550 / 900, loss = 2.48 (0.022 sec/batch)\n",
      "epoch 351, step 600 / 900, loss = 2.46 (0.021 sec/batch)\n",
      "epoch 351, step 650 / 900, loss = 2.97 (0.022 sec/batch)\n",
      "epoch 351, step 700 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "epoch 351, step 750 / 900, loss = 1.53 (0.021 sec/batch)\n",
      "epoch 351, step 800 / 900, loss = 1.82 (0.022 sec/batch)\n",
      "epoch 351, step 850 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "epoch 351, step 900 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.93\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.22\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.577545166015625\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 352, step 50 / 900, loss = 2.32 (0.021 sec/batch)\n",
      "epoch 352, step 100 / 900, loss = 2.74 (0.021 sec/batch)\n",
      "epoch 352, step 150 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 352, step 200 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 352, step 250 / 900, loss = 2.41 (0.021 sec/batch)\n",
      "epoch 352, step 300 / 900, loss = 1.87 (0.021 sec/batch)\n",
      "epoch 352, step 350 / 900, loss = 2.28 (0.022 sec/batch)\n",
      "epoch 352, step 400 / 900, loss = 2.41 (0.021 sec/batch)\n",
      "epoch 352, step 450 / 900, loss = 1.88 (0.021 sec/batch)\n",
      "epoch 352, step 500 / 900, loss = 2.61 (0.022 sec/batch)\n",
      "epoch 352, step 550 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 352, step 600 / 900, loss = 1.53 (0.021 sec/batch)\n",
      "epoch 352, step 650 / 900, loss = 2.15 (0.022 sec/batch)\n",
      "epoch 352, step 700 / 900, loss = 2.12 (0.022 sec/batch)\n",
      "epoch 352, step 750 / 900, loss = 1.89 (0.022 sec/batch)\n",
      "epoch 352, step 800 / 900, loss = 1.90 (0.021 sec/batch)\n",
      "epoch 352, step 850 / 900, loss = 1.81 (0.021 sec/batch)\n",
      "epoch 352, step 900 / 900, loss = 2.54 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.78\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.14\n",
      " avg loss = 2.92\n",
      "\n",
      "Epoch time: 24.59508514404297\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 353, step 50 / 900, loss = 2.55 (0.021 sec/batch)\n",
      "epoch 353, step 100 / 900, loss = 2.32 (0.022 sec/batch)\n",
      "epoch 353, step 150 / 900, loss = 1.61 (0.021 sec/batch)\n",
      "epoch 353, step 200 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 353, step 250 / 900, loss = 2.71 (0.022 sec/batch)\n",
      "epoch 353, step 300 / 900, loss = 1.71 (0.022 sec/batch)\n",
      "epoch 353, step 350 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 353, step 400 / 900, loss = 2.08 (0.022 sec/batch)\n",
      "epoch 353, step 450 / 900, loss = 2.13 (0.022 sec/batch)\n",
      "epoch 353, step 500 / 900, loss = 1.80 (0.022 sec/batch)\n",
      "epoch 353, step 550 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 353, step 600 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 353, step 650 / 900, loss = 1.67 (0.022 sec/batch)\n",
      "epoch 353, step 700 / 900, loss = 2.80 (0.021 sec/batch)\n",
      "epoch 353, step 750 / 900, loss = 3.00 (0.021 sec/batch)\n",
      "epoch 353, step 800 / 900, loss = 1.57 (0.021 sec/batch)\n",
      "epoch 353, step 850 / 900, loss = 2.20 (0.022 sec/batch)\n",
      "epoch 353, step 900 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.50\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.44\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.60262942314148\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 354, step 50 / 900, loss = 1.42 (0.021 sec/batch)\n",
      "epoch 354, step 100 / 900, loss = 2.39 (0.022 sec/batch)\n",
      "epoch 354, step 150 / 900, loss = 2.97 (0.022 sec/batch)\n",
      "epoch 354, step 200 / 900, loss = 1.68 (0.021 sec/batch)\n",
      "epoch 354, step 250 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "epoch 354, step 300 / 900, loss = 2.31 (0.022 sec/batch)\n",
      "epoch 354, step 350 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 354, step 400 / 900, loss = 2.59 (0.021 sec/batch)\n",
      "epoch 354, step 450 / 900, loss = 2.61 (0.021 sec/batch)\n",
      "epoch 354, step 500 / 900, loss = 1.61 (0.022 sec/batch)\n",
      "epoch 354, step 550 / 900, loss = 1.94 (0.022 sec/batch)\n",
      "epoch 354, step 600 / 900, loss = 3.18 (0.021 sec/batch)\n",
      "epoch 354, step 650 / 900, loss = 2.36 (0.022 sec/batch)\n",
      "epoch 354, step 700 / 900, loss = 2.06 (0.021 sec/batch)\n",
      "epoch 354, step 750 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 354, step 800 / 900, loss = 2.03 (0.022 sec/batch)\n",
      "epoch 354, step 850 / 900, loss = 2.26 (0.021 sec/batch)\n",
      "epoch 354, step 900 / 900, loss = 1.86 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.16\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.50\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.570875644683838\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 355, step 50 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 355, step 100 / 900, loss = 2.16 (0.022 sec/batch)\n",
      "epoch 355, step 150 / 900, loss = 2.96 (0.023 sec/batch)\n",
      "epoch 355, step 200 / 900, loss = 1.87 (0.021 sec/batch)\n",
      "epoch 355, step 250 / 900, loss = 1.92 (0.021 sec/batch)\n",
      "epoch 355, step 300 / 900, loss = 1.91 (0.021 sec/batch)\n",
      "epoch 355, step 350 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 355, step 400 / 900, loss = 1.56 (0.022 sec/batch)\n",
      "epoch 355, step 450 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 355, step 500 / 900, loss = 1.60 (0.021 sec/batch)\n",
      "epoch 355, step 550 / 900, loss = 2.06 (0.021 sec/batch)\n",
      "epoch 355, step 600 / 900, loss = 1.99 (0.021 sec/batch)\n",
      "epoch 355, step 650 / 900, loss = 2.77 (0.022 sec/batch)\n",
      "epoch 355, step 700 / 900, loss = 1.85 (0.021 sec/batch)\n",
      "epoch 355, step 750 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "epoch 355, step 800 / 900, loss = 1.59 (0.022 sec/batch)\n",
      "epoch 355, step 850 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 355, step 900 / 900, loss = 1.78 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.38\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.18\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.570003747940063\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 356, step 50 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 356, step 100 / 900, loss = 1.87 (0.021 sec/batch)\n",
      "epoch 356, step 150 / 900, loss = 2.67 (0.021 sec/batch)\n",
      "epoch 356, step 200 / 900, loss = 2.13 (0.022 sec/batch)\n",
      "epoch 356, step 250 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 356, step 300 / 900, loss = 2.22 (0.022 sec/batch)\n",
      "epoch 356, step 350 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 356, step 400 / 900, loss = 3.09 (0.021 sec/batch)\n",
      "epoch 356, step 450 / 900, loss = 2.88 (0.022 sec/batch)\n",
      "epoch 356, step 500 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 356, step 550 / 900, loss = 1.76 (0.021 sec/batch)\n",
      "epoch 356, step 600 / 900, loss = 2.52 (0.021 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 356, step 650 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 356, step 700 / 900, loss = 1.68 (0.021 sec/batch)\n",
      "epoch 356, step 750 / 900, loss = 1.63 (0.021 sec/batch)\n",
      "epoch 356, step 800 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 356, step 850 / 900, loss = 2.80 (0.022 sec/batch)\n",
      "epoch 356, step 900 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.58\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.60\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.612616062164307\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 357, step 50 / 900, loss = 2.74 (0.022 sec/batch)\n",
      "epoch 357, step 100 / 900, loss = 3.73 (0.022 sec/batch)\n",
      "epoch 357, step 150 / 900, loss = 2.57 (0.022 sec/batch)\n",
      "epoch 357, step 200 / 900, loss = 1.76 (0.022 sec/batch)\n",
      "epoch 357, step 250 / 900, loss = 2.74 (0.021 sec/batch)\n",
      "epoch 357, step 300 / 900, loss = 2.79 (0.021 sec/batch)\n",
      "epoch 357, step 350 / 900, loss = 2.08 (0.022 sec/batch)\n",
      "epoch 357, step 400 / 900, loss = 2.59 (0.021 sec/batch)\n",
      "epoch 357, step 450 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 357, step 500 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 357, step 550 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "epoch 357, step 600 / 900, loss = 2.77 (0.021 sec/batch)\n",
      "epoch 357, step 650 / 900, loss = 2.51 (0.021 sec/batch)\n",
      "epoch 357, step 700 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 357, step 750 / 900, loss = 3.22 (0.022 sec/batch)\n",
      "epoch 357, step 800 / 900, loss = 2.06 (0.021 sec/batch)\n",
      "epoch 357, step 850 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 357, step 900 / 900, loss = 2.04 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 56.07\n",
      " avg loss = 2.24\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.24\n",
      " avg loss = 2.83\n",
      "\n",
      "Epoch time: 24.5895938873291\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 358, step 50 / 900, loss = 2.56 (0.021 sec/batch)\n",
      "epoch 358, step 100 / 900, loss = 2.80 (0.021 sec/batch)\n",
      "epoch 358, step 150 / 900, loss = 1.80 (0.021 sec/batch)\n",
      "epoch 358, step 200 / 900, loss = 2.73 (0.021 sec/batch)\n",
      "epoch 358, step 250 / 900, loss = 2.45 (0.021 sec/batch)\n",
      "epoch 358, step 300 / 900, loss = 2.24 (0.021 sec/batch)\n",
      "epoch 358, step 350 / 900, loss = 1.65 (0.021 sec/batch)\n",
      "epoch 358, step 400 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 358, step 450 / 900, loss = 2.40 (0.022 sec/batch)\n",
      "epoch 358, step 500 / 900, loss = 1.75 (0.021 sec/batch)\n",
      "epoch 358, step 550 / 900, loss = 1.52 (0.021 sec/batch)\n",
      "epoch 358, step 600 / 900, loss = 1.87 (0.022 sec/batch)\n",
      "epoch 358, step 650 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 358, step 700 / 900, loss = 2.36 (0.022 sec/batch)\n",
      "epoch 358, step 750 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 358, step 800 / 900, loss = 3.37 (0.022 sec/batch)\n",
      "epoch 358, step 850 / 900, loss = 2.05 (0.022 sec/batch)\n",
      "epoch 358, step 900 / 900, loss = 1.65 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.47\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.22\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.595170259475708\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 359, step 50 / 900, loss = 2.37 (0.022 sec/batch)\n",
      "epoch 359, step 100 / 900, loss = 1.49 (0.021 sec/batch)\n",
      "epoch 359, step 150 / 900, loss = 2.50 (0.022 sec/batch)\n",
      "epoch 359, step 200 / 900, loss = 2.58 (0.021 sec/batch)\n",
      "epoch 359, step 250 / 900, loss = 2.96 (0.022 sec/batch)\n",
      "epoch 359, step 300 / 900, loss = 2.56 (0.022 sec/batch)\n",
      "epoch 359, step 350 / 900, loss = 1.81 (0.021 sec/batch)\n",
      "epoch 359, step 400 / 900, loss = 2.82 (0.021 sec/batch)\n",
      "epoch 359, step 450 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 359, step 500 / 900, loss = 1.45 (0.021 sec/batch)\n",
      "epoch 359, step 550 / 900, loss = 1.76 (0.021 sec/batch)\n",
      "epoch 359, step 600 / 900, loss = 2.25 (0.022 sec/batch)\n",
      "epoch 359, step 650 / 900, loss = 2.64 (0.022 sec/batch)\n",
      "epoch 359, step 700 / 900, loss = 2.06 (0.021 sec/batch)\n",
      "epoch 359, step 750 / 900, loss = 2.22 (0.022 sec/batch)\n",
      "epoch 359, step 800 / 900, loss = 1.73 (0.020 sec/batch)\n",
      "epoch 359, step 850 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 359, step 900 / 900, loss = 2.51 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.47\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.18\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.586790561676025\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 360, step 50 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "epoch 360, step 100 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "epoch 360, step 150 / 900, loss = 2.44 (0.022 sec/batch)\n",
      "epoch 360, step 200 / 900, loss = 2.46 (0.022 sec/batch)\n",
      "epoch 360, step 250 / 900, loss = 3.31 (0.022 sec/batch)\n",
      "epoch 360, step 300 / 900, loss = 1.88 (0.021 sec/batch)\n",
      "epoch 360, step 350 / 900, loss = 1.74 (0.021 sec/batch)\n",
      "epoch 360, step 400 / 900, loss = 1.84 (0.021 sec/batch)\n",
      "epoch 360, step 450 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 360, step 500 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 360, step 550 / 900, loss = 2.44 (0.021 sec/batch)\n",
      "epoch 360, step 600 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 360, step 650 / 900, loss = 3.03 (0.022 sec/batch)\n",
      "epoch 360, step 700 / 900, loss = 2.85 (0.021 sec/batch)\n",
      "epoch 360, step 750 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 360, step 800 / 900, loss = 1.95 (0.021 sec/batch)\n",
      "epoch 360, step 850 / 900, loss = 2.59 (0.022 sec/batch)\n",
      "epoch 360, step 900 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.85\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.08\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.58421802520752\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 361, step 50 / 900, loss = 2.34 (0.021 sec/batch)\n",
      "epoch 361, step 100 / 900, loss = 2.34 (0.022 sec/batch)\n",
      "epoch 361, step 150 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "epoch 361, step 200 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 361, step 250 / 900, loss = 2.63 (0.021 sec/batch)\n",
      "epoch 361, step 300 / 900, loss = 1.46 (0.021 sec/batch)\n",
      "epoch 361, step 350 / 900, loss = 1.39 (0.021 sec/batch)\n",
      "epoch 361, step 400 / 900, loss = 1.81 (0.021 sec/batch)\n",
      "epoch 361, step 450 / 900, loss = 1.75 (0.021 sec/batch)\n",
      "epoch 361, step 500 / 900, loss = 1.82 (0.021 sec/batch)\n",
      "epoch 361, step 550 / 900, loss = 1.84 (0.021 sec/batch)\n",
      "epoch 361, step 600 / 900, loss = 2.51 (0.022 sec/batch)\n",
      "epoch 361, step 650 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "epoch 361, step 700 / 900, loss = 2.58 (0.021 sec/batch)\n",
      "epoch 361, step 750 / 900, loss = 2.51 (0.021 sec/batch)\n",
      "epoch 361, step 800 / 900, loss = 2.26 (0.021 sec/batch)\n",
      "epoch 361, step 850 / 900, loss = 2.09 (0.022 sec/batch)\n",
      "epoch 361, step 900 / 900, loss = 3.32 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.59\n",
      " avg loss = 2.24\n",
      "\n",
      "Validation error:\n",
      " accuracy = 48.68\n",
      " avg loss = 2.93\n",
      "\n",
      "Epoch time: 24.61263108253479\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 362, step 50 / 900, loss = 2.47 (0.021 sec/batch)\n",
      "epoch 362, step 100 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 362, step 150 / 900, loss = 1.41 (0.022 sec/batch)\n",
      "epoch 362, step 200 / 900, loss = 1.79 (0.021 sec/batch)\n",
      "epoch 362, step 250 / 900, loss = 3.02 (0.021 sec/batch)\n",
      "epoch 362, step 300 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 362, step 350 / 900, loss = 2.71 (0.021 sec/batch)\n",
      "epoch 362, step 400 / 900, loss = 1.89 (0.022 sec/batch)\n",
      "epoch 362, step 450 / 900, loss = 2.67 (0.021 sec/batch)\n",
      "epoch 362, step 500 / 900, loss = 2.58 (0.021 sec/batch)\n",
      "epoch 362, step 550 / 900, loss = 2.49 (0.021 sec/batch)\n",
      "epoch 362, step 600 / 900, loss = 2.59 (0.022 sec/batch)\n",
      "epoch 362, step 650 / 900, loss = 1.90 (0.022 sec/batch)\n",
      "epoch 362, step 700 / 900, loss = 1.96 (0.021 sec/batch)\n",
      "epoch 362, step 750 / 900, loss = 1.80 (0.021 sec/batch)\n",
      "epoch 362, step 800 / 900, loss = 2.51 (0.021 sec/batch)\n",
      "epoch 362, step 850 / 900, loss = 1.65 (0.021 sec/batch)\n",
      "epoch 362, step 900 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.63\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.84\n",
      " avg loss = 2.85\n",
      "\n",
      "Epoch time: 24.58501148223877\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 363, step 50 / 900, loss = 2.60 (0.021 sec/batch)\n",
      "epoch 363, step 100 / 900, loss = 2.60 (0.021 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 363, step 150 / 900, loss = 1.91 (0.022 sec/batch)\n",
      "epoch 363, step 200 / 900, loss = 1.68 (0.021 sec/batch)\n",
      "epoch 363, step 250 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 363, step 300 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 363, step 350 / 900, loss = 1.77 (0.022 sec/batch)\n",
      "epoch 363, step 400 / 900, loss = 3.07 (0.021 sec/batch)\n",
      "epoch 363, step 450 / 900, loss = 2.61 (0.022 sec/batch)\n",
      "epoch 363, step 500 / 900, loss = 1.67 (0.022 sec/batch)\n",
      "epoch 363, step 550 / 900, loss = 2.59 (0.021 sec/batch)\n",
      "epoch 363, step 600 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 363, step 650 / 900, loss = 2.58 (0.022 sec/batch)\n",
      "epoch 363, step 700 / 900, loss = 1.87 (0.021 sec/batch)\n",
      "epoch 363, step 750 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 363, step 800 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 363, step 850 / 900, loss = 2.39 (0.021 sec/batch)\n",
      "epoch 363, step 900 / 900, loss = 2.52 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.51\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.40\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.566797971725464\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 364, step 50 / 900, loss = 2.38 (0.022 sec/batch)\n",
      "epoch 364, step 100 / 900, loss = 1.78 (0.021 sec/batch)\n",
      "epoch 364, step 150 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "epoch 364, step 200 / 900, loss = 1.80 (0.022 sec/batch)\n",
      "epoch 364, step 250 / 900, loss = 2.63 (0.021 sec/batch)\n",
      "epoch 364, step 300 / 900, loss = 1.64 (0.021 sec/batch)\n",
      "epoch 364, step 350 / 900, loss = 1.63 (0.021 sec/batch)\n",
      "epoch 364, step 400 / 900, loss = 2.07 (0.022 sec/batch)\n",
      "epoch 364, step 450 / 900, loss = 1.25 (0.022 sec/batch)\n",
      "epoch 364, step 500 / 900, loss = 1.27 (0.021 sec/batch)\n",
      "epoch 364, step 550 / 900, loss = 1.77 (0.021 sec/batch)\n",
      "epoch 364, step 600 / 900, loss = 2.51 (0.021 sec/batch)\n",
      "epoch 364, step 650 / 900, loss = 1.87 (0.022 sec/batch)\n",
      "epoch 364, step 700 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 364, step 750 / 900, loss = 1.90 (0.021 sec/batch)\n",
      "epoch 364, step 800 / 900, loss = 3.20 (0.021 sec/batch)\n",
      "epoch 364, step 850 / 900, loss = 3.20 (0.022 sec/batch)\n",
      "epoch 364, step 900 / 900, loss = 1.90 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.71\n",
      " avg loss = 2.24\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.12\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.584840059280396\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 365, step 50 / 900, loss = 2.40 (0.022 sec/batch)\n",
      "epoch 365, step 100 / 900, loss = 2.61 (0.021 sec/batch)\n",
      "epoch 365, step 150 / 900, loss = 1.75 (0.021 sec/batch)\n",
      "epoch 365, step 200 / 900, loss = 2.75 (0.022 sec/batch)\n",
      "epoch 365, step 250 / 900, loss = 2.34 (0.021 sec/batch)\n",
      "epoch 365, step 300 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 365, step 350 / 900, loss = 1.92 (0.021 sec/batch)\n",
      "epoch 365, step 400 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "epoch 365, step 450 / 900, loss = 1.88 (0.021 sec/batch)\n",
      "epoch 365, step 500 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 365, step 550 / 900, loss = 1.73 (0.021 sec/batch)\n",
      "epoch 365, step 600 / 900, loss = 2.41 (0.022 sec/batch)\n",
      "epoch 365, step 650 / 900, loss = 1.87 (0.021 sec/batch)\n",
      "epoch 365, step 700 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 365, step 750 / 900, loss = 2.64 (0.021 sec/batch)\n",
      "epoch 365, step 800 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 365, step 850 / 900, loss = 1.99 (0.021 sec/batch)\n",
      "epoch 365, step 900 / 900, loss = 2.34 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.40\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.96\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.577036142349243\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 366, step 50 / 900, loss = 1.26 (0.022 sec/batch)\n",
      "epoch 366, step 100 / 900, loss = 1.72 (0.021 sec/batch)\n",
      "epoch 366, step 150 / 900, loss = 2.86 (0.021 sec/batch)\n",
      "epoch 366, step 200 / 900, loss = 2.51 (0.021 sec/batch)\n",
      "epoch 366, step 250 / 900, loss = 2.70 (0.021 sec/batch)\n",
      "epoch 366, step 300 / 900, loss = 2.08 (0.022 sec/batch)\n",
      "epoch 366, step 350 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 366, step 400 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 366, step 450 / 900, loss = 2.28 (0.022 sec/batch)\n",
      "epoch 366, step 500 / 900, loss = 2.57 (0.021 sec/batch)\n",
      "epoch 366, step 550 / 900, loss = 2.57 (0.021 sec/batch)\n",
      "epoch 366, step 600 / 900, loss = 1.90 (0.021 sec/batch)\n",
      "epoch 366, step 650 / 900, loss = 2.82 (0.021 sec/batch)\n",
      "epoch 366, step 700 / 900, loss = 2.73 (0.021 sec/batch)\n",
      "epoch 366, step 750 / 900, loss = 2.88 (0.021 sec/batch)\n",
      "epoch 366, step 800 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "epoch 366, step 850 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 366, step 900 / 900, loss = 1.88 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.48\n",
      " avg loss = 2.28\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.96\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.60258197784424\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 367, step 50 / 900, loss = 1.61 (0.021 sec/batch)\n",
      "epoch 367, step 100 / 900, loss = 1.67 (0.021 sec/batch)\n",
      "epoch 367, step 150 / 900, loss = 2.45 (0.021 sec/batch)\n",
      "epoch 367, step 200 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 367, step 250 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "epoch 367, step 300 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 367, step 350 / 900, loss = 2.08 (0.022 sec/batch)\n",
      "epoch 367, step 400 / 900, loss = 1.84 (0.022 sec/batch)\n",
      "epoch 367, step 450 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 367, step 500 / 900, loss = 1.81 (0.021 sec/batch)\n",
      "epoch 367, step 550 / 900, loss = 1.98 (0.021 sec/batch)\n",
      "epoch 367, step 600 / 900, loss = 3.06 (0.021 sec/batch)\n",
      "epoch 367, step 650 / 900, loss = 2.41 (0.021 sec/batch)\n",
      "epoch 367, step 700 / 900, loss = 2.26 (0.021 sec/batch)\n",
      "epoch 367, step 750 / 900, loss = 3.53 (0.022 sec/batch)\n",
      "epoch 367, step 800 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "epoch 367, step 850 / 900, loss = 1.24 (0.021 sec/batch)\n",
      "epoch 367, step 900 / 900, loss = 1.80 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.72\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.50\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.579177856445312\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 368, step 50 / 900, loss = 1.96 (0.021 sec/batch)\n",
      "epoch 368, step 100 / 900, loss = 2.81 (0.022 sec/batch)\n",
      "epoch 368, step 150 / 900, loss = 2.68 (0.021 sec/batch)\n",
      "epoch 368, step 200 / 900, loss = 1.70 (0.020 sec/batch)\n",
      "epoch 368, step 250 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 368, step 300 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 368, step 350 / 900, loss = 2.44 (0.021 sec/batch)\n",
      "epoch 368, step 400 / 900, loss = 2.60 (0.021 sec/batch)\n",
      "epoch 368, step 450 / 900, loss = 2.56 (0.021 sec/batch)\n",
      "epoch 368, step 500 / 900, loss = 2.70 (0.021 sec/batch)\n",
      "epoch 368, step 550 / 900, loss = 3.53 (0.021 sec/batch)\n",
      "epoch 368, step 600 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 368, step 650 / 900, loss = 2.34 (0.022 sec/batch)\n",
      "epoch 368, step 700 / 900, loss = 2.06 (0.022 sec/batch)\n",
      "epoch 368, step 750 / 900, loss = 1.74 (0.021 sec/batch)\n",
      "epoch 368, step 800 / 900, loss = 3.10 (0.021 sec/batch)\n",
      "epoch 368, step 850 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 368, step 900 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.42\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.80\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.599759101867676\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 369, step 50 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 369, step 100 / 900, loss = 2.92 (0.021 sec/batch)\n",
      "epoch 369, step 150 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 369, step 200 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "epoch 369, step 250 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 369, step 300 / 900, loss = 1.63 (0.021 sec/batch)\n",
      "epoch 369, step 350 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 369, step 400 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 369, step 450 / 900, loss = 2.24 (0.020 sec/batch)\n",
      "epoch 369, step 500 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "epoch 369, step 550 / 900, loss = 2.51 (0.021 sec/batch)\n",
      "epoch 369, step 600 / 900, loss = 1.80 (0.022 sec/batch)\n",
      "epoch 369, step 650 / 900, loss = 2.63 (0.022 sec/batch)\n",
      "epoch 369, step 700 / 900, loss = 3.04 (0.021 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 369, step 750 / 900, loss = 2.06 (0.022 sec/batch)\n",
      "epoch 369, step 800 / 900, loss = 1.78 (0.021 sec/batch)\n",
      "epoch 369, step 850 / 900, loss = 2.68 (0.021 sec/batch)\n",
      "epoch 369, step 900 / 900, loss = 2.15 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.53\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.38\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.577523708343506\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 370, step 50 / 900, loss = 2.40 (0.022 sec/batch)\n",
      "epoch 370, step 100 / 900, loss = 2.34 (0.021 sec/batch)\n",
      "epoch 370, step 150 / 900, loss = 2.14 (0.022 sec/batch)\n",
      "epoch 370, step 200 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "epoch 370, step 250 / 900, loss = 1.98 (0.022 sec/batch)\n",
      "epoch 370, step 300 / 900, loss = 2.76 (0.021 sec/batch)\n",
      "epoch 370, step 350 / 900, loss = 2.06 (0.020 sec/batch)\n",
      "epoch 370, step 400 / 900, loss = 1.75 (0.021 sec/batch)\n",
      "epoch 370, step 450 / 900, loss = 1.25 (0.021 sec/batch)\n",
      "epoch 370, step 500 / 900, loss = 1.98 (0.022 sec/batch)\n",
      "epoch 370, step 550 / 900, loss = 3.06 (0.021 sec/batch)\n",
      "epoch 370, step 600 / 900, loss = 3.63 (0.020 sec/batch)\n",
      "epoch 370, step 650 / 900, loss = 1.76 (0.021 sec/batch)\n",
      "epoch 370, step 700 / 900, loss = 1.81 (0.020 sec/batch)\n",
      "epoch 370, step 750 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 370, step 800 / 900, loss = 3.11 (0.021 sec/batch)\n",
      "epoch 370, step 850 / 900, loss = 2.76 (0.021 sec/batch)\n",
      "epoch 370, step 900 / 900, loss = 1.96 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.47\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.14\n",
      " avg loss = 2.86\n",
      "\n",
      "Epoch time: 24.604398250579834\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 371, step 50 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 371, step 100 / 900, loss = 2.89 (0.022 sec/batch)\n",
      "epoch 371, step 150 / 900, loss = 1.71 (0.021 sec/batch)\n",
      "epoch 371, step 200 / 900, loss = 2.67 (0.021 sec/batch)\n",
      "epoch 371, step 250 / 900, loss = 1.84 (0.022 sec/batch)\n",
      "epoch 371, step 300 / 900, loss = 3.13 (0.021 sec/batch)\n",
      "epoch 371, step 350 / 900, loss = 1.81 (0.021 sec/batch)\n",
      "epoch 371, step 400 / 900, loss = 2.88 (0.021 sec/batch)\n",
      "epoch 371, step 450 / 900, loss = 1.91 (0.021 sec/batch)\n",
      "epoch 371, step 500 / 900, loss = 2.86 (0.020 sec/batch)\n",
      "epoch 371, step 550 / 900, loss = 2.40 (0.021 sec/batch)\n",
      "epoch 371, step 600 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "epoch 371, step 650 / 900, loss = 2.30 (0.022 sec/batch)\n",
      "epoch 371, step 700 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 371, step 750 / 900, loss = 2.80 (0.021 sec/batch)\n",
      "epoch 371, step 800 / 900, loss = 2.40 (0.022 sec/batch)\n",
      "epoch 371, step 850 / 900, loss = 2.75 (0.021 sec/batch)\n",
      "epoch 371, step 900 / 900, loss = 2.44 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.49\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.74\n",
      " avg loss = 2.86\n",
      "\n",
      "Epoch time: 24.639577865600586\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 372, step 50 / 900, loss = 2.79 (0.022 sec/batch)\n",
      "epoch 372, step 100 / 900, loss = 2.52 (0.020 sec/batch)\n",
      "epoch 372, step 150 / 900, loss = 1.86 (0.021 sec/batch)\n",
      "epoch 372, step 200 / 900, loss = 2.79 (0.022 sec/batch)\n",
      "epoch 372, step 250 / 900, loss = 1.57 (0.021 sec/batch)\n",
      "epoch 372, step 300 / 900, loss = 2.06 (0.022 sec/batch)\n",
      "epoch 372, step 350 / 900, loss = 1.73 (0.022 sec/batch)\n",
      "epoch 372, step 400 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 372, step 450 / 900, loss = 2.27 (0.022 sec/batch)\n",
      "epoch 372, step 500 / 900, loss = 1.64 (0.021 sec/batch)\n",
      "epoch 372, step 550 / 900, loss = 2.41 (0.021 sec/batch)\n",
      "epoch 372, step 600 / 900, loss = 2.68 (0.022 sec/batch)\n",
      "epoch 372, step 650 / 900, loss = 1.91 (0.022 sec/batch)\n",
      "epoch 372, step 700 / 900, loss = 2.79 (0.021 sec/batch)\n",
      "epoch 372, step 750 / 900, loss = 2.77 (0.022 sec/batch)\n",
      "epoch 372, step 800 / 900, loss = 1.79 (0.021 sec/batch)\n",
      "epoch 372, step 850 / 900, loss = 1.87 (0.022 sec/batch)\n",
      "epoch 372, step 900 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.39\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.04\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.580409049987793\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 373, step 50 / 900, loss = 2.87 (0.021 sec/batch)\n",
      "epoch 373, step 100 / 900, loss = 1.61 (0.022 sec/batch)\n",
      "epoch 373, step 150 / 900, loss = 1.86 (0.021 sec/batch)\n",
      "epoch 373, step 200 / 900, loss = 2.69 (0.022 sec/batch)\n",
      "epoch 373, step 250 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 373, step 300 / 900, loss = 3.04 (0.021 sec/batch)\n",
      "epoch 373, step 350 / 900, loss = 2.14 (0.022 sec/batch)\n",
      "epoch 373, step 400 / 900, loss = 3.37 (0.021 sec/batch)\n",
      "epoch 373, step 450 / 900, loss = 1.43 (0.021 sec/batch)\n",
      "epoch 373, step 500 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 373, step 550 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "epoch 373, step 600 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 373, step 650 / 900, loss = 2.26 (0.022 sec/batch)\n",
      "epoch 373, step 700 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 373, step 750 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 373, step 800 / 900, loss = 2.82 (0.021 sec/batch)\n",
      "epoch 373, step 850 / 900, loss = 2.57 (0.021 sec/batch)\n",
      "epoch 373, step 900 / 900, loss = 1.78 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.56\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.88\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.58327603340149\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 374, step 50 / 900, loss = 1.87 (0.021 sec/batch)\n",
      "epoch 374, step 100 / 900, loss = 2.69 (0.021 sec/batch)\n",
      "epoch 374, step 150 / 900, loss = 2.68 (0.021 sec/batch)\n",
      "epoch 374, step 200 / 900, loss = 2.24 (0.021 sec/batch)\n",
      "epoch 374, step 250 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 374, step 300 / 900, loss = 2.35 (0.022 sec/batch)\n",
      "epoch 374, step 350 / 900, loss = 1.59 (0.021 sec/batch)\n",
      "epoch 374, step 400 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "epoch 374, step 450 / 900, loss = 2.32 (0.022 sec/batch)\n",
      "epoch 374, step 500 / 900, loss = 2.62 (0.022 sec/batch)\n",
      "epoch 374, step 550 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 374, step 600 / 900, loss = 3.26 (0.021 sec/batch)\n",
      "epoch 374, step 650 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 374, step 700 / 900, loss = 1.53 (0.021 sec/batch)\n",
      "epoch 374, step 750 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "epoch 374, step 800 / 900, loss = 2.57 (0.022 sec/batch)\n",
      "epoch 374, step 850 / 900, loss = 1.85 (0.022 sec/batch)\n",
      "epoch 374, step 900 / 900, loss = 2.08 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.62\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.46\n",
      " avg loss = 2.92\n",
      "\n",
      "Epoch time: 24.58833384513855\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 375, step 50 / 900, loss = 1.98 (0.020 sec/batch)\n",
      "epoch 375, step 100 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 375, step 150 / 900, loss = 2.11 (0.022 sec/batch)\n",
      "epoch 375, step 200 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 375, step 250 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 375, step 300 / 900, loss = 1.73 (0.021 sec/batch)\n",
      "epoch 375, step 350 / 900, loss = 2.67 (0.022 sec/batch)\n",
      "epoch 375, step 400 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 375, step 450 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 375, step 500 / 900, loss = 2.75 (0.021 sec/batch)\n",
      "epoch 375, step 550 / 900, loss = 2.86 (0.021 sec/batch)\n",
      "epoch 375, step 600 / 900, loss = 1.79 (0.021 sec/batch)\n",
      "epoch 375, step 650 / 900, loss = 1.99 (0.022 sec/batch)\n",
      "epoch 375, step 700 / 900, loss = 2.82 (0.021 sec/batch)\n",
      "epoch 375, step 750 / 900, loss = 1.81 (0.022 sec/batch)\n",
      "epoch 375, step 800 / 900, loss = 3.28 (0.021 sec/batch)\n",
      "epoch 375, step 850 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 375, step 900 / 900, loss = 1.85 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.40\n",
      " avg loss = 2.28\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.98\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.58485460281372\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 376, step 50 / 900, loss = 1.99 (0.022 sec/batch)\n",
      "epoch 376, step 100 / 900, loss = 2.15 (0.021 sec/batch)\n",
      "epoch 376, step 150 / 900, loss = 3.40 (0.022 sec/batch)\n",
      "epoch 376, step 200 / 900, loss = 1.79 (0.022 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 376, step 250 / 900, loss = 1.33 (0.021 sec/batch)\n",
      "epoch 376, step 300 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 376, step 350 / 900, loss = 1.99 (0.022 sec/batch)\n",
      "epoch 376, step 400 / 900, loss = 1.95 (0.022 sec/batch)\n",
      "epoch 376, step 450 / 900, loss = 2.68 (0.021 sec/batch)\n",
      "epoch 376, step 500 / 900, loss = 1.77 (0.021 sec/batch)\n",
      "epoch 376, step 550 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 376, step 600 / 900, loss = 2.87 (0.021 sec/batch)\n",
      "epoch 376, step 650 / 900, loss = 1.39 (0.021 sec/batch)\n",
      "epoch 376, step 700 / 900, loss = 1.76 (0.021 sec/batch)\n",
      "epoch 376, step 750 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 376, step 800 / 900, loss = 1.99 (0.022 sec/batch)\n",
      "epoch 376, step 850 / 900, loss = 2.60 (0.021 sec/batch)\n",
      "epoch 376, step 900 / 900, loss = 2.86 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.53\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.56\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.608888626098633\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 377, step 50 / 900, loss = 2.07 (0.022 sec/batch)\n",
      "epoch 377, step 100 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 377, step 150 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 377, step 200 / 900, loss = 2.07 (0.022 sec/batch)\n",
      "epoch 377, step 250 / 900, loss = 2.72 (0.021 sec/batch)\n",
      "epoch 377, step 300 / 900, loss = 1.80 (0.021 sec/batch)\n",
      "epoch 377, step 350 / 900, loss = 1.82 (0.022 sec/batch)\n",
      "epoch 377, step 400 / 900, loss = 2.39 (0.022 sec/batch)\n",
      "epoch 377, step 450 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 377, step 500 / 900, loss = 2.58 (0.021 sec/batch)\n",
      "epoch 377, step 550 / 900, loss = 2.32 (0.021 sec/batch)\n",
      "epoch 377, step 600 / 900, loss = 2.06 (0.022 sec/batch)\n",
      "epoch 377, step 650 / 900, loss = 2.75 (0.022 sec/batch)\n",
      "epoch 377, step 700 / 900, loss = 1.65 (0.021 sec/batch)\n",
      "epoch 377, step 750 / 900, loss = 1.80 (0.021 sec/batch)\n",
      "epoch 377, step 800 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 377, step 850 / 900, loss = 2.84 (0.022 sec/batch)\n",
      "epoch 377, step 900 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.76\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.00\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.58164954185486\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 378, step 50 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "epoch 378, step 100 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 378, step 150 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 378, step 200 / 900, loss = 1.59 (0.021 sec/batch)\n",
      "epoch 378, step 250 / 900, loss = 2.15 (0.021 sec/batch)\n",
      "epoch 378, step 300 / 900, loss = 1.90 (0.021 sec/batch)\n",
      "epoch 378, step 350 / 900, loss = 2.06 (0.022 sec/batch)\n",
      "epoch 378, step 400 / 900, loss = 1.80 (0.022 sec/batch)\n",
      "epoch 378, step 450 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 378, step 500 / 900, loss = 3.13 (0.022 sec/batch)\n",
      "epoch 378, step 550 / 900, loss = 2.49 (0.021 sec/batch)\n",
      "epoch 378, step 600 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 378, step 650 / 900, loss = 2.40 (0.021 sec/batch)\n",
      "epoch 378, step 700 / 900, loss = 2.85 (0.021 sec/batch)\n",
      "epoch 378, step 750 / 900, loss = 1.93 (0.021 sec/batch)\n",
      "epoch 378, step 800 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 378, step 850 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 378, step 900 / 900, loss = 2.04 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.62\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.32\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.600739240646362\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 379, step 50 / 900, loss = 3.07 (0.021 sec/batch)\n",
      "epoch 379, step 100 / 900, loss = 2.76 (0.021 sec/batch)\n",
      "epoch 379, step 150 / 900, loss = 2.90 (0.022 sec/batch)\n",
      "epoch 379, step 200 / 900, loss = 2.49 (0.021 sec/batch)\n",
      "epoch 379, step 250 / 900, loss = 1.93 (0.021 sec/batch)\n",
      "epoch 379, step 300 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "epoch 379, step 350 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 379, step 400 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 379, step 450 / 900, loss = 2.76 (0.021 sec/batch)\n",
      "epoch 379, step 500 / 900, loss = 2.03 (0.022 sec/batch)\n",
      "epoch 379, step 550 / 900, loss = 1.43 (0.021 sec/batch)\n",
      "epoch 379, step 600 / 900, loss = 2.78 (0.022 sec/batch)\n",
      "epoch 379, step 650 / 900, loss = 2.20 (0.022 sec/batch)\n",
      "epoch 379, step 700 / 900, loss = 1.73 (0.021 sec/batch)\n",
      "epoch 379, step 750 / 900, loss = 2.59 (0.021 sec/batch)\n",
      "epoch 379, step 800 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "epoch 379, step 850 / 900, loss = 1.69 (0.021 sec/batch)\n",
      "epoch 379, step 900 / 900, loss = 2.68 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.72\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 48.72\n",
      " avg loss = 2.94\n",
      "\n",
      "Epoch time: 24.584251403808594\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 380, step 50 / 900, loss = 3.29 (0.021 sec/batch)\n",
      "epoch 380, step 100 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "epoch 380, step 150 / 900, loss = 1.69 (0.021 sec/batch)\n",
      "epoch 380, step 200 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "epoch 380, step 250 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 380, step 300 / 900, loss = 2.45 (0.022 sec/batch)\n",
      "epoch 380, step 350 / 900, loss = 1.85 (0.022 sec/batch)\n",
      "epoch 380, step 400 / 900, loss = 2.49 (0.022 sec/batch)\n",
      "epoch 380, step 450 / 900, loss = 2.54 (0.022 sec/batch)\n",
      "epoch 380, step 500 / 900, loss = 2.75 (0.022 sec/batch)\n",
      "epoch 380, step 550 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 380, step 600 / 900, loss = 3.02 (0.022 sec/batch)\n",
      "epoch 380, step 650 / 900, loss = 2.21 (0.022 sec/batch)\n",
      "epoch 380, step 700 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 380, step 750 / 900, loss = 1.44 (0.020 sec/batch)\n",
      "epoch 380, step 800 / 900, loss = 2.74 (0.021 sec/batch)\n",
      "epoch 380, step 850 / 900, loss = 2.36 (0.021 sec/batch)\n",
      "epoch 380, step 900 / 900, loss = 1.93 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.73\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 51.10\n",
      " avg loss = 2.82\n",
      "\n",
      "Epoch time: 24.632964372634888\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 381, step 50 / 900, loss = 1.96 (0.021 sec/batch)\n",
      "epoch 381, step 100 / 900, loss = 1.65 (0.022 sec/batch)\n",
      "epoch 381, step 150 / 900, loss = 2.63 (0.022 sec/batch)\n",
      "epoch 381, step 200 / 900, loss = 3.39 (0.021 sec/batch)\n",
      "epoch 381, step 250 / 900, loss = 1.67 (0.021 sec/batch)\n",
      "epoch 381, step 300 / 900, loss = 1.89 (0.022 sec/batch)\n",
      "epoch 381, step 350 / 900, loss = 1.78 (0.021 sec/batch)\n",
      "epoch 381, step 400 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "epoch 381, step 450 / 900, loss = 3.03 (0.021 sec/batch)\n",
      "epoch 381, step 500 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 381, step 550 / 900, loss = 2.60 (0.022 sec/batch)\n",
      "epoch 381, step 600 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 381, step 650 / 900, loss = 1.71 (0.022 sec/batch)\n",
      "epoch 381, step 700 / 900, loss = 2.77 (0.021 sec/batch)\n",
      "epoch 381, step 750 / 900, loss = 1.68 (0.021 sec/batch)\n",
      "epoch 381, step 800 / 900, loss = 1.90 (0.021 sec/batch)\n",
      "epoch 381, step 850 / 900, loss = 2.59 (0.021 sec/batch)\n",
      "epoch 381, step 900 / 900, loss = 1.95 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.75\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.54\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.60103940963745\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 382, step 50 / 900, loss = 1.43 (0.021 sec/batch)\n",
      "epoch 382, step 100 / 900, loss = 2.14 (0.022 sec/batch)\n",
      "epoch 382, step 150 / 900, loss = 2.77 (0.021 sec/batch)\n",
      "epoch 382, step 200 / 900, loss = 2.15 (0.022 sec/batch)\n",
      "epoch 382, step 250 / 900, loss = 1.92 (0.021 sec/batch)\n",
      "epoch 382, step 300 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 382, step 350 / 900, loss = 3.03 (0.021 sec/batch)\n",
      "epoch 382, step 400 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "epoch 382, step 450 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 382, step 500 / 900, loss = 1.54 (0.022 sec/batch)\n",
      "epoch 382, step 550 / 900, loss = 1.88 (0.021 sec/batch)\n",
      "epoch 382, step 600 / 900, loss = 2.36 (0.022 sec/batch)\n",
      "epoch 382, step 650 / 900, loss = 2.08 (0.022 sec/batch)\n",
      "epoch 382, step 700 / 900, loss = 3.15 (0.021 sec/batch)\n",
      "epoch 382, step 750 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "epoch 382, step 800 / 900, loss = 2.96 (0.020 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 382, step 850 / 900, loss = 2.29 (0.022 sec/batch)\n",
      "epoch 382, step 900 / 900, loss = 1.94 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.47\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.24\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.576504707336426\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 383, step 50 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 383, step 100 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "epoch 383, step 150 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "epoch 383, step 200 / 900, loss = 1.80 (0.022 sec/batch)\n",
      "epoch 383, step 250 / 900, loss = 1.76 (0.021 sec/batch)\n",
      "epoch 383, step 300 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "epoch 383, step 350 / 900, loss = 3.05 (0.021 sec/batch)\n",
      "epoch 383, step 400 / 900, loss = 1.66 (0.022 sec/batch)\n",
      "epoch 383, step 450 / 900, loss = 2.55 (0.021 sec/batch)\n",
      "epoch 383, step 500 / 900, loss = 1.54 (0.021 sec/batch)\n",
      "epoch 383, step 550 / 900, loss = 1.49 (0.023 sec/batch)\n",
      "epoch 383, step 600 / 900, loss = 1.58 (0.022 sec/batch)\n",
      "epoch 383, step 650 / 900, loss = 2.81 (0.022 sec/batch)\n",
      "epoch 383, step 700 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 383, step 750 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "epoch 383, step 800 / 900, loss = 1.89 (0.022 sec/batch)\n",
      "epoch 383, step 850 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 383, step 900 / 900, loss = 2.67 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.51\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.80\n",
      " avg loss = 2.84\n",
      "\n",
      "Epoch time: 24.591189861297607\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 384, step 50 / 900, loss = 2.11 (0.022 sec/batch)\n",
      "epoch 384, step 100 / 900, loss = 3.38 (0.021 sec/batch)\n",
      "epoch 384, step 150 / 900, loss = 2.91 (0.021 sec/batch)\n",
      "epoch 384, step 200 / 900, loss = 3.43 (0.022 sec/batch)\n",
      "epoch 384, step 250 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 384, step 300 / 900, loss = 2.46 (0.022 sec/batch)\n",
      "epoch 384, step 350 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 384, step 400 / 900, loss = 1.86 (0.021 sec/batch)\n",
      "epoch 384, step 450 / 900, loss = 1.74 (0.022 sec/batch)\n",
      "epoch 384, step 500 / 900, loss = 3.07 (0.022 sec/batch)\n",
      "epoch 384, step 550 / 900, loss = 2.66 (0.021 sec/batch)\n",
      "epoch 384, step 600 / 900, loss = 1.70 (0.021 sec/batch)\n",
      "epoch 384, step 650 / 900, loss = 2.75 (0.022 sec/batch)\n",
      "epoch 384, step 700 / 900, loss = 1.48 (0.021 sec/batch)\n",
      "epoch 384, step 750 / 900, loss = 2.49 (0.021 sec/batch)\n",
      "epoch 384, step 800 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "epoch 384, step 850 / 900, loss = 2.81 (0.021 sec/batch)\n",
      "epoch 384, step 900 / 900, loss = 2.70 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.43\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.62\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.59789276123047\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 385, step 50 / 900, loss = 1.79 (0.021 sec/batch)\n",
      "epoch 385, step 100 / 900, loss = 2.46 (0.021 sec/batch)\n",
      "epoch 385, step 150 / 900, loss = 3.06 (0.021 sec/batch)\n",
      "epoch 385, step 200 / 900, loss = 1.98 (0.021 sec/batch)\n",
      "epoch 385, step 250 / 900, loss = 2.37 (0.022 sec/batch)\n",
      "epoch 385, step 300 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 385, step 350 / 900, loss = 2.54 (0.021 sec/batch)\n",
      "epoch 385, step 400 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 385, step 450 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 385, step 500 / 900, loss = 3.17 (0.021 sec/batch)\n",
      "epoch 385, step 550 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 385, step 600 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 385, step 650 / 900, loss = 3.26 (0.022 sec/batch)\n",
      "epoch 385, step 700 / 900, loss = 1.95 (0.021 sec/batch)\n",
      "epoch 385, step 750 / 900, loss = 2.52 (0.022 sec/batch)\n",
      "epoch 385, step 800 / 900, loss = 2.26 (0.022 sec/batch)\n",
      "epoch 385, step 850 / 900, loss = 2.06 (0.022 sec/batch)\n",
      "epoch 385, step 900 / 900, loss = 2.62 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.35\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 48.62\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.620707750320435\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 386, step 50 / 900, loss = 3.18 (0.022 sec/batch)\n",
      "epoch 386, step 100 / 900, loss = 2.42 (0.020 sec/batch)\n",
      "epoch 386, step 150 / 900, loss = 2.83 (0.021 sec/batch)\n",
      "epoch 386, step 200 / 900, loss = 1.76 (0.021 sec/batch)\n",
      "epoch 386, step 250 / 900, loss = 2.06 (0.021 sec/batch)\n",
      "epoch 386, step 300 / 900, loss = 2.64 (0.022 sec/batch)\n",
      "epoch 386, step 350 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 386, step 400 / 900, loss = 1.92 (0.021 sec/batch)\n",
      "epoch 386, step 450 / 900, loss = 1.78 (0.022 sec/batch)\n",
      "epoch 386, step 500 / 900, loss = 2.66 (0.022 sec/batch)\n",
      "epoch 386, step 550 / 900, loss = 1.75 (0.021 sec/batch)\n",
      "epoch 386, step 600 / 900, loss = 2.80 (0.022 sec/batch)\n",
      "epoch 386, step 650 / 900, loss = 2.57 (0.022 sec/batch)\n",
      "epoch 386, step 700 / 900, loss = 2.85 (0.021 sec/batch)\n",
      "epoch 386, step 750 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 386, step 800 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 386, step 850 / 900, loss = 1.76 (0.022 sec/batch)\n",
      "epoch 386, step 900 / 900, loss = 1.96 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.30\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.16\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.586604356765747\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 387, step 50 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 387, step 100 / 900, loss = 1.59 (0.022 sec/batch)\n",
      "epoch 387, step 150 / 900, loss = 2.27 (0.022 sec/batch)\n",
      "epoch 387, step 200 / 900, loss = 2.50 (0.021 sec/batch)\n",
      "epoch 387, step 250 / 900, loss = 1.76 (0.022 sec/batch)\n",
      "epoch 387, step 300 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 387, step 350 / 900, loss = 2.69 (0.022 sec/batch)\n",
      "epoch 387, step 400 / 900, loss = 3.21 (0.021 sec/batch)\n",
      "epoch 387, step 450 / 900, loss = 2.92 (0.021 sec/batch)\n",
      "epoch 387, step 500 / 900, loss = 1.84 (0.022 sec/batch)\n",
      "epoch 387, step 550 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 387, step 600 / 900, loss = 1.92 (0.021 sec/batch)\n",
      "epoch 387, step 650 / 900, loss = 2.32 (0.022 sec/batch)\n",
      "epoch 387, step 700 / 900, loss = 2.62 (0.021 sec/batch)\n",
      "epoch 387, step 750 / 900, loss = 3.06 (0.021 sec/batch)\n",
      "epoch 387, step 800 / 900, loss = 1.82 (0.021 sec/batch)\n",
      "epoch 387, step 850 / 900, loss = 3.67 (0.022 sec/batch)\n",
      "epoch 387, step 900 / 900, loss = 1.74 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.46\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.32\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.595491886138916\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 388, step 50 / 900, loss = 2.32 (0.021 sec/batch)\n",
      "epoch 388, step 100 / 900, loss = 2.31 (0.022 sec/batch)\n",
      "epoch 388, step 150 / 900, loss = 2.81 (0.022 sec/batch)\n",
      "epoch 388, step 200 / 900, loss = 2.14 (0.020 sec/batch)\n",
      "epoch 388, step 250 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 388, step 300 / 900, loss = 2.01 (0.022 sec/batch)\n",
      "epoch 388, step 350 / 900, loss = 1.79 (0.022 sec/batch)\n",
      "epoch 388, step 400 / 900, loss = 2.75 (0.022 sec/batch)\n",
      "epoch 388, step 450 / 900, loss = 2.63 (0.021 sec/batch)\n",
      "epoch 388, step 500 / 900, loss = 2.14 (0.022 sec/batch)\n",
      "epoch 388, step 550 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 388, step 600 / 900, loss = 1.30 (0.021 sec/batch)\n",
      "epoch 388, step 650 / 900, loss = 2.88 (0.022 sec/batch)\n",
      "epoch 388, step 700 / 900, loss = 1.80 (0.022 sec/batch)\n",
      "epoch 388, step 750 / 900, loss = 2.15 (0.021 sec/batch)\n",
      "epoch 388, step 800 / 900, loss = 1.71 (0.021 sec/batch)\n",
      "epoch 388, step 850 / 900, loss = 2.67 (0.021 sec/batch)\n",
      "epoch 388, step 900 / 900, loss = 1.90 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.60\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 48.94\n",
      " avg loss = 2.95\n",
      "\n",
      "Epoch time: 24.59791326522827\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 389, step 50 / 900, loss = 2.46 (0.021 sec/batch)\n",
      "epoch 389, step 100 / 900, loss = 2.50 (0.021 sec/batch)\n",
      "epoch 389, step 150 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 389, step 200 / 900, loss = 1.34 (0.021 sec/batch)\n",
      "epoch 389, step 250 / 900, loss = 2.57 (0.021 sec/batch)\n",
      "epoch 389, step 300 / 900, loss = 1.88 (0.020 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 389, step 350 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 389, step 400 / 900, loss = 2.67 (0.021 sec/batch)\n",
      "epoch 389, step 450 / 900, loss = 2.56 (0.021 sec/batch)\n",
      "epoch 389, step 500 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "epoch 389, step 550 / 900, loss = 1.21 (0.021 sec/batch)\n",
      "epoch 389, step 600 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 389, step 650 / 900, loss = 2.26 (0.021 sec/batch)\n",
      "epoch 389, step 700 / 900, loss = 1.67 (0.021 sec/batch)\n",
      "epoch 389, step 750 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "epoch 389, step 800 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "epoch 389, step 850 / 900, loss = 2.60 (0.021 sec/batch)\n",
      "epoch 389, step 900 / 900, loss = 1.46 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.98\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.38\n",
      " avg loss = 2.86\n",
      "\n",
      "Epoch time: 24.584391117095947\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 390, step 50 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 390, step 100 / 900, loss = 2.20 (0.022 sec/batch)\n",
      "epoch 390, step 150 / 900, loss = 2.62 (0.022 sec/batch)\n",
      "epoch 390, step 200 / 900, loss = 1.64 (0.022 sec/batch)\n",
      "epoch 390, step 250 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 390, step 300 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 390, step 350 / 900, loss = 2.46 (0.021 sec/batch)\n",
      "epoch 390, step 400 / 900, loss = 1.85 (0.021 sec/batch)\n",
      "epoch 390, step 450 / 900, loss = 2.58 (0.021 sec/batch)\n",
      "epoch 390, step 500 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 390, step 550 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 390, step 600 / 900, loss = 2.29 (0.022 sec/batch)\n",
      "epoch 390, step 650 / 900, loss = 2.15 (0.022 sec/batch)\n",
      "epoch 390, step 700 / 900, loss = 0.90 (0.021 sec/batch)\n",
      "epoch 390, step 750 / 900, loss = 2.49 (0.022 sec/batch)\n",
      "epoch 390, step 800 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 390, step 850 / 900, loss = 3.12 (0.021 sec/batch)\n",
      "epoch 390, step 900 / 900, loss = 1.51 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.66\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.68\n",
      " avg loss = 2.95\n",
      "\n",
      "Epoch time: 24.591614484786987\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 391, step 50 / 900, loss = 3.22 (0.021 sec/batch)\n",
      "epoch 391, step 100 / 900, loss = 2.36 (0.022 sec/batch)\n",
      "epoch 391, step 150 / 900, loss = 1.84 (0.022 sec/batch)\n",
      "epoch 391, step 200 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 391, step 250 / 900, loss = 2.41 (0.021 sec/batch)\n",
      "epoch 391, step 300 / 900, loss = 1.95 (0.021 sec/batch)\n",
      "epoch 391, step 350 / 900, loss = 2.51 (0.021 sec/batch)\n",
      "epoch 391, step 400 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "epoch 391, step 450 / 900, loss = 1.84 (0.021 sec/batch)\n",
      "epoch 391, step 500 / 900, loss = 1.63 (0.021 sec/batch)\n",
      "epoch 391, step 550 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 391, step 600 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "epoch 391, step 650 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 391, step 700 / 900, loss = 1.96 (0.021 sec/batch)\n",
      "epoch 391, step 750 / 900, loss = 1.86 (0.022 sec/batch)\n",
      "epoch 391, step 800 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 391, step 850 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 391, step 900 / 900, loss = 2.40 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.61\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.46\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.58316946029663\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 392, step 50 / 900, loss = 1.90 (0.021 sec/batch)\n",
      "epoch 392, step 100 / 900, loss = 1.79 (0.021 sec/batch)\n",
      "epoch 392, step 150 / 900, loss = 2.68 (0.021 sec/batch)\n",
      "epoch 392, step 200 / 900, loss = 2.32 (0.020 sec/batch)\n",
      "epoch 392, step 250 / 900, loss = 1.86 (0.022 sec/batch)\n",
      "epoch 392, step 300 / 900, loss = 1.79 (0.021 sec/batch)\n",
      "epoch 392, step 350 / 900, loss = 2.72 (0.022 sec/batch)\n",
      "epoch 392, step 400 / 900, loss = 1.98 (0.021 sec/batch)\n",
      "epoch 392, step 450 / 900, loss = 2.80 (0.021 sec/batch)\n",
      "epoch 392, step 500 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 392, step 550 / 900, loss = 2.28 (0.022 sec/batch)\n",
      "epoch 392, step 600 / 900, loss = 2.62 (0.022 sec/batch)\n",
      "epoch 392, step 650 / 900, loss = 2.03 (0.022 sec/batch)\n",
      "epoch 392, step 700 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "epoch 392, step 750 / 900, loss = 2.63 (0.021 sec/batch)\n",
      "epoch 392, step 800 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 392, step 850 / 900, loss = 2.84 (0.022 sec/batch)\n",
      "epoch 392, step 900 / 900, loss = 1.92 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.81\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.52\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.58604645729065\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 393, step 50 / 900, loss = 2.80 (0.021 sec/batch)\n",
      "epoch 393, step 100 / 900, loss = 2.13 (0.022 sec/batch)\n",
      "epoch 393, step 150 / 900, loss = 2.15 (0.021 sec/batch)\n",
      "epoch 393, step 200 / 900, loss = 2.95 (0.022 sec/batch)\n",
      "epoch 393, step 250 / 900, loss = 2.72 (0.021 sec/batch)\n",
      "epoch 393, step 300 / 900, loss = 2.64 (0.021 sec/batch)\n",
      "epoch 393, step 350 / 900, loss = 1.86 (0.021 sec/batch)\n",
      "epoch 393, step 400 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "epoch 393, step 450 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "epoch 393, step 500 / 900, loss = 2.47 (0.021 sec/batch)\n",
      "epoch 393, step 550 / 900, loss = 2.70 (0.021 sec/batch)\n",
      "epoch 393, step 600 / 900, loss = 1.36 (0.021 sec/batch)\n",
      "epoch 393, step 650 / 900, loss = 1.99 (0.022 sec/batch)\n",
      "epoch 393, step 700 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 393, step 750 / 900, loss = 1.84 (0.023 sec/batch)\n",
      "epoch 393, step 800 / 900, loss = 1.78 (0.021 sec/batch)\n",
      "epoch 393, step 850 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "epoch 393, step 900 / 900, loss = 1.29 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.54\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.26\n",
      " avg loss = 2.92\n",
      "\n",
      "Epoch time: 24.605651378631592\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 394, step 50 / 900, loss = 2.61 (0.022 sec/batch)\n",
      "epoch 394, step 100 / 900, loss = 2.24 (0.022 sec/batch)\n",
      "epoch 394, step 150 / 900, loss = 1.96 (0.021 sec/batch)\n",
      "epoch 394, step 200 / 900, loss = 2.54 (0.021 sec/batch)\n",
      "epoch 394, step 250 / 900, loss = 1.63 (0.021 sec/batch)\n",
      "epoch 394, step 300 / 900, loss = 1.94 (0.022 sec/batch)\n",
      "epoch 394, step 350 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 394, step 400 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 394, step 450 / 900, loss = 2.15 (0.022 sec/batch)\n",
      "epoch 394, step 500 / 900, loss = 2.15 (0.021 sec/batch)\n",
      "epoch 394, step 550 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 394, step 600 / 900, loss = 1.69 (0.021 sec/batch)\n",
      "epoch 394, step 650 / 900, loss = 2.36 (0.021 sec/batch)\n",
      "epoch 394, step 700 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 394, step 750 / 900, loss = 3.73 (0.021 sec/batch)\n",
      "epoch 394, step 800 / 900, loss = 2.14 (0.022 sec/batch)\n",
      "epoch 394, step 850 / 900, loss = 2.10 (0.022 sec/batch)\n",
      "epoch 394, step 900 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.67\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.32\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.595629692077637\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 395, step 50 / 900, loss = 2.56 (0.021 sec/batch)\n",
      "epoch 395, step 100 / 900, loss = 2.57 (0.021 sec/batch)\n",
      "epoch 395, step 150 / 900, loss = 2.26 (0.022 sec/batch)\n",
      "epoch 395, step 200 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 395, step 250 / 900, loss = 2.73 (0.022 sec/batch)\n",
      "epoch 395, step 300 / 900, loss = 2.28 (0.022 sec/batch)\n",
      "epoch 395, step 350 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 395, step 400 / 900, loss = 3.80 (0.021 sec/batch)\n",
      "epoch 395, step 450 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 395, step 500 / 900, loss = 2.79 (0.021 sec/batch)\n",
      "epoch 395, step 550 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 395, step 600 / 900, loss = 2.75 (0.021 sec/batch)\n",
      "epoch 395, step 650 / 900, loss = 2.59 (0.021 sec/batch)\n",
      "epoch 395, step 700 / 900, loss = 1.47 (0.021 sec/batch)\n",
      "epoch 395, step 750 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 395, step 800 / 900, loss = 2.64 (0.021 sec/batch)\n",
      "epoch 395, step 850 / 900, loss = 1.58 (0.022 sec/batch)\n",
      "epoch 395, step 900 / 900, loss = 2.57 (0.021 sec/batch)\n",
      "Train error:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy = 55.34\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.46\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.604570388793945\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 396, step 50 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 396, step 100 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "epoch 396, step 150 / 900, loss = 2.92 (0.021 sec/batch)\n",
      "epoch 396, step 200 / 900, loss = 1.45 (0.021 sec/batch)\n",
      "epoch 396, step 250 / 900, loss = 1.82 (0.021 sec/batch)\n",
      "epoch 396, step 300 / 900, loss = 2.36 (0.021 sec/batch)\n",
      "epoch 396, step 350 / 900, loss = 2.29 (0.022 sec/batch)\n",
      "epoch 396, step 400 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 396, step 450 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 396, step 500 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 396, step 550 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 396, step 600 / 900, loss = 1.99 (0.021 sec/batch)\n",
      "epoch 396, step 650 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 396, step 700 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 396, step 750 / 900, loss = 2.93 (0.022 sec/batch)\n",
      "epoch 396, step 800 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "epoch 396, step 850 / 900, loss = 2.76 (0.021 sec/batch)\n",
      "epoch 396, step 900 / 900, loss = 1.64 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.81\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.34\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.579591035842896\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 397, step 50 / 900, loss = 2.28 (0.022 sec/batch)\n",
      "epoch 397, step 100 / 900, loss = 2.72 (0.022 sec/batch)\n",
      "epoch 397, step 150 / 900, loss = 2.16 (0.022 sec/batch)\n",
      "epoch 397, step 200 / 900, loss = 1.93 (0.021 sec/batch)\n",
      "epoch 397, step 250 / 900, loss = 1.81 (0.021 sec/batch)\n",
      "epoch 397, step 300 / 900, loss = 2.08 (0.022 sec/batch)\n",
      "epoch 397, step 350 / 900, loss = 3.00 (0.022 sec/batch)\n",
      "epoch 397, step 400 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 397, step 450 / 900, loss = 3.00 (0.022 sec/batch)\n",
      "epoch 397, step 500 / 900, loss = 2.18 (0.022 sec/batch)\n",
      "epoch 397, step 550 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 397, step 600 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 397, step 650 / 900, loss = 2.40 (0.022 sec/batch)\n",
      "epoch 397, step 700 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "epoch 397, step 750 / 900, loss = 1.80 (0.021 sec/batch)\n",
      "epoch 397, step 800 / 900, loss = 2.71 (0.021 sec/batch)\n",
      "epoch 397, step 850 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 397, step 900 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.72\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.86\n",
      " avg loss = 2.85\n",
      "\n",
      "Epoch time: 24.590003967285156\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 398, step 50 / 900, loss = 3.19 (0.021 sec/batch)\n",
      "epoch 398, step 100 / 900, loss = 2.68 (0.022 sec/batch)\n",
      "epoch 398, step 150 / 900, loss = 2.88 (0.021 sec/batch)\n",
      "epoch 398, step 200 / 900, loss = 1.98 (0.022 sec/batch)\n",
      "epoch 398, step 250 / 900, loss = 2.42 (0.022 sec/batch)\n",
      "epoch 398, step 300 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 398, step 350 / 900, loss = 1.31 (0.021 sec/batch)\n",
      "epoch 398, step 400 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "epoch 398, step 450 / 900, loss = 2.58 (0.022 sec/batch)\n",
      "epoch 398, step 500 / 900, loss = 2.23 (0.022 sec/batch)\n",
      "epoch 398, step 550 / 900, loss = 2.63 (0.021 sec/batch)\n",
      "epoch 398, step 600 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "epoch 398, step 650 / 900, loss = 2.59 (0.022 sec/batch)\n",
      "epoch 398, step 700 / 900, loss = 2.78 (0.021 sec/batch)\n",
      "epoch 398, step 750 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 398, step 800 / 900, loss = 3.09 (0.021 sec/batch)\n",
      "epoch 398, step 850 / 900, loss = 2.88 (0.021 sec/batch)\n",
      "epoch 398, step 900 / 900, loss = 2.83 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.49\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.42\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.597102642059326\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 399, step 50 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 399, step 100 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 399, step 150 / 900, loss = 2.64 (0.021 sec/batch)\n",
      "epoch 399, step 200 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 399, step 250 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "epoch 399, step 300 / 900, loss = 3.20 (0.022 sec/batch)\n",
      "epoch 399, step 350 / 900, loss = 2.34 (0.021 sec/batch)\n",
      "epoch 399, step 400 / 900, loss = 2.50 (0.022 sec/batch)\n",
      "epoch 399, step 450 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 399, step 500 / 900, loss = 1.61 (0.022 sec/batch)\n",
      "epoch 399, step 550 / 900, loss = 2.44 (0.021 sec/batch)\n",
      "epoch 399, step 600 / 900, loss = 2.12 (0.022 sec/batch)\n",
      "epoch 399, step 650 / 900, loss = 2.28 (0.022 sec/batch)\n",
      "epoch 399, step 700 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "epoch 399, step 750 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 399, step 800 / 900, loss = 2.66 (0.021 sec/batch)\n",
      "epoch 399, step 850 / 900, loss = 1.77 (0.021 sec/batch)\n",
      "epoch 399, step 900 / 900, loss = 1.74 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.46\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 48.92\n",
      " avg loss = 2.92\n",
      "\n",
      "Epoch time: 24.59870147705078\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 400, step 50 / 900, loss = 2.85 (0.021 sec/batch)\n",
      "epoch 400, step 100 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 400, step 150 / 900, loss = 2.98 (0.022 sec/batch)\n",
      "epoch 400, step 200 / 900, loss = 2.96 (0.021 sec/batch)\n",
      "epoch 400, step 250 / 900, loss = 1.59 (0.022 sec/batch)\n",
      "epoch 400, step 300 / 900, loss = 1.82 (0.021 sec/batch)\n",
      "epoch 400, step 350 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 400, step 400 / 900, loss = 2.66 (0.022 sec/batch)\n",
      "epoch 400, step 450 / 900, loss = 3.42 (0.021 sec/batch)\n",
      "epoch 400, step 500 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 400, step 550 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 400, step 600 / 900, loss = 1.88 (0.021 sec/batch)\n",
      "epoch 400, step 650 / 900, loss = 2.08 (0.022 sec/batch)\n",
      "epoch 400, step 700 / 900, loss = 2.51 (0.021 sec/batch)\n",
      "epoch 400, step 750 / 900, loss = 1.27 (0.021 sec/batch)\n",
      "epoch 400, step 800 / 900, loss = 1.69 (0.022 sec/batch)\n",
      "epoch 400, step 850 / 900, loss = 1.54 (0.021 sec/batch)\n",
      "epoch 400, step 900 / 900, loss = 2.63 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.70\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.26\n",
      " avg loss = 2.83\n",
      "\n",
      "Epoch time: 24.627479553222656\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 401, step 50 / 900, loss = 2.48 (0.022 sec/batch)\n",
      "epoch 401, step 100 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 401, step 150 / 900, loss = 1.96 (0.022 sec/batch)\n",
      "epoch 401, step 200 / 900, loss = 3.30 (0.021 sec/batch)\n",
      "epoch 401, step 250 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "epoch 401, step 300 / 900, loss = 1.82 (0.021 sec/batch)\n",
      "epoch 401, step 350 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 401, step 400 / 900, loss = 1.98 (0.022 sec/batch)\n",
      "epoch 401, step 450 / 900, loss = 1.19 (0.021 sec/batch)\n",
      "epoch 401, step 500 / 900, loss = 2.32 (0.021 sec/batch)\n",
      "epoch 401, step 550 / 900, loss = 2.96 (0.021 sec/batch)\n",
      "epoch 401, step 600 / 900, loss = 2.54 (0.022 sec/batch)\n",
      "epoch 401, step 650 / 900, loss = 1.96 (0.022 sec/batch)\n",
      "epoch 401, step 700 / 900, loss = 2.15 (0.021 sec/batch)\n",
      "epoch 401, step 750 / 900, loss = 2.01 (0.022 sec/batch)\n",
      "epoch 401, step 800 / 900, loss = 2.28 (0.022 sec/batch)\n",
      "epoch 401, step 850 / 900, loss = 2.52 (0.022 sec/batch)\n",
      "epoch 401, step 900 / 900, loss = 1.63 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.32\n",
      " avg loss = 2.28\n",
      "\n",
      "Validation error:\n",
      " accuracy = 48.18\n",
      " avg loss = 2.96\n",
      "\n",
      "Epoch time: 24.589072465896606\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 402, step 50 / 900, loss = 1.81 (0.022 sec/batch)\n",
      "epoch 402, step 100 / 900, loss = 2.36 (0.021 sec/batch)\n",
      "epoch 402, step 150 / 900, loss = 2.69 (0.021 sec/batch)\n",
      "epoch 402, step 200 / 900, loss = 2.58 (0.021 sec/batch)\n",
      "epoch 402, step 250 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 402, step 300 / 900, loss = 2.26 (0.022 sec/batch)\n",
      "epoch 402, step 350 / 900, loss = 2.73 (0.022 sec/batch)\n",
      "epoch 402, step 400 / 900, loss = 2.31 (0.022 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 402, step 450 / 900, loss = 3.00 (0.022 sec/batch)\n",
      "epoch 402, step 500 / 900, loss = 2.95 (0.021 sec/batch)\n",
      "epoch 402, step 550 / 900, loss = 1.98 (0.021 sec/batch)\n",
      "epoch 402, step 600 / 900, loss = 3.02 (0.022 sec/batch)\n",
      "epoch 402, step 650 / 900, loss = 2.39 (0.021 sec/batch)\n",
      "epoch 402, step 700 / 900, loss = 1.95 (0.022 sec/batch)\n",
      "epoch 402, step 750 / 900, loss = 1.44 (0.021 sec/batch)\n",
      "epoch 402, step 800 / 900, loss = 2.41 (0.021 sec/batch)\n",
      "epoch 402, step 850 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 402, step 900 / 900, loss = 1.34 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.51\n",
      " avg loss = 2.24\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.54\n",
      " avg loss = 2.92\n",
      "\n",
      "Epoch time: 24.590625286102295\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 403, step 50 / 900, loss = 2.55 (0.021 sec/batch)\n",
      "epoch 403, step 100 / 900, loss = 2.24 (0.022 sec/batch)\n",
      "epoch 403, step 150 / 900, loss = 2.55 (0.022 sec/batch)\n",
      "epoch 403, step 200 / 900, loss = 1.69 (0.021 sec/batch)\n",
      "epoch 403, step 250 / 900, loss = 2.70 (0.021 sec/batch)\n",
      "epoch 403, step 300 / 900, loss = 2.33 (0.022 sec/batch)\n",
      "epoch 403, step 350 / 900, loss = 1.71 (0.022 sec/batch)\n",
      "epoch 403, step 400 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 403, step 450 / 900, loss = 2.66 (0.021 sec/batch)\n",
      "epoch 403, step 500 / 900, loss = 2.16 (0.022 sec/batch)\n",
      "epoch 403, step 550 / 900, loss = 2.77 (0.021 sec/batch)\n",
      "epoch 403, step 600 / 900, loss = 3.09 (0.021 sec/batch)\n",
      "epoch 403, step 650 / 900, loss = 1.44 (0.022 sec/batch)\n",
      "epoch 403, step 700 / 900, loss = 2.41 (0.021 sec/batch)\n",
      "epoch 403, step 750 / 900, loss = 1.87 (0.022 sec/batch)\n",
      "epoch 403, step 800 / 900, loss = 2.84 (0.021 sec/batch)\n",
      "epoch 403, step 850 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 403, step 900 / 900, loss = 2.36 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.66\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.96\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.577574253082275\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 404, step 50 / 900, loss = 2.82 (0.021 sec/batch)\n",
      "epoch 404, step 100 / 900, loss = 2.93 (0.022 sec/batch)\n",
      "epoch 404, step 150 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 404, step 200 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "epoch 404, step 250 / 900, loss = 2.74 (0.021 sec/batch)\n",
      "epoch 404, step 300 / 900, loss = 1.03 (0.022 sec/batch)\n",
      "epoch 404, step 350 / 900, loss = 1.52 (0.021 sec/batch)\n",
      "epoch 404, step 400 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 404, step 450 / 900, loss = 2.74 (0.022 sec/batch)\n",
      "epoch 404, step 500 / 900, loss = 1.95 (0.021 sec/batch)\n",
      "epoch 404, step 550 / 900, loss = 1.66 (0.021 sec/batch)\n",
      "epoch 404, step 600 / 900, loss = 3.15 (0.021 sec/batch)\n",
      "epoch 404, step 650 / 900, loss = 2.45 (0.021 sec/batch)\n",
      "epoch 404, step 700 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 404, step 750 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 404, step 800 / 900, loss = 2.87 (0.021 sec/batch)\n",
      "epoch 404, step 850 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 404, step 900 / 900, loss = 2.02 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.59\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.78\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.625433683395386\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 405, step 50 / 900, loss = 1.71 (0.022 sec/batch)\n",
      "epoch 405, step 100 / 900, loss = 2.85 (0.021 sec/batch)\n",
      "epoch 405, step 150 / 900, loss = 2.04 (0.021 sec/batch)\n",
      "epoch 405, step 200 / 900, loss = 2.21 (0.022 sec/batch)\n",
      "epoch 405, step 250 / 900, loss = 3.33 (0.022 sec/batch)\n",
      "epoch 405, step 300 / 900, loss = 2.58 (0.021 sec/batch)\n",
      "epoch 405, step 350 / 900, loss = 1.74 (0.021 sec/batch)\n",
      "epoch 405, step 400 / 900, loss = 2.75 (0.021 sec/batch)\n",
      "epoch 405, step 450 / 900, loss = 2.14 (0.022 sec/batch)\n",
      "epoch 405, step 500 / 900, loss = 2.79 (0.022 sec/batch)\n",
      "epoch 405, step 550 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 405, step 600 / 900, loss = 1.76 (0.021 sec/batch)\n",
      "epoch 405, step 650 / 900, loss = 2.05 (0.022 sec/batch)\n",
      "epoch 405, step 700 / 900, loss = 1.40 (0.021 sec/batch)\n",
      "epoch 405, step 750 / 900, loss = 2.33 (0.022 sec/batch)\n",
      "epoch 405, step 800 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 405, step 850 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 405, step 900 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.29\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.54\n",
      " avg loss = 2.83\n",
      "\n",
      "Epoch time: 24.584516763687134\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 406, step 50 / 900, loss = 2.70 (0.022 sec/batch)\n",
      "epoch 406, step 100 / 900, loss = 1.74 (0.022 sec/batch)\n",
      "epoch 406, step 150 / 900, loss = 1.84 (0.021 sec/batch)\n",
      "epoch 406, step 200 / 900, loss = 1.67 (0.021 sec/batch)\n",
      "epoch 406, step 250 / 900, loss = 2.27 (0.022 sec/batch)\n",
      "epoch 406, step 300 / 900, loss = 1.68 (0.021 sec/batch)\n",
      "epoch 406, step 350 / 900, loss = 2.36 (0.022 sec/batch)\n",
      "epoch 406, step 400 / 900, loss = 1.61 (0.021 sec/batch)\n",
      "epoch 406, step 450 / 900, loss = 2.76 (0.022 sec/batch)\n",
      "epoch 406, step 500 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "epoch 406, step 550 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 406, step 600 / 900, loss = 1.75 (0.021 sec/batch)\n",
      "epoch 406, step 650 / 900, loss = 2.28 (0.022 sec/batch)\n",
      "epoch 406, step 700 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 406, step 750 / 900, loss = 2.96 (0.021 sec/batch)\n",
      "epoch 406, step 800 / 900, loss = 1.66 (0.021 sec/batch)\n",
      "epoch 406, step 850 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 406, step 900 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.42\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.96\n",
      " avg loss = 2.86\n",
      "\n",
      "Epoch time: 24.587212085723877\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 407, step 50 / 900, loss = 2.88 (0.021 sec/batch)\n",
      "epoch 407, step 100 / 900, loss = 2.26 (0.021 sec/batch)\n",
      "epoch 407, step 150 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 407, step 200 / 900, loss = 2.26 (0.021 sec/batch)\n",
      "epoch 407, step 250 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 407, step 300 / 900, loss = 3.20 (0.020 sec/batch)\n",
      "epoch 407, step 350 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 407, step 400 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 407, step 450 / 900, loss = 1.73 (0.021 sec/batch)\n",
      "epoch 407, step 500 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "epoch 407, step 550 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 407, step 600 / 900, loss = 1.92 (0.021 sec/batch)\n",
      "epoch 407, step 650 / 900, loss = 2.91 (0.021 sec/batch)\n",
      "epoch 407, step 700 / 900, loss = 1.69 (0.022 sec/batch)\n",
      "epoch 407, step 750 / 900, loss = 2.27 (0.022 sec/batch)\n",
      "epoch 407, step 800 / 900, loss = 1.96 (0.021 sec/batch)\n",
      "epoch 407, step 850 / 900, loss = 1.85 (0.020 sec/batch)\n",
      "epoch 407, step 900 / 900, loss = 2.30 (0.020 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.57\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.28\n",
      " avg loss = 2.85\n",
      "\n",
      "Epoch time: 24.58062720298767\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 408, step 50 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 408, step 100 / 900, loss = 1.53 (0.022 sec/batch)\n",
      "epoch 408, step 150 / 900, loss = 1.88 (0.021 sec/batch)\n",
      "epoch 408, step 200 / 900, loss = 2.84 (0.021 sec/batch)\n",
      "epoch 408, step 250 / 900, loss = 2.73 (0.021 sec/batch)\n",
      "epoch 408, step 300 / 900, loss = 2.70 (0.021 sec/batch)\n",
      "epoch 408, step 350 / 900, loss = 2.00 (0.022 sec/batch)\n",
      "epoch 408, step 400 / 900, loss = 1.55 (0.021 sec/batch)\n",
      "epoch 408, step 450 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "epoch 408, step 500 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "epoch 408, step 550 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "epoch 408, step 600 / 900, loss = 2.74 (0.021 sec/batch)\n",
      "epoch 408, step 650 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 408, step 700 / 900, loss = 2.46 (0.022 sec/batch)\n",
      "epoch 408, step 750 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 408, step 800 / 900, loss = 2.04 (0.021 sec/batch)\n",
      "epoch 408, step 850 / 900, loss = 2.69 (0.021 sec/batch)\n",
      "epoch 408, step 900 / 900, loss = 2.04 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.28\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.00\n",
      " avg loss = 2.92\n",
      "\n",
      "Epoch time: 24.578599452972412\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 409, step 50 / 900, loss = 2.82 (0.022 sec/batch)\n",
      "epoch 409, step 100 / 900, loss = 2.24 (0.022 sec/batch)\n",
      "epoch 409, step 150 / 900, loss = 2.96 (0.021 sec/batch)\n",
      "epoch 409, step 200 / 900, loss = 1.67 (0.021 sec/batch)\n",
      "epoch 409, step 250 / 900, loss = 2.64 (0.021 sec/batch)\n",
      "epoch 409, step 300 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 409, step 350 / 900, loss = 2.30 (0.022 sec/batch)\n",
      "epoch 409, step 400 / 900, loss = 2.74 (0.021 sec/batch)\n",
      "epoch 409, step 450 / 900, loss = 1.47 (0.021 sec/batch)\n",
      "epoch 409, step 500 / 900, loss = 1.90 (0.021 sec/batch)\n",
      "epoch 409, step 550 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 409, step 600 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 409, step 650 / 900, loss = 2.29 (0.022 sec/batch)\n",
      "epoch 409, step 700 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 409, step 750 / 900, loss = 1.67 (0.021 sec/batch)\n",
      "epoch 409, step 800 / 900, loss = 2.04 (0.021 sec/batch)\n",
      "epoch 409, step 850 / 900, loss = 2.59 (0.022 sec/batch)\n",
      "epoch 409, step 900 / 900, loss = 2.46 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.33\n",
      " avg loss = 2.28\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.70\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.613993644714355\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 410, step 50 / 900, loss = 1.95 (0.021 sec/batch)\n",
      "epoch 410, step 100 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 410, step 150 / 900, loss = 2.71 (0.021 sec/batch)\n",
      "epoch 410, step 200 / 900, loss = 1.67 (0.021 sec/batch)\n",
      "epoch 410, step 250 / 900, loss = 2.71 (0.021 sec/batch)\n",
      "epoch 410, step 300 / 900, loss = 2.22 (0.022 sec/batch)\n",
      "epoch 410, step 350 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 410, step 400 / 900, loss = 2.47 (0.021 sec/batch)\n",
      "epoch 410, step 450 / 900, loss = 2.85 (0.020 sec/batch)\n",
      "epoch 410, step 500 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "epoch 410, step 550 / 900, loss = 1.26 (0.021 sec/batch)\n",
      "epoch 410, step 600 / 900, loss = 1.75 (0.021 sec/batch)\n",
      "epoch 410, step 650 / 900, loss = 2.51 (0.022 sec/batch)\n",
      "epoch 410, step 700 / 900, loss = 2.76 (0.021 sec/batch)\n",
      "epoch 410, step 750 / 900, loss = 1.83 (0.022 sec/batch)\n",
      "epoch 410, step 800 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 410, step 850 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 410, step 900 / 900, loss = 1.31 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.59\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 48.94\n",
      " avg loss = 2.96\n",
      "\n",
      "Epoch time: 24.57336711883545\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 411, step 50 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "epoch 411, step 100 / 900, loss = 1.36 (0.021 sec/batch)\n",
      "epoch 411, step 150 / 900, loss = 2.11 (0.022 sec/batch)\n",
      "epoch 411, step 200 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "epoch 411, step 250 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 411, step 300 / 900, loss = 1.74 (0.022 sec/batch)\n",
      "epoch 411, step 350 / 900, loss = 1.75 (0.021 sec/batch)\n",
      "epoch 411, step 400 / 900, loss = 1.72 (0.022 sec/batch)\n",
      "epoch 411, step 450 / 900, loss = 2.57 (0.022 sec/batch)\n",
      "epoch 411, step 500 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 411, step 550 / 900, loss = 1.79 (0.022 sec/batch)\n",
      "epoch 411, step 600 / 900, loss = 2.34 (0.021 sec/batch)\n",
      "epoch 411, step 650 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 411, step 700 / 900, loss = 2.40 (0.021 sec/batch)\n",
      "epoch 411, step 750 / 900, loss = 2.46 (0.022 sec/batch)\n",
      "epoch 411, step 800 / 900, loss = 2.60 (0.021 sec/batch)\n",
      "epoch 411, step 850 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 411, step 900 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.55\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.64\n",
      " avg loss = 2.81\n",
      "\n",
      "Epoch time: 24.59784770011902\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 412, step 50 / 900, loss = 2.57 (0.021 sec/batch)\n",
      "epoch 412, step 100 / 900, loss = 3.11 (0.022 sec/batch)\n",
      "epoch 412, step 150 / 900, loss = 2.68 (0.021 sec/batch)\n",
      "epoch 412, step 200 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 412, step 250 / 900, loss = 2.55 (0.022 sec/batch)\n",
      "epoch 412, step 300 / 900, loss = 1.76 (0.021 sec/batch)\n",
      "epoch 412, step 350 / 900, loss = 1.66 (0.022 sec/batch)\n",
      "epoch 412, step 400 / 900, loss = 1.67 (0.021 sec/batch)\n",
      "epoch 412, step 450 / 900, loss = 2.67 (0.021 sec/batch)\n",
      "epoch 412, step 500 / 900, loss = 2.05 (0.022 sec/batch)\n",
      "epoch 412, step 550 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 412, step 600 / 900, loss = 1.99 (0.021 sec/batch)\n",
      "epoch 412, step 650 / 900, loss = 1.47 (0.021 sec/batch)\n",
      "epoch 412, step 700 / 900, loss = 2.93 (0.021 sec/batch)\n",
      "epoch 412, step 750 / 900, loss = 1.54 (0.021 sec/batch)\n",
      "epoch 412, step 800 / 900, loss = 1.83 (0.022 sec/batch)\n",
      "epoch 412, step 850 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 412, step 900 / 900, loss = 1.74 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.67\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.76\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.593515634536743\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 413, step 50 / 900, loss = 1.95 (0.022 sec/batch)\n",
      "epoch 413, step 100 / 900, loss = 2.75 (0.021 sec/batch)\n",
      "epoch 413, step 150 / 900, loss = 1.61 (0.021 sec/batch)\n",
      "epoch 413, step 200 / 900, loss = 2.04 (0.021 sec/batch)\n",
      "epoch 413, step 250 / 900, loss = 2.24 (0.022 sec/batch)\n",
      "epoch 413, step 300 / 900, loss = 1.50 (0.022 sec/batch)\n",
      "epoch 413, step 350 / 900, loss = 1.85 (0.022 sec/batch)\n",
      "epoch 413, step 400 / 900, loss = 2.73 (0.021 sec/batch)\n",
      "epoch 413, step 450 / 900, loss = 1.81 (0.021 sec/batch)\n",
      "epoch 413, step 500 / 900, loss = 1.91 (0.021 sec/batch)\n",
      "epoch 413, step 550 / 900, loss = 1.42 (0.022 sec/batch)\n",
      "epoch 413, step 600 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "epoch 413, step 650 / 900, loss = 2.81 (0.022 sec/batch)\n",
      "epoch 413, step 700 / 900, loss = 1.98 (0.021 sec/batch)\n",
      "epoch 413, step 750 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 413, step 800 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 413, step 850 / 900, loss = 2.51 (0.021 sec/batch)\n",
      "epoch 413, step 900 / 900, loss = 1.74 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.62\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.88\n",
      " avg loss = 2.92\n",
      "\n",
      "Epoch time: 24.586498975753784\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 414, step 50 / 900, loss = 3.02 (0.021 sec/batch)\n",
      "epoch 414, step 100 / 900, loss = 2.67 (0.021 sec/batch)\n",
      "epoch 414, step 150 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 414, step 200 / 900, loss = 1.81 (0.021 sec/batch)\n",
      "epoch 414, step 250 / 900, loss = 1.70 (0.021 sec/batch)\n",
      "epoch 414, step 300 / 900, loss = 2.90 (0.021 sec/batch)\n",
      "epoch 414, step 350 / 900, loss = 2.60 (0.021 sec/batch)\n",
      "epoch 414, step 400 / 900, loss = 2.04 (0.021 sec/batch)\n",
      "epoch 414, step 450 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "epoch 414, step 500 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 414, step 550 / 900, loss = 2.28 (0.022 sec/batch)\n",
      "epoch 414, step 600 / 900, loss = 3.02 (0.021 sec/batch)\n",
      "epoch 414, step 650 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "epoch 414, step 700 / 900, loss = 2.46 (0.022 sec/batch)\n",
      "epoch 414, step 750 / 900, loss = 2.40 (0.021 sec/batch)\n",
      "epoch 414, step 800 / 900, loss = 3.24 (0.021 sec/batch)\n",
      "epoch 414, step 850 / 900, loss = 2.76 (0.021 sec/batch)\n",
      "epoch 414, step 900 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.90\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.74\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.593047857284546\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 415, step 50 / 900, loss = 1.36 (0.021 sec/batch)\n",
      "epoch 415, step 100 / 900, loss = 1.82 (0.022 sec/batch)\n",
      "epoch 415, step 150 / 900, loss = 2.36 (0.021 sec/batch)\n",
      "epoch 415, step 200 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 415, step 250 / 900, loss = 1.91 (0.022 sec/batch)\n",
      "epoch 415, step 300 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 415, step 350 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 415, step 400 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 415, step 450 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "epoch 415, step 500 / 900, loss = 2.52 (0.022 sec/batch)\n",
      "epoch 415, step 550 / 900, loss = 2.50 (0.021 sec/batch)\n",
      "epoch 415, step 600 / 900, loss = 2.53 (0.022 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 415, step 650 / 900, loss = 3.88 (0.023 sec/batch)\n",
      "epoch 415, step 700 / 900, loss = 1.98 (0.022 sec/batch)\n",
      "epoch 415, step 750 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 415, step 800 / 900, loss = 2.15 (0.021 sec/batch)\n",
      "epoch 415, step 850 / 900, loss = 1.58 (0.022 sec/batch)\n",
      "epoch 415, step 900 / 900, loss = 1.74 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.66\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.22\n",
      " avg loss = 2.94\n",
      "\n",
      "Epoch time: 24.59643316268921\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 416, step 50 / 900, loss = 3.24 (0.021 sec/batch)\n",
      "epoch 416, step 100 / 900, loss = 2.47 (0.022 sec/batch)\n",
      "epoch 416, step 150 / 900, loss = 1.75 (0.021 sec/batch)\n",
      "epoch 416, step 200 / 900, loss = 3.06 (0.021 sec/batch)\n",
      "epoch 416, step 250 / 900, loss = 2.40 (0.021 sec/batch)\n",
      "epoch 416, step 300 / 900, loss = 1.69 (0.022 sec/batch)\n",
      "epoch 416, step 350 / 900, loss = 1.48 (0.022 sec/batch)\n",
      "epoch 416, step 400 / 900, loss = 1.44 (0.021 sec/batch)\n",
      "epoch 416, step 450 / 900, loss = 2.66 (0.021 sec/batch)\n",
      "epoch 416, step 500 / 900, loss = 2.93 (0.021 sec/batch)\n",
      "epoch 416, step 550 / 900, loss = 2.67 (0.022 sec/batch)\n",
      "epoch 416, step 600 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 416, step 650 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 416, step 700 / 900, loss = 1.91 (0.022 sec/batch)\n",
      "epoch 416, step 750 / 900, loss = 1.92 (0.021 sec/batch)\n",
      "epoch 416, step 800 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "epoch 416, step 850 / 900, loss = 2.36 (0.022 sec/batch)\n",
      "epoch 416, step 900 / 900, loss = 3.17 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.62\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.14\n",
      " avg loss = 2.93\n",
      "\n",
      "Epoch time: 24.595528841018677\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 417, step 50 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 417, step 100 / 900, loss = 1.63 (0.022 sec/batch)\n",
      "epoch 417, step 150 / 900, loss = 1.54 (0.021 sec/batch)\n",
      "epoch 417, step 200 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "epoch 417, step 250 / 900, loss = 3.53 (0.021 sec/batch)\n",
      "epoch 417, step 300 / 900, loss = 2.60 (0.021 sec/batch)\n",
      "epoch 417, step 350 / 900, loss = 2.98 (0.022 sec/batch)\n",
      "epoch 417, step 400 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 417, step 450 / 900, loss = 2.78 (0.021 sec/batch)\n",
      "epoch 417, step 500 / 900, loss = 2.96 (0.021 sec/batch)\n",
      "epoch 417, step 550 / 900, loss = 2.82 (0.021 sec/batch)\n",
      "epoch 417, step 600 / 900, loss = 2.07 (0.022 sec/batch)\n",
      "epoch 417, step 650 / 900, loss = 1.91 (0.021 sec/batch)\n",
      "epoch 417, step 700 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 417, step 750 / 900, loss = 1.63 (0.022 sec/batch)\n",
      "epoch 417, step 800 / 900, loss = 3.06 (0.021 sec/batch)\n",
      "epoch 417, step 850 / 900, loss = 1.84 (0.021 sec/batch)\n",
      "epoch 417, step 900 / 900, loss = 1.86 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.62\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.04\n",
      " avg loss = 2.86\n",
      "\n",
      "Epoch time: 24.59254217147827\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 418, step 50 / 900, loss = 2.74 (0.021 sec/batch)\n",
      "epoch 418, step 100 / 900, loss = 2.55 (0.022 sec/batch)\n",
      "epoch 418, step 150 / 900, loss = 2.15 (0.021 sec/batch)\n",
      "epoch 418, step 200 / 900, loss = 1.80 (0.022 sec/batch)\n",
      "epoch 418, step 250 / 900, loss = 2.05 (0.022 sec/batch)\n",
      "epoch 418, step 300 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 418, step 350 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 418, step 400 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 418, step 450 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 418, step 500 / 900, loss = 2.87 (0.021 sec/batch)\n",
      "epoch 418, step 550 / 900, loss = 2.24 (0.021 sec/batch)\n",
      "epoch 418, step 600 / 900, loss = 1.98 (0.021 sec/batch)\n",
      "epoch 418, step 650 / 900, loss = 1.90 (0.023 sec/batch)\n",
      "epoch 418, step 700 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 418, step 750 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 418, step 800 / 900, loss = 1.98 (0.022 sec/batch)\n",
      "epoch 418, step 850 / 900, loss = 2.28 (0.020 sec/batch)\n",
      "epoch 418, step 900 / 900, loss = 2.39 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.70\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.32\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.593249559402466\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 419, step 50 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 419, step 100 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 419, step 150 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 419, step 200 / 900, loss = 2.51 (0.021 sec/batch)\n",
      "epoch 419, step 250 / 900, loss = 2.14 (0.022 sec/batch)\n",
      "epoch 419, step 300 / 900, loss = 2.24 (0.022 sec/batch)\n",
      "epoch 419, step 350 / 900, loss = 2.46 (0.021 sec/batch)\n",
      "epoch 419, step 400 / 900, loss = 2.77 (0.021 sec/batch)\n",
      "epoch 419, step 450 / 900, loss = 2.63 (0.021 sec/batch)\n",
      "epoch 419, step 500 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "epoch 419, step 550 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 419, step 600 / 900, loss = 2.70 (0.021 sec/batch)\n",
      "epoch 419, step 650 / 900, loss = 2.06 (0.022 sec/batch)\n",
      "epoch 419, step 700 / 900, loss = 1.65 (0.021 sec/batch)\n",
      "epoch 419, step 750 / 900, loss = 2.55 (0.021 sec/batch)\n",
      "epoch 419, step 800 / 900, loss = 1.81 (0.021 sec/batch)\n",
      "epoch 419, step 850 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 419, step 900 / 900, loss = 1.77 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.80\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.24\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.60115146636963\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 420, step 50 / 900, loss = 2.26 (0.022 sec/batch)\n",
      "epoch 420, step 100 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 420, step 150 / 900, loss = 2.46 (0.021 sec/batch)\n",
      "epoch 420, step 200 / 900, loss = 2.39 (0.021 sec/batch)\n",
      "epoch 420, step 250 / 900, loss = 1.72 (0.021 sec/batch)\n",
      "epoch 420, step 300 / 900, loss = 1.93 (0.022 sec/batch)\n",
      "epoch 420, step 350 / 900, loss = 2.36 (0.022 sec/batch)\n",
      "epoch 420, step 400 / 900, loss = 1.47 (0.021 sec/batch)\n",
      "epoch 420, step 450 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 420, step 500 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 420, step 550 / 900, loss = 1.47 (0.022 sec/batch)\n",
      "epoch 420, step 600 / 900, loss = 2.70 (0.022 sec/batch)\n",
      "epoch 420, step 650 / 900, loss = 1.95 (0.022 sec/batch)\n",
      "epoch 420, step 700 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "epoch 420, step 750 / 900, loss = 2.49 (0.021 sec/batch)\n",
      "epoch 420, step 800 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "epoch 420, step 850 / 900, loss = 2.07 (0.022 sec/batch)\n",
      "epoch 420, step 900 / 900, loss = 2.32 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.33\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 51.14\n",
      " avg loss = 2.81\n",
      "\n",
      "Epoch time: 24.565672874450684\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 421, step 50 / 900, loss = 1.99 (0.021 sec/batch)\n",
      "epoch 421, step 100 / 900, loss = 2.01 (0.022 sec/batch)\n",
      "epoch 421, step 150 / 900, loss = 1.87 (0.021 sec/batch)\n",
      "epoch 421, step 200 / 900, loss = 1.69 (0.021 sec/batch)\n",
      "epoch 421, step 250 / 900, loss = 1.77 (0.022 sec/batch)\n",
      "epoch 421, step 300 / 900, loss = 1.58 (0.021 sec/batch)\n",
      "epoch 421, step 350 / 900, loss = 1.98 (0.021 sec/batch)\n",
      "epoch 421, step 400 / 900, loss = 1.99 (0.021 sec/batch)\n",
      "epoch 421, step 450 / 900, loss = 2.50 (0.021 sec/batch)\n",
      "epoch 421, step 500 / 900, loss = 2.84 (0.021 sec/batch)\n",
      "epoch 421, step 550 / 900, loss = 2.36 (0.022 sec/batch)\n",
      "epoch 421, step 600 / 900, loss = 2.21 (0.022 sec/batch)\n",
      "epoch 421, step 650 / 900, loss = 3.00 (0.021 sec/batch)\n",
      "epoch 421, step 700 / 900, loss = 2.84 (0.021 sec/batch)\n",
      "epoch 421, step 750 / 900, loss = 1.66 (0.021 sec/batch)\n",
      "epoch 421, step 800 / 900, loss = 2.61 (0.021 sec/batch)\n",
      "epoch 421, step 850 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "epoch 421, step 900 / 900, loss = 3.05 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.47\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.42\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.59777069091797\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 422, step 50 / 900, loss = 2.73 (0.021 sec/batch)\n",
      "epoch 422, step 100 / 900, loss = 1.97 (0.021 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 422, step 150 / 900, loss = 1.68 (0.021 sec/batch)\n",
      "epoch 422, step 200 / 900, loss = 2.61 (0.021 sec/batch)\n",
      "epoch 422, step 250 / 900, loss = 2.04 (0.021 sec/batch)\n",
      "epoch 422, step 300 / 900, loss = 2.17 (0.020 sec/batch)\n",
      "epoch 422, step 350 / 900, loss = 2.17 (0.022 sec/batch)\n",
      "epoch 422, step 400 / 900, loss = 2.49 (0.021 sec/batch)\n",
      "epoch 422, step 450 / 900, loss = 2.64 (0.021 sec/batch)\n",
      "epoch 422, step 500 / 900, loss = 2.24 (0.021 sec/batch)\n",
      "epoch 422, step 550 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 422, step 600 / 900, loss = 1.92 (0.021 sec/batch)\n",
      "epoch 422, step 650 / 900, loss = 1.78 (0.022 sec/batch)\n",
      "epoch 422, step 700 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 422, step 750 / 900, loss = 2.40 (0.022 sec/batch)\n",
      "epoch 422, step 800 / 900, loss = 3.66 (0.021 sec/batch)\n",
      "epoch 422, step 850 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 422, step 900 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.35\n",
      " avg loss = 2.29\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.08\n",
      " avg loss = 2.92\n",
      "\n",
      "Epoch time: 24.578477144241333\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 423, step 50 / 900, loss = 2.56 (0.022 sec/batch)\n",
      "epoch 423, step 100 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 423, step 150 / 900, loss = 1.55 (0.021 sec/batch)\n",
      "epoch 423, step 200 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 423, step 250 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "epoch 423, step 300 / 900, loss = 2.57 (0.021 sec/batch)\n",
      "epoch 423, step 350 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "epoch 423, step 400 / 900, loss = 2.06 (0.021 sec/batch)\n",
      "epoch 423, step 450 / 900, loss = 1.75 (0.021 sec/batch)\n",
      "epoch 423, step 500 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 423, step 550 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "epoch 423, step 600 / 900, loss = 2.66 (0.021 sec/batch)\n",
      "epoch 423, step 650 / 900, loss = 2.90 (0.021 sec/batch)\n",
      "epoch 423, step 700 / 900, loss = 2.98 (0.022 sec/batch)\n",
      "epoch 423, step 750 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 423, step 800 / 900, loss = 2.24 (0.021 sec/batch)\n",
      "epoch 423, step 850 / 900, loss = 1.94 (0.022 sec/batch)\n",
      "epoch 423, step 900 / 900, loss = 1.27 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.73\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.94\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.58083176612854\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 424, step 50 / 900, loss = 2.27 (0.020 sec/batch)\n",
      "epoch 424, step 100 / 900, loss = 3.66 (0.021 sec/batch)\n",
      "epoch 424, step 150 / 900, loss = 2.55 (0.021 sec/batch)\n",
      "epoch 424, step 200 / 900, loss = 1.96 (0.022 sec/batch)\n",
      "epoch 424, step 250 / 900, loss = 1.64 (0.021 sec/batch)\n",
      "epoch 424, step 300 / 900, loss = 2.71 (0.021 sec/batch)\n",
      "epoch 424, step 350 / 900, loss = 2.67 (0.021 sec/batch)\n",
      "epoch 424, step 400 / 900, loss = 2.40 (0.021 sec/batch)\n",
      "epoch 424, step 450 / 900, loss = 3.22 (0.021 sec/batch)\n",
      "epoch 424, step 500 / 900, loss = 2.64 (0.022 sec/batch)\n",
      "epoch 424, step 550 / 900, loss = 2.80 (0.021 sec/batch)\n",
      "epoch 424, step 600 / 900, loss = 1.98 (0.021 sec/batch)\n",
      "epoch 424, step 650 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "epoch 424, step 700 / 900, loss = 2.54 (0.021 sec/batch)\n",
      "epoch 424, step 750 / 900, loss = 2.26 (0.022 sec/batch)\n",
      "epoch 424, step 800 / 900, loss = 2.84 (0.021 sec/batch)\n",
      "epoch 424, step 850 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 424, step 900 / 900, loss = 1.73 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.69\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.76\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.59906506538391\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 425, step 50 / 900, loss = 3.51 (0.021 sec/batch)\n",
      "epoch 425, step 100 / 900, loss = 2.46 (0.021 sec/batch)\n",
      "epoch 425, step 150 / 900, loss = 1.88 (0.021 sec/batch)\n",
      "epoch 425, step 200 / 900, loss = 2.60 (0.022 sec/batch)\n",
      "epoch 425, step 250 / 900, loss = 3.31 (0.022 sec/batch)\n",
      "epoch 425, step 300 / 900, loss = 2.45 (0.021 sec/batch)\n",
      "epoch 425, step 350 / 900, loss = 2.24 (0.022 sec/batch)\n",
      "epoch 425, step 400 / 900, loss = 2.73 (0.021 sec/batch)\n",
      "epoch 425, step 450 / 900, loss = 1.74 (0.021 sec/batch)\n",
      "epoch 425, step 500 / 900, loss = 3.21 (0.021 sec/batch)\n",
      "epoch 425, step 550 / 900, loss = 1.72 (0.021 sec/batch)\n",
      "epoch 425, step 600 / 900, loss = 1.92 (0.022 sec/batch)\n",
      "epoch 425, step 650 / 900, loss = 2.23 (0.022 sec/batch)\n",
      "epoch 425, step 700 / 900, loss = 1.84 (0.021 sec/batch)\n",
      "epoch 425, step 750 / 900, loss = 3.03 (0.021 sec/batch)\n",
      "epoch 425, step 800 / 900, loss = 2.89 (0.021 sec/batch)\n",
      "epoch 425, step 850 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 425, step 900 / 900, loss = 0.82 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.60\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.60\n",
      " avg loss = 2.86\n",
      "\n",
      "Epoch time: 24.55902910232544\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 426, step 50 / 900, loss = 2.06 (0.021 sec/batch)\n",
      "epoch 426, step 100 / 900, loss = 2.27 (0.022 sec/batch)\n",
      "epoch 426, step 150 / 900, loss = 2.68 (0.021 sec/batch)\n",
      "epoch 426, step 200 / 900, loss = 1.76 (0.023 sec/batch)\n",
      "epoch 426, step 250 / 900, loss = 2.06 (0.021 sec/batch)\n",
      "epoch 426, step 300 / 900, loss = 2.75 (0.021 sec/batch)\n",
      "epoch 426, step 350 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 426, step 400 / 900, loss = 1.37 (0.021 sec/batch)\n",
      "epoch 426, step 450 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 426, step 500 / 900, loss = 1.81 (0.021 sec/batch)\n",
      "epoch 426, step 550 / 900, loss = 2.24 (0.022 sec/batch)\n",
      "epoch 426, step 600 / 900, loss = 2.59 (0.021 sec/batch)\n",
      "epoch 426, step 650 / 900, loss = 2.56 (0.022 sec/batch)\n",
      "epoch 426, step 700 / 900, loss = 2.74 (0.021 sec/batch)\n",
      "epoch 426, step 750 / 900, loss = 1.50 (0.021 sec/batch)\n",
      "epoch 426, step 800 / 900, loss = 1.52 (0.021 sec/batch)\n",
      "epoch 426, step 850 / 900, loss = 1.68 (0.021 sec/batch)\n",
      "epoch 426, step 900 / 900, loss = 2.01 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.77\n",
      " avg loss = 2.24\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.64\n",
      " avg loss = 2.93\n",
      "\n",
      "Epoch time: 24.58528232574463\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 427, step 50 / 900, loss = 2.74 (0.021 sec/batch)\n",
      "epoch 427, step 100 / 900, loss = 1.72 (0.022 sec/batch)\n",
      "epoch 427, step 150 / 900, loss = 2.36 (0.021 sec/batch)\n",
      "epoch 427, step 200 / 900, loss = 2.02 (0.022 sec/batch)\n",
      "epoch 427, step 250 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 427, step 300 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 427, step 350 / 900, loss = 1.54 (0.021 sec/batch)\n",
      "epoch 427, step 400 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 427, step 450 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "epoch 427, step 500 / 900, loss = 1.57 (0.021 sec/batch)\n",
      "epoch 427, step 550 / 900, loss = 2.19 (0.022 sec/batch)\n",
      "epoch 427, step 600 / 900, loss = 2.36 (0.022 sec/batch)\n",
      "epoch 427, step 650 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "epoch 427, step 700 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 427, step 750 / 900, loss = 1.54 (0.021 sec/batch)\n",
      "epoch 427, step 800 / 900, loss = 2.24 (0.022 sec/batch)\n",
      "epoch 427, step 850 / 900, loss = 1.76 (0.021 sec/batch)\n",
      "epoch 427, step 900 / 900, loss = 2.22 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.50\n",
      " avg loss = 2.24\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.38\n",
      " avg loss = 2.92\n",
      "\n",
      "Epoch time: 24.58574080467224\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 428, step 50 / 900, loss = 1.79 (0.021 sec/batch)\n",
      "epoch 428, step 100 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "epoch 428, step 150 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 428, step 200 / 900, loss = 1.64 (0.022 sec/batch)\n",
      "epoch 428, step 250 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 428, step 300 / 900, loss = 1.51 (0.021 sec/batch)\n",
      "epoch 428, step 350 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 428, step 400 / 900, loss = 3.12 (0.021 sec/batch)\n",
      "epoch 428, step 450 / 900, loss = 2.01 (0.022 sec/batch)\n",
      "epoch 428, step 500 / 900, loss = 2.34 (0.021 sec/batch)\n",
      "epoch 428, step 550 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "epoch 428, step 600 / 900, loss = 1.50 (0.022 sec/batch)\n",
      "epoch 428, step 650 / 900, loss = 1.82 (0.022 sec/batch)\n",
      "epoch 428, step 700 / 900, loss = 1.63 (0.021 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 428, step 750 / 900, loss = 1.96 (0.021 sec/batch)\n",
      "epoch 428, step 800 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 428, step 850 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 428, step 900 / 900, loss = 2.36 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.66\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 48.82\n",
      " avg loss = 2.94\n",
      "\n",
      "Epoch time: 24.597639322280884\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 429, step 50 / 900, loss = 1.90 (0.021 sec/batch)\n",
      "epoch 429, step 100 / 900, loss = 2.58 (0.021 sec/batch)\n",
      "epoch 429, step 150 / 900, loss = 2.13 (0.022 sec/batch)\n",
      "epoch 429, step 200 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 429, step 250 / 900, loss = 3.11 (0.021 sec/batch)\n",
      "epoch 429, step 300 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 429, step 350 / 900, loss = 2.50 (0.021 sec/batch)\n",
      "epoch 429, step 400 / 900, loss = 3.06 (0.022 sec/batch)\n",
      "epoch 429, step 450 / 900, loss = 2.95 (0.021 sec/batch)\n",
      "epoch 429, step 500 / 900, loss = 1.61 (0.021 sec/batch)\n",
      "epoch 429, step 550 / 900, loss = 2.85 (0.021 sec/batch)\n",
      "epoch 429, step 600 / 900, loss = 2.46 (0.021 sec/batch)\n",
      "epoch 429, step 650 / 900, loss = 2.72 (0.021 sec/batch)\n",
      "epoch 429, step 700 / 900, loss = 2.93 (0.022 sec/batch)\n",
      "epoch 429, step 750 / 900, loss = 3.58 (0.021 sec/batch)\n",
      "epoch 429, step 800 / 900, loss = 2.08 (0.022 sec/batch)\n",
      "epoch 429, step 850 / 900, loss = 2.34 (0.022 sec/batch)\n",
      "epoch 429, step 900 / 900, loss = 1.61 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.29\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.82\n",
      " avg loss = 2.85\n",
      "\n",
      "Epoch time: 24.594155311584473\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 430, step 50 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 430, step 100 / 900, loss = 2.70 (0.021 sec/batch)\n",
      "epoch 430, step 150 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 430, step 200 / 900, loss = 2.26 (0.022 sec/batch)\n",
      "epoch 430, step 250 / 900, loss = 2.71 (0.021 sec/batch)\n",
      "epoch 430, step 300 / 900, loss = 2.26 (0.021 sec/batch)\n",
      "epoch 430, step 350 / 900, loss = 1.36 (0.021 sec/batch)\n",
      "epoch 430, step 400 / 900, loss = 2.53 (0.022 sec/batch)\n",
      "epoch 430, step 450 / 900, loss = 1.71 (0.021 sec/batch)\n",
      "epoch 430, step 500 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 430, step 550 / 900, loss = 1.84 (0.021 sec/batch)\n",
      "epoch 430, step 600 / 900, loss = 3.20 (0.021 sec/batch)\n",
      "epoch 430, step 650 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "epoch 430, step 700 / 900, loss = 3.02 (0.021 sec/batch)\n",
      "epoch 430, step 750 / 900, loss = 1.99 (0.022 sec/batch)\n",
      "epoch 430, step 800 / 900, loss = 2.66 (0.021 sec/batch)\n",
      "epoch 430, step 850 / 900, loss = 1.71 (0.022 sec/batch)\n",
      "epoch 430, step 900 / 900, loss = 2.22 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.87\n",
      " avg loss = 2.24\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.66\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.563976049423218\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 431, step 50 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 431, step 100 / 900, loss = 1.66 (0.021 sec/batch)\n",
      "epoch 431, step 150 / 900, loss = 1.75 (0.021 sec/batch)\n",
      "epoch 431, step 200 / 900, loss = 1.79 (0.022 sec/batch)\n",
      "epoch 431, step 250 / 900, loss = 1.35 (0.021 sec/batch)\n",
      "epoch 431, step 300 / 900, loss = 1.80 (0.021 sec/batch)\n",
      "epoch 431, step 350 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 431, step 400 / 900, loss = 2.47 (0.022 sec/batch)\n",
      "epoch 431, step 450 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 431, step 500 / 900, loss = 1.63 (0.022 sec/batch)\n",
      "epoch 431, step 550 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 431, step 600 / 900, loss = 2.81 (0.021 sec/batch)\n",
      "epoch 431, step 650 / 900, loss = 1.79 (0.021 sec/batch)\n",
      "epoch 431, step 700 / 900, loss = 1.95 (0.021 sec/batch)\n",
      "epoch 431, step 750 / 900, loss = 2.98 (0.022 sec/batch)\n",
      "epoch 431, step 800 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 431, step 850 / 900, loss = 2.54 (0.021 sec/batch)\n",
      "epoch 431, step 900 / 900, loss = 2.06 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.67\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.26\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.574142932891846\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 432, step 50 / 900, loss = 2.75 (0.022 sec/batch)\n",
      "epoch 432, step 100 / 900, loss = 2.76 (0.021 sec/batch)\n",
      "epoch 432, step 150 / 900, loss = 1.85 (0.021 sec/batch)\n",
      "epoch 432, step 200 / 900, loss = 3.22 (0.022 sec/batch)\n",
      "epoch 432, step 250 / 900, loss = 2.37 (0.022 sec/batch)\n",
      "epoch 432, step 300 / 900, loss = 2.25 (0.022 sec/batch)\n",
      "epoch 432, step 350 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 432, step 400 / 900, loss = 2.81 (0.021 sec/batch)\n",
      "epoch 432, step 450 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 432, step 500 / 900, loss = 2.76 (0.021 sec/batch)\n",
      "epoch 432, step 550 / 900, loss = 1.82 (0.021 sec/batch)\n",
      "epoch 432, step 600 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 432, step 650 / 900, loss = 2.69 (0.022 sec/batch)\n",
      "epoch 432, step 700 / 900, loss = 2.60 (0.022 sec/batch)\n",
      "epoch 432, step 750 / 900, loss = 2.79 (0.021 sec/batch)\n",
      "epoch 432, step 800 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 432, step 850 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 432, step 900 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.50\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.32\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.57510805130005\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 433, step 50 / 900, loss = 1.37 (0.022 sec/batch)\n",
      "epoch 433, step 100 / 900, loss = 2.63 (0.021 sec/batch)\n",
      "epoch 433, step 150 / 900, loss = 2.44 (0.021 sec/batch)\n",
      "epoch 433, step 200 / 900, loss = 2.78 (0.022 sec/batch)\n",
      "epoch 433, step 250 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "epoch 433, step 300 / 900, loss = 1.80 (0.022 sec/batch)\n",
      "epoch 433, step 350 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "epoch 433, step 400 / 900, loss = 2.62 (0.021 sec/batch)\n",
      "epoch 433, step 450 / 900, loss = 2.68 (0.021 sec/batch)\n",
      "epoch 433, step 500 / 900, loss = 2.24 (0.021 sec/batch)\n",
      "epoch 433, step 550 / 900, loss = 1.74 (0.021 sec/batch)\n",
      "epoch 433, step 600 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 433, step 650 / 900, loss = 2.01 (0.022 sec/batch)\n",
      "epoch 433, step 700 / 900, loss = 1.55 (0.022 sec/batch)\n",
      "epoch 433, step 750 / 900, loss = 2.57 (0.021 sec/batch)\n",
      "epoch 433, step 800 / 900, loss = 2.06 (0.022 sec/batch)\n",
      "epoch 433, step 850 / 900, loss = 2.71 (0.021 sec/batch)\n",
      "epoch 433, step 900 / 900, loss = 2.30 (0.023 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.76\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.76\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.577414751052856\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 434, step 50 / 900, loss = 2.50 (0.021 sec/batch)\n",
      "epoch 434, step 100 / 900, loss = 2.30 (0.022 sec/batch)\n",
      "epoch 434, step 150 / 900, loss = 1.55 (0.021 sec/batch)\n",
      "epoch 434, step 200 / 900, loss = 2.55 (0.021 sec/batch)\n",
      "epoch 434, step 250 / 900, loss = 2.46 (0.021 sec/batch)\n",
      "epoch 434, step 300 / 900, loss = 1.69 (0.021 sec/batch)\n",
      "epoch 434, step 350 / 900, loss = 2.06 (0.021 sec/batch)\n",
      "epoch 434, step 400 / 900, loss = 1.73 (0.021 sec/batch)\n",
      "epoch 434, step 450 / 900, loss = 1.66 (0.022 sec/batch)\n",
      "epoch 434, step 500 / 900, loss = 2.60 (0.022 sec/batch)\n",
      "epoch 434, step 550 / 900, loss = 1.71 (0.021 sec/batch)\n",
      "epoch 434, step 600 / 900, loss = 2.48 (0.022 sec/batch)\n",
      "epoch 434, step 650 / 900, loss = 2.39 (0.021 sec/batch)\n",
      "epoch 434, step 700 / 900, loss = 1.84 (0.022 sec/batch)\n",
      "epoch 434, step 750 / 900, loss = 2.40 (0.021 sec/batch)\n",
      "epoch 434, step 800 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 434, step 850 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "epoch 434, step 900 / 900, loss = 1.74 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.38\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.54\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.581318378448486\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 435, step 50 / 900, loss = 2.03 (0.022 sec/batch)\n",
      "epoch 435, step 100 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 435, step 150 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 435, step 200 / 900, loss = 2.58 (0.021 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 435, step 250 / 900, loss = 2.04 (0.021 sec/batch)\n",
      "epoch 435, step 300 / 900, loss = 2.56 (0.021 sec/batch)\n",
      "epoch 435, step 350 / 900, loss = 1.71 (0.021 sec/batch)\n",
      "epoch 435, step 400 / 900, loss = 2.35 (0.022 sec/batch)\n",
      "epoch 435, step 450 / 900, loss = 1.86 (0.022 sec/batch)\n",
      "epoch 435, step 500 / 900, loss = 2.37 (0.022 sec/batch)\n",
      "epoch 435, step 550 / 900, loss = 2.48 (0.022 sec/batch)\n",
      "epoch 435, step 600 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "epoch 435, step 650 / 900, loss = 1.39 (0.022 sec/batch)\n",
      "epoch 435, step 700 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "epoch 435, step 750 / 900, loss = 1.63 (0.021 sec/batch)\n",
      "epoch 435, step 800 / 900, loss = 1.61 (0.023 sec/batch)\n",
      "epoch 435, step 850 / 900, loss = 1.72 (0.021 sec/batch)\n",
      "epoch 435, step 900 / 900, loss = 1.82 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.65\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 51.22\n",
      " avg loss = 2.85\n",
      "\n",
      "Epoch time: 24.570833683013916\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 436, step 50 / 900, loss = 2.31 (0.022 sec/batch)\n",
      "epoch 436, step 100 / 900, loss = 2.29 (0.020 sec/batch)\n",
      "epoch 436, step 150 / 900, loss = 2.58 (0.021 sec/batch)\n",
      "epoch 436, step 200 / 900, loss = 2.64 (0.022 sec/batch)\n",
      "epoch 436, step 250 / 900, loss = 1.90 (0.021 sec/batch)\n",
      "epoch 436, step 300 / 900, loss = 1.82 (0.021 sec/batch)\n",
      "epoch 436, step 350 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 436, step 400 / 900, loss = 1.69 (0.021 sec/batch)\n",
      "epoch 436, step 450 / 900, loss = 2.34 (0.021 sec/batch)\n",
      "epoch 436, step 500 / 900, loss = 1.81 (0.021 sec/batch)\n",
      "epoch 436, step 550 / 900, loss = 2.32 (0.021 sec/batch)\n",
      "epoch 436, step 600 / 900, loss = 1.87 (0.022 sec/batch)\n",
      "epoch 436, step 650 / 900, loss = 1.69 (0.021 sec/batch)\n",
      "epoch 436, step 700 / 900, loss = 1.91 (0.021 sec/batch)\n",
      "epoch 436, step 750 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "epoch 436, step 800 / 900, loss = 1.55 (0.021 sec/batch)\n",
      "epoch 436, step 850 / 900, loss = 2.19 (0.022 sec/batch)\n",
      "epoch 436, step 900 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.90\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.94\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.56466054916382\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 437, step 50 / 900, loss = 2.90 (0.021 sec/batch)\n",
      "epoch 437, step 100 / 900, loss = 2.80 (0.021 sec/batch)\n",
      "epoch 437, step 150 / 900, loss = 2.38 (0.022 sec/batch)\n",
      "epoch 437, step 200 / 900, loss = 2.17 (0.022 sec/batch)\n",
      "epoch 437, step 250 / 900, loss = 2.63 (0.021 sec/batch)\n",
      "epoch 437, step 300 / 900, loss = 2.41 (0.021 sec/batch)\n",
      "epoch 437, step 350 / 900, loss = 1.77 (0.021 sec/batch)\n",
      "epoch 437, step 400 / 900, loss = 2.16 (0.022 sec/batch)\n",
      "epoch 437, step 450 / 900, loss = 2.79 (0.022 sec/batch)\n",
      "epoch 437, step 500 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "epoch 437, step 550 / 900, loss = 2.41 (0.022 sec/batch)\n",
      "epoch 437, step 600 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 437, step 650 / 900, loss = 2.40 (0.022 sec/batch)\n",
      "epoch 437, step 700 / 900, loss = 2.72 (0.021 sec/batch)\n",
      "epoch 437, step 750 / 900, loss = 3.19 (0.021 sec/batch)\n",
      "epoch 437, step 800 / 900, loss = 2.40 (0.023 sec/batch)\n",
      "epoch 437, step 850 / 900, loss = 1.64 (0.022 sec/batch)\n",
      "epoch 437, step 900 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.69\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.68\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.57526707649231\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 438, step 50 / 900, loss = 2.62 (0.021 sec/batch)\n",
      "epoch 438, step 100 / 900, loss = 2.39 (0.021 sec/batch)\n",
      "epoch 438, step 150 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 438, step 200 / 900, loss = 1.55 (0.022 sec/batch)\n",
      "epoch 438, step 250 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 438, step 300 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 438, step 350 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 438, step 400 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 438, step 450 / 900, loss = 2.95 (0.022 sec/batch)\n",
      "epoch 438, step 500 / 900, loss = 2.06 (0.021 sec/batch)\n",
      "epoch 438, step 550 / 900, loss = 1.92 (0.021 sec/batch)\n",
      "epoch 438, step 600 / 900, loss = 2.63 (0.022 sec/batch)\n",
      "epoch 438, step 650 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "epoch 438, step 700 / 900, loss = 2.16 (0.022 sec/batch)\n",
      "epoch 438, step 750 / 900, loss = 1.72 (0.021 sec/batch)\n",
      "epoch 438, step 800 / 900, loss = 1.20 (0.021 sec/batch)\n",
      "epoch 438, step 850 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 438, step 900 / 900, loss = 2.48 (0.023 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.71\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.02\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.588101387023926\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 439, step 50 / 900, loss = 1.85 (0.021 sec/batch)\n",
      "epoch 439, step 100 / 900, loss = 2.81 (0.022 sec/batch)\n",
      "epoch 439, step 150 / 900, loss = 1.74 (0.021 sec/batch)\n",
      "epoch 439, step 200 / 900, loss = 2.48 (0.022 sec/batch)\n",
      "epoch 439, step 250 / 900, loss = 2.19 (0.022 sec/batch)\n",
      "epoch 439, step 300 / 900, loss = 2.06 (0.022 sec/batch)\n",
      "epoch 439, step 350 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 439, step 400 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 439, step 450 / 900, loss = 1.70 (0.022 sec/batch)\n",
      "epoch 439, step 500 / 900, loss = 3.25 (0.021 sec/batch)\n",
      "epoch 439, step 550 / 900, loss = 2.83 (0.021 sec/batch)\n",
      "epoch 439, step 600 / 900, loss = 1.64 (0.021 sec/batch)\n",
      "epoch 439, step 650 / 900, loss = 1.72 (0.022 sec/batch)\n",
      "epoch 439, step 700 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "epoch 439, step 750 / 900, loss = 1.44 (0.021 sec/batch)\n",
      "epoch 439, step 800 / 900, loss = 1.99 (0.022 sec/batch)\n",
      "epoch 439, step 850 / 900, loss = 2.15 (0.022 sec/batch)\n",
      "epoch 439, step 900 / 900, loss = 2.81 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.94\n",
      " avg loss = 2.24\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.22\n",
      " avg loss = 2.82\n",
      "\n",
      "Epoch time: 24.580620527267456\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 440, step 50 / 900, loss = 1.84 (0.021 sec/batch)\n",
      "epoch 440, step 100 / 900, loss = 1.74 (0.022 sec/batch)\n",
      "epoch 440, step 150 / 900, loss = 1.37 (0.021 sec/batch)\n",
      "epoch 440, step 200 / 900, loss = 2.20 (0.022 sec/batch)\n",
      "epoch 440, step 250 / 900, loss = 2.08 (0.022 sec/batch)\n",
      "epoch 440, step 300 / 900, loss = 1.39 (0.021 sec/batch)\n",
      "epoch 440, step 350 / 900, loss = 2.58 (0.021 sec/batch)\n",
      "epoch 440, step 400 / 900, loss = 1.82 (0.021 sec/batch)\n",
      "epoch 440, step 450 / 900, loss = 1.18 (0.022 sec/batch)\n",
      "epoch 440, step 500 / 900, loss = 1.98 (0.021 sec/batch)\n",
      "epoch 440, step 550 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 440, step 600 / 900, loss = 1.85 (0.021 sec/batch)\n",
      "epoch 440, step 650 / 900, loss = 1.65 (0.021 sec/batch)\n",
      "epoch 440, step 700 / 900, loss = 2.84 (0.021 sec/batch)\n",
      "epoch 440, step 750 / 900, loss = 2.25 (0.022 sec/batch)\n",
      "epoch 440, step 800 / 900, loss = 1.56 (0.021 sec/batch)\n",
      "epoch 440, step 850 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 440, step 900 / 900, loss = 2.45 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.76\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.20\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.604926109313965\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 441, step 50 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 441, step 100 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 441, step 150 / 900, loss = 2.82 (0.021 sec/batch)\n",
      "epoch 441, step 200 / 900, loss = 2.27 (0.022 sec/batch)\n",
      "epoch 441, step 250 / 900, loss = 1.77 (0.021 sec/batch)\n",
      "epoch 441, step 300 / 900, loss = 1.52 (0.021 sec/batch)\n",
      "epoch 441, step 350 / 900, loss = 1.79 (0.021 sec/batch)\n",
      "epoch 441, step 400 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 441, step 450 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 441, step 500 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 441, step 550 / 900, loss = 1.76 (0.021 sec/batch)\n",
      "epoch 441, step 600 / 900, loss = 2.39 (0.021 sec/batch)\n",
      "epoch 441, step 650 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "epoch 441, step 700 / 900, loss = 2.78 (0.020 sec/batch)\n",
      "epoch 441, step 750 / 900, loss = 2.91 (0.022 sec/batch)\n",
      "epoch 441, step 800 / 900, loss = 2.46 (0.021 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 441, step 850 / 900, loss = 1.65 (0.021 sec/batch)\n",
      "epoch 441, step 900 / 900, loss = 1.53 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.78\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.68\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.577432870864868\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 442, step 50 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "epoch 442, step 100 / 900, loss = 2.58 (0.021 sec/batch)\n",
      "epoch 442, step 150 / 900, loss = 1.96 (0.021 sec/batch)\n",
      "epoch 442, step 200 / 900, loss = 2.68 (0.022 sec/batch)\n",
      "epoch 442, step 250 / 900, loss = 2.35 (0.022 sec/batch)\n",
      "epoch 442, step 300 / 900, loss = 1.83 (0.022 sec/batch)\n",
      "epoch 442, step 350 / 900, loss = 1.52 (0.021 sec/batch)\n",
      "epoch 442, step 400 / 900, loss = 2.62 (0.021 sec/batch)\n",
      "epoch 442, step 450 / 900, loss = 2.50 (0.021 sec/batch)\n",
      "epoch 442, step 500 / 900, loss = 1.94 (0.022 sec/batch)\n",
      "epoch 442, step 550 / 900, loss = 1.74 (0.021 sec/batch)\n",
      "epoch 442, step 600 / 900, loss = 3.66 (0.022 sec/batch)\n",
      "epoch 442, step 650 / 900, loss = 1.79 (0.021 sec/batch)\n",
      "epoch 442, step 700 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 442, step 750 / 900, loss = 2.24 (0.022 sec/batch)\n",
      "epoch 442, step 800 / 900, loss = 2.57 (0.021 sec/batch)\n",
      "epoch 442, step 850 / 900, loss = 2.23 (0.022 sec/batch)\n",
      "epoch 442, step 900 / 900, loss = 1.69 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.66\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.60\n",
      " avg loss = 2.85\n",
      "\n",
      "Epoch time: 24.572017908096313\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 443, step 50 / 900, loss = 2.36 (0.021 sec/batch)\n",
      "epoch 443, step 100 / 900, loss = 1.93 (0.022 sec/batch)\n",
      "epoch 443, step 150 / 900, loss = 1.85 (0.022 sec/batch)\n",
      "epoch 443, step 200 / 900, loss = 2.40 (0.021 sec/batch)\n",
      "epoch 443, step 250 / 900, loss = 1.96 (0.021 sec/batch)\n",
      "epoch 443, step 300 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 443, step 350 / 900, loss = 1.95 (0.021 sec/batch)\n",
      "epoch 443, step 400 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 443, step 450 / 900, loss = 3.58 (0.021 sec/batch)\n",
      "epoch 443, step 500 / 900, loss = 1.86 (0.021 sec/batch)\n",
      "epoch 443, step 550 / 900, loss = 2.81 (0.021 sec/batch)\n",
      "epoch 443, step 600 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 443, step 650 / 900, loss = 1.86 (0.021 sec/batch)\n",
      "epoch 443, step 700 / 900, loss = 1.58 (0.022 sec/batch)\n",
      "epoch 443, step 750 / 900, loss = 2.17 (0.022 sec/batch)\n",
      "epoch 443, step 800 / 900, loss = 2.59 (0.021 sec/batch)\n",
      "epoch 443, step 850 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 443, step 900 / 900, loss = 2.00 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.80\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.96\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.605401754379272\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 444, step 50 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 444, step 100 / 900, loss = 3.15 (0.021 sec/batch)\n",
      "epoch 444, step 150 / 900, loss = 2.40 (0.022 sec/batch)\n",
      "epoch 444, step 200 / 900, loss = 2.10 (0.022 sec/batch)\n",
      "epoch 444, step 250 / 900, loss = 2.43 (0.022 sec/batch)\n",
      "epoch 444, step 300 / 900, loss = 2.74 (0.021 sec/batch)\n",
      "epoch 444, step 350 / 900, loss = 3.01 (0.022 sec/batch)\n",
      "epoch 444, step 400 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 444, step 450 / 900, loss = 2.76 (0.022 sec/batch)\n",
      "epoch 444, step 500 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 444, step 550 / 900, loss = 1.78 (0.022 sec/batch)\n",
      "epoch 444, step 600 / 900, loss = 2.70 (0.022 sec/batch)\n",
      "epoch 444, step 650 / 900, loss = 2.50 (0.022 sec/batch)\n",
      "epoch 444, step 700 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 444, step 750 / 900, loss = 1.83 (0.022 sec/batch)\n",
      "epoch 444, step 800 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 444, step 850 / 900, loss = 2.94 (0.021 sec/batch)\n",
      "epoch 444, step 900 / 900, loss = 2.35 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.78\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.10\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.582407474517822\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 445, step 50 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "epoch 445, step 100 / 900, loss = 2.25 (0.022 sec/batch)\n",
      "epoch 445, step 150 / 900, loss = 1.80 (0.022 sec/batch)\n",
      "epoch 445, step 200 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 445, step 250 / 900, loss = 2.97 (0.021 sec/batch)\n",
      "epoch 445, step 300 / 900, loss = 1.70 (0.021 sec/batch)\n",
      "epoch 445, step 350 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 445, step 400 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "epoch 445, step 450 / 900, loss = 1.33 (0.022 sec/batch)\n",
      "epoch 445, step 500 / 900, loss = 2.24 (0.021 sec/batch)\n",
      "epoch 445, step 550 / 900, loss = 1.76 (0.021 sec/batch)\n",
      "epoch 445, step 600 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 445, step 650 / 900, loss = 2.50 (0.022 sec/batch)\n",
      "epoch 445, step 700 / 900, loss = 2.36 (0.022 sec/batch)\n",
      "epoch 445, step 750 / 900, loss = 2.55 (0.021 sec/batch)\n",
      "epoch 445, step 800 / 900, loss = 1.72 (0.022 sec/batch)\n",
      "epoch 445, step 850 / 900, loss = 1.85 (0.021 sec/batch)\n",
      "epoch 445, step 900 / 900, loss = 2.83 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.36\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.92\n",
      " avg loss = 2.94\n",
      "\n",
      "Epoch time: 24.573636531829834\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 446, step 50 / 900, loss = 3.41 (0.021 sec/batch)\n",
      "epoch 446, step 100 / 900, loss = 1.80 (0.021 sec/batch)\n",
      "epoch 446, step 150 / 900, loss = 2.18 (0.022 sec/batch)\n",
      "epoch 446, step 200 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "epoch 446, step 250 / 900, loss = 2.70 (0.021 sec/batch)\n",
      "epoch 446, step 300 / 900, loss = 2.44 (0.021 sec/batch)\n",
      "epoch 446, step 350 / 900, loss = 2.87 (0.021 sec/batch)\n",
      "epoch 446, step 400 / 900, loss = 1.33 (0.021 sec/batch)\n",
      "epoch 446, step 450 / 900, loss = 1.93 (0.022 sec/batch)\n",
      "epoch 446, step 500 / 900, loss = 2.71 (0.021 sec/batch)\n",
      "epoch 446, step 550 / 900, loss = 1.19 (0.021 sec/batch)\n",
      "epoch 446, step 600 / 900, loss = 2.48 (0.022 sec/batch)\n",
      "epoch 446, step 650 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "epoch 446, step 700 / 900, loss = 3.33 (0.022 sec/batch)\n",
      "epoch 446, step 750 / 900, loss = 2.13 (0.022 sec/batch)\n",
      "epoch 446, step 800 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 446, step 850 / 900, loss = 2.04 (0.021 sec/batch)\n",
      "epoch 446, step 900 / 900, loss = 1.91 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.41\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.22\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.556026697158813\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 447, step 50 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 447, step 100 / 900, loss = 2.54 (0.021 sec/batch)\n",
      "epoch 447, step 150 / 900, loss = 1.65 (0.021 sec/batch)\n",
      "epoch 447, step 200 / 900, loss = 1.99 (0.022 sec/batch)\n",
      "epoch 447, step 250 / 900, loss = 2.46 (0.022 sec/batch)\n",
      "epoch 447, step 300 / 900, loss = 1.85 (0.022 sec/batch)\n",
      "epoch 447, step 350 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 447, step 400 / 900, loss = 2.68 (0.021 sec/batch)\n",
      "epoch 447, step 450 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 447, step 500 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 447, step 550 / 900, loss = 2.05 (0.022 sec/batch)\n",
      "epoch 447, step 600 / 900, loss = 1.73 (0.021 sec/batch)\n",
      "epoch 447, step 650 / 900, loss = 2.26 (0.022 sec/batch)\n",
      "epoch 447, step 700 / 900, loss = 2.67 (0.023 sec/batch)\n",
      "epoch 447, step 750 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 447, step 800 / 900, loss = 2.34 (0.021 sec/batch)\n",
      "epoch 447, step 850 / 900, loss = 3.54 (0.021 sec/batch)\n",
      "epoch 447, step 900 / 900, loss = 2.74 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.25\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 48.90\n",
      " avg loss = 2.95\n",
      "\n",
      "Epoch time: 24.56009030342102\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 448, step 50 / 900, loss = 1.91 (0.021 sec/batch)\n",
      "epoch 448, step 100 / 900, loss = 1.52 (0.021 sec/batch)\n",
      "epoch 448, step 150 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "epoch 448, step 200 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 448, step 250 / 900, loss = 2.46 (0.021 sec/batch)\n",
      "epoch 448, step 300 / 900, loss = 2.38 (0.022 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 448, step 350 / 900, loss = 2.75 (0.021 sec/batch)\n",
      "epoch 448, step 400 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 448, step 450 / 900, loss = 2.45 (0.021 sec/batch)\n",
      "epoch 448, step 500 / 900, loss = 3.06 (0.021 sec/batch)\n",
      "epoch 448, step 550 / 900, loss = 1.61 (0.022 sec/batch)\n",
      "epoch 448, step 600 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 448, step 650 / 900, loss = 2.51 (0.021 sec/batch)\n",
      "epoch 448, step 700 / 900, loss = 1.99 (0.022 sec/batch)\n",
      "epoch 448, step 750 / 900, loss = 1.52 (0.021 sec/batch)\n",
      "epoch 448, step 800 / 900, loss = 2.99 (0.021 sec/batch)\n",
      "epoch 448, step 850 / 900, loss = 2.15 (0.020 sec/batch)\n",
      "epoch 448, step 900 / 900, loss = 1.66 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.94\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.84\n",
      " avg loss = 2.93\n",
      "\n",
      "Epoch time: 24.586148262023926\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 449, step 50 / 900, loss = 1.62 (0.022 sec/batch)\n",
      "epoch 449, step 100 / 900, loss = 2.15 (0.021 sec/batch)\n",
      "epoch 449, step 150 / 900, loss = 2.39 (0.021 sec/batch)\n",
      "epoch 449, step 200 / 900, loss = 2.74 (0.021 sec/batch)\n",
      "epoch 449, step 250 / 900, loss = 2.64 (0.021 sec/batch)\n",
      "epoch 449, step 300 / 900, loss = 1.57 (0.021 sec/batch)\n",
      "epoch 449, step 350 / 900, loss = 1.81 (0.021 sec/batch)\n",
      "epoch 449, step 400 / 900, loss = 1.79 (0.021 sec/batch)\n",
      "epoch 449, step 450 / 900, loss = 2.05 (0.022 sec/batch)\n",
      "epoch 449, step 500 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 449, step 550 / 900, loss = 1.61 (0.021 sec/batch)\n",
      "epoch 449, step 600 / 900, loss = 2.06 (0.021 sec/batch)\n",
      "epoch 449, step 650 / 900, loss = 2.24 (0.022 sec/batch)\n",
      "epoch 449, step 700 / 900, loss = 2.41 (0.021 sec/batch)\n",
      "epoch 449, step 750 / 900, loss = 2.11 (0.022 sec/batch)\n",
      "epoch 449, step 800 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "epoch 449, step 850 / 900, loss = 2.96 (0.022 sec/batch)\n",
      "epoch 449, step 900 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.40\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.82\n",
      " avg loss = 2.86\n",
      "\n",
      "Epoch time: 24.545854091644287\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 450, step 50 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 450, step 100 / 900, loss = 1.73 (0.021 sec/batch)\n",
      "epoch 450, step 150 / 900, loss = 1.62 (0.022 sec/batch)\n",
      "epoch 450, step 200 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 450, step 250 / 900, loss = 2.24 (0.021 sec/batch)\n",
      "epoch 450, step 300 / 900, loss = 1.49 (0.021 sec/batch)\n",
      "epoch 450, step 350 / 900, loss = 2.02 (0.022 sec/batch)\n",
      "epoch 450, step 400 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 450, step 450 / 900, loss = 2.75 (0.022 sec/batch)\n",
      "epoch 450, step 500 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 450, step 550 / 900, loss = 3.01 (0.021 sec/batch)\n",
      "epoch 450, step 600 / 900, loss = 1.98 (0.021 sec/batch)\n",
      "epoch 450, step 650 / 900, loss = 2.06 (0.021 sec/batch)\n",
      "epoch 450, step 700 / 900, loss = 2.02 (0.022 sec/batch)\n",
      "epoch 450, step 750 / 900, loss = 1.91 (0.022 sec/batch)\n",
      "epoch 450, step 800 / 900, loss = 2.73 (0.022 sec/batch)\n",
      "epoch 450, step 850 / 900, loss = 2.44 (0.021 sec/batch)\n",
      "epoch 450, step 900 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.62\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.02\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.589383602142334\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 451, step 50 / 900, loss = 2.40 (0.021 sec/batch)\n",
      "epoch 451, step 100 / 900, loss = 2.78 (0.021 sec/batch)\n",
      "epoch 451, step 150 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 451, step 200 / 900, loss = 2.73 (0.021 sec/batch)\n",
      "epoch 451, step 250 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 451, step 300 / 900, loss = 1.59 (0.021 sec/batch)\n",
      "epoch 451, step 350 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 451, step 400 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 451, step 450 / 900, loss = 1.69 (0.022 sec/batch)\n",
      "epoch 451, step 500 / 900, loss = 1.91 (0.022 sec/batch)\n",
      "epoch 451, step 550 / 900, loss = 2.82 (0.021 sec/batch)\n",
      "epoch 451, step 600 / 900, loss = 2.46 (0.022 sec/batch)\n",
      "epoch 451, step 650 / 900, loss = 1.67 (0.021 sec/batch)\n",
      "epoch 451, step 700 / 900, loss = 2.51 (0.022 sec/batch)\n",
      "epoch 451, step 750 / 900, loss = 2.50 (0.022 sec/batch)\n",
      "epoch 451, step 800 / 900, loss = 2.70 (0.021 sec/batch)\n",
      "epoch 451, step 850 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 451, step 900 / 900, loss = 3.45 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.22\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.44\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.54857301712036\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 452, step 50 / 900, loss = 2.38 (0.022 sec/batch)\n",
      "epoch 452, step 100 / 900, loss = 2.03 (0.022 sec/batch)\n",
      "epoch 452, step 150 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 452, step 200 / 900, loss = 2.24 (0.021 sec/batch)\n",
      "epoch 452, step 250 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 452, step 300 / 900, loss = 2.61 (0.022 sec/batch)\n",
      "epoch 452, step 350 / 900, loss = 3.04 (0.022 sec/batch)\n",
      "epoch 452, step 400 / 900, loss = 1.59 (0.021 sec/batch)\n",
      "epoch 452, step 450 / 900, loss = 3.07 (0.022 sec/batch)\n",
      "epoch 452, step 500 / 900, loss = 2.91 (0.021 sec/batch)\n",
      "epoch 452, step 550 / 900, loss = 2.32 (0.022 sec/batch)\n",
      "epoch 452, step 600 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 452, step 650 / 900, loss = 1.81 (0.021 sec/batch)\n",
      "epoch 452, step 700 / 900, loss = 2.45 (0.021 sec/batch)\n",
      "epoch 452, step 750 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 452, step 800 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 452, step 850 / 900, loss = 1.78 (0.021 sec/batch)\n",
      "epoch 452, step 900 / 900, loss = 2.47 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.50\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.32\n",
      " avg loss = 2.81\n",
      "\n",
      "Epoch time: 24.59866690635681\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 453, step 50 / 900, loss = 1.85 (0.021 sec/batch)\n",
      "epoch 453, step 100 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 453, step 150 / 900, loss = 2.19 (0.022 sec/batch)\n",
      "epoch 453, step 200 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 453, step 250 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 453, step 300 / 900, loss = 2.01 (0.022 sec/batch)\n",
      "epoch 453, step 350 / 900, loss = 2.34 (0.021 sec/batch)\n",
      "epoch 453, step 400 / 900, loss = 2.98 (0.021 sec/batch)\n",
      "epoch 453, step 450 / 900, loss = 2.47 (0.021 sec/batch)\n",
      "epoch 453, step 500 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 453, step 550 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "epoch 453, step 600 / 900, loss = 2.66 (0.021 sec/batch)\n",
      "epoch 453, step 650 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "epoch 453, step 700 / 900, loss = 2.38 (0.022 sec/batch)\n",
      "epoch 453, step 750 / 900, loss = 2.89 (0.021 sec/batch)\n",
      "epoch 453, step 800 / 900, loss = 2.26 (0.021 sec/batch)\n",
      "epoch 453, step 850 / 900, loss = 2.76 (0.021 sec/batch)\n",
      "epoch 453, step 900 / 900, loss = 1.98 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.97\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.68\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.58010458946228\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 454, step 50 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 454, step 100 / 900, loss = 2.25 (0.022 sec/batch)\n",
      "epoch 454, step 150 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 454, step 200 / 900, loss = 1.60 (0.021 sec/batch)\n",
      "epoch 454, step 250 / 900, loss = 2.85 (0.022 sec/batch)\n",
      "epoch 454, step 300 / 900, loss = 1.66 (0.021 sec/batch)\n",
      "epoch 454, step 350 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 454, step 400 / 900, loss = 3.09 (0.021 sec/batch)\n",
      "epoch 454, step 450 / 900, loss = 1.69 (0.022 sec/batch)\n",
      "epoch 454, step 500 / 900, loss = 3.17 (0.022 sec/batch)\n",
      "epoch 454, step 550 / 900, loss = 2.77 (0.021 sec/batch)\n",
      "epoch 454, step 600 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "epoch 454, step 650 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 454, step 700 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 454, step 750 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 454, step 800 / 900, loss = 2.39 (0.021 sec/batch)\n",
      "epoch 454, step 850 / 900, loss = 3.02 (0.021 sec/batch)\n",
      "epoch 454, step 900 / 900, loss = 1.60 (0.022 sec/batch)\n",
      "Train error:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy = 55.74\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.34\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.55722427368164\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 455, step 50 / 900, loss = 2.99 (0.021 sec/batch)\n",
      "epoch 455, step 100 / 900, loss = 1.47 (0.021 sec/batch)\n",
      "epoch 455, step 150 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "epoch 455, step 200 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 455, step 250 / 900, loss = 2.49 (0.022 sec/batch)\n",
      "epoch 455, step 300 / 900, loss = 2.49 (0.021 sec/batch)\n",
      "epoch 455, step 350 / 900, loss = 2.35 (0.022 sec/batch)\n",
      "epoch 455, step 400 / 900, loss = 2.47 (0.021 sec/batch)\n",
      "epoch 455, step 450 / 900, loss = 2.85 (0.021 sec/batch)\n",
      "epoch 455, step 500 / 900, loss = 1.77 (0.022 sec/batch)\n",
      "epoch 455, step 550 / 900, loss = 1.61 (0.021 sec/batch)\n",
      "epoch 455, step 600 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 455, step 650 / 900, loss = 2.59 (0.022 sec/batch)\n",
      "epoch 455, step 700 / 900, loss = 1.76 (0.022 sec/batch)\n",
      "epoch 455, step 750 / 900, loss = 2.32 (0.021 sec/batch)\n",
      "epoch 455, step 800 / 900, loss = 2.34 (0.022 sec/batch)\n",
      "epoch 455, step 850 / 900, loss = 3.02 (0.021 sec/batch)\n",
      "epoch 455, step 900 / 900, loss = 1.55 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.94\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.34\n",
      " avg loss = 2.94\n",
      "\n",
      "Epoch time: 24.55519151687622\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 456, step 50 / 900, loss = 2.72 (0.021 sec/batch)\n",
      "epoch 456, step 100 / 900, loss = 2.09 (0.022 sec/batch)\n",
      "epoch 456, step 150 / 900, loss = 2.46 (0.021 sec/batch)\n",
      "epoch 456, step 200 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "epoch 456, step 250 / 900, loss = 2.46 (0.021 sec/batch)\n",
      "epoch 456, step 300 / 900, loss = 1.80 (0.021 sec/batch)\n",
      "epoch 456, step 350 / 900, loss = 2.47 (0.021 sec/batch)\n",
      "epoch 456, step 400 / 900, loss = 1.79 (0.021 sec/batch)\n",
      "epoch 456, step 450 / 900, loss = 1.58 (0.021 sec/batch)\n",
      "epoch 456, step 500 / 900, loss = 1.95 (0.021 sec/batch)\n",
      "epoch 456, step 550 / 900, loss = 1.93 (0.021 sec/batch)\n",
      "epoch 456, step 600 / 900, loss = 1.96 (0.021 sec/batch)\n",
      "epoch 456, step 650 / 900, loss = 1.86 (0.022 sec/batch)\n",
      "epoch 456, step 700 / 900, loss = 2.28 (0.022 sec/batch)\n",
      "epoch 456, step 750 / 900, loss = 2.56 (0.021 sec/batch)\n",
      "epoch 456, step 800 / 900, loss = 3.32 (0.021 sec/batch)\n",
      "epoch 456, step 850 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 456, step 900 / 900, loss = 2.70 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.50\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.14\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.561502695083618\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 457, step 50 / 900, loss = 3.07 (0.021 sec/batch)\n",
      "epoch 457, step 100 / 900, loss = 2.34 (0.021 sec/batch)\n",
      "epoch 457, step 150 / 900, loss = 3.06 (0.022 sec/batch)\n",
      "epoch 457, step 200 / 900, loss = 1.50 (0.021 sec/batch)\n",
      "epoch 457, step 250 / 900, loss = 1.80 (0.021 sec/batch)\n",
      "epoch 457, step 300 / 900, loss = 1.31 (0.021 sec/batch)\n",
      "epoch 457, step 350 / 900, loss = 1.95 (0.021 sec/batch)\n",
      "epoch 457, step 400 / 900, loss = 1.53 (0.022 sec/batch)\n",
      "epoch 457, step 450 / 900, loss = 1.76 (0.021 sec/batch)\n",
      "epoch 457, step 500 / 900, loss = 2.40 (0.021 sec/batch)\n",
      "epoch 457, step 550 / 900, loss = 2.86 (0.021 sec/batch)\n",
      "epoch 457, step 600 / 900, loss = 2.88 (0.021 sec/batch)\n",
      "epoch 457, step 650 / 900, loss = 1.73 (0.021 sec/batch)\n",
      "epoch 457, step 700 / 900, loss = 1.71 (0.022 sec/batch)\n",
      "epoch 457, step 750 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 457, step 800 / 900, loss = 2.64 (0.022 sec/batch)\n",
      "epoch 457, step 850 / 900, loss = 2.26 (0.021 sec/batch)\n",
      "epoch 457, step 900 / 900, loss = 1.96 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.54\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.34\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.606428384780884\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 458, step 50 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "epoch 458, step 100 / 900, loss = 2.30 (0.022 sec/batch)\n",
      "epoch 458, step 150 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 458, step 200 / 900, loss = 1.93 (0.021 sec/batch)\n",
      "epoch 458, step 250 / 900, loss = 1.88 (0.021 sec/batch)\n",
      "epoch 458, step 300 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 458, step 350 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "epoch 458, step 400 / 900, loss = 2.59 (0.022 sec/batch)\n",
      "epoch 458, step 450 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 458, step 500 / 900, loss = 1.87 (0.021 sec/batch)\n",
      "epoch 458, step 550 / 900, loss = 2.63 (0.022 sec/batch)\n",
      "epoch 458, step 600 / 900, loss = 2.89 (0.021 sec/batch)\n",
      "epoch 458, step 650 / 900, loss = 1.61 (0.022 sec/batch)\n",
      "epoch 458, step 700 / 900, loss = 1.73 (0.022 sec/batch)\n",
      "epoch 458, step 750 / 900, loss = 1.47 (0.022 sec/batch)\n",
      "epoch 458, step 800 / 900, loss = 2.00 (0.022 sec/batch)\n",
      "epoch 458, step 850 / 900, loss = 1.82 (0.021 sec/batch)\n",
      "epoch 458, step 900 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.61\n",
      " avg loss = 2.24\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.28\n",
      " avg loss = 2.92\n",
      "\n",
      "Epoch time: 24.576462984085083\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 459, step 50 / 900, loss = 2.98 (0.022 sec/batch)\n",
      "epoch 459, step 100 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 459, step 150 / 900, loss = 2.16 (0.022 sec/batch)\n",
      "epoch 459, step 200 / 900, loss = 2.77 (0.022 sec/batch)\n",
      "epoch 459, step 250 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 459, step 300 / 900, loss = 1.73 (0.022 sec/batch)\n",
      "epoch 459, step 350 / 900, loss = 1.47 (0.022 sec/batch)\n",
      "epoch 459, step 400 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 459, step 450 / 900, loss = 2.58 (0.022 sec/batch)\n",
      "epoch 459, step 500 / 900, loss = 2.50 (0.022 sec/batch)\n",
      "epoch 459, step 550 / 900, loss = 1.31 (0.021 sec/batch)\n",
      "epoch 459, step 600 / 900, loss = 1.37 (0.021 sec/batch)\n",
      "epoch 459, step 650 / 900, loss = 2.04 (0.022 sec/batch)\n",
      "epoch 459, step 700 / 900, loss = 1.70 (0.022 sec/batch)\n",
      "epoch 459, step 750 / 900, loss = 3.13 (0.022 sec/batch)\n",
      "epoch 459, step 800 / 900, loss = 2.15 (0.021 sec/batch)\n",
      "epoch 459, step 850 / 900, loss = 2.04 (0.021 sec/batch)\n",
      "epoch 459, step 900 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.67\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.66\n",
      " avg loss = 2.86\n",
      "\n",
      "Epoch time: 24.579604864120483\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 460, step 50 / 900, loss = 2.32 (0.022 sec/batch)\n",
      "epoch 460, step 100 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 460, step 150 / 900, loss = 3.02 (0.021 sec/batch)\n",
      "epoch 460, step 200 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 460, step 250 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 460, step 300 / 900, loss = 1.98 (0.021 sec/batch)\n",
      "epoch 460, step 350 / 900, loss = 2.89 (0.021 sec/batch)\n",
      "epoch 460, step 400 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 460, step 450 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 460, step 500 / 900, loss = 2.41 (0.021 sec/batch)\n",
      "epoch 460, step 550 / 900, loss = 1.76 (0.022 sec/batch)\n",
      "epoch 460, step 600 / 900, loss = 2.59 (0.021 sec/batch)\n",
      "epoch 460, step 650 / 900, loss = 2.95 (0.022 sec/batch)\n",
      "epoch 460, step 700 / 900, loss = 1.73 (0.022 sec/batch)\n",
      "epoch 460, step 750 / 900, loss = 1.89 (0.021 sec/batch)\n",
      "epoch 460, step 800 / 900, loss = 2.52 (0.022 sec/batch)\n",
      "epoch 460, step 850 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 460, step 900 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.58\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.66\n",
      " avg loss = 2.83\n",
      "\n",
      "Epoch time: 24.593566417694092\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 461, step 50 / 900, loss = 2.23 (0.022 sec/batch)\n",
      "epoch 461, step 100 / 900, loss = 2.37 (0.022 sec/batch)\n",
      "epoch 461, step 150 / 900, loss = 1.93 (0.021 sec/batch)\n",
      "epoch 461, step 200 / 900, loss = 2.71 (0.021 sec/batch)\n",
      "epoch 461, step 250 / 900, loss = 2.77 (0.021 sec/batch)\n",
      "epoch 461, step 300 / 900, loss = 1.85 (0.022 sec/batch)\n",
      "epoch 461, step 350 / 900, loss = 2.47 (0.021 sec/batch)\n",
      "epoch 461, step 400 / 900, loss = 3.27 (0.021 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 461, step 450 / 900, loss = 2.39 (0.022 sec/batch)\n",
      "epoch 461, step 500 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "epoch 461, step 550 / 900, loss = 2.66 (0.022 sec/batch)\n",
      "epoch 461, step 600 / 900, loss = 2.10 (0.022 sec/batch)\n",
      "epoch 461, step 650 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 461, step 700 / 900, loss = 3.00 (0.021 sec/batch)\n",
      "epoch 461, step 750 / 900, loss = 2.29 (0.022 sec/batch)\n",
      "epoch 461, step 800 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 461, step 850 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "epoch 461, step 900 / 900, loss = 1.53 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.60\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.08\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.593740224838257\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 462, step 50 / 900, loss = 1.37 (0.021 sec/batch)\n",
      "epoch 462, step 100 / 900, loss = 1.73 (0.022 sec/batch)\n",
      "epoch 462, step 150 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 462, step 200 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 462, step 250 / 900, loss = 2.00 (0.022 sec/batch)\n",
      "epoch 462, step 300 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "epoch 462, step 350 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 462, step 400 / 900, loss = 2.97 (0.021 sec/batch)\n",
      "epoch 462, step 450 / 900, loss = 1.44 (0.021 sec/batch)\n",
      "epoch 462, step 500 / 900, loss = 2.13 (0.022 sec/batch)\n",
      "epoch 462, step 550 / 900, loss = 1.90 (0.021 sec/batch)\n",
      "epoch 462, step 600 / 900, loss = 3.11 (0.021 sec/batch)\n",
      "epoch 462, step 650 / 900, loss = 1.79 (0.022 sec/batch)\n",
      "epoch 462, step 700 / 900, loss = 2.36 (0.022 sec/batch)\n",
      "epoch 462, step 750 / 900, loss = 1.79 (0.021 sec/batch)\n",
      "epoch 462, step 800 / 900, loss = 1.81 (0.022 sec/batch)\n",
      "epoch 462, step 850 / 900, loss = 2.74 (0.021 sec/batch)\n",
      "epoch 462, step 900 / 900, loss = 1.82 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.89\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.62\n",
      " avg loss = 2.96\n",
      "\n",
      "Epoch time: 24.592154026031494\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 463, step 50 / 900, loss = 1.75 (0.021 sec/batch)\n",
      "epoch 463, step 100 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 463, step 150 / 900, loss = 2.91 (0.021 sec/batch)\n",
      "epoch 463, step 200 / 900, loss = 2.01 (0.022 sec/batch)\n",
      "epoch 463, step 250 / 900, loss = 2.00 (0.021 sec/batch)\n",
      "epoch 463, step 300 / 900, loss = 1.82 (0.022 sec/batch)\n",
      "epoch 463, step 350 / 900, loss = 2.83 (0.021 sec/batch)\n",
      "epoch 463, step 400 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 463, step 450 / 900, loss = 1.97 (0.022 sec/batch)\n",
      "epoch 463, step 500 / 900, loss = 2.83 (0.021 sec/batch)\n",
      "epoch 463, step 550 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 463, step 600 / 900, loss = 1.77 (0.021 sec/batch)\n",
      "epoch 463, step 650 / 900, loss = 2.86 (0.021 sec/batch)\n",
      "epoch 463, step 700 / 900, loss = 1.79 (0.023 sec/batch)\n",
      "epoch 463, step 750 / 900, loss = 2.25 (0.022 sec/batch)\n",
      "epoch 463, step 800 / 900, loss = 1.53 (0.021 sec/batch)\n",
      "epoch 463, step 850 / 900, loss = 1.62 (0.022 sec/batch)\n",
      "epoch 463, step 900 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.40\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.58\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.579678297042847\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 464, step 50 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 464, step 100 / 900, loss = 2.06 (0.021 sec/batch)\n",
      "epoch 464, step 150 / 900, loss = 3.36 (0.021 sec/batch)\n",
      "epoch 464, step 200 / 900, loss = 2.55 (0.022 sec/batch)\n",
      "epoch 464, step 250 / 900, loss = 2.84 (0.021 sec/batch)\n",
      "epoch 464, step 300 / 900, loss = 2.50 (0.021 sec/batch)\n",
      "epoch 464, step 350 / 900, loss = 1.92 (0.021 sec/batch)\n",
      "epoch 464, step 400 / 900, loss = 2.71 (0.021 sec/batch)\n",
      "epoch 464, step 450 / 900, loss = 2.08 (0.022 sec/batch)\n",
      "epoch 464, step 500 / 900, loss = 2.55 (0.021 sec/batch)\n",
      "epoch 464, step 550 / 900, loss = 1.81 (0.021 sec/batch)\n",
      "epoch 464, step 600 / 900, loss = 1.34 (0.021 sec/batch)\n",
      "epoch 464, step 650 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 464, step 700 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 464, step 750 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 464, step 800 / 900, loss = 2.29 (0.022 sec/batch)\n",
      "epoch 464, step 850 / 900, loss = 2.58 (0.022 sec/batch)\n",
      "epoch 464, step 900 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.72\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.70\n",
      " avg loss = 2.83\n",
      "\n",
      "Epoch time: 24.58131766319275\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 465, step 50 / 900, loss = 2.73 (0.021 sec/batch)\n",
      "epoch 465, step 100 / 900, loss = 2.55 (0.022 sec/batch)\n",
      "epoch 465, step 150 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 465, step 200 / 900, loss = 2.43 (0.022 sec/batch)\n",
      "epoch 465, step 250 / 900, loss = 2.41 (0.022 sec/batch)\n",
      "epoch 465, step 300 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "epoch 465, step 350 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 465, step 400 / 900, loss = 1.86 (0.021 sec/batch)\n",
      "epoch 465, step 450 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 465, step 500 / 900, loss = 2.79 (0.021 sec/batch)\n",
      "epoch 465, step 550 / 900, loss = 2.74 (0.021 sec/batch)\n",
      "epoch 465, step 600 / 900, loss = 1.70 (0.021 sec/batch)\n",
      "epoch 465, step 650 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 465, step 700 / 900, loss = 2.54 (0.022 sec/batch)\n",
      "epoch 465, step 750 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 465, step 800 / 900, loss = 1.67 (0.020 sec/batch)\n",
      "epoch 465, step 850 / 900, loss = 2.87 (0.021 sec/batch)\n",
      "epoch 465, step 900 / 900, loss = 2.73 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 56.10\n",
      " avg loss = 2.24\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.06\n",
      " avg loss = 2.85\n",
      "\n",
      "Epoch time: 24.57748031616211\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 466, step 50 / 900, loss = 2.15 (0.021 sec/batch)\n",
      "epoch 466, step 100 / 900, loss = 2.32 (0.021 sec/batch)\n",
      "epoch 466, step 150 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 466, step 200 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 466, step 250 / 900, loss = 2.47 (0.022 sec/batch)\n",
      "epoch 466, step 300 / 900, loss = 2.50 (0.021 sec/batch)\n",
      "epoch 466, step 350 / 900, loss = 2.13 (0.022 sec/batch)\n",
      "epoch 466, step 400 / 900, loss = 1.78 (0.021 sec/batch)\n",
      "epoch 466, step 450 / 900, loss = 2.88 (0.021 sec/batch)\n",
      "epoch 466, step 500 / 900, loss = 2.45 (0.021 sec/batch)\n",
      "epoch 466, step 550 / 900, loss = 1.82 (0.022 sec/batch)\n",
      "epoch 466, step 600 / 900, loss = 2.11 (0.022 sec/batch)\n",
      "epoch 466, step 650 / 900, loss = 2.41 (0.021 sec/batch)\n",
      "epoch 466, step 700 / 900, loss = 2.35 (0.022 sec/batch)\n",
      "epoch 466, step 750 / 900, loss = 2.31 (0.022 sec/batch)\n",
      "epoch 466, step 800 / 900, loss = 2.92 (0.021 sec/batch)\n",
      "epoch 466, step 850 / 900, loss = 2.46 (0.021 sec/batch)\n",
      "epoch 466, step 900 / 900, loss = 1.92 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.72\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.24\n",
      " avg loss = 2.94\n",
      "\n",
      "Epoch time: 24.576931953430176\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 467, step 50 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 467, step 100 / 900, loss = 2.26 (0.021 sec/batch)\n",
      "epoch 467, step 150 / 900, loss = 2.46 (0.021 sec/batch)\n",
      "epoch 467, step 200 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 467, step 250 / 900, loss = 2.14 (0.022 sec/batch)\n",
      "epoch 467, step 300 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 467, step 350 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 467, step 400 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 467, step 450 / 900, loss = 2.37 (0.022 sec/batch)\n",
      "epoch 467, step 500 / 900, loss = 2.34 (0.021 sec/batch)\n",
      "epoch 467, step 550 / 900, loss = 2.05 (0.022 sec/batch)\n",
      "epoch 467, step 600 / 900, loss = 1.87 (0.022 sec/batch)\n",
      "epoch 467, step 650 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 467, step 700 / 900, loss = 1.96 (0.022 sec/batch)\n",
      "epoch 467, step 750 / 900, loss = 2.32 (0.022 sec/batch)\n",
      "epoch 467, step 800 / 900, loss = 2.11 (0.022 sec/batch)\n",
      "epoch 467, step 850 / 900, loss = 3.19 (0.021 sec/batch)\n",
      "epoch 467, step 900 / 900, loss = 1.72 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.89\n",
      " avg loss = 2.23\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.54\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.620251417160034\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 468, step 50 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "epoch 468, step 100 / 900, loss = 1.52 (0.021 sec/batch)\n",
      "epoch 468, step 150 / 900, loss = 2.11 (0.021 sec/batch)\n",
      "epoch 468, step 200 / 900, loss = 1.93 (0.022 sec/batch)\n",
      "epoch 468, step 250 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 468, step 300 / 900, loss = 3.17 (0.021 sec/batch)\n",
      "epoch 468, step 350 / 900, loss = 1.86 (0.022 sec/batch)\n",
      "epoch 468, step 400 / 900, loss = 1.85 (0.021 sec/batch)\n",
      "epoch 468, step 450 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 468, step 500 / 900, loss = 1.40 (0.021 sec/batch)\n",
      "epoch 468, step 550 / 900, loss = 1.87 (0.022 sec/batch)\n",
      "epoch 468, step 600 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "epoch 468, step 650 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "epoch 468, step 700 / 900, loss = 2.03 (0.022 sec/batch)\n",
      "epoch 468, step 750 / 900, loss = 2.27 (0.022 sec/batch)\n",
      "epoch 468, step 800 / 900, loss = 2.94 (0.021 sec/batch)\n",
      "epoch 468, step 850 / 900, loss = 2.86 (0.021 sec/batch)\n",
      "epoch 468, step 900 / 900, loss = 2.77 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.62\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.80\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.579421520233154\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 469, step 50 / 900, loss = 2.15 (0.021 sec/batch)\n",
      "epoch 469, step 100 / 900, loss = 2.32 (0.021 sec/batch)\n",
      "epoch 469, step 150 / 900, loss = 2.70 (0.022 sec/batch)\n",
      "epoch 469, step 200 / 900, loss = 2.46 (0.021 sec/batch)\n",
      "epoch 469, step 250 / 900, loss = 1.09 (0.022 sec/batch)\n",
      "epoch 469, step 300 / 900, loss = 1.91 (0.021 sec/batch)\n",
      "epoch 469, step 350 / 900, loss = 0.91 (0.021 sec/batch)\n",
      "epoch 469, step 400 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 469, step 450 / 900, loss = 1.95 (0.021 sec/batch)\n",
      "epoch 469, step 500 / 900, loss = 2.32 (0.021 sec/batch)\n",
      "epoch 469, step 550 / 900, loss = 2.85 (0.021 sec/batch)\n",
      "epoch 469, step 600 / 900, loss = 2.79 (0.021 sec/batch)\n",
      "epoch 469, step 650 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "epoch 469, step 700 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 469, step 750 / 900, loss = 2.51 (0.021 sec/batch)\n",
      "epoch 469, step 800 / 900, loss = 2.48 (0.022 sec/batch)\n",
      "epoch 469, step 850 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 469, step 900 / 900, loss = 2.54 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.43\n",
      " avg loss = 2.28\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.34\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.584506273269653\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 470, step 50 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "epoch 470, step 100 / 900, loss = 2.24 (0.021 sec/batch)\n",
      "epoch 470, step 150 / 900, loss = 2.36 (0.022 sec/batch)\n",
      "epoch 470, step 200 / 900, loss = 1.70 (0.022 sec/batch)\n",
      "epoch 470, step 250 / 900, loss = 1.98 (0.022 sec/batch)\n",
      "epoch 470, step 300 / 900, loss = 2.74 (0.021 sec/batch)\n",
      "epoch 470, step 350 / 900, loss = 1.82 (0.022 sec/batch)\n",
      "epoch 470, step 400 / 900, loss = 1.56 (0.021 sec/batch)\n",
      "epoch 470, step 450 / 900, loss = 1.70 (0.021 sec/batch)\n",
      "epoch 470, step 500 / 900, loss = 2.21 (0.022 sec/batch)\n",
      "epoch 470, step 550 / 900, loss = 2.54 (0.021 sec/batch)\n",
      "epoch 470, step 600 / 900, loss = 3.57 (0.021 sec/batch)\n",
      "epoch 470, step 650 / 900, loss = 2.54 (0.020 sec/batch)\n",
      "epoch 470, step 700 / 900, loss = 1.34 (0.021 sec/batch)\n",
      "epoch 470, step 750 / 900, loss = 2.12 (0.022 sec/batch)\n",
      "epoch 470, step 800 / 900, loss = 2.96 (0.021 sec/batch)\n",
      "epoch 470, step 850 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 470, step 900 / 900, loss = 3.16 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.52\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.54\n",
      " avg loss = 2.93\n",
      "\n",
      "Epoch time: 24.574947834014893\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 471, step 50 / 900, loss = 3.01 (0.021 sec/batch)\n",
      "epoch 471, step 100 / 900, loss = 1.58 (0.021 sec/batch)\n",
      "epoch 471, step 150 / 900, loss = 2.58 (0.021 sec/batch)\n",
      "epoch 471, step 200 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 471, step 250 / 900, loss = 1.88 (0.022 sec/batch)\n",
      "epoch 471, step 300 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "epoch 471, step 350 / 900, loss = 2.76 (0.021 sec/batch)\n",
      "epoch 471, step 400 / 900, loss = 3.25 (0.021 sec/batch)\n",
      "epoch 471, step 450 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 471, step 500 / 900, loss = 1.62 (0.021 sec/batch)\n",
      "epoch 471, step 550 / 900, loss = 1.77 (0.021 sec/batch)\n",
      "epoch 471, step 600 / 900, loss = 2.45 (0.021 sec/batch)\n",
      "epoch 471, step 650 / 900, loss = 2.05 (0.022 sec/batch)\n",
      "epoch 471, step 700 / 900, loss = 1.88 (0.022 sec/batch)\n",
      "epoch 471, step 750 / 900, loss = 3.05 (0.021 sec/batch)\n",
      "epoch 471, step 800 / 900, loss = 2.50 (0.021 sec/batch)\n",
      "epoch 471, step 850 / 900, loss = 1.76 (0.021 sec/batch)\n",
      "epoch 471, step 900 / 900, loss = 1.39 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.55\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.46\n",
      " avg loss = 2.86\n",
      "\n",
      "Epoch time: 24.562349319458008\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 472, step 50 / 900, loss = 1.72 (0.021 sec/batch)\n",
      "epoch 472, step 100 / 900, loss = 2.55 (0.021 sec/batch)\n",
      "epoch 472, step 150 / 900, loss = 2.68 (0.021 sec/batch)\n",
      "epoch 472, step 200 / 900, loss = 1.94 (0.022 sec/batch)\n",
      "epoch 472, step 250 / 900, loss = 1.98 (0.022 sec/batch)\n",
      "epoch 472, step 300 / 900, loss = 2.04 (0.021 sec/batch)\n",
      "epoch 472, step 350 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 472, step 400 / 900, loss = 2.48 (0.022 sec/batch)\n",
      "epoch 472, step 450 / 900, loss = 2.79 (0.022 sec/batch)\n",
      "epoch 472, step 500 / 900, loss = 2.04 (0.022 sec/batch)\n",
      "epoch 472, step 550 / 900, loss = 2.24 (0.020 sec/batch)\n",
      "epoch 472, step 600 / 900, loss = 2.98 (0.021 sec/batch)\n",
      "epoch 472, step 650 / 900, loss = 1.81 (0.021 sec/batch)\n",
      "epoch 472, step 700 / 900, loss = 1.79 (0.022 sec/batch)\n",
      "epoch 472, step 750 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 472, step 800 / 900, loss = 1.78 (0.021 sec/batch)\n",
      "epoch 472, step 850 / 900, loss = 1.93 (0.022 sec/batch)\n",
      "epoch 472, step 900 / 900, loss = 2.92 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.87\n",
      " avg loss = 2.24\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.40\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.617393016815186\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 473, step 50 / 900, loss = 2.53 (0.022 sec/batch)\n",
      "epoch 473, step 100 / 900, loss = 2.07 (0.022 sec/batch)\n",
      "epoch 473, step 150 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 473, step 200 / 900, loss = 2.63 (0.021 sec/batch)\n",
      "epoch 473, step 250 / 900, loss = 2.20 (0.022 sec/batch)\n",
      "epoch 473, step 300 / 900, loss = 1.88 (0.021 sec/batch)\n",
      "epoch 473, step 350 / 900, loss = 2.58 (0.022 sec/batch)\n",
      "epoch 473, step 400 / 900, loss = 2.71 (0.021 sec/batch)\n",
      "epoch 473, step 450 / 900, loss = 2.74 (0.021 sec/batch)\n",
      "epoch 473, step 500 / 900, loss = 1.81 (0.021 sec/batch)\n",
      "epoch 473, step 550 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 473, step 600 / 900, loss = 2.73 (0.021 sec/batch)\n",
      "epoch 473, step 650 / 900, loss = 2.68 (0.021 sec/batch)\n",
      "epoch 473, step 700 / 900, loss = 1.36 (0.021 sec/batch)\n",
      "epoch 473, step 750 / 900, loss = 1.63 (0.022 sec/batch)\n",
      "epoch 473, step 800 / 900, loss = 1.85 (0.021 sec/batch)\n",
      "epoch 473, step 850 / 900, loss = 2.76 (0.021 sec/batch)\n",
      "epoch 473, step 900 / 900, loss = 1.96 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.34\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.74\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.573099851608276\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 474, step 50 / 900, loss = 2.64 (0.021 sec/batch)\n",
      "epoch 474, step 100 / 900, loss = 2.32 (0.021 sec/batch)\n",
      "epoch 474, step 150 / 900, loss = 2.32 (0.021 sec/batch)\n",
      "epoch 474, step 200 / 900, loss = 1.68 (0.021 sec/batch)\n",
      "epoch 474, step 250 / 900, loss = 2.94 (0.022 sec/batch)\n",
      "epoch 474, step 300 / 900, loss = 2.92 (0.021 sec/batch)\n",
      "epoch 474, step 350 / 900, loss = 2.61 (0.022 sec/batch)\n",
      "epoch 474, step 400 / 900, loss = 2.55 (0.022 sec/batch)\n",
      "epoch 474, step 450 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 474, step 500 / 900, loss = 2.04 (0.021 sec/batch)\n",
      "epoch 474, step 550 / 900, loss = 2.31 (0.022 sec/batch)\n",
      "epoch 474, step 600 / 900, loss = 0.96 (0.022 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 474, step 650 / 900, loss = 1.62 (0.021 sec/batch)\n",
      "epoch 474, step 700 / 900, loss = 1.36 (0.021 sec/batch)\n",
      "epoch 474, step 750 / 900, loss = 2.99 (0.021 sec/batch)\n",
      "epoch 474, step 800 / 900, loss = 3.00 (0.021 sec/batch)\n",
      "epoch 474, step 850 / 900, loss = 2.06 (0.021 sec/batch)\n",
      "epoch 474, step 900 / 900, loss = 1.70 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.65\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 48.98\n",
      " avg loss = 2.92\n",
      "\n",
      "Epoch time: 24.588899612426758\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 475, step 50 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 475, step 100 / 900, loss = 2.53 (0.022 sec/batch)\n",
      "epoch 475, step 150 / 900, loss = 1.85 (0.022 sec/batch)\n",
      "epoch 475, step 200 / 900, loss = 2.09 (0.022 sec/batch)\n",
      "epoch 475, step 250 / 900, loss = 1.44 (0.022 sec/batch)\n",
      "epoch 475, step 300 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "epoch 475, step 350 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 475, step 400 / 900, loss = 1.68 (0.022 sec/batch)\n",
      "epoch 475, step 450 / 900, loss = 2.12 (0.022 sec/batch)\n",
      "epoch 475, step 500 / 900, loss = 1.90 (0.021 sec/batch)\n",
      "epoch 475, step 550 / 900, loss = 1.59 (0.021 sec/batch)\n",
      "epoch 475, step 600 / 900, loss = 2.34 (0.021 sec/batch)\n",
      "epoch 475, step 650 / 900, loss = 1.95 (0.022 sec/batch)\n",
      "epoch 475, step 700 / 900, loss = 2.11 (0.022 sec/batch)\n",
      "epoch 475, step 750 / 900, loss = 2.25 (0.022 sec/batch)\n",
      "epoch 475, step 800 / 900, loss = 1.91 (0.021 sec/batch)\n",
      "epoch 475, step 850 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 475, step 900 / 900, loss = 2.49 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.40\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.76\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.574535369873047\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 476, step 50 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 476, step 100 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 476, step 150 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 476, step 200 / 900, loss = 1.60 (0.021 sec/batch)\n",
      "epoch 476, step 250 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 476, step 300 / 900, loss = 2.24 (0.021 sec/batch)\n",
      "epoch 476, step 350 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 476, step 400 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 476, step 450 / 900, loss = 2.72 (0.021 sec/batch)\n",
      "epoch 476, step 500 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 476, step 550 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 476, step 600 / 900, loss = 1.74 (0.021 sec/batch)\n",
      "epoch 476, step 650 / 900, loss = 1.60 (0.022 sec/batch)\n",
      "epoch 476, step 700 / 900, loss = 1.71 (0.022 sec/batch)\n",
      "epoch 476, step 750 / 900, loss = 2.58 (0.021 sec/batch)\n",
      "epoch 476, step 800 / 900, loss = 1.50 (0.022 sec/batch)\n",
      "epoch 476, step 850 / 900, loss = 2.05 (0.020 sec/batch)\n",
      "epoch 476, step 900 / 900, loss = 1.69 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.68\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.48\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.610514640808105\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 477, step 50 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 477, step 100 / 900, loss = 3.18 (0.022 sec/batch)\n",
      "epoch 477, step 150 / 900, loss = 1.92 (0.021 sec/batch)\n",
      "epoch 477, step 200 / 900, loss = 3.41 (0.021 sec/batch)\n",
      "epoch 477, step 250 / 900, loss = 1.33 (0.022 sec/batch)\n",
      "epoch 477, step 300 / 900, loss = 2.96 (0.021 sec/batch)\n",
      "epoch 477, step 350 / 900, loss = 2.73 (0.021 sec/batch)\n",
      "epoch 477, step 400 / 900, loss = 2.83 (0.022 sec/batch)\n",
      "epoch 477, step 450 / 900, loss = 1.99 (0.021 sec/batch)\n",
      "epoch 477, step 500 / 900, loss = 1.69 (0.021 sec/batch)\n",
      "epoch 477, step 550 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 477, step 600 / 900, loss = 3.34 (0.021 sec/batch)\n",
      "epoch 477, step 650 / 900, loss = 2.73 (0.021 sec/batch)\n",
      "epoch 477, step 700 / 900, loss = 2.38 (0.022 sec/batch)\n",
      "epoch 477, step 750 / 900, loss = 1.66 (0.021 sec/batch)\n",
      "epoch 477, step 800 / 900, loss = 1.75 (0.021 sec/batch)\n",
      "epoch 477, step 850 / 900, loss = 2.15 (0.022 sec/batch)\n",
      "epoch 477, step 900 / 900, loss = 2.37 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.80\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.54\n",
      " avg loss = 2.92\n",
      "\n",
      "Epoch time: 24.60267949104309\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 478, step 50 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "epoch 478, step 100 / 900, loss = 1.62 (0.021 sec/batch)\n",
      "epoch 478, step 150 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 478, step 200 / 900, loss = 2.67 (0.021 sec/batch)\n",
      "epoch 478, step 250 / 900, loss = 1.85 (0.021 sec/batch)\n",
      "epoch 478, step 300 / 900, loss = 1.93 (0.021 sec/batch)\n",
      "epoch 478, step 350 / 900, loss = 3.09 (0.021 sec/batch)\n",
      "epoch 478, step 400 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 478, step 450 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 478, step 500 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "epoch 478, step 550 / 900, loss = 1.98 (0.021 sec/batch)\n",
      "epoch 478, step 600 / 900, loss = 3.31 (0.021 sec/batch)\n",
      "epoch 478, step 650 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "epoch 478, step 700 / 900, loss = 2.12 (0.022 sec/batch)\n",
      "epoch 478, step 750 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 478, step 800 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 478, step 850 / 900, loss = 1.82 (0.021 sec/batch)\n",
      "epoch 478, step 900 / 900, loss = 1.74 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.81\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.20\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.60387134552002\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 479, step 50 / 900, loss = 2.55 (0.021 sec/batch)\n",
      "epoch 479, step 100 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 479, step 150 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 479, step 200 / 900, loss = 1.46 (0.021 sec/batch)\n",
      "epoch 479, step 250 / 900, loss = 1.54 (0.023 sec/batch)\n",
      "epoch 479, step 300 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "epoch 479, step 350 / 900, loss = 2.89 (0.021 sec/batch)\n",
      "epoch 479, step 400 / 900, loss = 2.44 (0.021 sec/batch)\n",
      "epoch 479, step 450 / 900, loss = 1.72 (0.021 sec/batch)\n",
      "epoch 479, step 500 / 900, loss = 1.21 (0.022 sec/batch)\n",
      "epoch 479, step 550 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 479, step 600 / 900, loss = 3.49 (0.022 sec/batch)\n",
      "epoch 479, step 650 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 479, step 700 / 900, loss = 2.44 (0.021 sec/batch)\n",
      "epoch 479, step 750 / 900, loss = 2.25 (0.022 sec/batch)\n",
      "epoch 479, step 800 / 900, loss = 2.58 (0.021 sec/batch)\n",
      "epoch 479, step 850 / 900, loss = 2.57 (0.021 sec/batch)\n",
      "epoch 479, step 900 / 900, loss = 2.81 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.41\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.08\n",
      " avg loss = 2.88\n",
      "\n",
      "Epoch time: 24.575161457061768\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 480, step 50 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 480, step 100 / 900, loss = 3.10 (0.021 sec/batch)\n",
      "epoch 480, step 150 / 900, loss = 1.67 (0.021 sec/batch)\n",
      "epoch 480, step 200 / 900, loss = 1.69 (0.021 sec/batch)\n",
      "epoch 480, step 250 / 900, loss = 2.42 (0.022 sec/batch)\n",
      "epoch 480, step 300 / 900, loss = 2.34 (0.022 sec/batch)\n",
      "epoch 480, step 350 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 480, step 400 / 900, loss = 1.82 (0.021 sec/batch)\n",
      "epoch 480, step 450 / 900, loss = 1.99 (0.021 sec/batch)\n",
      "epoch 480, step 500 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 480, step 550 / 900, loss = 1.90 (0.022 sec/batch)\n",
      "epoch 480, step 600 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 480, step 650 / 900, loss = 1.59 (0.021 sec/batch)\n",
      "epoch 480, step 700 / 900, loss = 2.20 (0.022 sec/batch)\n",
      "epoch 480, step 750 / 900, loss = 2.23 (0.021 sec/batch)\n",
      "epoch 480, step 800 / 900, loss = 1.82 (0.022 sec/batch)\n",
      "epoch 480, step 850 / 900, loss = 2.26 (0.022 sec/batch)\n",
      "epoch 480, step 900 / 900, loss = 2.15 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.75\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 48.98\n",
      " avg loss = 2.96\n",
      "\n",
      "Epoch time: 24.575420141220093\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 481, step 50 / 900, loss = 1.97 (0.021 sec/batch)\n",
      "epoch 481, step 100 / 900, loss = 2.72 (0.021 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 481, step 150 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 481, step 200 / 900, loss = 2.30 (0.022 sec/batch)\n",
      "epoch 481, step 250 / 900, loss = 1.64 (0.022 sec/batch)\n",
      "epoch 481, step 300 / 900, loss = 1.67 (0.022 sec/batch)\n",
      "epoch 481, step 350 / 900, loss = 1.76 (0.021 sec/batch)\n",
      "epoch 481, step 400 / 900, loss = 2.79 (0.021 sec/batch)\n",
      "epoch 481, step 450 / 900, loss = 2.95 (0.021 sec/batch)\n",
      "epoch 481, step 500 / 900, loss = 1.64 (0.021 sec/batch)\n",
      "epoch 481, step 550 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 481, step 600 / 900, loss = 2.61 (0.021 sec/batch)\n",
      "epoch 481, step 650 / 900, loss = 2.10 (0.022 sec/batch)\n",
      "epoch 481, step 700 / 900, loss = 2.06 (0.021 sec/batch)\n",
      "epoch 481, step 750 / 900, loss = 2.61 (0.021 sec/batch)\n",
      "epoch 481, step 800 / 900, loss = 2.68 (0.021 sec/batch)\n",
      "epoch 481, step 850 / 900, loss = 1.71 (0.022 sec/batch)\n",
      "epoch 481, step 900 / 900, loss = 2.44 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.40\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.50\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.619444608688354\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 482, step 50 / 900, loss = 1.60 (0.021 sec/batch)\n",
      "epoch 482, step 100 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 482, step 150 / 900, loss = 2.24 (0.021 sec/batch)\n",
      "epoch 482, step 200 / 900, loss = 2.18 (0.022 sec/batch)\n",
      "epoch 482, step 250 / 900, loss = 2.71 (0.021 sec/batch)\n",
      "epoch 482, step 300 / 900, loss = 1.86 (0.021 sec/batch)\n",
      "epoch 482, step 350 / 900, loss = 3.31 (0.021 sec/batch)\n",
      "epoch 482, step 400 / 900, loss = 2.72 (0.021 sec/batch)\n",
      "epoch 482, step 450 / 900, loss = 2.03 (0.023 sec/batch)\n",
      "epoch 482, step 500 / 900, loss = 1.98 (0.021 sec/batch)\n",
      "epoch 482, step 550 / 900, loss = 2.17 (0.021 sec/batch)\n",
      "epoch 482, step 600 / 900, loss = 1.87 (0.021 sec/batch)\n",
      "epoch 482, step 650 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 482, step 700 / 900, loss = 1.67 (0.021 sec/batch)\n",
      "epoch 482, step 750 / 900, loss = 2.30 (0.022 sec/batch)\n",
      "epoch 482, step 800 / 900, loss = 1.55 (0.021 sec/batch)\n",
      "epoch 482, step 850 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 482, step 900 / 900, loss = 2.77 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.63\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.50\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.585498809814453\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 483, step 50 / 900, loss = 2.07 (0.022 sec/batch)\n",
      "epoch 483, step 100 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "epoch 483, step 150 / 900, loss = 2.65 (0.022 sec/batch)\n",
      "epoch 483, step 200 / 900, loss = 1.39 (0.021 sec/batch)\n",
      "epoch 483, step 250 / 900, loss = 1.77 (0.022 sec/batch)\n",
      "epoch 483, step 300 / 900, loss = 2.02 (0.021 sec/batch)\n",
      "epoch 483, step 350 / 900, loss = 2.81 (0.022 sec/batch)\n",
      "epoch 483, step 400 / 900, loss = 2.32 (0.022 sec/batch)\n",
      "epoch 483, step 450 / 900, loss = 2.80 (0.021 sec/batch)\n",
      "epoch 483, step 500 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 483, step 550 / 900, loss = 3.32 (0.022 sec/batch)\n",
      "epoch 483, step 600 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 483, step 650 / 900, loss = 1.89 (0.022 sec/batch)\n",
      "epoch 483, step 700 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 483, step 750 / 900, loss = 2.07 (0.022 sec/batch)\n",
      "epoch 483, step 800 / 900, loss = 2.06 (0.022 sec/batch)\n",
      "epoch 483, step 850 / 900, loss = 2.22 (0.021 sec/batch)\n",
      "epoch 483, step 900 / 900, loss = 2.24 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.63\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.46\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.597532033920288\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 484, step 50 / 900, loss = 2.21 (0.022 sec/batch)\n",
      "epoch 484, step 100 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 484, step 150 / 900, loss = 2.39 (0.021 sec/batch)\n",
      "epoch 484, step 200 / 900, loss = 1.21 (0.021 sec/batch)\n",
      "epoch 484, step 250 / 900, loss = 2.05 (0.022 sec/batch)\n",
      "epoch 484, step 300 / 900, loss = 2.49 (0.021 sec/batch)\n",
      "epoch 484, step 350 / 900, loss = 3.01 (0.021 sec/batch)\n",
      "epoch 484, step 400 / 900, loss = 2.24 (0.021 sec/batch)\n",
      "epoch 484, step 450 / 900, loss = 2.26 (0.022 sec/batch)\n",
      "epoch 484, step 500 / 900, loss = 2.11 (0.022 sec/batch)\n",
      "epoch 484, step 550 / 900, loss = 2.56 (0.021 sec/batch)\n",
      "epoch 484, step 600 / 900, loss = 2.11 (0.022 sec/batch)\n",
      "epoch 484, step 650 / 900, loss = 2.79 (0.021 sec/batch)\n",
      "epoch 484, step 700 / 900, loss = 2.54 (0.022 sec/batch)\n",
      "epoch 484, step 750 / 900, loss = 1.78 (0.022 sec/batch)\n",
      "epoch 484, step 800 / 900, loss = 2.57 (0.021 sec/batch)\n",
      "epoch 484, step 850 / 900, loss = 2.25 (0.022 sec/batch)\n",
      "epoch 484, step 900 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.65\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.02\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.573939085006714\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 485, step 50 / 900, loss = 2.47 (0.021 sec/batch)\n",
      "epoch 485, step 100 / 900, loss = 1.76 (0.021 sec/batch)\n",
      "epoch 485, step 150 / 900, loss = 3.33 (0.021 sec/batch)\n",
      "epoch 485, step 200 / 900, loss = 2.40 (0.022 sec/batch)\n",
      "epoch 485, step 250 / 900, loss = 2.81 (0.022 sec/batch)\n",
      "epoch 485, step 300 / 900, loss = 2.34 (0.021 sec/batch)\n",
      "epoch 485, step 350 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 485, step 400 / 900, loss = 1.93 (0.021 sec/batch)\n",
      "epoch 485, step 450 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 485, step 500 / 900, loss = 1.67 (0.021 sec/batch)\n",
      "epoch 485, step 550 / 900, loss = 2.06 (0.021 sec/batch)\n",
      "epoch 485, step 600 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "epoch 485, step 650 / 900, loss = 2.46 (0.021 sec/batch)\n",
      "epoch 485, step 700 / 900, loss = 1.80 (0.022 sec/batch)\n",
      "epoch 485, step 750 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "epoch 485, step 800 / 900, loss = 2.50 (0.021 sec/batch)\n",
      "epoch 485, step 850 / 900, loss = 2.82 (0.021 sec/batch)\n",
      "epoch 485, step 900 / 900, loss = 2.40 (0.022 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.61\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.78\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.586424589157104\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 486, step 50 / 900, loss = 3.62 (0.021 sec/batch)\n",
      "epoch 486, step 100 / 900, loss = 2.43 (0.021 sec/batch)\n",
      "epoch 486, step 150 / 900, loss = 2.36 (0.021 sec/batch)\n",
      "epoch 486, step 200 / 900, loss = 2.35 (0.022 sec/batch)\n",
      "epoch 486, step 250 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 486, step 300 / 900, loss = 2.57 (0.022 sec/batch)\n",
      "epoch 486, step 350 / 900, loss = 2.69 (0.021 sec/batch)\n",
      "epoch 486, step 400 / 900, loss = 1.30 (0.021 sec/batch)\n",
      "epoch 486, step 450 / 900, loss = 0.94 (0.022 sec/batch)\n",
      "epoch 486, step 500 / 900, loss = 2.47 (0.022 sec/batch)\n",
      "epoch 486, step 550 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "epoch 486, step 600 / 900, loss = 2.72 (0.022 sec/batch)\n",
      "epoch 486, step 650 / 900, loss = 1.61 (0.021 sec/batch)\n",
      "epoch 486, step 700 / 900, loss = 1.63 (0.022 sec/batch)\n",
      "epoch 486, step 750 / 900, loss = 3.06 (0.021 sec/batch)\n",
      "epoch 486, step 800 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "epoch 486, step 850 / 900, loss = 2.40 (0.022 sec/batch)\n",
      "epoch 486, step 900 / 900, loss = 1.81 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.48\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.28\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.60012412071228\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 487, step 50 / 900, loss = 2.08 (0.021 sec/batch)\n",
      "epoch 487, step 100 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "epoch 487, step 150 / 900, loss = 1.56 (0.021 sec/batch)\n",
      "epoch 487, step 200 / 900, loss = 2.40 (0.021 sec/batch)\n",
      "epoch 487, step 250 / 900, loss = 2.71 (0.022 sec/batch)\n",
      "epoch 487, step 300 / 900, loss = 2.45 (0.021 sec/batch)\n",
      "epoch 487, step 350 / 900, loss = 3.68 (0.021 sec/batch)\n",
      "epoch 487, step 400 / 900, loss = 2.24 (0.022 sec/batch)\n",
      "epoch 487, step 450 / 900, loss = 1.54 (0.022 sec/batch)\n",
      "epoch 487, step 500 / 900, loss = 2.13 (0.022 sec/batch)\n",
      "epoch 487, step 550 / 900, loss = 2.32 (0.022 sec/batch)\n",
      "epoch 487, step 600 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 487, step 650 / 900, loss = 1.74 (0.021 sec/batch)\n",
      "epoch 487, step 700 / 900, loss = 1.82 (0.022 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 487, step 750 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 487, step 800 / 900, loss = 2.06 (0.022 sec/batch)\n",
      "epoch 487, step 850 / 900, loss = 2.09 (0.021 sec/batch)\n",
      "epoch 487, step 900 / 900, loss = 1.53 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.74\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.72\n",
      " avg loss = 2.83\n",
      "\n",
      "Epoch time: 24.585982084274292\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 488, step 50 / 900, loss = 2.70 (0.022 sec/batch)\n",
      "epoch 488, step 100 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 488, step 150 / 900, loss = 2.26 (0.021 sec/batch)\n",
      "epoch 488, step 200 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 488, step 250 / 900, loss = 2.44 (0.021 sec/batch)\n",
      "epoch 488, step 300 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "epoch 488, step 350 / 900, loss = 1.72 (0.021 sec/batch)\n",
      "epoch 488, step 400 / 900, loss = 2.55 (0.021 sec/batch)\n",
      "epoch 488, step 450 / 900, loss = 2.05 (0.021 sec/batch)\n",
      "epoch 488, step 500 / 900, loss = 1.59 (0.021 sec/batch)\n",
      "epoch 488, step 550 / 900, loss = 2.56 (0.021 sec/batch)\n",
      "epoch 488, step 600 / 900, loss = 2.74 (0.022 sec/batch)\n",
      "epoch 488, step 650 / 900, loss = 1.84 (0.021 sec/batch)\n",
      "epoch 488, step 700 / 900, loss = 2.47 (0.021 sec/batch)\n",
      "epoch 488, step 750 / 900, loss = 2.41 (0.021 sec/batch)\n",
      "epoch 488, step 800 / 900, loss = 1.45 (0.021 sec/batch)\n",
      "epoch 488, step 850 / 900, loss = 3.24 (0.021 sec/batch)\n",
      "epoch 488, step 900 / 900, loss = 2.50 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.81\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.40\n",
      " avg loss = 2.84\n",
      "\n",
      "Epoch time: 24.58428931236267\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 489, step 50 / 900, loss = 1.65 (0.021 sec/batch)\n",
      "epoch 489, step 100 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "epoch 489, step 150 / 900, loss = 3.23 (0.021 sec/batch)\n",
      "epoch 489, step 200 / 900, loss = 2.14 (0.021 sec/batch)\n",
      "epoch 489, step 250 / 900, loss = 2.45 (0.022 sec/batch)\n",
      "epoch 489, step 300 / 900, loss = 2.51 (0.022 sec/batch)\n",
      "epoch 489, step 350 / 900, loss = 2.24 (0.021 sec/batch)\n",
      "epoch 489, step 400 / 900, loss = 1.80 (0.022 sec/batch)\n",
      "epoch 489, step 450 / 900, loss = 2.85 (0.021 sec/batch)\n",
      "epoch 489, step 500 / 900, loss = 3.02 (0.022 sec/batch)\n",
      "epoch 489, step 550 / 900, loss = 2.54 (0.021 sec/batch)\n",
      "epoch 489, step 600 / 900, loss = 2.37 (0.022 sec/batch)\n",
      "epoch 489, step 650 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "epoch 489, step 700 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 489, step 750 / 900, loss = 2.58 (0.021 sec/batch)\n",
      "epoch 489, step 800 / 900, loss = 2.02 (0.022 sec/batch)\n",
      "epoch 489, step 850 / 900, loss = 3.07 (0.021 sec/batch)\n",
      "epoch 489, step 900 / 900, loss = 2.70 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.79\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.72\n",
      " avg loss = 2.84\n",
      "\n",
      "Epoch time: 24.58978819847107\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 490, step 50 / 900, loss = 2.67 (0.022 sec/batch)\n",
      "epoch 490, step 100 / 900, loss = 3.23 (0.021 sec/batch)\n",
      "epoch 490, step 150 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 490, step 200 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 490, step 250 / 900, loss = 1.66 (0.022 sec/batch)\n",
      "epoch 490, step 300 / 900, loss = 2.51 (0.021 sec/batch)\n",
      "epoch 490, step 350 / 900, loss = 2.35 (0.022 sec/batch)\n",
      "epoch 490, step 400 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 490, step 450 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 490, step 500 / 900, loss = 2.31 (0.021 sec/batch)\n",
      "epoch 490, step 550 / 900, loss = 2.30 (0.021 sec/batch)\n",
      "epoch 490, step 600 / 900, loss = 2.47 (0.021 sec/batch)\n",
      "epoch 490, step 650 / 900, loss = 2.32 (0.022 sec/batch)\n",
      "epoch 490, step 700 / 900, loss = 2.46 (0.022 sec/batch)\n",
      "epoch 490, step 750 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 490, step 800 / 900, loss = 3.80 (0.021 sec/batch)\n",
      "epoch 490, step 850 / 900, loss = 2.20 (0.022 sec/batch)\n",
      "epoch 490, step 900 / 900, loss = 2.57 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.48\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.82\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.57335066795349\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 491, step 50 / 900, loss = 2.54 (0.022 sec/batch)\n",
      "epoch 491, step 100 / 900, loss = 1.70 (0.021 sec/batch)\n",
      "epoch 491, step 150 / 900, loss = 3.03 (0.021 sec/batch)\n",
      "epoch 491, step 200 / 900, loss = 1.93 (0.022 sec/batch)\n",
      "epoch 491, step 250 / 900, loss = 2.66 (0.022 sec/batch)\n",
      "epoch 491, step 300 / 900, loss = 3.08 (0.021 sec/batch)\n",
      "epoch 491, step 350 / 900, loss = 1.77 (0.021 sec/batch)\n",
      "epoch 491, step 400 / 900, loss = 1.91 (0.022 sec/batch)\n",
      "epoch 491, step 450 / 900, loss = 2.61 (0.021 sec/batch)\n",
      "epoch 491, step 500 / 900, loss = 1.66 (0.021 sec/batch)\n",
      "epoch 491, step 550 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "epoch 491, step 600 / 900, loss = 2.62 (0.021 sec/batch)\n",
      "epoch 491, step 650 / 900, loss = 2.23 (0.022 sec/batch)\n",
      "epoch 491, step 700 / 900, loss = 2.13 (0.022 sec/batch)\n",
      "epoch 491, step 750 / 900, loss = 3.03 (0.022 sec/batch)\n",
      "epoch 491, step 800 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 491, step 850 / 900, loss = 2.96 (0.021 sec/batch)\n",
      "epoch 491, step 900 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.48\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.42\n",
      " avg loss = 2.92\n",
      "\n",
      "Epoch time: 24.601828575134277\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 492, step 50 / 900, loss = 1.80 (0.022 sec/batch)\n",
      "epoch 492, step 100 / 900, loss = 2.60 (0.021 sec/batch)\n",
      "epoch 492, step 150 / 900, loss = 1.71 (0.021 sec/batch)\n",
      "epoch 492, step 200 / 900, loss = 1.89 (0.022 sec/batch)\n",
      "epoch 492, step 250 / 900, loss = 1.54 (0.022 sec/batch)\n",
      "epoch 492, step 300 / 900, loss = 2.09 (0.022 sec/batch)\n",
      "epoch 492, step 350 / 900, loss = 1.91 (0.021 sec/batch)\n",
      "epoch 492, step 400 / 900, loss = 1.74 (0.022 sec/batch)\n",
      "epoch 492, step 450 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 492, step 500 / 900, loss = 1.78 (0.021 sec/batch)\n",
      "epoch 492, step 550 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 492, step 600 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "epoch 492, step 650 / 900, loss = 2.46 (0.021 sec/batch)\n",
      "epoch 492, step 700 / 900, loss = 3.18 (0.022 sec/batch)\n",
      "epoch 492, step 750 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "epoch 492, step 800 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 492, step 850 / 900, loss = 2.38 (0.022 sec/batch)\n",
      "epoch 492, step 900 / 900, loss = 1.92 (0.023 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.45\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.56\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.59951877593994\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 493, step 50 / 900, loss = 2.17 (0.022 sec/batch)\n",
      "epoch 493, step 100 / 900, loss = 1.58 (0.022 sec/batch)\n",
      "epoch 493, step 150 / 900, loss = 1.95 (0.022 sec/batch)\n",
      "epoch 493, step 200 / 900, loss = 2.46 (0.021 sec/batch)\n",
      "epoch 493, step 250 / 900, loss = 2.28 (0.022 sec/batch)\n",
      "epoch 493, step 300 / 900, loss = 2.11 (0.022 sec/batch)\n",
      "epoch 493, step 350 / 900, loss = 2.45 (0.021 sec/batch)\n",
      "epoch 493, step 400 / 900, loss = 2.39 (0.021 sec/batch)\n",
      "epoch 493, step 450 / 900, loss = 2.12 (0.021 sec/batch)\n",
      "epoch 493, step 500 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 493, step 550 / 900, loss = 1.90 (0.021 sec/batch)\n",
      "epoch 493, step 600 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "epoch 493, step 650 / 900, loss = 1.89 (0.022 sec/batch)\n",
      "epoch 493, step 700 / 900, loss = 2.62 (0.022 sec/batch)\n",
      "epoch 493, step 750 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "epoch 493, step 800 / 900, loss = 2.48 (0.021 sec/batch)\n",
      "epoch 493, step 850 / 900, loss = 1.24 (0.021 sec/batch)\n",
      "epoch 493, step 900 / 900, loss = 2.23 (0.020 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.70\n",
      " avg loss = 2.24\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.68\n",
      " avg loss = 2.87\n",
      "\n",
      "Epoch time: 24.604379177093506\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 494, step 50 / 900, loss = 1.99 (0.022 sec/batch)\n",
      "epoch 494, step 100 / 900, loss = 2.37 (0.021 sec/batch)\n",
      "epoch 494, step 150 / 900, loss = 3.32 (0.021 sec/batch)\n",
      "epoch 494, step 200 / 900, loss = 1.99 (0.021 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 494, step 250 / 900, loss = 2.21 (0.022 sec/batch)\n",
      "epoch 494, step 300 / 900, loss = 2.54 (0.022 sec/batch)\n",
      "epoch 494, step 350 / 900, loss = 2.54 (0.021 sec/batch)\n",
      "epoch 494, step 400 / 900, loss = 2.07 (0.021 sec/batch)\n",
      "epoch 494, step 450 / 900, loss = 1.88 (0.020 sec/batch)\n",
      "epoch 494, step 500 / 900, loss = 1.70 (0.021 sec/batch)\n",
      "epoch 494, step 550 / 900, loss = 2.79 (0.021 sec/batch)\n",
      "epoch 494, step 600 / 900, loss = 3.10 (0.022 sec/batch)\n",
      "epoch 494, step 650 / 900, loss = 1.75 (0.021 sec/batch)\n",
      "epoch 494, step 700 / 900, loss = 1.78 (0.022 sec/batch)\n",
      "epoch 494, step 750 / 900, loss = 2.09 (0.022 sec/batch)\n",
      "epoch 494, step 800 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 494, step 850 / 900, loss = 1.90 (0.021 sec/batch)\n",
      "epoch 494, step 900 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.44\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.32\n",
      " avg loss = 2.93\n",
      "\n",
      "Epoch time: 24.576133251190186\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 495, step 50 / 900, loss = 1.77 (0.021 sec/batch)\n",
      "epoch 495, step 100 / 900, loss = 2.42 (0.021 sec/batch)\n",
      "epoch 495, step 150 / 900, loss = 3.00 (0.021 sec/batch)\n",
      "epoch 495, step 200 / 900, loss = 1.91 (0.022 sec/batch)\n",
      "epoch 495, step 250 / 900, loss = 3.06 (0.022 sec/batch)\n",
      "epoch 495, step 300 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 495, step 350 / 900, loss = 1.91 (0.022 sec/batch)\n",
      "epoch 495, step 400 / 900, loss = 1.90 (0.021 sec/batch)\n",
      "epoch 495, step 450 / 900, loss = 2.61 (0.022 sec/batch)\n",
      "epoch 495, step 500 / 900, loss = 2.59 (0.022 sec/batch)\n",
      "epoch 495, step 550 / 900, loss = 2.04 (0.021 sec/batch)\n",
      "epoch 495, step 600 / 900, loss = 3.18 (0.021 sec/batch)\n",
      "epoch 495, step 650 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 495, step 700 / 900, loss = 1.41 (0.021 sec/batch)\n",
      "epoch 495, step 750 / 900, loss = 1.72 (0.021 sec/batch)\n",
      "epoch 495, step 800 / 900, loss = 2.58 (0.021 sec/batch)\n",
      "epoch 495, step 850 / 900, loss = 1.73 (0.021 sec/batch)\n",
      "epoch 495, step 900 / 900, loss = 2.38 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.78\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.50\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.596262216567993\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 496, step 50 / 900, loss = 1.79 (0.021 sec/batch)\n",
      "epoch 496, step 100 / 900, loss = 2.20 (0.021 sec/batch)\n",
      "epoch 496, step 150 / 900, loss = 2.25 (0.022 sec/batch)\n",
      "epoch 496, step 200 / 900, loss = 2.60 (0.022 sec/batch)\n",
      "epoch 496, step 250 / 900, loss = 1.50 (0.021 sec/batch)\n",
      "epoch 496, step 300 / 900, loss = 2.31 (0.022 sec/batch)\n",
      "epoch 496, step 350 / 900, loss = 2.45 (0.021 sec/batch)\n",
      "epoch 496, step 400 / 900, loss = 1.87 (0.021 sec/batch)\n",
      "epoch 496, step 450 / 900, loss = 2.70 (0.021 sec/batch)\n",
      "epoch 496, step 500 / 900, loss = 2.72 (0.021 sec/batch)\n",
      "epoch 496, step 550 / 900, loss = 2.18 (0.021 sec/batch)\n",
      "epoch 496, step 600 / 900, loss = 1.42 (0.021 sec/batch)\n",
      "epoch 496, step 650 / 900, loss = 1.77 (0.021 sec/batch)\n",
      "epoch 496, step 700 / 900, loss = 2.10 (0.021 sec/batch)\n",
      "epoch 496, step 750 / 900, loss = 2.72 (0.022 sec/batch)\n",
      "epoch 496, step 800 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "epoch 496, step 850 / 900, loss = 2.14 (0.022 sec/batch)\n",
      "epoch 496, step 900 / 900, loss = 2.86 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.81\n",
      " avg loss = 2.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.36\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.57859468460083\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 497, step 50 / 900, loss = 1.85 (0.022 sec/batch)\n",
      "epoch 497, step 100 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 497, step 150 / 900, loss = 1.71 (0.021 sec/batch)\n",
      "epoch 497, step 200 / 900, loss = 3.20 (0.022 sec/batch)\n",
      "epoch 497, step 250 / 900, loss = 2.27 (0.021 sec/batch)\n",
      "epoch 497, step 300 / 900, loss = 3.38 (0.022 sec/batch)\n",
      "epoch 497, step 350 / 900, loss = 1.73 (0.021 sec/batch)\n",
      "epoch 497, step 400 / 900, loss = 2.96 (0.022 sec/batch)\n",
      "epoch 497, step 450 / 900, loss = 2.88 (0.021 sec/batch)\n",
      "epoch 497, step 500 / 900, loss = 2.23 (0.022 sec/batch)\n",
      "epoch 497, step 550 / 900, loss = 2.86 (0.021 sec/batch)\n",
      "epoch 497, step 600 / 900, loss = 2.51 (0.022 sec/batch)\n",
      "epoch 497, step 650 / 900, loss = 1.99 (0.021 sec/batch)\n",
      "epoch 497, step 700 / 900, loss = 2.53 (0.021 sec/batch)\n",
      "epoch 497, step 750 / 900, loss = 2.15 (0.022 sec/batch)\n",
      "epoch 497, step 800 / 900, loss = 1.98 (0.022 sec/batch)\n",
      "epoch 497, step 850 / 900, loss = 1.65 (0.022 sec/batch)\n",
      "epoch 497, step 900 / 900, loss = 2.49 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.80\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 47.96\n",
      " avg loss = 2.98\n",
      "\n",
      "Epoch time: 24.574999809265137\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 498, step 50 / 900, loss = 2.67 (0.022 sec/batch)\n",
      "epoch 498, step 100 / 900, loss = 3.34 (0.021 sec/batch)\n",
      "epoch 498, step 150 / 900, loss = 1.83 (0.021 sec/batch)\n",
      "epoch 498, step 200 / 900, loss = 2.79 (0.021 sec/batch)\n",
      "epoch 498, step 250 / 900, loss = 2.21 (0.021 sec/batch)\n",
      "epoch 498, step 300 / 900, loss = 2.26 (0.022 sec/batch)\n",
      "epoch 498, step 350 / 900, loss = 2.71 (0.021 sec/batch)\n",
      "epoch 498, step 400 / 900, loss = 1.56 (0.021 sec/batch)\n",
      "epoch 498, step 450 / 900, loss = 2.45 (0.022 sec/batch)\n",
      "epoch 498, step 500 / 900, loss = 2.03 (0.021 sec/batch)\n",
      "epoch 498, step 550 / 900, loss = 2.01 (0.021 sec/batch)\n",
      "epoch 498, step 600 / 900, loss = 2.34 (0.021 sec/batch)\n",
      "epoch 498, step 650 / 900, loss = 1.42 (0.021 sec/batch)\n",
      "epoch 498, step 700 / 900, loss = 2.10 (0.022 sec/batch)\n",
      "epoch 498, step 750 / 900, loss = 2.29 (0.021 sec/batch)\n",
      "epoch 498, step 800 / 900, loss = 2.11 (0.022 sec/batch)\n",
      "epoch 498, step 850 / 900, loss = 2.69 (0.022 sec/batch)\n",
      "epoch 498, step 900 / 900, loss = 2.88 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.70\n",
      " avg loss = 2.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.00\n",
      " avg loss = 2.91\n",
      "\n",
      "Epoch time: 24.573126554489136\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 499, step 50 / 900, loss = 2.16 (0.021 sec/batch)\n",
      "epoch 499, step 100 / 900, loss = 2.13 (0.021 sec/batch)\n",
      "epoch 499, step 150 / 900, loss = 2.54 (0.021 sec/batch)\n",
      "epoch 499, step 200 / 900, loss = 2.25 (0.021 sec/batch)\n",
      "epoch 499, step 250 / 900, loss = 2.24 (0.022 sec/batch)\n",
      "epoch 499, step 300 / 900, loss = 2.34 (0.021 sec/batch)\n",
      "epoch 499, step 350 / 900, loss = 2.19 (0.021 sec/batch)\n",
      "epoch 499, step 400 / 900, loss = 2.65 (0.021 sec/batch)\n",
      "epoch 499, step 450 / 900, loss = 2.33 (0.021 sec/batch)\n",
      "epoch 499, step 500 / 900, loss = 2.90 (0.022 sec/batch)\n",
      "epoch 499, step 550 / 900, loss = 1.31 (0.022 sec/batch)\n",
      "epoch 499, step 600 / 900, loss = 3.10 (0.023 sec/batch)\n",
      "epoch 499, step 650 / 900, loss = 2.39 (0.021 sec/batch)\n",
      "epoch 499, step 700 / 900, loss = 1.94 (0.021 sec/batch)\n",
      "epoch 499, step 750 / 900, loss = 2.51 (0.021 sec/batch)\n",
      "epoch 499, step 800 / 900, loss = 3.09 (0.021 sec/batch)\n",
      "epoch 499, step 850 / 900, loss = 2.45 (0.021 sec/batch)\n",
      "epoch 499, step 900 / 900, loss = 1.80 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.55\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.14\n",
      " avg loss = 2.89\n",
      "\n",
      "Epoch time: 24.5864999294281\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 500, step 50 / 900, loss = 1.49 (0.022 sec/batch)\n",
      "epoch 500, step 100 / 900, loss = 1.63 (0.021 sec/batch)\n",
      "epoch 500, step 150 / 900, loss = 2.40 (0.021 sec/batch)\n",
      "epoch 500, step 200 / 900, loss = 2.55 (0.021 sec/batch)\n",
      "epoch 500, step 250 / 900, loss = 2.72 (0.022 sec/batch)\n",
      "epoch 500, step 300 / 900, loss = 2.40 (0.021 sec/batch)\n",
      "epoch 500, step 350 / 900, loss = 2.55 (0.022 sec/batch)\n",
      "epoch 500, step 400 / 900, loss = 1.98 (0.021 sec/batch)\n",
      "epoch 500, step 450 / 900, loss = 2.26 (0.021 sec/batch)\n",
      "epoch 500, step 500 / 900, loss = 2.35 (0.021 sec/batch)\n",
      "epoch 500, step 550 / 900, loss = 2.28 (0.021 sec/batch)\n",
      "epoch 500, step 600 / 900, loss = 1.82 (0.022 sec/batch)\n",
      "epoch 500, step 650 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "epoch 500, step 700 / 900, loss = 1.52 (0.021 sec/batch)\n",
      "epoch 500, step 750 / 900, loss = 1.82 (0.022 sec/batch)\n",
      "epoch 500, step 800 / 900, loss = 2.44 (0.021 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 500, step 850 / 900, loss = 1.49 (0.021 sec/batch)\n",
      "epoch 500, step 900 / 900, loss = 2.52 (0.021 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.38\n",
      " avg loss = 2.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.50\n",
      " avg loss = 2.90\n",
      "\n",
      "Epoch time: 24.56404137611389\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "Total train time: 12500.25497174263\n"
     ]
    }
   ],
   "source": [
    "session = tf.Session()\n",
    "train_start = time.time()\n",
    "\n",
    "plot_data={}\n",
    "train_cifar(session, train_x, train_y, valid_x, valid_y, config, plot_data)\n",
    "\n",
    "print(\"Total train time:\", time.time() - train_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7YAAAHiCAYAAADcelBQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n45wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27d0b3f91d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_training_progress(plot_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Graph\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "Y_ = tf.placeholder(tf.int32, [None,])\n",
    "logits = model_alexnet(X, variables=variables_alexnet())\n",
    "\n",
    "# loss\n",
    "regularizers = l2_loss(weights.values())\n",
    "loss_per_sample = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=Y_)\n",
    "data_loss = tf.reduce_mean(loss_per_sample)\n",
    "loss = data_loss + weight_decay*regularizers\n",
    "\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(0.01, global_step, 900, 0.9, staircase=True)\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, step 50 / 900, loss = 9.02 (0.018 sec/batch)\n",
      "epoch 1, step 100 / 900, loss = 8.65 (0.018 sec/batch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Korisnik\\Anaconda3\\lib\\site-packages\\skimage\\util\\dtype.py:122: UserWarning: Possible precision loss when converting from float64 to uint8\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, step 150 / 900, loss = 10.83 (0.018 sec/batch)\n",
      "epoch 1, step 200 / 900, loss = 10.90 (0.019 sec/batch)\n",
      "epoch 1, step 250 / 900, loss = 9.35 (0.018 sec/batch)\n",
      "epoch 1, step 300 / 900, loss = 10.28 (0.018 sec/batch)\n",
      "epoch 1, step 350 / 900, loss = 7.76 (0.018 sec/batch)\n",
      "epoch 1, step 400 / 900, loss = 7.61 (0.019 sec/batch)\n",
      "epoch 1, step 450 / 900, loss = 9.59 (0.019 sec/batch)\n",
      "epoch 1, step 500 / 900, loss = 7.80 (0.019 sec/batch)\n",
      "epoch 1, step 550 / 900, loss = 9.22 (0.019 sec/batch)\n",
      "epoch 1, step 600 / 900, loss = 9.46 (0.020 sec/batch)\n",
      "epoch 1, step 650 / 900, loss = 9.29 (0.019 sec/batch)\n",
      "epoch 1, step 700 / 900, loss = 7.56 (0.018 sec/batch)\n",
      "epoch 1, step 750 / 900, loss = 6.76 (0.018 sec/batch)\n",
      "epoch 1, step 800 / 900, loss = 6.68 (0.018 sec/batch)\n",
      "epoch 1, step 850 / 900, loss = 6.00 (0.018 sec/batch)\n",
      "epoch 1, step 900 / 900, loss = 7.29 (0.019 sec/batch)\n",
      "Train error:\n",
      " accuracy = 16.89\n",
      " avg loss = 6.82\n",
      "\n",
      "Validation error:\n",
      " accuracy = 17.28\n",
      " avg loss = 6.89\n",
      "\n",
      "Epoch time: 22.330379486083984\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 2, step 50 / 900, loss = 8.46 (0.019 sec/batch)\n",
      "epoch 2, step 100 / 900, loss = 7.31 (0.018 sec/batch)\n",
      "epoch 2, step 150 / 900, loss = 6.42 (0.019 sec/batch)\n",
      "epoch 2, step 200 / 900, loss = 5.47 (0.021 sec/batch)\n",
      "epoch 2, step 250 / 900, loss = 5.69 (0.019 sec/batch)\n",
      "epoch 2, step 300 / 900, loss = 5.76 (0.021 sec/batch)\n",
      "epoch 2, step 350 / 900, loss = 4.97 (0.021 sec/batch)\n",
      "epoch 2, step 400 / 900, loss = 5.77 (0.021 sec/batch)\n",
      "epoch 2, step 450 / 900, loss = 5.70 (0.019 sec/batch)\n",
      "epoch 2, step 500 / 900, loss = 5.80 (0.020 sec/batch)\n",
      "epoch 2, step 550 / 900, loss = 4.38 (0.019 sec/batch)\n",
      "epoch 2, step 600 / 900, loss = 4.67 (0.021 sec/batch)\n",
      "epoch 2, step 650 / 900, loss = 5.80 (0.019 sec/batch)\n",
      "epoch 2, step 700 / 900, loss = 4.89 (0.020 sec/batch)\n",
      "epoch 2, step 750 / 900, loss = 5.23 (0.019 sec/batch)\n",
      "epoch 2, step 800 / 900, loss = 5.86 (0.021 sec/batch)\n",
      "epoch 2, step 850 / 900, loss = 5.19 (0.021 sec/batch)\n",
      "epoch 2, step 900 / 900, loss = 5.00 (0.019 sec/batch)\n",
      "Train error:\n",
      " accuracy = 21.43\n",
      " avg loss = 4.94\n",
      "\n",
      "Validation error:\n",
      " accuracy = 21.04\n",
      " avg loss = 4.96\n",
      "\n",
      "Epoch time: 23.342925786972046\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 3, step 50 / 900, loss = 5.15 (0.021 sec/batch)\n",
      "epoch 3, step 100 / 900, loss = 5.18 (0.019 sec/batch)\n",
      "epoch 3, step 150 / 900, loss = 4.76 (0.021 sec/batch)\n",
      "epoch 3, step 200 / 900, loss = 4.33 (0.019 sec/batch)\n",
      "epoch 3, step 250 / 900, loss = 4.44 (0.021 sec/batch)\n",
      "epoch 3, step 300 / 900, loss = 3.71 (0.019 sec/batch)\n",
      "epoch 3, step 350 / 900, loss = 4.16 (0.021 sec/batch)\n",
      "epoch 3, step 400 / 900, loss = 4.10 (0.019 sec/batch)\n",
      "epoch 3, step 450 / 900, loss = 3.97 (0.019 sec/batch)\n",
      "epoch 3, step 500 / 900, loss = 3.81 (0.019 sec/batch)\n",
      "epoch 3, step 550 / 900, loss = 3.87 (0.019 sec/batch)\n",
      "epoch 3, step 600 / 900, loss = 4.44 (0.022 sec/batch)\n",
      "epoch 3, step 650 / 900, loss = 4.14 (0.019 sec/batch)\n",
      "epoch 3, step 700 / 900, loss = 4.45 (0.021 sec/batch)\n",
      "epoch 3, step 750 / 900, loss = 4.05 (0.019 sec/batch)\n",
      "epoch 3, step 800 / 900, loss = 3.58 (0.019 sec/batch)\n",
      "epoch 3, step 850 / 900, loss = 3.23 (0.021 sec/batch)\n",
      "epoch 3, step 900 / 900, loss = 3.38 (0.019 sec/batch)\n",
      "Train error:\n",
      " accuracy = 26.12\n",
      " avg loss = 3.58\n",
      "\n",
      "Validation error:\n",
      " accuracy = 26.74\n",
      " avg loss = 3.60\n",
      "\n",
      "Epoch time: 23.398608446121216\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 4, step 50 / 900, loss = 3.31 (0.019 sec/batch)\n",
      "epoch 4, step 100 / 900, loss = 3.02 (0.019 sec/batch)\n",
      "epoch 4, step 150 / 900, loss = 4.00 (0.021 sec/batch)\n",
      "epoch 4, step 200 / 900, loss = 2.58 (0.020 sec/batch)\n",
      "epoch 4, step 250 / 900, loss = 3.69 (0.019 sec/batch)\n",
      "epoch 4, step 300 / 900, loss = 3.08 (0.019 sec/batch)\n",
      "epoch 4, step 350 / 900, loss = 3.73 (0.019 sec/batch)\n",
      "epoch 4, step 400 / 900, loss = 2.98 (0.018 sec/batch)\n",
      "epoch 4, step 450 / 900, loss = 3.57 (0.019 sec/batch)\n",
      "epoch 4, step 500 / 900, loss = 3.31 (0.019 sec/batch)\n",
      "epoch 4, step 550 / 900, loss = 4.18 (0.018 sec/batch)\n",
      "epoch 4, step 600 / 900, loss = 2.76 (0.019 sec/batch)\n",
      "epoch 4, step 650 / 900, loss = 3.14 (0.018 sec/batch)\n",
      "epoch 4, step 700 / 900, loss = 3.11 (0.018 sec/batch)\n",
      "epoch 4, step 750 / 900, loss = 2.76 (0.019 sec/batch)\n",
      "epoch 4, step 800 / 900, loss = 3.25 (0.018 sec/batch)\n",
      "epoch 4, step 850 / 900, loss = 3.57 (0.019 sec/batch)\n",
      "epoch 4, step 900 / 900, loss = 3.11 (0.019 sec/batch)\n",
      "Train error:\n",
      " accuracy = 26.57\n",
      " avg loss = 3.18\n",
      "\n",
      "Validation error:\n",
      " accuracy = 26.82\n",
      " avg loss = 3.19\n",
      "\n",
      "Epoch time: 22.548630714416504\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 5, step 50 / 900, loss = 2.73 (0.019 sec/batch)\n",
      "epoch 5, step 100 / 900, loss = 2.48 (0.019 sec/batch)\n",
      "epoch 5, step 150 / 900, loss = 2.79 (0.020 sec/batch)\n",
      "epoch 5, step 200 / 900, loss = 2.70 (0.019 sec/batch)\n",
      "epoch 5, step 250 / 900, loss = 3.03 (0.019 sec/batch)\n",
      "epoch 5, step 300 / 900, loss = 2.64 (0.019 sec/batch)\n",
      "epoch 5, step 350 / 900, loss = 2.66 (0.020 sec/batch)\n",
      "epoch 5, step 400 / 900, loss = 2.42 (0.019 sec/batch)\n",
      "epoch 5, step 450 / 900, loss = 2.39 (0.018 sec/batch)\n",
      "epoch 5, step 500 / 900, loss = 3.02 (0.019 sec/batch)\n",
      "epoch 5, step 550 / 900, loss = 2.55 (0.019 sec/batch)\n",
      "epoch 5, step 600 / 900, loss = 2.14 (0.018 sec/batch)\n",
      "epoch 5, step 650 / 900, loss = 2.28 (0.019 sec/batch)\n",
      "epoch 5, step 700 / 900, loss = 2.00 (0.019 sec/batch)\n",
      "epoch 5, step 750 / 900, loss = 2.97 (0.019 sec/batch)\n",
      "epoch 5, step 800 / 900, loss = 2.59 (0.019 sec/batch)\n",
      "epoch 5, step 850 / 900, loss = 2.51 (0.018 sec/batch)\n",
      "epoch 5, step 900 / 900, loss = 2.07 (0.019 sec/batch)\n",
      "Train error:\n",
      " accuracy = 34.10\n",
      " avg loss = 2.34\n",
      "\n",
      "Validation error:\n",
      " accuracy = 33.98\n",
      " avg loss = 2.36\n",
      "\n",
      "Epoch time: 22.25391411781311\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 6, step 50 / 900, loss = 2.58 (0.019 sec/batch)\n",
      "epoch 6, step 100 / 900, loss = 2.64 (0.019 sec/batch)\n",
      "epoch 6, step 150 / 900, loss = 2.13 (0.018 sec/batch)\n",
      "epoch 6, step 200 / 900, loss = 2.16 (0.019 sec/batch)\n",
      "epoch 6, step 250 / 900, loss = 2.32 (0.018 sec/batch)\n",
      "epoch 6, step 300 / 900, loss = 2.05 (0.019 sec/batch)\n",
      "epoch 6, step 350 / 900, loss = 2.59 (0.019 sec/batch)\n",
      "epoch 6, step 400 / 900, loss = 2.20 (0.018 sec/batch)\n",
      "epoch 6, step 450 / 900, loss = 2.76 (0.019 sec/batch)\n",
      "epoch 6, step 500 / 900, loss = 2.25 (0.019 sec/batch)\n",
      "epoch 6, step 550 / 900, loss = 2.81 (0.019 sec/batch)\n",
      "epoch 6, step 600 / 900, loss = 2.16 (0.019 sec/batch)\n",
      "epoch 6, step 650 / 900, loss = 2.10 (0.018 sec/batch)\n",
      "epoch 6, step 700 / 900, loss = 2.56 (0.019 sec/batch)\n",
      "epoch 6, step 750 / 900, loss = 2.16 (0.018 sec/batch)\n",
      "epoch 6, step 800 / 900, loss = 1.93 (0.019 sec/batch)\n",
      "epoch 6, step 850 / 900, loss = 1.80 (0.018 sec/batch)\n",
      "epoch 6, step 900 / 900, loss = 2.18 (0.019 sec/batch)\n",
      "Train error:\n",
      " accuracy = 36.26\n",
      " avg loss = 2.15\n",
      "\n",
      "Validation error:\n",
      " accuracy = 36.52\n",
      " avg loss = 2.17\n",
      "\n",
      "Epoch time: 22.4801983833313\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 7, step 50 / 900, loss = 1.91 (0.019 sec/batch)\n",
      "epoch 7, step 100 / 900, loss = 1.96 (0.018 sec/batch)\n",
      "epoch 7, step 150 / 900, loss = 1.79 (0.019 sec/batch)\n",
      "epoch 7, step 200 / 900, loss = 1.99 (0.019 sec/batch)\n",
      "epoch 7, step 250 / 900, loss = 1.57 (0.018 sec/batch)\n",
      "epoch 7, step 300 / 900, loss = 2.00 (0.019 sec/batch)\n",
      "epoch 7, step 350 / 900, loss = 2.12 (0.018 sec/batch)\n",
      "epoch 7, step 400 / 900, loss = 2.30 (0.018 sec/batch)\n",
      "epoch 7, step 450 / 900, loss = 1.92 (0.018 sec/batch)\n",
      "epoch 7, step 500 / 900, loss = 2.08 (0.018 sec/batch)\n",
      "epoch 7, step 550 / 900, loss = 2.06 (0.018 sec/batch)\n",
      "epoch 7, step 600 / 900, loss = 1.51 (0.018 sec/batch)\n",
      "epoch 7, step 650 / 900, loss = 2.33 (0.018 sec/batch)\n",
      "epoch 7, step 700 / 900, loss = 1.90 (0.018 sec/batch)\n",
      "epoch 7, step 750 / 900, loss = 1.77 (0.019 sec/batch)\n",
      "epoch 7, step 800 / 900, loss = 1.87 (0.019 sec/batch)\n",
      "epoch 7, step 850 / 900, loss = 2.28 (0.019 sec/batch)\n",
      "epoch 7, step 900 / 900, loss = 2.24 (0.018 sec/batch)\n",
      "Train error:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy = 38.18\n",
      " avg loss = 1.92\n",
      "\n",
      "Validation error:\n",
      " accuracy = 37.42\n",
      " avg loss = 1.97\n",
      "\n",
      "Epoch time: 22.035736322402954\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 8, step 50 / 900, loss = 1.72 (0.018 sec/batch)\n",
      "epoch 8, step 100 / 900, loss = 1.75 (0.018 sec/batch)\n",
      "epoch 8, step 150 / 900, loss = 1.86 (0.018 sec/batch)\n",
      "epoch 8, step 200 / 900, loss = 1.84 (0.018 sec/batch)\n",
      "epoch 8, step 250 / 900, loss = 1.82 (0.018 sec/batch)\n",
      "epoch 8, step 300 / 900, loss = 2.00 (0.018 sec/batch)\n",
      "epoch 8, step 350 / 900, loss = 1.95 (0.018 sec/batch)\n",
      "epoch 8, step 400 / 900, loss = 1.83 (0.018 sec/batch)\n",
      "epoch 8, step 450 / 900, loss = 1.68 (0.019 sec/batch)\n",
      "epoch 8, step 500 / 900, loss = 1.72 (0.018 sec/batch)\n",
      "epoch 8, step 550 / 900, loss = 1.82 (0.019 sec/batch)\n",
      "epoch 8, step 600 / 900, loss = 1.45 (0.018 sec/batch)\n",
      "epoch 8, step 650 / 900, loss = 1.80 (0.018 sec/batch)\n",
      "epoch 8, step 700 / 900, loss = 1.69 (0.018 sec/batch)\n",
      "epoch 8, step 750 / 900, loss = 1.81 (0.018 sec/batch)\n",
      "epoch 8, step 800 / 900, loss = 1.92 (0.018 sec/batch)\n",
      "epoch 8, step 850 / 900, loss = 1.67 (0.018 sec/batch)\n",
      "epoch 8, step 900 / 900, loss = 1.81 (0.019 sec/batch)\n",
      "Train error:\n",
      " accuracy = 40.39\n",
      " avg loss = 1.82\n",
      "\n",
      "Validation error:\n",
      " accuracy = 39.56\n",
      " avg loss = 1.87\n",
      "\n",
      "Epoch time: 22.13152027130127\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 9, step 50 / 900, loss = 1.57 (0.018 sec/batch)\n",
      "epoch 9, step 100 / 900, loss = 1.66 (0.018 sec/batch)\n",
      "epoch 9, step 150 / 900, loss = 1.92 (0.019 sec/batch)\n",
      "epoch 9, step 200 / 900, loss = 1.75 (0.018 sec/batch)\n",
      "epoch 9, step 250 / 900, loss = 1.84 (0.018 sec/batch)\n",
      "epoch 9, step 300 / 900, loss = 1.99 (0.018 sec/batch)\n",
      "epoch 9, step 350 / 900, loss = 2.07 (0.020 sec/batch)\n",
      "epoch 9, step 400 / 900, loss = 1.88 (0.018 sec/batch)\n",
      "epoch 9, step 450 / 900, loss = 1.51 (0.018 sec/batch)\n",
      "epoch 9, step 500 / 900, loss = 2.04 (0.018 sec/batch)\n",
      "epoch 9, step 550 / 900, loss = 1.72 (0.018 sec/batch)\n",
      "epoch 9, step 600 / 900, loss = 1.86 (0.018 sec/batch)\n",
      "epoch 9, step 650 / 900, loss = 1.49 (0.018 sec/batch)\n",
      "epoch 9, step 700 / 900, loss = 1.63 (0.018 sec/batch)\n",
      "epoch 9, step 750 / 900, loss = 1.56 (0.018 sec/batch)\n",
      "epoch 9, step 800 / 900, loss = 1.49 (0.018 sec/batch)\n",
      "epoch 9, step 850 / 900, loss = 1.69 (0.018 sec/batch)\n",
      "epoch 9, step 900 / 900, loss = 2.13 (0.019 sec/batch)\n",
      "Train error:\n",
      " accuracy = 43.28\n",
      " avg loss = 1.70\n",
      "\n",
      "Validation error:\n",
      " accuracy = 42.78\n",
      " avg loss = 1.74\n",
      "\n",
      "Epoch time: 22.007999420166016\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 10, step 50 / 900, loss = 1.83 (0.018 sec/batch)\n",
      "epoch 10, step 100 / 900, loss = 1.47 (0.018 sec/batch)\n",
      "epoch 10, step 150 / 900, loss = 1.72 (0.018 sec/batch)\n",
      "epoch 10, step 200 / 900, loss = 1.49 (0.018 sec/batch)\n",
      "epoch 10, step 250 / 900, loss = 1.82 (0.018 sec/batch)\n",
      "epoch 10, step 300 / 900, loss = 2.08 (0.019 sec/batch)\n",
      "epoch 10, step 350 / 900, loss = 1.46 (0.018 sec/batch)\n",
      "epoch 10, step 400 / 900, loss = 1.90 (0.018 sec/batch)\n",
      "epoch 10, step 450 / 900, loss = 1.61 (0.019 sec/batch)\n",
      "epoch 10, step 500 / 900, loss = 1.61 (0.019 sec/batch)\n",
      "epoch 10, step 550 / 900, loss = 1.78 (0.018 sec/batch)\n",
      "epoch 10, step 600 / 900, loss = 1.75 (0.018 sec/batch)\n",
      "epoch 10, step 650 / 900, loss = 1.76 (0.018 sec/batch)\n",
      "epoch 10, step 700 / 900, loss = 1.58 (0.019 sec/batch)\n",
      "epoch 10, step 750 / 900, loss = 1.64 (0.018 sec/batch)\n",
      "epoch 10, step 800 / 900, loss = 1.55 (0.018 sec/batch)\n",
      "epoch 10, step 850 / 900, loss = 1.75 (0.019 sec/batch)\n",
      "epoch 10, step 900 / 900, loss = 2.09 (0.019 sec/batch)\n",
      "Train error:\n",
      " accuracy = 44.89\n",
      " avg loss = 1.60\n",
      "\n",
      "Validation error:\n",
      " accuracy = 43.92\n",
      " avg loss = 1.65\n",
      "\n",
      "Epoch time: 21.96559166908264\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 11, step 50 / 900, loss = 1.54 (0.019 sec/batch)\n",
      "epoch 11, step 100 / 900, loss = 1.36 (0.018 sec/batch)\n",
      "epoch 11, step 150 / 900, loss = 1.53 (0.018 sec/batch)\n",
      "epoch 11, step 200 / 900, loss = 1.67 (0.018 sec/batch)\n",
      "epoch 11, step 250 / 900, loss = 1.73 (0.019 sec/batch)\n",
      "epoch 11, step 300 / 900, loss = 1.84 (0.019 sec/batch)\n",
      "epoch 11, step 350 / 900, loss = 1.39 (0.018 sec/batch)\n",
      "epoch 11, step 400 / 900, loss = 1.46 (0.018 sec/batch)\n",
      "epoch 11, step 450 / 900, loss = 1.52 (0.020 sec/batch)\n",
      "epoch 11, step 500 / 900, loss = 1.44 (0.019 sec/batch)\n",
      "epoch 11, step 550 / 900, loss = 1.53 (0.018 sec/batch)\n",
      "epoch 11, step 600 / 900, loss = 1.42 (0.019 sec/batch)\n",
      "epoch 11, step 650 / 900, loss = 2.05 (0.019 sec/batch)\n",
      "epoch 11, step 700 / 900, loss = 1.71 (0.018 sec/batch)\n",
      "epoch 11, step 750 / 900, loss = 1.67 (0.018 sec/batch)\n",
      "epoch 11, step 800 / 900, loss = 1.64 (0.018 sec/batch)\n",
      "epoch 11, step 850 / 900, loss = 1.34 (0.018 sec/batch)\n",
      "epoch 11, step 900 / 900, loss = 1.61 (0.018 sec/batch)\n",
      "Train error:\n",
      " accuracy = 45.63\n",
      " avg loss = 1.57\n",
      "\n",
      "Validation error:\n",
      " accuracy = 44.60\n",
      " avg loss = 1.62\n",
      "\n",
      "Epoch time: 21.984362602233887\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 12, step 50 / 900, loss = 1.70 (0.018 sec/batch)\n",
      "epoch 12, step 100 / 900, loss = 1.50 (0.018 sec/batch)\n",
      "epoch 12, step 150 / 900, loss = 1.51 (0.018 sec/batch)\n",
      "epoch 12, step 200 / 900, loss = 1.48 (0.018 sec/batch)\n",
      "epoch 12, step 250 / 900, loss = 1.21 (0.018 sec/batch)\n",
      "epoch 12, step 300 / 900, loss = 1.73 (0.018 sec/batch)\n",
      "epoch 12, step 350 / 900, loss = 1.69 (0.018 sec/batch)\n",
      "epoch 12, step 400 / 900, loss = 1.38 (0.018 sec/batch)\n",
      "epoch 12, step 450 / 900, loss = 1.64 (0.019 sec/batch)\n",
      "epoch 12, step 500 / 900, loss = 1.22 (0.019 sec/batch)\n",
      "epoch 12, step 550 / 900, loss = 1.63 (0.018 sec/batch)\n",
      "epoch 12, step 600 / 900, loss = 1.29 (0.018 sec/batch)\n",
      "epoch 12, step 650 / 900, loss = 1.63 (0.018 sec/batch)\n",
      "epoch 12, step 700 / 900, loss = 1.51 (0.018 sec/batch)\n",
      "epoch 12, step 750 / 900, loss = 1.52 (0.018 sec/batch)\n",
      "epoch 12, step 800 / 900, loss = 1.66 (0.018 sec/batch)\n",
      "epoch 12, step 850 / 900, loss = 1.79 (0.019 sec/batch)\n",
      "epoch 12, step 900 / 900, loss = 1.41 (0.018 sec/batch)\n",
      "Train error:\n",
      " accuracy = 47.83\n",
      " avg loss = 1.49\n",
      "\n",
      "Validation error:\n",
      " accuracy = 45.78\n",
      " avg loss = 1.56\n",
      "\n",
      "Epoch time: 22.019826650619507\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 13, step 50 / 900, loss = 1.75 (0.018 sec/batch)\n",
      "epoch 13, step 100 / 900, loss = 1.33 (0.018 sec/batch)\n",
      "epoch 13, step 150 / 900, loss = 1.46 (0.018 sec/batch)\n",
      "epoch 13, step 200 / 900, loss = 1.31 (0.018 sec/batch)\n",
      "epoch 13, step 250 / 900, loss = 1.61 (0.018 sec/batch)\n",
      "epoch 13, step 300 / 900, loss = 2.05 (0.019 sec/batch)\n",
      "epoch 13, step 350 / 900, loss = 1.20 (0.019 sec/batch)\n",
      "epoch 13, step 400 / 900, loss = 1.14 (0.019 sec/batch)\n",
      "epoch 13, step 450 / 900, loss = 1.71 (0.018 sec/batch)\n",
      "epoch 13, step 500 / 900, loss = 1.60 (0.019 sec/batch)\n",
      "epoch 13, step 550 / 900, loss = 1.64 (0.019 sec/batch)\n",
      "epoch 13, step 600 / 900, loss = 1.55 (0.018 sec/batch)\n",
      "epoch 13, step 650 / 900, loss = 1.83 (0.018 sec/batch)\n",
      "epoch 13, step 700 / 900, loss = 1.42 (0.018 sec/batch)\n",
      "epoch 13, step 750 / 900, loss = 1.76 (0.018 sec/batch)\n",
      "epoch 13, step 800 / 900, loss = 1.71 (0.019 sec/batch)\n",
      "epoch 13, step 850 / 900, loss = 1.22 (0.018 sec/batch)\n",
      "epoch 13, step 900 / 900, loss = 1.41 (0.018 sec/batch)\n",
      "Train error:\n",
      " accuracy = 48.24\n",
      " avg loss = 1.48\n",
      "\n",
      "Validation error:\n",
      " accuracy = 46.60\n",
      " avg loss = 1.53\n",
      "\n",
      "Epoch time: 21.994924068450928\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 14, step 50 / 900, loss = 1.53 (0.019 sec/batch)\n",
      "epoch 14, step 100 / 900, loss = 1.48 (0.018 sec/batch)\n",
      "epoch 14, step 150 / 900, loss = 1.50 (0.018 sec/batch)\n",
      "epoch 14, step 200 / 900, loss = 1.24 (0.018 sec/batch)\n",
      "epoch 14, step 250 / 900, loss = 1.42 (0.018 sec/batch)\n",
      "epoch 14, step 300 / 900, loss = 1.41 (0.018 sec/batch)\n",
      "epoch 14, step 350 / 900, loss = 1.50 (0.018 sec/batch)\n",
      "epoch 14, step 400 / 900, loss = 1.22 (0.018 sec/batch)\n",
      "epoch 14, step 450 / 900, loss = 1.31 (0.018 sec/batch)\n",
      "epoch 14, step 500 / 900, loss = 1.66 (0.018 sec/batch)\n",
      "epoch 14, step 550 / 900, loss = 1.35 (0.018 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14, step 600 / 900, loss = 1.60 (0.018 sec/batch)\n",
      "epoch 14, step 650 / 900, loss = 1.64 (0.018 sec/batch)\n",
      "epoch 14, step 700 / 900, loss = 1.86 (0.018 sec/batch)\n",
      "epoch 14, step 750 / 900, loss = 1.62 (0.018 sec/batch)\n",
      "epoch 14, step 800 / 900, loss = 1.68 (0.018 sec/batch)\n",
      "epoch 14, step 850 / 900, loss = 1.83 (0.018 sec/batch)\n",
      "epoch 14, step 900 / 900, loss = 1.39 (0.018 sec/batch)\n",
      "Train error:\n",
      " accuracy = 49.78\n",
      " avg loss = 1.43\n",
      "\n",
      "Validation error:\n",
      " accuracy = 47.62\n",
      " avg loss = 1.51\n",
      "\n",
      "Epoch time: 21.98235273361206\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 15, step 50 / 900, loss = 1.30 (0.019 sec/batch)\n",
      "epoch 15, step 100 / 900, loss = 1.26 (0.018 sec/batch)\n",
      "epoch 15, step 150 / 900, loss = 1.41 (0.018 sec/batch)\n",
      "epoch 15, step 200 / 900, loss = 1.14 (0.019 sec/batch)\n",
      "epoch 15, step 250 / 900, loss = 1.47 (0.018 sec/batch)\n",
      "epoch 15, step 300 / 900, loss = 1.36 (0.019 sec/batch)\n",
      "epoch 15, step 350 / 900, loss = 1.39 (0.019 sec/batch)\n",
      "epoch 15, step 400 / 900, loss = 1.14 (0.018 sec/batch)\n",
      "epoch 15, step 450 / 900, loss = 1.63 (0.019 sec/batch)\n",
      "epoch 15, step 500 / 900, loss = 1.49 (0.018 sec/batch)\n",
      "epoch 15, step 550 / 900, loss = 1.47 (0.018 sec/batch)\n",
      "epoch 15, step 600 / 900, loss = 1.43 (0.018 sec/batch)\n",
      "epoch 15, step 650 / 900, loss = 1.64 (0.019 sec/batch)\n",
      "epoch 15, step 700 / 900, loss = 1.09 (0.018 sec/batch)\n",
      "epoch 15, step 750 / 900, loss = 1.44 (0.018 sec/batch)\n",
      "epoch 15, step 800 / 900, loss = 1.28 (0.019 sec/batch)\n",
      "epoch 15, step 850 / 900, loss = 1.39 (0.018 sec/batch)\n",
      "epoch 15, step 900 / 900, loss = 1.21 (0.018 sec/batch)\n",
      "Train error:\n",
      " accuracy = 49.99\n",
      " avg loss = 1.42\n",
      "\n",
      "Validation error:\n",
      " accuracy = 47.64\n",
      " avg loss = 1.50\n",
      "\n",
      "Epoch time: 21.959476470947266\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 16, step 50 / 900, loss = 1.55 (0.018 sec/batch)\n",
      "epoch 16, step 100 / 900, loss = 1.46 (0.019 sec/batch)\n",
      "epoch 16, step 150 / 900, loss = 1.40 (0.019 sec/batch)\n",
      "epoch 16, step 200 / 900, loss = 1.21 (0.018 sec/batch)\n",
      "epoch 16, step 250 / 900, loss = 1.51 (0.018 sec/batch)\n",
      "epoch 16, step 300 / 900, loss = 1.55 (0.019 sec/batch)\n",
      "epoch 16, step 350 / 900, loss = 1.34 (0.019 sec/batch)\n",
      "epoch 16, step 400 / 900, loss = 1.46 (0.018 sec/batch)\n",
      "epoch 16, step 450 / 900, loss = 1.30 (0.019 sec/batch)\n",
      "epoch 16, step 500 / 900, loss = 1.52 (0.018 sec/batch)\n",
      "epoch 16, step 550 / 900, loss = 1.40 (0.019 sec/batch)\n",
      "epoch 16, step 600 / 900, loss = 1.14 (0.018 sec/batch)\n",
      "epoch 16, step 650 / 900, loss = 1.47 (0.018 sec/batch)\n",
      "epoch 16, step 700 / 900, loss = 1.27 (0.019 sec/batch)\n",
      "epoch 16, step 750 / 900, loss = 1.36 (0.020 sec/batch)\n",
      "epoch 16, step 800 / 900, loss = 1.43 (0.018 sec/batch)\n",
      "epoch 16, step 850 / 900, loss = 1.30 (0.019 sec/batch)\n",
      "epoch 16, step 900 / 900, loss = 1.29 (0.019 sec/batch)\n",
      "Train error:\n",
      " accuracy = 50.38\n",
      " avg loss = 1.41\n",
      "\n",
      "Validation error:\n",
      " accuracy = 47.90\n",
      " avg loss = 1.50\n",
      "\n",
      "Epoch time: 22.174726009368896\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 17, step 50 / 900, loss = 1.11 (0.019 sec/batch)\n",
      "epoch 17, step 100 / 900, loss = 1.22 (0.018 sec/batch)\n",
      "epoch 17, step 150 / 900, loss = 1.30 (0.018 sec/batch)\n",
      "epoch 17, step 200 / 900, loss = 1.56 (0.019 sec/batch)\n",
      "epoch 17, step 250 / 900, loss = 1.13 (0.018 sec/batch)\n",
      "epoch 17, step 300 / 900, loss = 1.10 (0.019 sec/batch)\n",
      "epoch 17, step 350 / 900, loss = 1.27 (0.019 sec/batch)\n",
      "epoch 17, step 400 / 900, loss = 1.44 (0.019 sec/batch)\n",
      "epoch 17, step 450 / 900, loss = 1.54 (0.018 sec/batch)\n",
      "epoch 17, step 500 / 900, loss = 1.14 (0.019 sec/batch)\n",
      "epoch 17, step 550 / 900, loss = 1.48 (0.018 sec/batch)\n",
      "epoch 17, step 600 / 900, loss = 1.13 (0.018 sec/batch)\n",
      "epoch 17, step 650 / 900, loss = 1.41 (0.018 sec/batch)\n",
      "epoch 17, step 700 / 900, loss = 1.57 (0.019 sec/batch)\n",
      "epoch 17, step 750 / 900, loss = 1.28 (0.018 sec/batch)\n",
      "epoch 17, step 800 / 900, loss = 1.53 (0.018 sec/batch)\n",
      "epoch 17, step 850 / 900, loss = 1.59 (0.018 sec/batch)\n",
      "epoch 17, step 900 / 900, loss = 1.25 (0.018 sec/batch)\n",
      "Train error:\n",
      " accuracy = 51.66\n",
      " avg loss = 1.37\n",
      "\n",
      "Validation error:\n",
      " accuracy = 48.44\n",
      " avg loss = 1.46\n",
      "\n",
      "Epoch time: 22.07829475402832\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 18, step 50 / 900, loss = 1.20 (0.019 sec/batch)\n",
      "epoch 18, step 100 / 900, loss = 1.34 (0.019 sec/batch)\n",
      "epoch 18, step 150 / 900, loss = 1.13 (0.018 sec/batch)\n",
      "epoch 18, step 200 / 900, loss = 1.35 (0.019 sec/batch)\n",
      "epoch 18, step 250 / 900, loss = 1.49 (0.018 sec/batch)\n",
      "epoch 18, step 300 / 900, loss = 1.48 (0.018 sec/batch)\n",
      "epoch 18, step 350 / 900, loss = 1.53 (0.019 sec/batch)\n",
      "epoch 18, step 400 / 900, loss = 1.56 (0.019 sec/batch)\n",
      "epoch 18, step 450 / 900, loss = 1.61 (0.018 sec/batch)\n",
      "epoch 18, step 500 / 900, loss = 1.86 (0.018 sec/batch)\n",
      "epoch 18, step 550 / 900, loss = 1.42 (0.019 sec/batch)\n",
      "epoch 18, step 600 / 900, loss = 1.65 (0.018 sec/batch)\n",
      "epoch 18, step 650 / 900, loss = 0.94 (0.018 sec/batch)\n",
      "epoch 18, step 700 / 900, loss = 1.28 (0.020 sec/batch)\n",
      "epoch 18, step 750 / 900, loss = 1.32 (0.020 sec/batch)\n",
      "epoch 18, step 800 / 900, loss = 1.21 (0.018 sec/batch)\n",
      "epoch 18, step 850 / 900, loss = 1.56 (0.019 sec/batch)\n",
      "epoch 18, step 900 / 900, loss = 1.37 (0.018 sec/batch)\n",
      "Train error:\n",
      " accuracy = 51.65\n",
      " avg loss = 1.37\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.00\n",
      " avg loss = 1.44\n",
      "\n",
      "Epoch time: 22.259907245635986\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 19, step 50 / 900, loss = 1.61 (0.018 sec/batch)\n",
      "epoch 19, step 100 / 900, loss = 1.37 (0.018 sec/batch)\n",
      "epoch 19, step 150 / 900, loss = 1.46 (0.018 sec/batch)\n",
      "epoch 19, step 200 / 900, loss = 1.73 (0.018 sec/batch)\n",
      "epoch 19, step 250 / 900, loss = 1.21 (0.019 sec/batch)\n",
      "epoch 19, step 300 / 900, loss = 1.12 (0.018 sec/batch)\n",
      "epoch 19, step 350 / 900, loss = 1.40 (0.018 sec/batch)\n",
      "epoch 19, step 400 / 900, loss = 1.29 (0.019 sec/batch)\n",
      "epoch 19, step 450 / 900, loss = 1.60 (0.018 sec/batch)\n",
      "epoch 19, step 500 / 900, loss = 1.44 (0.018 sec/batch)\n",
      "epoch 19, step 550 / 900, loss = 1.70 (0.018 sec/batch)\n",
      "epoch 19, step 600 / 900, loss = 1.40 (0.018 sec/batch)\n",
      "epoch 19, step 650 / 900, loss = 1.42 (0.018 sec/batch)\n",
      "epoch 19, step 700 / 900, loss = 1.48 (0.018 sec/batch)\n",
      "epoch 19, step 750 / 900, loss = 1.26 (0.018 sec/batch)\n",
      "epoch 19, step 800 / 900, loss = 1.17 (0.019 sec/batch)\n",
      "epoch 19, step 850 / 900, loss = 1.33 (0.018 sec/batch)\n",
      "epoch 19, step 900 / 900, loss = 1.33 (0.018 sec/batch)\n",
      "Train error:\n",
      " accuracy = 52.28\n",
      " avg loss = 1.35\n",
      "\n",
      "Validation error:\n",
      " accuracy = 49.36\n",
      " avg loss = 1.44\n",
      "\n",
      "Epoch time: 22.157233715057373\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 20, step 50 / 900, loss = 1.18 (0.018 sec/batch)\n",
      "epoch 20, step 100 / 900, loss = 1.29 (0.019 sec/batch)\n",
      "epoch 20, step 150 / 900, loss = 1.58 (0.019 sec/batch)\n",
      "epoch 20, step 200 / 900, loss = 1.47 (0.019 sec/batch)\n",
      "epoch 20, step 250 / 900, loss = 1.29 (0.018 sec/batch)\n",
      "epoch 20, step 300 / 900, loss = 1.38 (0.018 sec/batch)\n",
      "epoch 20, step 350 / 900, loss = 1.27 (0.019 sec/batch)\n",
      "epoch 20, step 400 / 900, loss = 1.23 (0.018 sec/batch)\n",
      "epoch 20, step 450 / 900, loss = 1.29 (0.018 sec/batch)\n",
      "epoch 20, step 500 / 900, loss = 1.46 (0.018 sec/batch)\n",
      "epoch 20, step 550 / 900, loss = 1.51 (0.018 sec/batch)\n",
      "epoch 20, step 600 / 900, loss = 1.60 (0.018 sec/batch)\n",
      "epoch 20, step 650 / 900, loss = 1.42 (0.019 sec/batch)\n",
      "epoch 20, step 700 / 900, loss = 1.19 (0.019 sec/batch)\n",
      "epoch 20, step 750 / 900, loss = 1.25 (0.018 sec/batch)\n",
      "epoch 20, step 800 / 900, loss = 1.25 (0.019 sec/batch)\n",
      "epoch 20, step 850 / 900, loss = 1.27 (0.019 sec/batch)\n",
      "epoch 20, step 900 / 900, loss = 1.33 (0.019 sec/batch)\n",
      "Train error:\n",
      " accuracy = 52.94\n",
      " avg loss = 1.34\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.16\n",
      " avg loss = 1.43\n",
      "\n",
      "Epoch time: 22.146348237991333\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 21, step 50 / 900, loss = 1.38 (0.018 sec/batch)\n",
      "epoch 21, step 100 / 900, loss = 1.16 (0.018 sec/batch)\n",
      "epoch 21, step 150 / 900, loss = 1.27 (0.019 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21, step 200 / 900, loss = 1.36 (0.019 sec/batch)\n",
      "epoch 21, step 250 / 900, loss = 1.24 (0.019 sec/batch)\n",
      "epoch 21, step 300 / 900, loss = 1.53 (0.018 sec/batch)\n",
      "epoch 21, step 350 / 900, loss = 1.61 (0.019 sec/batch)\n",
      "epoch 21, step 400 / 900, loss = 1.23 (0.018 sec/batch)\n",
      "epoch 21, step 450 / 900, loss = 1.65 (0.019 sec/batch)\n",
      "epoch 21, step 500 / 900, loss = 1.53 (0.019 sec/batch)\n",
      "epoch 21, step 550 / 900, loss = 1.15 (0.018 sec/batch)\n",
      "epoch 21, step 600 / 900, loss = 1.40 (0.018 sec/batch)\n",
      "epoch 21, step 650 / 900, loss = 1.25 (0.019 sec/batch)\n",
      "epoch 21, step 700 / 900, loss = 1.26 (0.019 sec/batch)\n",
      "epoch 21, step 750 / 900, loss = 1.20 (0.019 sec/batch)\n",
      "epoch 21, step 800 / 900, loss = 1.51 (0.018 sec/batch)\n",
      "epoch 21, step 850 / 900, loss = 1.23 (0.019 sec/batch)\n",
      "epoch 21, step 900 / 900, loss = 1.26 (0.020 sec/batch)\n",
      "Train error:\n",
      " accuracy = 53.30\n",
      " avg loss = 1.33\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.34\n",
      " avg loss = 1.44\n",
      "\n",
      "Epoch time: 22.638614416122437\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 22, step 50 / 900, loss = 1.75 (0.019 sec/batch)\n",
      "epoch 22, step 100 / 900, loss = 1.48 (0.019 sec/batch)\n",
      "epoch 22, step 150 / 900, loss = 1.45 (0.018 sec/batch)\n",
      "epoch 22, step 200 / 900, loss = 1.24 (0.018 sec/batch)\n",
      "epoch 22, step 250 / 900, loss = 1.08 (0.018 sec/batch)\n",
      "epoch 22, step 300 / 900, loss = 1.29 (0.020 sec/batch)\n",
      "epoch 22, step 350 / 900, loss = 1.31 (0.019 sec/batch)\n",
      "epoch 22, step 400 / 900, loss = 1.34 (0.019 sec/batch)\n",
      "epoch 22, step 450 / 900, loss = 1.41 (0.020 sec/batch)\n",
      "epoch 22, step 500 / 900, loss = 1.20 (0.018 sec/batch)\n",
      "epoch 22, step 550 / 900, loss = 1.15 (0.019 sec/batch)\n",
      "epoch 22, step 600 / 900, loss = 1.26 (0.019 sec/batch)\n",
      "epoch 22, step 650 / 900, loss = 1.39 (0.018 sec/batch)\n",
      "epoch 22, step 700 / 900, loss = 0.99 (0.019 sec/batch)\n",
      "epoch 22, step 750 / 900, loss = 1.42 (0.018 sec/batch)\n",
      "epoch 22, step 800 / 900, loss = 1.40 (0.019 sec/batch)\n",
      "epoch 22, step 850 / 900, loss = 1.45 (0.018 sec/batch)\n",
      "epoch 22, step 900 / 900, loss = 1.63 (0.019 sec/batch)\n",
      "Train error:\n",
      " accuracy = 53.12\n",
      " avg loss = 1.33\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.06\n",
      " avg loss = 1.43\n",
      "\n",
      "Epoch time: 22.27705407142639\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 23, step 50 / 900, loss = 1.26 (0.019 sec/batch)\n",
      "epoch 23, step 100 / 900, loss = 1.44 (0.020 sec/batch)\n",
      "epoch 23, step 150 / 900, loss = 1.41 (0.020 sec/batch)\n",
      "epoch 23, step 200 / 900, loss = 1.42 (0.020 sec/batch)\n",
      "epoch 23, step 250 / 900, loss = 1.44 (0.019 sec/batch)\n",
      "epoch 23, step 300 / 900, loss = 1.25 (0.020 sec/batch)\n",
      "epoch 23, step 350 / 900, loss = 1.20 (0.019 sec/batch)\n",
      "epoch 23, step 400 / 900, loss = 1.10 (0.019 sec/batch)\n",
      "epoch 23, step 450 / 900, loss = 1.28 (0.019 sec/batch)\n",
      "epoch 23, step 500 / 900, loss = 1.49 (0.019 sec/batch)\n",
      "epoch 23, step 550 / 900, loss = 1.20 (0.022 sec/batch)\n",
      "epoch 23, step 600 / 900, loss = 1.03 (0.019 sec/batch)\n",
      "epoch 23, step 650 / 900, loss = 1.35 (0.018 sec/batch)\n",
      "epoch 23, step 700 / 900, loss = 1.56 (0.019 sec/batch)\n",
      "epoch 23, step 750 / 900, loss = 1.51 (0.019 sec/batch)\n",
      "epoch 23, step 800 / 900, loss = 1.35 (0.019 sec/batch)\n",
      "epoch 23, step 850 / 900, loss = 1.45 (0.018 sec/batch)\n",
      "epoch 23, step 900 / 900, loss = 1.07 (0.019 sec/batch)\n",
      "Train error:\n",
      " accuracy = 53.74\n",
      " avg loss = 1.31\n",
      "\n",
      "Validation error:\n",
      " accuracy = 51.10\n",
      " avg loss = 1.40\n",
      "\n",
      "Epoch time: 22.966917991638184\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 24, step 50 / 900, loss = 1.25 (0.019 sec/batch)\n",
      "epoch 24, step 100 / 900, loss = 1.23 (0.018 sec/batch)\n",
      "epoch 24, step 150 / 900, loss = 1.03 (0.019 sec/batch)\n",
      "epoch 24, step 200 / 900, loss = 1.09 (0.019 sec/batch)\n",
      "epoch 24, step 250 / 900, loss = 1.34 (0.019 sec/batch)\n",
      "epoch 24, step 300 / 900, loss = 1.57 (0.019 sec/batch)\n",
      "epoch 24, step 350 / 900, loss = 0.96 (0.019 sec/batch)\n",
      "epoch 24, step 400 / 900, loss = 0.88 (0.020 sec/batch)\n",
      "epoch 24, step 450 / 900, loss = 1.57 (0.019 sec/batch)\n",
      "epoch 24, step 500 / 900, loss = 1.57 (0.018 sec/batch)\n",
      "epoch 24, step 550 / 900, loss = 1.55 (0.019 sec/batch)\n",
      "epoch 24, step 600 / 900, loss = 1.13 (0.019 sec/batch)\n",
      "epoch 24, step 650 / 900, loss = 1.32 (0.018 sec/batch)\n",
      "epoch 24, step 700 / 900, loss = 1.46 (0.020 sec/batch)\n",
      "epoch 24, step 750 / 900, loss = 1.13 (0.019 sec/batch)\n",
      "epoch 24, step 800 / 900, loss = 1.17 (0.020 sec/batch)\n",
      "epoch 24, step 850 / 900, loss = 1.43 (0.019 sec/batch)\n",
      "epoch 24, step 900 / 900, loss = 1.39 (0.019 sec/batch)\n",
      "Train error:\n",
      " accuracy = 53.88\n",
      " avg loss = 1.31\n",
      "\n",
      "Validation error:\n",
      " accuracy = 50.50\n",
      " avg loss = 1.41\n",
      "\n",
      "Epoch time: 22.292752265930176\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 25, step 50 / 900, loss = 1.03 (0.019 sec/batch)\n",
      "epoch 25, step 100 / 900, loss = 1.16 (0.019 sec/batch)\n",
      "epoch 25, step 150 / 900, loss = 1.15 (0.020 sec/batch)\n",
      "epoch 25, step 200 / 900, loss = 1.14 (0.019 sec/batch)\n",
      "epoch 25, step 250 / 900, loss = 1.11 (0.018 sec/batch)\n",
      "epoch 25, step 300 / 900, loss = 1.47 (0.019 sec/batch)\n",
      "epoch 25, step 350 / 900, loss = 1.54 (0.020 sec/batch)\n",
      "epoch 25, step 400 / 900, loss = 1.58 (0.019 sec/batch)\n",
      "epoch 25, step 450 / 900, loss = 1.10 (0.018 sec/batch)\n",
      "epoch 25, step 500 / 900, loss = 1.37 (0.019 sec/batch)\n",
      "epoch 25, step 550 / 900, loss = 1.47 (0.019 sec/batch)\n",
      "epoch 25, step 600 / 900, loss = 1.34 (0.020 sec/batch)\n",
      "epoch 25, step 650 / 900, loss = 1.52 (0.019 sec/batch)\n",
      "epoch 25, step 700 / 900, loss = 1.19 (0.018 sec/batch)\n",
      "epoch 25, step 750 / 900, loss = 1.29 (0.018 sec/batch)\n",
      "epoch 25, step 800 / 900, loss = 1.16 (0.019 sec/batch)\n",
      "epoch 25, step 850 / 900, loss = 1.35 (0.019 sec/batch)\n",
      "epoch 25, step 900 / 900, loss = 1.29 (0.018 sec/batch)\n",
      "Train error:\n",
      " accuracy = 54.21\n",
      " avg loss = 1.31\n",
      "\n",
      "Validation error:\n",
      " accuracy = 51.04\n",
      " avg loss = 1.42\n",
      "\n",
      "Epoch time: 22.25591278076172\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 26, step 50 / 900, loss = 1.20 (0.019 sec/batch)\n",
      "epoch 26, step 100 / 900, loss = 1.13 (0.018 sec/batch)\n",
      "epoch 26, step 150 / 900, loss = 1.23 (0.020 sec/batch)\n",
      "epoch 26, step 200 / 900, loss = 1.62 (0.018 sec/batch)\n",
      "epoch 26, step 250 / 900, loss = 1.72 (0.019 sec/batch)\n",
      "epoch 26, step 300 / 900, loss = 1.42 (0.018 sec/batch)\n",
      "epoch 26, step 350 / 900, loss = 1.19 (0.020 sec/batch)\n",
      "epoch 26, step 400 / 900, loss = 1.18 (0.019 sec/batch)\n",
      "epoch 26, step 450 / 900, loss = 1.42 (0.019 sec/batch)\n",
      "epoch 26, step 500 / 900, loss = 1.31 (0.019 sec/batch)\n",
      "epoch 26, step 550 / 900, loss = 1.36 (0.018 sec/batch)\n",
      "epoch 26, step 600 / 900, loss = 1.16 (0.019 sec/batch)\n",
      "epoch 26, step 650 / 900, loss = 1.55 (0.018 sec/batch)\n",
      "epoch 26, step 700 / 900, loss = 1.20 (0.019 sec/batch)\n",
      "epoch 26, step 750 / 900, loss = 1.46 (0.018 sec/batch)\n",
      "epoch 26, step 800 / 900, loss = 1.14 (0.019 sec/batch)\n",
      "epoch 26, step 850 / 900, loss = 1.20 (0.019 sec/batch)\n",
      "epoch 26, step 900 / 900, loss = 1.12 (0.019 sec/batch)\n",
      "Train error:\n",
      " accuracy = 54.42\n",
      " avg loss = 1.30\n",
      "\n",
      "Validation error:\n",
      " accuracy = 51.24\n",
      " avg loss = 1.41\n",
      "\n",
      "Epoch time: 22.262613773345947\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 27, step 50 / 900, loss = 1.44 (0.019 sec/batch)\n",
      "epoch 27, step 100 / 900, loss = 1.36 (0.019 sec/batch)\n",
      "epoch 27, step 150 / 900, loss = 1.56 (0.019 sec/batch)\n",
      "epoch 27, step 200 / 900, loss = 1.52 (0.019 sec/batch)\n",
      "epoch 27, step 250 / 900, loss = 1.15 (0.018 sec/batch)\n",
      "epoch 27, step 300 / 900, loss = 1.32 (0.019 sec/batch)\n",
      "epoch 27, step 350 / 900, loss = 1.59 (0.018 sec/batch)\n",
      "epoch 27, step 400 / 900, loss = 1.56 (0.020 sec/batch)\n",
      "epoch 27, step 450 / 900, loss = 1.57 (0.019 sec/batch)\n",
      "epoch 27, step 500 / 900, loss = 1.11 (0.020 sec/batch)\n",
      "epoch 27, step 550 / 900, loss = 1.24 (0.019 sec/batch)\n",
      "epoch 27, step 600 / 900, loss = 1.63 (0.019 sec/batch)\n",
      "epoch 27, step 650 / 900, loss = 1.22 (0.018 sec/batch)\n",
      "epoch 27, step 700 / 900, loss = 1.24 (0.018 sec/batch)\n",
      "epoch 27, step 750 / 900, loss = 1.27 (0.020 sec/batch)\n",
      "epoch 27, step 800 / 900, loss = 1.29 (0.019 sec/batch)\n",
      "epoch 27, step 850 / 900, loss = 1.23 (0.019 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 27, step 900 / 900, loss = 1.44 (0.019 sec/batch)\n",
      "Train error:\n",
      " accuracy = 54.47\n",
      " avg loss = 1.30\n",
      "\n",
      "Validation error:\n",
      " accuracy = 51.62\n",
      " avg loss = 1.40\n",
      "\n",
      "Epoch time: 22.320930004119873\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 28, step 50 / 900, loss = 1.44 (0.018 sec/batch)\n",
      "epoch 28, step 100 / 900, loss = 1.45 (0.019 sec/batch)\n",
      "epoch 28, step 150 / 900, loss = 1.53 (0.019 sec/batch)\n",
      "epoch 28, step 200 / 900, loss = 1.34 (0.018 sec/batch)\n",
      "epoch 28, step 250 / 900, loss = 1.13 (0.019 sec/batch)\n",
      "epoch 28, step 300 / 900, loss = 1.83 (0.018 sec/batch)\n",
      "epoch 28, step 350 / 900, loss = 1.14 (0.019 sec/batch)\n",
      "epoch 28, step 400 / 900, loss = 1.10 (0.018 sec/batch)\n",
      "epoch 28, step 450 / 900, loss = 1.01 (0.019 sec/batch)\n",
      "epoch 28, step 500 / 900, loss = 1.33 (0.018 sec/batch)\n",
      "epoch 28, step 550 / 900, loss = 1.58 (0.018 sec/batch)\n",
      "epoch 28, step 600 / 900, loss = 1.40 (0.019 sec/batch)\n",
      "epoch 28, step 650 / 900, loss = 1.32 (0.018 sec/batch)\n",
      "epoch 28, step 700 / 900, loss = 1.33 (0.018 sec/batch)\n",
      "epoch 28, step 750 / 900, loss = 1.22 (0.018 sec/batch)\n",
      "epoch 28, step 800 / 900, loss = 1.23 (0.019 sec/batch)\n",
      "epoch 28, step 850 / 900, loss = 1.39 (0.018 sec/batch)\n",
      "epoch 28, step 900 / 900, loss = 1.02 (0.018 sec/batch)\n",
      "Train error:\n",
      " accuracy = 54.94\n",
      " avg loss = 1.29\n",
      "\n",
      "Validation error:\n",
      " accuracy = 51.28\n",
      " avg loss = 1.39\n",
      "\n",
      "Epoch time: 21.980079412460327\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 29, step 50 / 900, loss = 1.69 (0.019 sec/batch)\n",
      "epoch 29, step 100 / 900, loss = 1.38 (0.018 sec/batch)\n",
      "epoch 29, step 150 / 900, loss = 1.14 (0.018 sec/batch)\n",
      "epoch 29, step 200 / 900, loss = 1.31 (0.018 sec/batch)\n",
      "epoch 29, step 250 / 900, loss = 0.99 (0.018 sec/batch)\n",
      "epoch 29, step 300 / 900, loss = 1.24 (0.018 sec/batch)\n",
      "epoch 29, step 350 / 900, loss = 1.32 (0.018 sec/batch)\n",
      "epoch 29, step 400 / 900, loss = 1.27 (0.018 sec/batch)\n",
      "epoch 29, step 450 / 900, loss = 1.36 (0.018 sec/batch)\n",
      "epoch 29, step 500 / 900, loss = 1.11 (0.020 sec/batch)\n",
      "epoch 29, step 550 / 900, loss = 1.21 (0.018 sec/batch)\n",
      "epoch 29, step 600 / 900, loss = 1.53 (0.018 sec/batch)\n",
      "epoch 29, step 650 / 900, loss = 1.50 (0.018 sec/batch)\n",
      "epoch 29, step 700 / 900, loss = 1.53 (0.018 sec/batch)\n",
      "epoch 29, step 750 / 900, loss = 1.13 (0.018 sec/batch)\n",
      "epoch 29, step 800 / 900, loss = 1.20 (0.018 sec/batch)\n",
      "epoch 29, step 850 / 900, loss = 1.20 (0.018 sec/batch)\n",
      "epoch 29, step 900 / 900, loss = 1.46 (0.019 sec/batch)\n",
      "Train error:\n",
      " accuracy = 54.73\n",
      " avg loss = 1.29\n",
      "\n",
      "Validation error:\n",
      " accuracy = 51.64\n",
      " avg loss = 1.38\n",
      "\n",
      "Epoch time: 22.00245475769043\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 30, step 50 / 900, loss = 1.45 (0.018 sec/batch)\n",
      "epoch 30, step 100 / 900, loss = 1.09 (0.018 sec/batch)\n",
      "epoch 30, step 150 / 900, loss = 1.26 (0.019 sec/batch)\n",
      "epoch 30, step 200 / 900, loss = 1.25 (0.018 sec/batch)\n",
      "epoch 30, step 250 / 900, loss = 1.12 (0.018 sec/batch)\n",
      "epoch 30, step 300 / 900, loss = 1.21 (0.018 sec/batch)\n",
      "epoch 30, step 350 / 900, loss = 1.46 (0.018 sec/batch)\n",
      "epoch 30, step 400 / 900, loss = 1.11 (0.019 sec/batch)\n",
      "epoch 30, step 450 / 900, loss = 1.53 (0.018 sec/batch)\n",
      "epoch 30, step 500 / 900, loss = 1.50 (0.018 sec/batch)\n",
      "epoch 30, step 550 / 900, loss = 1.22 (0.018 sec/batch)\n",
      "epoch 30, step 600 / 900, loss = 1.70 (0.018 sec/batch)\n",
      "epoch 30, step 650 / 900, loss = 1.13 (0.018 sec/batch)\n",
      "epoch 30, step 700 / 900, loss = 1.09 (0.018 sec/batch)\n",
      "epoch 30, step 750 / 900, loss = 1.35 (0.018 sec/batch)\n",
      "epoch 30, step 800 / 900, loss = 1.25 (0.018 sec/batch)\n",
      "epoch 30, step 850 / 900, loss = 1.34 (0.018 sec/batch)\n",
      "epoch 30, step 900 / 900, loss = 1.25 (0.018 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.29\n",
      " avg loss = 1.28\n",
      "\n",
      "Validation error:\n",
      " accuracy = 51.78\n",
      " avg loss = 1.38\n",
      "\n",
      "Epoch time: 22.01216197013855\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 31, step 50 / 900, loss = 1.40 (0.019 sec/batch)\n",
      "epoch 31, step 100 / 900, loss = 1.22 (0.018 sec/batch)\n",
      "epoch 31, step 150 / 900, loss = 1.22 (0.019 sec/batch)\n",
      "epoch 31, step 200 / 900, loss = 1.10 (0.019 sec/batch)\n",
      "epoch 31, step 250 / 900, loss = 1.06 (0.018 sec/batch)\n",
      "epoch 31, step 300 / 900, loss = 1.33 (0.019 sec/batch)\n",
      "epoch 31, step 350 / 900, loss = 1.21 (0.019 sec/batch)\n",
      "epoch 31, step 400 / 900, loss = 1.64 (0.018 sec/batch)\n",
      "epoch 31, step 450 / 900, loss = 1.36 (0.019 sec/batch)\n",
      "epoch 31, step 500 / 900, loss = 1.20 (0.018 sec/batch)\n",
      "epoch 31, step 550 / 900, loss = 1.23 (0.018 sec/batch)\n",
      "epoch 31, step 600 / 900, loss = 1.06 (0.018 sec/batch)\n",
      "epoch 31, step 650 / 900, loss = 0.98 (0.018 sec/batch)\n",
      "epoch 31, step 700 / 900, loss = 1.25 (0.018 sec/batch)\n",
      "epoch 31, step 750 / 900, loss = 1.55 (0.018 sec/batch)\n",
      "epoch 31, step 800 / 900, loss = 1.14 (0.019 sec/batch)\n",
      "epoch 31, step 850 / 900, loss = 1.55 (0.018 sec/batch)\n",
      "epoch 31, step 900 / 900, loss = 0.89 (0.018 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.06\n",
      " avg loss = 1.28\n",
      "\n",
      "Validation error:\n",
      " accuracy = 52.96\n",
      " avg loss = 1.39\n",
      "\n",
      "Epoch time: 21.982654094696045\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 32, step 50 / 900, loss = 1.52 (0.018 sec/batch)\n",
      "epoch 32, step 100 / 900, loss = 1.54 (0.018 sec/batch)\n",
      "epoch 32, step 150 / 900, loss = 1.09 (0.018 sec/batch)\n",
      "epoch 32, step 200 / 900, loss = 1.34 (0.018 sec/batch)\n",
      "epoch 32, step 250 / 900, loss = 1.70 (0.018 sec/batch)\n",
      "epoch 32, step 300 / 900, loss = 1.18 (0.018 sec/batch)\n",
      "epoch 32, step 350 / 900, loss = 1.07 (0.019 sec/batch)\n",
      "epoch 32, step 400 / 900, loss = 1.39 (0.019 sec/batch)\n",
      "epoch 32, step 450 / 900, loss = 1.52 (0.018 sec/batch)\n",
      "epoch 32, step 500 / 900, loss = 1.19 (0.018 sec/batch)\n",
      "epoch 32, step 550 / 900, loss = 1.40 (0.019 sec/batch)\n",
      "epoch 32, step 600 / 900, loss = 1.19 (0.019 sec/batch)\n",
      "epoch 32, step 650 / 900, loss = 1.43 (0.018 sec/batch)\n",
      "epoch 32, step 700 / 900, loss = 1.06 (0.018 sec/batch)\n",
      "epoch 32, step 750 / 900, loss = 1.18 (0.018 sec/batch)\n",
      "epoch 32, step 800 / 900, loss = 1.50 (0.020 sec/batch)\n",
      "epoch 32, step 850 / 900, loss = 1.31 (0.019 sec/batch)\n",
      "epoch 32, step 900 / 900, loss = 1.19 (0.018 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.40\n",
      " avg loss = 1.28\n",
      "\n",
      "Validation error:\n",
      " accuracy = 51.98\n",
      " avg loss = 1.37\n",
      "\n",
      "Epoch time: 22.192042589187622\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 33, step 50 / 900, loss = 1.17 (0.019 sec/batch)\n",
      "epoch 33, step 100 / 900, loss = 1.22 (0.018 sec/batch)\n",
      "epoch 33, step 150 / 900, loss = 1.07 (0.020 sec/batch)\n",
      "epoch 33, step 200 / 900, loss = 1.28 (0.018 sec/batch)\n",
      "epoch 33, step 250 / 900, loss = 1.11 (0.019 sec/batch)\n",
      "epoch 33, step 300 / 900, loss = 1.36 (0.019 sec/batch)\n",
      "epoch 33, step 350 / 900, loss = 1.90 (0.019 sec/batch)\n",
      "epoch 33, step 400 / 900, loss = 1.40 (0.018 sec/batch)\n",
      "epoch 33, step 450 / 900, loss = 1.32 (0.018 sec/batch)\n",
      "epoch 33, step 500 / 900, loss = 1.55 (0.020 sec/batch)\n",
      "epoch 33, step 550 / 900, loss = 1.33 (0.018 sec/batch)\n",
      "epoch 33, step 600 / 900, loss = 1.22 (0.018 sec/batch)\n",
      "epoch 33, step 650 / 900, loss = 1.24 (0.018 sec/batch)\n",
      "epoch 33, step 700 / 900, loss = 1.32 (0.019 sec/batch)\n",
      "epoch 33, step 750 / 900, loss = 1.48 (0.018 sec/batch)\n",
      "epoch 33, step 800 / 900, loss = 1.46 (0.018 sec/batch)\n",
      "epoch 33, step 850 / 900, loss = 1.11 (0.018 sec/batch)\n",
      "epoch 33, step 900 / 900, loss = 1.34 (0.018 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.33\n",
      " avg loss = 1.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 51.50\n",
      " avg loss = 1.38\n",
      "\n",
      "Epoch time: 22.19560194015503\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 34, step 50 / 900, loss = 1.10 (0.018 sec/batch)\n",
      "epoch 34, step 100 / 900, loss = 1.28 (0.019 sec/batch)\n",
      "epoch 34, step 150 / 900, loss = 1.31 (0.018 sec/batch)\n",
      "epoch 34, step 200 / 900, loss = 1.21 (0.018 sec/batch)\n",
      "epoch 34, step 250 / 900, loss = 1.19 (0.018 sec/batch)\n",
      "epoch 34, step 300 / 900, loss = 1.27 (0.019 sec/batch)\n",
      "epoch 34, step 350 / 900, loss = 1.12 (0.019 sec/batch)\n",
      "epoch 34, step 400 / 900, loss = 1.14 (0.018 sec/batch)\n",
      "epoch 34, step 450 / 900, loss = 1.31 (0.018 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 34, step 500 / 900, loss = 1.21 (0.018 sec/batch)\n",
      "epoch 34, step 550 / 900, loss = 1.47 (0.018 sec/batch)\n",
      "epoch 34, step 600 / 900, loss = 1.30 (0.018 sec/batch)\n",
      "epoch 34, step 650 / 900, loss = 1.39 (0.019 sec/batch)\n",
      "epoch 34, step 700 / 900, loss = 1.07 (0.018 sec/batch)\n",
      "epoch 34, step 750 / 900, loss = 1.18 (0.019 sec/batch)\n",
      "epoch 34, step 800 / 900, loss = 1.17 (0.018 sec/batch)\n",
      "epoch 34, step 850 / 900, loss = 1.13 (0.018 sec/batch)\n",
      "epoch 34, step 900 / 900, loss = 1.29 (0.018 sec/batch)\n",
      "Train error:\n",
      " accuracy = 54.95\n",
      " avg loss = 1.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 51.98\n",
      " avg loss = 1.37\n",
      "\n",
      "Epoch time: 22.038471698760986\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 35, step 50 / 900, loss = 1.18 (0.018 sec/batch)\n",
      "epoch 35, step 100 / 900, loss = 1.46 (0.018 sec/batch)\n",
      "epoch 35, step 150 / 900, loss = 1.16 (0.018 sec/batch)\n",
      "epoch 35, step 200 / 900, loss = 1.18 (0.018 sec/batch)\n",
      "epoch 35, step 250 / 900, loss = 1.07 (0.018 sec/batch)\n",
      "epoch 35, step 300 / 900, loss = 1.39 (0.019 sec/batch)\n",
      "epoch 35, step 350 / 900, loss = 1.26 (0.018 sec/batch)\n",
      "epoch 35, step 400 / 900, loss = 1.45 (0.018 sec/batch)\n",
      "epoch 35, step 450 / 900, loss = 1.12 (0.018 sec/batch)\n",
      "epoch 35, step 500 / 900, loss = 1.17 (0.019 sec/batch)\n",
      "epoch 35, step 550 / 900, loss = 1.06 (0.018 sec/batch)\n",
      "epoch 35, step 600 / 900, loss = 1.31 (0.018 sec/batch)\n",
      "epoch 35, step 650 / 900, loss = 1.03 (0.019 sec/batch)\n",
      "epoch 35, step 700 / 900, loss = 1.17 (0.019 sec/batch)\n",
      "epoch 35, step 750 / 900, loss = 1.49 (0.019 sec/batch)\n",
      "epoch 35, step 800 / 900, loss = 1.35 (0.019 sec/batch)\n",
      "epoch 35, step 850 / 900, loss = 1.16 (0.018 sec/batch)\n",
      "epoch 35, step 900 / 900, loss = 1.24 (0.018 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.63\n",
      " avg loss = 1.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 52.66\n",
      " avg loss = 1.37\n",
      "\n",
      "Epoch time: 21.998798847198486\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 36, step 50 / 900, loss = 1.17 (0.018 sec/batch)\n",
      "epoch 36, step 100 / 900, loss = 1.51 (0.018 sec/batch)\n",
      "epoch 36, step 150 / 900, loss = 1.14 (0.019 sec/batch)\n",
      "epoch 36, step 200 / 900, loss = 1.23 (0.018 sec/batch)\n",
      "epoch 36, step 250 / 900, loss = 1.20 (0.018 sec/batch)\n",
      "epoch 36, step 300 / 900, loss = 1.13 (0.019 sec/batch)\n",
      "epoch 36, step 350 / 900, loss = 1.28 (0.018 sec/batch)\n",
      "epoch 36, step 400 / 900, loss = 1.48 (0.018 sec/batch)\n",
      "epoch 36, step 450 / 900, loss = 0.98 (0.018 sec/batch)\n",
      "epoch 36, step 500 / 900, loss = 1.08 (0.018 sec/batch)\n",
      "epoch 36, step 550 / 900, loss = 1.32 (0.018 sec/batch)\n",
      "epoch 36, step 600 / 900, loss = 1.32 (0.019 sec/batch)\n",
      "epoch 36, step 650 / 900, loss = 1.27 (0.018 sec/batch)\n",
      "epoch 36, step 700 / 900, loss = 1.48 (0.018 sec/batch)\n",
      "epoch 36, step 750 / 900, loss = 1.32 (0.018 sec/batch)\n",
      "epoch 36, step 800 / 900, loss = 1.28 (0.019 sec/batch)\n",
      "epoch 36, step 850 / 900, loss = 1.16 (0.019 sec/batch)\n",
      "epoch 36, step 900 / 900, loss = 1.18 (0.019 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.57\n",
      " avg loss = 1.27\n",
      "\n",
      "Validation error:\n",
      " accuracy = 51.82\n",
      " avg loss = 1.38\n",
      "\n",
      "Epoch time: 22.006465673446655\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 37, step 50 / 900, loss = 1.47 (0.018 sec/batch)\n",
      "epoch 37, step 100 / 900, loss = 1.04 (0.018 sec/batch)\n",
      "epoch 37, step 150 / 900, loss = 1.42 (0.020 sec/batch)\n",
      "epoch 37, step 200 / 900, loss = 1.21 (0.018 sec/batch)\n",
      "epoch 37, step 250 / 900, loss = 1.37 (0.018 sec/batch)\n",
      "epoch 37, step 300 / 900, loss = 1.10 (0.018 sec/batch)\n",
      "epoch 37, step 350 / 900, loss = 1.06 (0.018 sec/batch)\n",
      "epoch 37, step 400 / 900, loss = 1.17 (0.018 sec/batch)\n",
      "epoch 37, step 450 / 900, loss = 1.49 (0.019 sec/batch)\n",
      "epoch 37, step 500 / 900, loss = 1.33 (0.018 sec/batch)\n",
      "epoch 37, step 550 / 900, loss = 1.10 (0.019 sec/batch)\n",
      "epoch 37, step 600 / 900, loss = 1.31 (0.019 sec/batch)\n",
      "epoch 37, step 650 / 900, loss = 1.19 (0.019 sec/batch)\n",
      "epoch 37, step 700 / 900, loss = 1.04 (0.018 sec/batch)\n",
      "epoch 37, step 750 / 900, loss = 1.07 (0.019 sec/batch)\n",
      "epoch 37, step 800 / 900, loss = 1.24 (0.019 sec/batch)\n",
      "epoch 37, step 850 / 900, loss = 1.35 (0.018 sec/batch)\n",
      "epoch 37, step 900 / 900, loss = 1.53 (0.018 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.87\n",
      " avg loss = 1.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 52.20\n",
      " avg loss = 1.37\n",
      "\n",
      "Epoch time: 21.991945028305054\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 38, step 50 / 900, loss = 1.08 (0.018 sec/batch)\n",
      "epoch 38, step 100 / 900, loss = 1.35 (0.018 sec/batch)\n",
      "epoch 38, step 150 / 900, loss = 1.19 (0.018 sec/batch)\n",
      "epoch 38, step 200 / 900, loss = 1.45 (0.019 sec/batch)\n",
      "epoch 38, step 250 / 900, loss = 1.60 (0.018 sec/batch)\n",
      "epoch 38, step 300 / 900, loss = 1.22 (0.018 sec/batch)\n",
      "epoch 38, step 350 / 900, loss = 1.21 (0.019 sec/batch)\n",
      "epoch 38, step 400 / 900, loss = 1.33 (0.018 sec/batch)\n",
      "epoch 38, step 450 / 900, loss = 1.32 (0.018 sec/batch)\n",
      "epoch 38, step 500 / 900, loss = 1.27 (0.018 sec/batch)\n",
      "epoch 38, step 550 / 900, loss = 1.22 (0.018 sec/batch)\n",
      "epoch 38, step 600 / 900, loss = 1.39 (0.020 sec/batch)\n",
      "epoch 38, step 650 / 900, loss = 1.19 (0.018 sec/batch)\n",
      "epoch 38, step 700 / 900, loss = 1.42 (0.018 sec/batch)\n",
      "epoch 38, step 750 / 900, loss = 1.26 (0.018 sec/batch)\n",
      "epoch 38, step 800 / 900, loss = 1.23 (0.019 sec/batch)\n",
      "epoch 38, step 850 / 900, loss = 1.31 (0.019 sec/batch)\n",
      "epoch 38, step 900 / 900, loss = 1.24 (0.018 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.75\n",
      " avg loss = 1.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 52.00\n",
      " avg loss = 1.38\n",
      "\n",
      "Epoch time: 22.003566026687622\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 39, step 50 / 900, loss = 1.31 (0.019 sec/batch)\n",
      "epoch 39, step 100 / 900, loss = 1.06 (0.018 sec/batch)\n",
      "epoch 39, step 150 / 900, loss = 1.13 (0.019 sec/batch)\n",
      "epoch 39, step 200 / 900, loss = 1.18 (0.019 sec/batch)\n",
      "epoch 39, step 250 / 900, loss = 1.10 (0.018 sec/batch)\n",
      "epoch 39, step 300 / 900, loss = 1.37 (0.018 sec/batch)\n",
      "epoch 39, step 350 / 900, loss = 1.20 (0.018 sec/batch)\n",
      "epoch 39, step 400 / 900, loss = 1.26 (0.019 sec/batch)\n",
      "epoch 39, step 450 / 900, loss = 1.57 (0.019 sec/batch)\n",
      "epoch 39, step 500 / 900, loss = 1.21 (0.018 sec/batch)\n",
      "epoch 39, step 550 / 900, loss = 1.59 (0.018 sec/batch)\n",
      "epoch 39, step 600 / 900, loss = 1.10 (0.018 sec/batch)\n",
      "epoch 39, step 650 / 900, loss = 1.30 (0.018 sec/batch)\n",
      "epoch 39, step 700 / 900, loss = 1.18 (0.018 sec/batch)\n",
      "epoch 39, step 750 / 900, loss = 1.17 (0.019 sec/batch)\n",
      "epoch 39, step 800 / 900, loss = 1.51 (0.018 sec/batch)\n",
      "epoch 39, step 850 / 900, loss = 1.25 (0.018 sec/batch)\n",
      "epoch 39, step 900 / 900, loss = 1.14 (0.019 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.80\n",
      " avg loss = 1.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 52.70\n",
      " avg loss = 1.37\n",
      "\n",
      "Epoch time: 21.99539589881897\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 40, step 50 / 900, loss = 1.38 (0.019 sec/batch)\n",
      "epoch 40, step 100 / 900, loss = 1.28 (0.019 sec/batch)\n",
      "epoch 40, step 150 / 900, loss = 1.42 (0.019 sec/batch)\n",
      "epoch 40, step 200 / 900, loss = 1.33 (0.018 sec/batch)\n",
      "epoch 40, step 250 / 900, loss = 1.65 (0.019 sec/batch)\n",
      "epoch 40, step 300 / 900, loss = 1.36 (0.018 sec/batch)\n",
      "epoch 40, step 350 / 900, loss = 1.58 (0.018 sec/batch)\n",
      "epoch 40, step 400 / 900, loss = 1.34 (0.019 sec/batch)\n",
      "epoch 40, step 450 / 900, loss = 1.44 (0.018 sec/batch)\n",
      "epoch 40, step 500 / 900, loss = 1.47 (0.019 sec/batch)\n",
      "epoch 40, step 550 / 900, loss = 1.39 (0.019 sec/batch)\n",
      "epoch 40, step 600 / 900, loss = 1.33 (0.018 sec/batch)\n",
      "epoch 40, step 650 / 900, loss = 1.16 (0.018 sec/batch)\n",
      "epoch 40, step 700 / 900, loss = 1.07 (0.018 sec/batch)\n",
      "epoch 40, step 750 / 900, loss = 1.22 (0.018 sec/batch)\n",
      "epoch 40, step 800 / 900, loss = 1.17 (0.018 sec/batch)\n",
      "epoch 40, step 850 / 900, loss = 0.95 (0.018 sec/batch)\n",
      "epoch 40, step 900 / 900, loss = 1.36 (0.018 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.71\n",
      " avg loss = 1.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 51.48\n",
      " avg loss = 1.37\n",
      "\n",
      "Epoch time: 22.00317692756653\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 41, step 50 / 900, loss = 0.96 (0.019 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 41, step 100 / 900, loss = 0.99 (0.018 sec/batch)\n",
      "epoch 41, step 150 / 900, loss = 0.98 (0.019 sec/batch)\n",
      "epoch 41, step 200 / 900, loss = 1.10 (0.018 sec/batch)\n",
      "epoch 41, step 250 / 900, loss = 1.27 (0.018 sec/batch)\n",
      "epoch 41, step 300 / 900, loss = 1.28 (0.019 sec/batch)\n",
      "epoch 41, step 350 / 900, loss = 1.29 (0.018 sec/batch)\n",
      "epoch 41, step 400 / 900, loss = 1.56 (0.019 sec/batch)\n",
      "epoch 41, step 450 / 900, loss = 1.26 (0.018 sec/batch)\n",
      "epoch 41, step 500 / 900, loss = 1.18 (0.018 sec/batch)\n",
      "epoch 41, step 550 / 900, loss = 1.00 (0.018 sec/batch)\n",
      "epoch 41, step 600 / 900, loss = 1.28 (0.018 sec/batch)\n",
      "epoch 41, step 650 / 900, loss = 1.20 (0.018 sec/batch)\n",
      "epoch 41, step 700 / 900, loss = 1.24 (0.018 sec/batch)\n",
      "epoch 41, step 750 / 900, loss = 1.31 (0.018 sec/batch)\n",
      "epoch 41, step 800 / 900, loss = 1.40 (0.018 sec/batch)\n",
      "epoch 41, step 850 / 900, loss = 1.16 (0.018 sec/batch)\n",
      "epoch 41, step 900 / 900, loss = 1.19 (0.019 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.90\n",
      " avg loss = 1.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 51.94\n",
      " avg loss = 1.38\n",
      "\n",
      "Epoch time: 22.24697470664978\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 42, step 50 / 900, loss = 1.20 (0.018 sec/batch)\n",
      "epoch 42, step 100 / 900, loss = 1.27 (0.019 sec/batch)\n",
      "epoch 42, step 150 / 900, loss = 1.04 (0.018 sec/batch)\n",
      "epoch 42, step 200 / 900, loss = 1.56 (0.018 sec/batch)\n",
      "epoch 42, step 250 / 900, loss = 1.10 (0.019 sec/batch)\n",
      "epoch 42, step 300 / 900, loss = 1.16 (0.018 sec/batch)\n",
      "epoch 42, step 350 / 900, loss = 1.00 (0.019 sec/batch)\n",
      "epoch 42, step 400 / 900, loss = 1.21 (0.018 sec/batch)\n",
      "epoch 42, step 450 / 900, loss = 1.10 (0.019 sec/batch)\n",
      "epoch 42, step 500 / 900, loss = 1.52 (0.018 sec/batch)\n",
      "epoch 42, step 550 / 900, loss = 1.44 (0.018 sec/batch)\n",
      "epoch 42, step 600 / 900, loss = 1.49 (0.018 sec/batch)\n",
      "epoch 42, step 650 / 900, loss = 1.37 (0.019 sec/batch)\n",
      "epoch 42, step 700 / 900, loss = 1.39 (0.019 sec/batch)\n",
      "epoch 42, step 750 / 900, loss = 1.36 (0.019 sec/batch)\n",
      "epoch 42, step 800 / 900, loss = 1.23 (0.018 sec/batch)\n",
      "epoch 42, step 850 / 900, loss = 1.08 (0.018 sec/batch)\n",
      "epoch 42, step 900 / 900, loss = 1.20 (0.018 sec/batch)\n",
      "Train error:\n",
      " accuracy = 56.00\n",
      " avg loss = 1.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 52.12\n",
      " avg loss = 1.39\n",
      "\n",
      "Epoch time: 22.04427409172058\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 43, step 50 / 900, loss = 1.40 (0.018 sec/batch)\n",
      "epoch 43, step 100 / 900, loss = 1.31 (0.018 sec/batch)\n",
      "epoch 43, step 150 / 900, loss = 1.27 (0.019 sec/batch)\n",
      "epoch 43, step 200 / 900, loss = 1.25 (0.018 sec/batch)\n",
      "epoch 43, step 250 / 900, loss = 1.35 (0.019 sec/batch)\n",
      "epoch 43, step 300 / 900, loss = 1.25 (0.019 sec/batch)\n",
      "epoch 43, step 350 / 900, loss = 1.41 (0.018 sec/batch)\n",
      "epoch 43, step 400 / 900, loss = 1.26 (0.018 sec/batch)\n",
      "epoch 43, step 450 / 900, loss = 1.11 (0.018 sec/batch)\n",
      "epoch 43, step 500 / 900, loss = 1.13 (0.018 sec/batch)\n",
      "epoch 43, step 550 / 900, loss = 1.37 (0.019 sec/batch)\n",
      "epoch 43, step 600 / 900, loss = 1.03 (0.018 sec/batch)\n",
      "epoch 43, step 650 / 900, loss = 1.42 (0.019 sec/batch)\n",
      "epoch 43, step 700 / 900, loss = 1.05 (0.018 sec/batch)\n",
      "epoch 43, step 750 / 900, loss = 1.23 (0.018 sec/batch)\n",
      "epoch 43, step 800 / 900, loss = 1.30 (0.018 sec/batch)\n",
      "epoch 43, step 850 / 900, loss = 1.39 (0.018 sec/batch)\n",
      "epoch 43, step 900 / 900, loss = 1.35 (0.018 sec/batch)\n",
      "Train error:\n",
      " accuracy = 56.14\n",
      " avg loss = 1.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 52.40\n",
      " avg loss = 1.36\n",
      "\n",
      "Epoch time: 22.0088312625885\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 44, step 50 / 900, loss = 1.13 (0.018 sec/batch)\n",
      "epoch 44, step 100 / 900, loss = 1.23 (0.019 sec/batch)\n",
      "epoch 44, step 150 / 900, loss = 1.44 (0.018 sec/batch)\n",
      "epoch 44, step 200 / 900, loss = 1.14 (0.019 sec/batch)\n",
      "epoch 44, step 250 / 900, loss = 1.06 (0.019 sec/batch)\n",
      "epoch 44, step 300 / 900, loss = 1.41 (0.018 sec/batch)\n",
      "epoch 44, step 350 / 900, loss = 1.24 (0.018 sec/batch)\n",
      "epoch 44, step 400 / 900, loss = 1.54 (0.018 sec/batch)\n",
      "epoch 44, step 450 / 900, loss = 1.33 (0.018 sec/batch)\n",
      "epoch 44, step 500 / 900, loss = 1.35 (0.018 sec/batch)\n",
      "epoch 44, step 550 / 900, loss = 1.40 (0.019 sec/batch)\n",
      "epoch 44, step 600 / 900, loss = 1.29 (0.018 sec/batch)\n",
      "epoch 44, step 650 / 900, loss = 1.42 (0.018 sec/batch)\n",
      "epoch 44, step 700 / 900, loss = 1.38 (0.019 sec/batch)\n",
      "epoch 44, step 750 / 900, loss = 1.20 (0.018 sec/batch)\n",
      "epoch 44, step 800 / 900, loss = 1.18 (0.018 sec/batch)\n",
      "epoch 44, step 850 / 900, loss = 1.61 (0.019 sec/batch)\n",
      "epoch 44, step 900 / 900, loss = 1.44 (0.018 sec/batch)\n",
      "Train error:\n",
      " accuracy = 55.91\n",
      " avg loss = 1.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 52.08\n",
      " avg loss = 1.38\n",
      "\n",
      "Epoch time: 22.022172212600708\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 45, step 50 / 900, loss = 1.36 (0.018 sec/batch)\n",
      "epoch 45, step 100 / 900, loss = 1.23 (0.018 sec/batch)\n",
      "epoch 45, step 150 / 900, loss = 1.30 (0.018 sec/batch)\n",
      "epoch 45, step 200 / 900, loss = 1.30 (0.018 sec/batch)\n",
      "epoch 45, step 250 / 900, loss = 1.30 (0.018 sec/batch)\n",
      "epoch 45, step 300 / 900, loss = 1.46 (0.019 sec/batch)\n",
      "epoch 45, step 350 / 900, loss = 1.38 (0.019 sec/batch)\n",
      "epoch 45, step 400 / 900, loss = 1.21 (0.018 sec/batch)\n",
      "epoch 45, step 450 / 900, loss = 1.27 (0.018 sec/batch)\n",
      "epoch 45, step 500 / 900, loss = 1.40 (0.019 sec/batch)\n",
      "epoch 45, step 550 / 900, loss = 0.90 (0.018 sec/batch)\n",
      "epoch 45, step 600 / 900, loss = 1.48 (0.019 sec/batch)\n",
      "epoch 45, step 650 / 900, loss = 1.19 (0.018 sec/batch)\n",
      "epoch 45, step 700 / 900, loss = 1.21 (0.019 sec/batch)\n",
      "epoch 45, step 750 / 900, loss = 1.28 (0.019 sec/batch)\n",
      "epoch 45, step 800 / 900, loss = 1.15 (0.018 sec/batch)\n",
      "epoch 45, step 850 / 900, loss = 1.11 (0.018 sec/batch)\n",
      "epoch 45, step 900 / 900, loss = 1.27 (0.018 sec/batch)\n",
      "Train error:\n",
      " accuracy = 56.17\n",
      " avg loss = 1.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 52.56\n",
      " avg loss = 1.38\n",
      "\n",
      "Epoch time: 21.98775362968445\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 46, step 50 / 900, loss = 1.23 (0.019 sec/batch)\n",
      "epoch 46, step 100 / 900, loss = 1.26 (0.018 sec/batch)\n",
      "epoch 46, step 150 / 900, loss = 1.49 (0.019 sec/batch)\n",
      "epoch 46, step 200 / 900, loss = 1.13 (0.018 sec/batch)\n",
      "epoch 46, step 250 / 900, loss = 1.34 (0.018 sec/batch)\n",
      "epoch 46, step 300 / 900, loss = 1.29 (0.018 sec/batch)\n",
      "epoch 46, step 350 / 900, loss = 1.10 (0.019 sec/batch)\n",
      "epoch 46, step 400 / 900, loss = 1.21 (0.018 sec/batch)\n",
      "epoch 46, step 450 / 900, loss = 1.24 (0.018 sec/batch)\n",
      "epoch 46, step 500 / 900, loss = 1.36 (0.018 sec/batch)\n",
      "epoch 46, step 550 / 900, loss = 1.24 (0.018 sec/batch)\n",
      "epoch 46, step 600 / 900, loss = 1.08 (0.018 sec/batch)\n",
      "epoch 46, step 650 / 900, loss = 1.10 (0.019 sec/batch)\n",
      "epoch 46, step 700 / 900, loss = 0.89 (0.018 sec/batch)\n",
      "epoch 46, step 750 / 900, loss = 1.61 (0.018 sec/batch)\n",
      "epoch 46, step 800 / 900, loss = 1.30 (0.018 sec/batch)\n",
      "epoch 46, step 850 / 900, loss = 1.37 (0.019 sec/batch)\n",
      "epoch 46, step 900 / 900, loss = 1.41 (0.018 sec/batch)\n",
      "Train error:\n",
      " accuracy = 56.19\n",
      " avg loss = 1.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 52.38\n",
      " avg loss = 1.37\n",
      "\n",
      "Epoch time: 21.99902606010437\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 47, step 50 / 900, loss = 1.04 (0.019 sec/batch)\n",
      "epoch 47, step 100 / 900, loss = 1.36 (0.018 sec/batch)\n",
      "epoch 47, step 150 / 900, loss = 1.53 (0.019 sec/batch)\n",
      "epoch 47, step 200 / 900, loss = 1.24 (0.018 sec/batch)\n",
      "epoch 47, step 250 / 900, loss = 1.47 (0.019 sec/batch)\n",
      "epoch 47, step 300 / 900, loss = 1.06 (0.019 sec/batch)\n",
      "epoch 47, step 350 / 900, loss = 1.21 (0.018 sec/batch)\n",
      "epoch 47, step 400 / 900, loss = 1.23 (0.019 sec/batch)\n",
      "epoch 47, step 450 / 900, loss = 1.13 (0.018 sec/batch)\n",
      "epoch 47, step 500 / 900, loss = 1.27 (0.019 sec/batch)\n",
      "epoch 47, step 550 / 900, loss = 1.08 (0.018 sec/batch)\n",
      "epoch 47, step 600 / 900, loss = 1.44 (0.019 sec/batch)\n",
      "epoch 47, step 650 / 900, loss = 1.43 (0.018 sec/batch)\n",
      "epoch 47, step 700 / 900, loss = 1.26 (0.018 sec/batch)\n",
      "epoch 47, step 750 / 900, loss = 1.29 (0.018 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 47, step 800 / 900, loss = 1.12 (0.019 sec/batch)\n",
      "epoch 47, step 850 / 900, loss = 1.00 (0.019 sec/batch)\n",
      "epoch 47, step 900 / 900, loss = 1.22 (0.018 sec/batch)\n",
      "Train error:\n",
      " accuracy = 56.02\n",
      " avg loss = 1.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 52.40\n",
      " avg loss = 1.37\n",
      "\n",
      "Epoch time: 22.005401372909546\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 48, step 50 / 900, loss = 1.26 (0.019 sec/batch)\n",
      "epoch 48, step 100 / 900, loss = 1.28 (0.018 sec/batch)\n",
      "epoch 48, step 150 / 900, loss = 1.49 (0.018 sec/batch)\n",
      "epoch 48, step 200 / 900, loss = 1.56 (0.018 sec/batch)\n",
      "epoch 48, step 250 / 900, loss = 1.07 (0.018 sec/batch)\n",
      "epoch 48, step 300 / 900, loss = 1.22 (0.018 sec/batch)\n",
      "epoch 48, step 350 / 900, loss = 1.29 (0.018 sec/batch)\n",
      "epoch 48, step 400 / 900, loss = 1.46 (0.018 sec/batch)\n",
      "epoch 48, step 450 / 900, loss = 1.00 (0.018 sec/batch)\n",
      "epoch 48, step 500 / 900, loss = 1.18 (0.018 sec/batch)\n",
      "epoch 48, step 550 / 900, loss = 1.30 (0.018 sec/batch)\n",
      "epoch 48, step 600 / 900, loss = 1.17 (0.019 sec/batch)\n",
      "epoch 48, step 650 / 900, loss = 1.10 (0.019 sec/batch)\n",
      "epoch 48, step 700 / 900, loss = 1.02 (0.018 sec/batch)\n",
      "epoch 48, step 750 / 900, loss = 1.30 (0.019 sec/batch)\n",
      "epoch 48, step 800 / 900, loss = 1.37 (0.018 sec/batch)\n",
      "epoch 48, step 850 / 900, loss = 0.93 (0.018 sec/batch)\n",
      "epoch 48, step 900 / 900, loss = 1.27 (0.018 sec/batch)\n",
      "Train error:\n",
      " accuracy = 56.21\n",
      " avg loss = 1.26\n",
      "\n",
      "Validation error:\n",
      " accuracy = 52.48\n",
      " avg loss = 1.37\n",
      "\n",
      "Epoch time: 21.983662605285645\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 49, step 50 / 900, loss = 1.24 (0.018 sec/batch)\n",
      "epoch 49, step 100 / 900, loss = 1.24 (0.018 sec/batch)\n",
      "epoch 49, step 150 / 900, loss = 1.60 (0.019 sec/batch)\n",
      "epoch 49, step 200 / 900, loss = 1.17 (0.019 sec/batch)\n",
      "epoch 49, step 250 / 900, loss = 1.05 (0.018 sec/batch)\n",
      "epoch 49, step 300 / 900, loss = 1.11 (0.018 sec/batch)\n",
      "epoch 49, step 350 / 900, loss = 1.29 (0.018 sec/batch)\n",
      "epoch 49, step 400 / 900, loss = 1.18 (0.018 sec/batch)\n",
      "epoch 49, step 450 / 900, loss = 1.36 (0.018 sec/batch)\n",
      "epoch 49, step 500 / 900, loss = 1.32 (0.018 sec/batch)\n",
      "epoch 49, step 550 / 900, loss = 1.13 (0.018 sec/batch)\n",
      "epoch 49, step 600 / 900, loss = 1.37 (0.018 sec/batch)\n",
      "epoch 49, step 650 / 900, loss = 1.34 (0.018 sec/batch)\n",
      "epoch 49, step 700 / 900, loss = 1.28 (0.019 sec/batch)\n",
      "epoch 49, step 750 / 900, loss = 1.36 (0.018 sec/batch)\n",
      "epoch 49, step 800 / 900, loss = 1.16 (0.018 sec/batch)\n",
      "epoch 49, step 850 / 900, loss = 1.42 (0.018 sec/batch)\n",
      "epoch 49, step 900 / 900, loss = 1.35 (0.018 sec/batch)\n",
      "Train error:\n",
      " accuracy = 56.21\n",
      " avg loss = 1.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 52.82\n",
      " avg loss = 1.37\n",
      "\n",
      "Epoch time: 22.009215593338013\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 50, step 50 / 900, loss = 1.30 (0.018 sec/batch)\n",
      "epoch 50, step 100 / 900, loss = 1.14 (0.019 sec/batch)\n",
      "epoch 50, step 150 / 900, loss = 1.10 (0.019 sec/batch)\n",
      "epoch 50, step 200 / 900, loss = 1.47 (0.018 sec/batch)\n",
      "epoch 50, step 250 / 900, loss = 1.38 (0.018 sec/batch)\n",
      "epoch 50, step 300 / 900, loss = 1.27 (0.019 sec/batch)\n",
      "epoch 50, step 350 / 900, loss = 1.42 (0.019 sec/batch)\n",
      "epoch 50, step 400 / 900, loss = 1.18 (0.018 sec/batch)\n",
      "epoch 50, step 450 / 900, loss = 1.08 (0.018 sec/batch)\n",
      "epoch 50, step 500 / 900, loss = 1.38 (0.018 sec/batch)\n",
      "epoch 50, step 550 / 900, loss = 1.28 (0.018 sec/batch)\n",
      "epoch 50, step 600 / 900, loss = 1.33 (0.018 sec/batch)\n",
      "epoch 50, step 650 / 900, loss = 1.32 (0.019 sec/batch)\n",
      "epoch 50, step 700 / 900, loss = 1.18 (0.019 sec/batch)\n",
      "epoch 50, step 750 / 900, loss = 1.27 (0.018 sec/batch)\n",
      "epoch 50, step 800 / 900, loss = 1.40 (0.019 sec/batch)\n",
      "epoch 50, step 850 / 900, loss = 1.35 (0.018 sec/batch)\n",
      "epoch 50, step 900 / 900, loss = 1.42 (0.018 sec/batch)\n",
      "Train error:\n",
      " accuracy = 56.26\n",
      " avg loss = 1.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 52.14\n",
      " avg loss = 1.35\n",
      "\n",
      "Epoch time: 21.99069333076477\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 51, step 50 / 900, loss = 1.26 (0.018 sec/batch)\n",
      "epoch 51, step 100 / 900, loss = 1.43 (0.019 sec/batch)\n",
      "epoch 51, step 150 / 900, loss = 1.28 (0.018 sec/batch)\n",
      "epoch 51, step 200 / 900, loss = 1.25 (0.019 sec/batch)\n",
      "epoch 51, step 250 / 900, loss = 1.42 (0.018 sec/batch)\n",
      "epoch 51, step 300 / 900, loss = 1.38 (0.019 sec/batch)\n",
      "epoch 51, step 350 / 900, loss = 1.20 (0.018 sec/batch)\n",
      "epoch 51, step 400 / 900, loss = 1.26 (0.018 sec/batch)\n",
      "epoch 51, step 450 / 900, loss = 1.41 (0.018 sec/batch)\n",
      "epoch 51, step 500 / 900, loss = 1.22 (0.018 sec/batch)\n",
      "epoch 51, step 550 / 900, loss = 1.17 (0.019 sec/batch)\n",
      "epoch 51, step 600 / 900, loss = 1.05 (0.019 sec/batch)\n",
      "epoch 51, step 650 / 900, loss = 1.31 (0.019 sec/batch)\n",
      "epoch 51, step 700 / 900, loss = 1.14 (0.018 sec/batch)\n",
      "epoch 51, step 750 / 900, loss = 1.36 (0.018 sec/batch)\n",
      "epoch 51, step 800 / 900, loss = 1.24 (0.018 sec/batch)\n",
      "epoch 51, step 850 / 900, loss = 1.27 (0.018 sec/batch)\n",
      "epoch 51, step 900 / 900, loss = 1.02 (0.018 sec/batch)\n",
      "Train error:\n",
      " accuracy = 56.18\n",
      " avg loss = 1.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 52.26\n",
      " avg loss = 1.37\n",
      "\n",
      "Epoch time: 22.022363901138306\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 52, step 50 / 900, loss = 1.29 (0.019 sec/batch)\n",
      "epoch 52, step 100 / 900, loss = 1.34 (0.018 sec/batch)\n",
      "epoch 52, step 150 / 900, loss = 1.30 (0.018 sec/batch)\n",
      "epoch 52, step 200 / 900, loss = 1.19 (0.018 sec/batch)\n",
      "epoch 52, step 250 / 900, loss = 1.23 (0.018 sec/batch)\n",
      "epoch 52, step 300 / 900, loss = 1.28 (0.018 sec/batch)\n",
      "epoch 52, step 350 / 900, loss = 1.17 (0.018 sec/batch)\n",
      "epoch 52, step 400 / 900, loss = 0.82 (0.019 sec/batch)\n",
      "epoch 52, step 450 / 900, loss = 1.25 (0.019 sec/batch)\n",
      "epoch 52, step 500 / 900, loss = 1.12 (0.019 sec/batch)\n",
      "epoch 52, step 550 / 900, loss = 1.37 (0.018 sec/batch)\n",
      "epoch 52, step 600 / 900, loss = 1.39 (0.018 sec/batch)\n",
      "epoch 52, step 650 / 900, loss = 1.29 (0.018 sec/batch)\n",
      "epoch 52, step 700 / 900, loss = 1.30 (0.018 sec/batch)\n",
      "epoch 52, step 750 / 900, loss = 1.28 (0.019 sec/batch)\n",
      "epoch 52, step 800 / 900, loss = 1.62 (0.018 sec/batch)\n",
      "epoch 52, step 850 / 900, loss = 1.17 (0.018 sec/batch)\n",
      "epoch 52, step 900 / 900, loss = 1.38 (0.018 sec/batch)\n",
      "Train error:\n",
      " accuracy = 56.25\n",
      " avg loss = 1.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 52.98\n",
      " avg loss = 1.36\n",
      "\n",
      "Epoch time: 22.014434099197388\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 53, step 50 / 900, loss = 1.26 (0.018 sec/batch)\n",
      "epoch 53, step 100 / 900, loss = 1.61 (0.018 sec/batch)\n",
      "epoch 53, step 150 / 900, loss = 1.55 (0.019 sec/batch)\n",
      "epoch 53, step 200 / 900, loss = 1.03 (0.018 sec/batch)\n",
      "epoch 53, step 250 / 900, loss = 1.35 (0.019 sec/batch)\n",
      "epoch 53, step 300 / 900, loss = 0.96 (0.019 sec/batch)\n",
      "epoch 53, step 350 / 900, loss = 1.18 (0.018 sec/batch)\n",
      "epoch 53, step 400 / 900, loss = 1.39 (0.018 sec/batch)\n",
      "epoch 53, step 450 / 900, loss = 1.37 (0.018 sec/batch)\n",
      "epoch 53, step 500 / 900, loss = 1.07 (0.020 sec/batch)\n",
      "epoch 53, step 550 / 900, loss = 1.07 (0.018 sec/batch)\n",
      "epoch 53, step 600 / 900, loss = 1.35 (0.018 sec/batch)\n",
      "epoch 53, step 650 / 900, loss = 1.53 (0.019 sec/batch)\n",
      "epoch 53, step 700 / 900, loss = 1.15 (0.018 sec/batch)\n",
      "epoch 53, step 750 / 900, loss = 1.36 (0.018 sec/batch)\n",
      "epoch 53, step 800 / 900, loss = 1.18 (0.018 sec/batch)\n",
      "epoch 53, step 850 / 900, loss = 1.15 (0.019 sec/batch)\n",
      "epoch 53, step 900 / 900, loss = 1.07 (0.019 sec/batch)\n",
      "Train error:\n",
      " accuracy = 56.22\n",
      " avg loss = 1.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 52.10\n",
      " avg loss = 1.37\n",
      "\n",
      "Epoch time: 22.00482678413391\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n",
      "epoch 54, step 50 / 900, loss = 1.07 (0.019 sec/batch)\n",
      "epoch 54, step 100 / 900, loss = 1.25 (0.018 sec/batch)\n",
      "epoch 54, step 150 / 900, loss = 1.45 (0.019 sec/batch)\n",
      "epoch 54, step 200 / 900, loss = 1.47 (0.018 sec/batch)\n",
      "epoch 54, step 250 / 900, loss = 1.05 (0.018 sec/batch)\n",
      "epoch 54, step 300 / 900, loss = 1.27 (0.018 sec/batch)\n",
      "epoch 54, step 350 / 900, loss = 1.11 (0.018 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 54, step 400 / 900, loss = 1.50 (0.018 sec/batch)\n",
      "epoch 54, step 450 / 900, loss = 1.35 (0.018 sec/batch)\n",
      "epoch 54, step 500 / 900, loss = 1.21 (0.019 sec/batch)\n",
      "epoch 54, step 550 / 900, loss = 1.57 (0.018 sec/batch)\n",
      "epoch 54, step 600 / 900, loss = 1.20 (0.018 sec/batch)\n",
      "epoch 54, step 650 / 900, loss = 1.11 (0.018 sec/batch)\n",
      "epoch 54, step 700 / 900, loss = 1.25 (0.018 sec/batch)\n",
      "epoch 54, step 750 / 900, loss = 1.14 (0.018 sec/batch)\n",
      "epoch 54, step 800 / 900, loss = 1.18 (0.018 sec/batch)\n",
      "epoch 54, step 850 / 900, loss = 1.28 (0.019 sec/batch)\n",
      "epoch 54, step 900 / 900, loss = 1.28 (0.019 sec/batch)\n",
      "Train error:\n",
      " accuracy = 56.09\n",
      " avg loss = 1.25\n",
      "\n",
      "Validation error:\n",
      " accuracy = 52.22\n",
      " avg loss = 1.37\n",
      "\n",
      "Epoch time: 22.048449277877808\n",
      "Plotting in:  C:\\Users\\Korisnik\\Desktop\\du_lab2\\Deep-Learning\\2_lab\\zad4_images\\training_plot.pdf\n"
     ]
    }
   ],
   "source": [
    "session = tf.Session()\n",
    "train_start = time.time()\n",
    "\n",
    "plot_data={}\n",
    "train_cifar(session, train_x, train_y, valid_x, valid_y, config, plot_data)\n",
    "\n",
    "print(\"Total train time:\", time.time() - train_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_training_progress(plot_data)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "import os\n",
    "import math\n",
    "\n",
    "import skimage as ski\n",
    "import skimage.io\n",
    "\n",
    "from im2col_cython import col2im_cython, im2col_cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_init = np.zeros\n",
    "\n",
    "def variance_scaling_initializer(shape, fan_in, factor=2.0, seed=None):\n",
    "    sigma = np.sqrt(factor / fan_in)\n",
    "    return stats.truncnorm(-2, 2, loc=0, scale=sigma).rvs(shape)\n",
    "\n",
    "\n",
    "# -- ABSTRACT CLASS DEFINITION --\n",
    "class Layer(metaclass = ABCMeta):\n",
    "    \"Interface for layers\"\n",
    "    # See documentation of abstract base classes (ABC): https://docs.python.org/3/library/abc.html\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          inputs: ndarray tensor.\n",
    "        Returns:\n",
    "          ndarray tensor, result of the forward pass.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward_inputs(self, grads):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          grads: gradient of the loss with respect to the output of the layer.\n",
    "        Returns:\n",
    "          Gradient of the loss with respect to the input of the layer.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def backward_params(self, grads):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          grads: gradient of the loss with respect to the output of the layer.\n",
    "        Returns:\n",
    "          Gradient of the loss with respect to all the parameters of the layer as a list\n",
    "          [[w0, g0], ..., [wk, gk], self.name] where w are parameter weights and g their gradient.\n",
    "          Note that wk and gk must have the same shape.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "# -- CONVOLUTION LAYER --\n",
    "class Convolution(Layer):\n",
    "    \"N-dimensional convolution layer\"\n",
    "\n",
    "    def __init__(self, input_layer, num_filters, kernel_size, name, padding='SAME',\n",
    "               weights_initializer_fn=variance_scaling_initializer,\n",
    "               bias_initializer_fn=zero_init):\n",
    "        self.input_shape = input_layer.shape\n",
    "        N, C, H, W = input_layer.shape\n",
    "        self.C = C\n",
    "        self.N = N\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        assert kernel_size % 2 == 1\n",
    "\n",
    "        self.padding = padding\n",
    "\n",
    "        if padding == 'SAME':\n",
    "          # with zero padding\n",
    "          self.shape = (N, num_filters, H, W)\n",
    "          self.pad = (kernel_size - 1) // 2\n",
    "        else:\n",
    "          # without padding\n",
    "          self.shape = (N, num_filters, H - kernel_size + 1, W - kernel_size + 1)\n",
    "          self.pad = 0\n",
    "\n",
    "        fan_in = C * kernel_size**2\n",
    "        self.weights = weights_initializer_fn([num_filters, kernel_size**2 * C], fan_in)\n",
    "        self.bias = bias_initializer_fn([num_filters])\n",
    "        # this implementation doesn't support strided convolutions\n",
    "        self.stride = 1\n",
    "        self.name = name\n",
    "        self.has_params = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        k = self.kernel_size\n",
    "        self.x_cols = im2col_cython(x, k, k, self.pad, self.stride)\n",
    "        res = self.weights.dot(self.x_cols) + self.bias.reshape(-1, 1)\n",
    "        N, C, H, W = x.shape\n",
    "        out = res.reshape(self.num_filters, self.shape[2], self.shape[3], N)\n",
    "        return out.transpose(3, 0, 1, 2)\n",
    "\n",
    "    def backward_inputs(self, grad_out):\n",
    "        # nice trick from CS231n, backward pass can be done with just matrix mul and col2im\n",
    "        grad_out = grad_out.transpose(1, 2, 3, 0).reshape(self.num_filters, -1)\n",
    "        grad_x_cols = self.weights.T.dot(grad_out)\n",
    "        N, C, H, W = self.input_shape\n",
    "        k = self.kernel_size\n",
    "        grad_x = col2im_cython(grad_x_cols, N, C, H, W, k, k, self.pad, self.stride)\n",
    "        return grad_x\n",
    "\n",
    "    def backward_params(self, grad_out):\n",
    "        grad_bias = np.sum(grad_out, axis=(0, 2, 3))\n",
    "        grad_out = grad_out.transpose(1, 2, 3, 0).reshape(self.num_filters, -1)\n",
    "        grad_weights = grad_out.dot(self.x_cols.T).reshape(self.weights.shape)\n",
    "        return [[self.weights, grad_weights], [self.bias, grad_bias], self.name]\n",
    "\n",
    "\n",
    "class MaxPooling(Layer):\n",
    "    def __init__(self, input_layer, name, pool_size=2, stride=2):\n",
    "        self.name = name\n",
    "        self.input_shape = input_layer.shape\n",
    "        N, C, H, W = self.input_shape\n",
    "        self.stride = stride\n",
    "        self.shape = (N, C, H // stride, W // stride)\n",
    "        self.pool_size = pool_size\n",
    "        assert pool_size == stride, 'Invalid pooling params'\n",
    "        assert H % pool_size == 0\n",
    "        assert W % pool_size == 0\n",
    "        self.has_params = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        self.input_shape = x.shape\n",
    "        # with this clever reshaping we can implement pooling where pool_size == stride\n",
    "        self.x = x.reshape(N, C, H // self.pool_size, self.pool_size,\n",
    "                           W // self.pool_size, self.pool_size)\n",
    "        self.out = self.x.max(axis=3).max(axis=4)\n",
    "        # if you are returning class member be sure to return a copy\n",
    "        return self.out.copy()\n",
    "\n",
    "    def backward_inputs(self, grad_out):\n",
    "        grad_x = np.zeros_like(self.x)\n",
    "        out_newaxis = self.out[:, :, :, np.newaxis, :, np.newaxis]\n",
    "        mask = (self.x == out_newaxis)\n",
    "        dout_newaxis = grad_out[:, :, :, np.newaxis, :, np.newaxis]\n",
    "        dout_broadcast, _ = np.broadcast_arrays(dout_newaxis, grad_x)\n",
    "        # this is almost the same as the real backward pass\n",
    "        grad_x[mask] = dout_broadcast[mask]\n",
    "        # in the very rare case that more then one input have the same max value\n",
    "        # we can aprox the real gradient routing by evenly distributing across multiple inputs\n",
    "        # but in almost all cases this sum will be 1\n",
    "        grad_x /= np.sum(mask, axis=(3, 5), keepdims=True)\n",
    "        grad_x = grad_x.reshape(self.input_shape)\n",
    "        return grad_x\n",
    "\n",
    "\n",
    "class Flatten(Layer):\n",
    "    def __init__(self, input_layer, name):\n",
    "        self.input_shape = input_layer.shape\n",
    "        self.N = self.input_shape[0]\n",
    "        self.num_outputs = 1\n",
    "        for i in range(1, len(self.input_shape)):\n",
    "            self.num_outputs *= self.input_shape[i]\n",
    "        self.shape = (self.N, self.num_outputs)\n",
    "        self.has_params = False\n",
    "        self.name = name\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.input_shape = inputs.shape\n",
    "        inputs_flat = inputs.reshape(self.input_shape[0], -1)\n",
    "        self.shape = inputs_flat.shape\n",
    "        return inputs_flat\n",
    "\n",
    "    def backward_inputs(self, grads):\n",
    "        return grads.reshape(self.input_shape)\n",
    "\n",
    "\n",
    "class FC(Layer):\n",
    "    def __init__(self, input_layer, num_outputs, name,\n",
    "               weights_initializer_fn=variance_scaling_initializer,\n",
    "               bias_initializer_fn=zero_init):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          input_layer: layer below\n",
    "          num_outputs: number of neurons in this layer\n",
    "          weights_initializer_fn: initializer function for weights,\n",
    "          bias_initializer_fn: initializer function for biases\n",
    "        \"\"\"\n",
    "\n",
    "        self.input_shape = input_layer.shape\n",
    "        self.N = self.input_shape[0]\n",
    "        self.shape = (self.N, num_outputs)\n",
    "        self.num_outputs = num_outputs\n",
    "\n",
    "        self.num_inputs = 1\n",
    "        for i in range(1, len(self.input_shape)):\n",
    "            self.num_inputs *= self.input_shape[i]\n",
    "\n",
    "        self.weights = weights_initializer_fn([num_outputs, self.num_inputs], fan_in=self.num_inputs)\n",
    "        self.bias = bias_initializer_fn([num_outputs])\n",
    "        self.name = name\n",
    "        self.has_params = True\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          inputs: ndarray of shape (N, num_inputs)\n",
    "        Returns:\n",
    "          An ndarray of shape (N, num_outputs)\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def backward_inputs(self, grads):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          grads: ndarray of shape (N, num_outputs)\n",
    "        Returns:\n",
    "          An ndarray of shape (N, num_inputs)\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def backward_params(self, grads):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          grads: ndarray of shape (N, num_outputs)\n",
    "        Returns:\n",
    "          List of params and gradient pairs.\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        grad_weights = ...\n",
    "        grad_bias = ...\n",
    "        return [[self.weights, grad_weights], [self.bias, grad_bias], self.name]\n",
    "\n",
    "\n",
    "\n",
    "class ReLU(Layer):\n",
    "    def __init__(self, input_layer, name):\n",
    "        self.shape = input_layer.shape\n",
    "        self.name = name\n",
    "        self.has_params = False\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          inputs: ndarray of shape (N, C, H, W).\n",
    "        Returns:\n",
    "          ndarray of shape (N, C, H, W).\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def backward_inputs(self, grads):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          grads: ndarray of shape (N, C, H, W).\n",
    "        Returns:\n",
    "          ndarray of shape (N, C, H, W).\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "\n",
    "class SoftmaxCrossEntropyWithLogits():\n",
    "    def __init__(self):\n",
    "        self.has_params = False\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x: ndarray of shape (N, num_classes).\n",
    "          y: ndarray of shape (N, num_classes).\n",
    "        Returns:\n",
    "          Scalar, average loss over N examples.\n",
    "          It is better to compute average loss here instead of just sum\n",
    "          because then learning rate and weight decay won't depend on batch size.\n",
    "\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def backward_inputs(self, x, y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x: ndarray of shape (N, num_classes).\n",
    "          y: ndarray of shape (N, num_classes).\n",
    "        Returns:\n",
    "          Gradient with respect to the x, ndarray of shape (N, num_classes).\n",
    "        \"\"\"\n",
    "        # Hint: don't forget that we took the average in the forward pass\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "\n",
    "class L2Regularizer():\n",
    "    def __init__(self, weights, weight_decay, name):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          weights: parameters which will be regularizerized\n",
    "          weight_decay: lambda, regularization strength\n",
    "          name: layer name\n",
    "        \"\"\"\n",
    "        # this is still a reference to original tensor so don't change self.weights\n",
    "        self.weights = weights\n",
    "        self.weight_decay = weight_decay\n",
    "        self.name = name\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "         Returns:\n",
    "          Scalar, loss due to the L2 regularization.\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def backward_params(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "          Gradient of the L2 loss with respect to the regularized weights.\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        grad_weights = ...\n",
    "        return [[self.weights, grad_weights], self.name]\n",
    "\n",
    "\n",
    "class RegularizedLoss():\n",
    "    def __init__(self, data_loss, regularizer_losses):\n",
    "        self.data_loss = data_loss\n",
    "        self.regularizer_losses = regularizer_losses\n",
    "        self.has_params = True\n",
    "        self.name = 'RegularizedLoss'\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        loss_val = self.data_loss.forward(x, y)\n",
    "        for loss in self.regularizer_losses:\n",
    "            loss_val += loss.forward()\n",
    "        return loss_val\n",
    "\n",
    "    def backward_inputs(self, x, y):\n",
    "        return self.data_loss.backward_inputs(x, y)\n",
    "\n",
    "    def backward_params(self):\n",
    "        grads = []\n",
    "        for loss in self.regularizer_losses:\n",
    "            grads += [loss.backward_params()]\n",
    "        return grads\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "def eval_numerical_gradient(f, x, df, h=1e-5):\n",
    "    \"\"\"\n",
    "    Evaluate a numeric gradient for a function that accepts a numpy\n",
    "    array and returns a numpy array.\n",
    "    - f should be a function that takes a single argument\n",
    "    - x is the point (numpy array) to evaluate the gradient at\n",
    "    \"\"\"\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h\n",
    "        # evaluate f(x + h)\n",
    "        pos = f(x.copy()).copy()\n",
    "        x[ix] = oldval - h\n",
    "        # evaluate f(x - h)\n",
    "        neg = f(x.copy()).copy()\n",
    "        x[ix] = oldval\n",
    "\n",
    "        # compute the partial derivative with centered formula\n",
    "        grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n",
    "        # step to next dimension\n",
    "        it.iternext()\n",
    "    return grad\n",
    "\n",
    "def check_grad_inputs(layer, x, grad_out):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    layer: Layer object\n",
    "    x: ndarray tensor input data\n",
    "    grad_out: ndarray tensor gradient from the next layer\n",
    "    \"\"\"\n",
    "    grad_x_num = eval_numerical_gradient(layer.forward, x, grad_out)\n",
    "    grad_x = layer.backward_inputs(grad_out)\n",
    "    print(\"Relative error = \", rel_error(grad_x_num, grad_x))\n",
    "    print(\"Error norm = \", np.linalg.norm(grad_x_num - grad_x))\n",
    "\n",
    "def check_grad_params(layer, x, w, b, grad_out):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    layer: Layer object\n",
    "    x: ndarray tensor input data\n",
    "    w: ndarray tensor layer weights\n",
    "    b: ndarray tensor layer biases\n",
    "    grad_out: ndarray tensor gradient from the next layer\n",
    "    \"\"\"\n",
    "    func = lambda params: layer.forward(x)\n",
    "    grad_w_num = eval_numerical_gradient(func, w, grad_out)\n",
    "    grad_b_num = eval_numerical_gradient(func, b, grad_out)\n",
    "    grads = layer.backward_params(grad_out)\n",
    "    grad_w = grads[0][1]\n",
    "    grad_b = grads[1][1]\n",
    "    print(\"Check weights:\")\n",
    "    print(\"Relative error = \", rel_error(grad_w_num, grad_w))\n",
    "    print(\"Error norm = \", np.linalg.norm(grad_w_num - grad_w))\n",
    "    print(\"Check biases:\")\n",
    "    print(\"Relative error = \", rel_error(grad_b_num, grad_b))\n",
    "    print(\"Error norm = \", np.linalg.norm(grad_b_num - grad_b))\n",
    "\n",
    "print(\"Convolution\")\n",
    "x = np.random.randn(4, 3, 5, 5)\n",
    "grad_out = np.random.randn(4, 2, 5, 5)\n",
    "conv = Convolution(x, 2, 3, \"conv1\")\n",
    "print(\"Check grad wrt input\")\n",
    "check_grad_inputs(conv, x, grad_out)\n",
    "print(\"Check grad wrt params\")\n",
    "check_grad_params(conv, x, conv.weights, conv.bias, grad_out)\n",
    "\n",
    "print(\"\\nMaxPooling\")\n",
    "x = np.random.randn(5, 4, 8, 8)\n",
    "grad_out = np.random.randn(5, 4, 4, 4)\n",
    "pool = MaxPooling(x, \"pool\", 2, 2)\n",
    "print(\"Check grad wrt input\")\n",
    "check_grad_inputs(pool, x, grad_out)\n",
    "\n",
    "print(\"\\nReLU\")\n",
    "x = np.random.randn(4, 3, 5, 5)\n",
    "grad_out = np.random.randn(4, 3, 5, 5)\n",
    "relu = ReLU(x, \"relu\")\n",
    "print(\"Check grad wrt input\")\n",
    "check_grad_inputs(relu, x, grad_out)\n",
    "\n",
    "print(\"\\nFC\")\n",
    "x = np.random.randn(20, 40)\n",
    "grad_out = np.random.randn(20, 30)\n",
    "fc = FC(x, 30, \"fc\")\n",
    "print(\"Check grad wrt input\")\n",
    "check_grad_inputs(fc, x, grad_out)\n",
    "print(\"Check grad wrt params\")\n",
    "check_grad_params(fc, x, fc.weights, fc.bias, grad_out)\n",
    "\n",
    "print(\"\\nSoftmaxCrossEntropyWithLogits\")\n",
    "x = np.random.randn(50, 20)\n",
    "y = np.zeros([50, 20])\n",
    "y[:,0] = 1\n",
    "loss = SoftmaxCrossEntropyWithLogits()\n",
    "grad_x_num = eval_numerical_gradient(lambda x: loss.forward(x, y), x, 1)\n",
    "out = loss.forward(x, y)\n",
    "grad_x = loss.backward_inputs(x, y)\n",
    "print(\"Relative error = \", rel_error(grad_x_num, grad_x))\n",
    "print(\"Error norm = \", np.linalg.norm(grad_x_num - grad_x))\n",
    "\n",
    "print(\"\\nL2Regularizer\")\n",
    "x = np.random.randn(5, 4, 8, 8)\n",
    "grad_out = np.random.randn(5, 4, 4, 4)\n",
    "l2reg = L2Regularizer(x, 1e-2, 'L2reg')\n",
    "print(\"Check grad wrt params\")\n",
    "func = lambda params: l2reg.forward()\n",
    "grad_num = eval_numerical_gradient(func, l2reg.weights, 1)\n",
    "grads = l2reg.backward_params()\n",
    "grad = grads[0][1]\n",
    "print(\"Relative error = \", rel_error(grad_num, grad))\n",
    "print(\"Error norm = \", np.linalg.norm(grad_num - grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_pass(net, inputs):\n",
    "    output = inputs\n",
    "    for layer in net:\n",
    "        output = layer.forward(output)\n",
    "    return output\n",
    "\n",
    "\n",
    "def backward_pass(net, loss, x, y):\n",
    "    grads = []\n",
    "    grad_out = loss.backward_inputs(x, y)\n",
    "    if loss.has_params:\n",
    "        grads += loss.backward_params()\n",
    "    for layer in reversed(net):\n",
    "        grad_inputs = layer.backward_inputs(grad_out)\n",
    "        if layer.has_params:\n",
    "            grads += [layer.backward_params(grad_out)]\n",
    "        grad_out = grad_inputs\n",
    "    return grads\n",
    "\n",
    "def sgd_update_params(grads, config):\n",
    "    lr = config['lr']\n",
    "    for layer_grads in grads:\n",
    "        for i in range(len(layer_grads) - 1):\n",
    "            params = layer_grads[i][0]\n",
    "            grads = layer_grads[i][1]\n",
    "            #print(layer_grads[-1], \" -> \", grads.sum())\n",
    "            params -= lr * grads\n",
    "\n",
    "\n",
    "def draw_conv_filters(epoch, step, layer, save_dir):\n",
    "    C = layer.C\n",
    "    w = layer.weights.copy()\n",
    "    num_filters = w.shape[0]\n",
    "    k = int(np.sqrt(w.shape[1] / C))\n",
    "    w = w.reshape(num_filters, C, k, k)\n",
    "    w -= w.min()\n",
    "    w /= w.max()\n",
    "    border = 1\n",
    "    cols = 8\n",
    "    rows = math.ceil(num_filters / cols)\n",
    "    width = cols * k + (cols-1) * border\n",
    "    height = rows * k + (rows-1) * border\n",
    "    #for i in range(C):\n",
    "    for i in range(1):\n",
    "        img = np.zeros([height, width])\n",
    "        \n",
    "        for j in range(num_filters):\n",
    "            r = int(j / cols) * (k + border)\n",
    "            c = int(j % cols) * (k + border)\n",
    "            img[r:r+k,c:c+k] = w[j,i]\n",
    "            \n",
    "        filename = '%s_epoch_%02d_step_%06d_input_%03d.png' % (layer.name, epoch, step, i)\n",
    "        ski.io.imsave(os.path.join(save_dir, filename), img)\n",
    "\n",
    "\n",
    "def train(train_x, train_y, valid_x, valid_y, net, loss, config):\n",
    "    lr_policy = config['lr_policy']\n",
    "    batch_size = config['batch_size']\n",
    "    max_epochs = config['max_epochs']\n",
    "    save_dir = config['save_dir']\n",
    "    num_examples = train_x.shape[0]\n",
    "    assert num_examples % batch_size == 0\n",
    "    num_batches = num_examples // batch_size\n",
    "    for epoch in range(1, max_epochs+1):\n",
    "        if epoch in lr_policy:\n",
    "            solver_config = lr_policy[epoch]\n",
    "            \n",
    "        cnt_correct = 0\n",
    "        #for i in range(num_batches):\n",
    "        # shuffle the data at the beggining of each epoch\n",
    "        permutation_idx = np.random.permutation(num_examples)\n",
    "        train_x = train_x[permutation_idx]\n",
    "        train_y = train_y[permutation_idx]\n",
    "        #for i in range(100):\n",
    "        for i in range(num_batches):\n",
    "            # store mini-batch to ndarray\n",
    "            batch_x = train_x[i*batch_size:(i+1)*batch_size, :]\n",
    "            batch_y = train_y[i*batch_size:(i+1)*batch_size, :]\n",
    "            logits = forward_pass(net, batch_x)\n",
    "            loss_val = loss.forward(logits, batch_y)\n",
    "            # compute classification accuracy\n",
    "            yp = np.argmax(logits, 1)\n",
    "            yt = np.argmax(batch_y, 1)\n",
    "            cnt_correct += (yp == yt).sum()\n",
    "            grads = backward_pass(net, loss, logits, batch_y)\n",
    "            sgd_update_params(grads, solver_config)\n",
    "            \n",
    "            if i % 5 == 0:\n",
    "                print(\"epoch %d, step %d/%d, batch loss = %.2f\" % (epoch, i*batch_size, num_examples, loss_val))\n",
    "                \n",
    "            if i % 100 == 0:\n",
    "                draw_conv_filters(epoch, i*batch_size, net[0], save_dir)\n",
    "                #draw_conv_filters(epoch, i*batch_size, net[3])\n",
    "            if i > 0 and i % 50 == 0:\n",
    "                print(\"Train accuracy = %.2f\" % (cnt_correct / ((i+1)*batch_size) * 100))\n",
    "        print(\"Train accuracy = %.2f\" % (cnt_correct / num_examples * 100))\n",
    "        evaluate(\"Validation\", valid_x, valid_y, net, loss, config)\n",
    "    return net\n",
    "\n",
    "\n",
    "def evaluate(name, x, y, net, loss, config):\n",
    "    print(\"\\nRunning evaluation: \", name)\n",
    "    batch_size = config['batch_size']\n",
    "    num_examples = x.shape[0]\n",
    "    assert num_examples % batch_size == 0\n",
    "    num_batches = num_examples // batch_size\n",
    "    cnt_correct = 0\n",
    "    loss_avg = 0\n",
    "    for i in range(num_batches):\n",
    "        batch_x = x[i*batch_size:(i+1)*batch_size, :]\n",
    "        batch_y = y[i*batch_size:(i+1)*batch_size, :]\n",
    "        logits = forward_pass(net, batch_x)\n",
    "        yp = np.argmax(logits, 1)\n",
    "        yt = np.argmax(batch_y, 1)\n",
    "        cnt_correct += (yp == yt).sum()\n",
    "        loss_val = loss.forward(logits, batch_y)\n",
    "        loss_avg += loss_val\n",
    "        #print(\"step %d / %d, loss = %.2f\" % (i*batch_size, num_examples, loss_val / batch_size))\n",
    "    valid_acc = cnt_correct / num_examples * 100\n",
    "    loss_avg /= num_batches\n",
    "    print(name + \" accuracy = %.2f\" % valid_acc)\n",
    "    print(name + \" avg loss = %.2f\\n\" % loss_avg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import nn\n",
    "\n",
    "DATA_DIR = '/home/kivan/datasets/MNIST/'\n",
    "SAVE_DIR = \"/home/kivan/source/fer/out/\"\n",
    "\n",
    "config = {}\n",
    "config['max_epochs'] = 8\n",
    "config['batch_size'] = 50\n",
    "config['save_dir'] = SAVE_DIR\n",
    "config['lr_policy'] = {1:{'lr':1e-1}, 3:{'lr':1e-2}, 5:{'lr':1e-3}, 7:{'lr':1e-4}}\n",
    "\n",
    "#np.random.seed(100) \n",
    "np.random.seed(int(time.time() * 1e6) % 2**31)\n",
    "dataset = input_data.read_data_sets(DATA_DIR, one_hot=True)\n",
    "train_x = dataset.train.images\n",
    "train_x = train_x.reshape([-1, 1, 28, 28])\n",
    "train_y = dataset.train.labels\n",
    "valid_x = dataset.validation.images\n",
    "valid_x = valid_x.reshape([-1, 1, 28, 28])\n",
    "valid_y = dataset.validation.labels\n",
    "test_x = dataset.test.images\n",
    "test_x = test_x.reshape([-1, 1, 28, 28])\n",
    "test_y = dataset.test.labels\n",
    "train_mean = train_x.mean()\n",
    "train_x -= train_mean\n",
    "valid_x -= train_mean\n",
    "test_x -= train_mean\n",
    "\n",
    "\n",
    "net = []\n",
    "inputs = np.random.randn(config['batch_size'], 1, 28, 28)\n",
    "net += [layers.Convolution(inputs, 16, 5, \"conv1\")]\n",
    "net += [layers.MaxPooling(net[-1], \"pool1\")]\n",
    "net += [layers.ReLU(net[-1], \"relu1\")]\n",
    "net += [layers.Convolution(net[-1], 32, 5, \"conv2\")]\n",
    "net += [layers.MaxPooling(net[-1], \"pool2\")]\n",
    "net += [layers.ReLU(net[-1], \"relu2\")]\n",
    "# out = 7x7\n",
    "net += [layers.Flatten(net[-1], \"flatten3\")]\n",
    "net += [layers.FC(net[-1], 512, \"fc3\")]\n",
    "net += [layers.ReLU(net[-1], \"relu3\")]\n",
    "net += [layers.FC(net[-1], 10, \"logits\")]\n",
    "\n",
    "loss = layers.SoftmaxCrossEntropyWithLogits()\n",
    "\n",
    "nn.train(train_x, train_y, valid_x, valid_y, net, loss, config)\n",
    "nn.evaluate(\"Test\", test_x, test_y, net, loss, config)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. vježba: modeliranje nizova povratnim neuronskim mrežama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pdb\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadatak 1: učitavanje podataka i batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hardcoded paths - change if necessary\n",
    "root = 'data'\n",
    "\n",
    "# this one you need to download from the dataset\n",
    "full_dataset = 'data/movie_lines.txt'\n",
    "\n",
    "output_destination = 'data/selected_conversations.txt'\n",
    "movie_selection = 'data/selected_movies.txt'\n",
    "\n",
    "# separator used in the original dataset\n",
    "separator = ' +++$+++ '\n",
    "\n",
    "# movie ID file\n",
    "MOVIE_ID = 0\n",
    "\n",
    "# full conversation dataset file\n",
    "MOVIE_ID_FULL = 2\n",
    "# reverse indexing\n",
    "CHARACTER_NAME = -2\n",
    "CHARACTER_LINE = -1\n",
    "\n",
    "# keep just these characters for simplicity (and utf8 breaking)\n",
    "repl = r'[^A-Za-z0-9()\\,!\\?\\'\\`\\. ]'\n",
    "\n",
    "\n",
    "# regex replace\n",
    "def filter(string):\n",
    "    return re.sub(repl, '', string)\n",
    "\n",
    "\n",
    "# from a movie ID string (e.g. M134), output the number (134)\n",
    "def number_from_id(id):\n",
    "    return int(id[1:])\n",
    "\n",
    "\n",
    "# read just movie ID's, rest is for readability\n",
    "def read_selected(path_to_selected_movies):\n",
    "    selected_movies = set()\n",
    "\n",
    "    with open(path_to_selected_movies, 'r') as infile:\n",
    "        for line in infile:\n",
    "            parts = line.strip().split(separator)\n",
    "            selected_movies.add(parts[MOVIE_ID].strip())\n",
    "    return selected_movies\n",
    "\n",
    "\n",
    "# select and write to output file\n",
    "def select_and_write(path_to_full_dataset, path_to_output, selected_movies):\n",
    "    movies = {}\n",
    "\n",
    "    with open(path_to_full_dataset, 'r', encoding=\"ISO-8859-1\") as infile:\n",
    "\n",
    "        for line in infile:\n",
    "\n",
    "            parts = line.strip().split(separator)\n",
    "\n",
    "            if parts[MOVIE_ID_FULL].strip() not in selected_movies:\n",
    "                continue\n",
    "\n",
    "            # take data and transform to tuple\n",
    "            ID = parts[MOVIE_ID_FULL]\n",
    "            char_name = parts[CHARACTER_NAME]\n",
    "            char_line = parts[CHARACTER_LINE]\n",
    "\n",
    "            tup = (number_from_id(ID), char_name, char_line)\n",
    "\n",
    "            # add to map\n",
    "            if ID not in movies:\n",
    "                movies[ID] = []\n",
    "            movies[ID].append(tup)\n",
    "\n",
    "    with open(path_to_output, 'w') as out:\n",
    "        for movie in movies:\n",
    "            # sort by line number\n",
    "            dialogue = sorted(movies[movie], key=lambda t: t[0])\n",
    "            for n, name, text in dialogue:\n",
    "                out.write(filter(name) + ':\\n' + filter(text) + '\\n\\n')\n",
    "\n",
    "\n",
    "def main():\n",
    "    # uses hardcoded paths\n",
    "    selection = os.path.join(root, movie_selection)\n",
    "    selected_movies = read_selected(selection)\n",
    "\n",
    "    dataset = os.path.join(root, full_dataset)\n",
    "    output = os.path.join(root, output_destination)\n",
    "    select_and_write(dataset, output, selected_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def preprocess(self, input_file):\n",
    "        with open(input_file, \"r\") as f:\n",
    "            data = f.read()\n",
    "\n",
    "        # count and sort most frequent characters\n",
    "        characters, counts = np.unique(list(data), return_index=True)\n",
    "        # self.sorted chars contains just the characters ordered descending by frequency\n",
    "        self.sorted_characters = characters[np.argsort(-counts)]\n",
    "        self.vocab_size = len(self.sorted_characters)\n",
    "\n",
    "        self.char2id = dict(zip(self.sorted_characters, range(len(self.sorted_characters)))) \n",
    "        # reverse the mapping\n",
    "        self.id2char = {k:v for v,k in self.char2id.items()}\n",
    "        # convert the data to ids\n",
    "        self.x = np.array(list(map(self.char2id.get, data)))\n",
    "\n",
    "    def encode(self, sequence):\n",
    "        encoded_sequence = np.array([self.char2id[c] for c in sequence])\n",
    "        return encoded_sequence\n",
    "\n",
    "    def decode(self, sequence):\n",
    "        decoded_sequence = [self.id2char[c] for c in sequence]\n",
    "        return decoded_sequence\n",
    "        \n",
    "    # creates minibatches with batch_size * sequence length letters\n",
    "    def create_minibatches(self, batch_size, sequence_length):\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.batch_index = 0\n",
    "        \n",
    "        self.num_batches = int(len(self.x) / (batch_size * sequence_length)) # calculate the number of batches\n",
    "        self.batches = np.zeros([self.num_batches, self.batch_size, self.sequence_length + 1], dtype=np.int32)  \n",
    "    \n",
    "        for minibatch_index in range(self.num_batches):\n",
    "            for sequence_index in range(self.batch_size):\n",
    "                start = minibatch_index * sequence_length + sequence_index * (self.num_batches * sequence_length)\n",
    "                end = start + self.sequence_length + 1 \n",
    "                self.batches[minibatch_index, sequence_index, :] = self.x[start : end]\n",
    "\n",
    "    def next_minibatch(self):        \n",
    "        if self.batch_index == self.num_batches:\n",
    "            self.batch_index = 0\n",
    "\n",
    "        batch = self.batches[self.batch_index, :, :]\n",
    "        self.batch_index += 1\n",
    "        \n",
    "        batch_x = batch[:, :-1]\n",
    "        batch_y = batch[:, 1:]\n",
    "        \n",
    "        return self.batch_index == self.num_batches, batch_x, batch_y\n",
    "    \n",
    "    def _as_one_hot(self, x, vocab):\n",
    "        n = len(x)\n",
    "        Yoh = np.zeros((n, vocab))\n",
    "        Yoh[np.arange(n), x] = 1\n",
    "        \n",
    "        return Yoh\n",
    "    \n",
    "    def one_hot(self, batch):\n",
    "        if batch.ndim == 1:\n",
    "            return self._as_one_hot(batch, self.vocab_size)\n",
    "        else:\n",
    "            return np.array([self._as_one_hot(b, self.vocab_size) for b in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "['D', 'a', 'y']\n",
      "['T', 'o', 'm', 'o', 'r', 'r', 'o', 'w']\n"
     ]
    }
   ],
   "source": [
    "# test 1\n",
    "dat = Dataset()\n",
    "dat.preprocess(\"data/selected_conversations.txt\")\n",
    "\n",
    "encoded = dat.encode(\"Day\")\n",
    "decoded = dat.decode(encoded)\n",
    "print(dat.one_hot(encoded))\n",
    "print(decoded)\n",
    "print(dat.decode(dat.encode(\"Tomorrow\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadatak 2: obična jednoslojna povratna neuronska mreža"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN():\n",
    "    # inicijalizacija parametara\n",
    "    def __init__(self, hidden_size, sequence_length, vocabulary_size, learning_rate):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        print(\"Number of parameters in the RNN: \" + str(2*vocabulary_size*hidden_size + hidden_size + hidden_size*hidden_size + vocabulary_size))\n",
    "        print(\"Hidden size: \" + str(hidden_size))\n",
    "        print(\"Sequence length: \" + str(sequence_length))\n",
    "        print(\"Vocabulary size: \" + str(vocabulary_size))\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        self.U = np.random.normal(size=[vocabulary_size, hidden_size], scale=1.0 / np.sqrt(hidden_size))  # ... input projection\n",
    "        self.W = np.random.normal(size=[hidden_size, hidden_size], scale=1.0 / np.sqrt(hidden_size))  # ... hidden-to-hidden projection\n",
    "        self.b = np.zeros([1, hidden_size])\n",
    "\n",
    "        self.V = np.random.normal(size=[hidden_size, vocabulary_size], scale=1.0 / np.sqrt(vocabulary_size)) # ... output projection\n",
    "        self.c = np.zeros([1, vocabulary_size]) # ... output bias\n",
    "\n",
    "        # memory of past gradients - rolling sum of squares for Adagrad\n",
    "        self.memory_U = np.zeros_like(self.U)\n",
    "        self.memory_W = np.zeros_like(self.W)\n",
    "        self.memory_V = np.zeros_like(self.V)\n",
    "        self.memory_b = np.zeros_like(self.b)\n",
    "        self.memory_c = np.zeros_like(self.c)\n",
    "        \n",
    "    def rnn_step_forward(self, x, h_prev, U, W, b):\n",
    "        # A single time step forward of a recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity.\n",
    "\n",
    "        # x - input data (minibatch size x input dimension)\n",
    "        # h_prev - previous hidden state (minibatch size x hidden size)\n",
    "        # U - input projection matrix (input dimension x hidden size)\n",
    "        # W - hidden to hidden projection matrix (hidden size x hidden size)\n",
    "        # b - bias of shape (hidden size x 1)\n",
    "\n",
    "        h_current, cache = None, None\n",
    "        h_current = np.tanh(np.dot(h_prev, W) + np.dot(x, U) + b)\n",
    "        cache = (W, x, h_prev, h_current)\n",
    "\n",
    "        # return the new hidden state and a tuple of values needed for the backward step\n",
    "\n",
    "        return h_current, cache\n",
    "\n",
    "\n",
    "    def rnn_forward(self, x, h0, U, W, b):\n",
    "        # Full unroll forward of the recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity\n",
    "\n",
    "        # x - input data for the whole time-series (minibatch size x sequence_length x input dimension)\n",
    "        # h0 - initial hidden state (minibatch size x hidden size)\n",
    "        # U - input projection matrix (input dimension x hidden size)\n",
    "        # W - hidden to hidden projection matrix (hidden size x hidden size)\n",
    "        # b - bias of shape (hidden size x 1)\n",
    "        \n",
    "        h, cache = None, None\n",
    "        hs = [h0]\n",
    "        caches = []\n",
    "        \n",
    "        for sequence in range(self.sequence_length):\n",
    "            data = x[:, sequence, :]\n",
    "            h_current, cache_current = self.rnn_step_forward(data, hs[-1], U, W, b)\n",
    "            \n",
    "            hs.append(h_current)\n",
    "            caches.append(cache_current)\n",
    "            \n",
    "        # transposes the hs matrix according to indexes\n",
    "        hs = np.array(hs[1:]).transpose((1, 0, 2))\n",
    "        \n",
    "        # return the hidden states for the whole time series (T+1) and a tuple of values needed for the backward step\n",
    "        return hs, caches\n",
    "    \n",
    "    def rnn_step_backward(self, grad_next, cache):\n",
    "        # A single time step backward of a recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity.\n",
    "\n",
    "        # grad_next - upstream gradient of the loss with respect to the next hidden state and current output\n",
    "        # cache - cached information from the forward pass        \n",
    "        dh_prev, dU, dW, db = None, None, None, None\n",
    "        W, x, h_prev, h_current = cache\n",
    "        \n",
    "        # compute and return gradients with respect to each parameter\n",
    "        # HINT: you can use the chain rule to compute the derivative of the\n",
    "        # hyperbolic tangent function and use it to compute the gradient\n",
    "        # with respect to the remaining parameters\n",
    "        \n",
    "        # 65. i 66. slajd formule\n",
    "        \n",
    "        # gradijent tangensa hiperbolnog\n",
    "        d_tanh = 1 - h_current**2\n",
    "        \n",
    "        da = grad_next * (d_tanh)\n",
    "        \n",
    "        # gradijenti na parametre\n",
    "        dU = np.dot(x.T, da)\n",
    "        dW = np.dot(h_prev.T, da)\n",
    "        db = np.sum(da, axis=0)\n",
    "        \n",
    "        # gradijenti na ulaze\n",
    "        dh_prev = np.dot(da, W.T)\n",
    "\n",
    "        return dh_prev, dU, dW, db\n",
    "\n",
    "\n",
    "    def rnn_backward(self, dh, cache):\n",
    "        # Full unroll forward of the recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity\n",
    "\n",
    "        dU = np.zeros_like(self.U)\n",
    "        dW = np.zeros_like(self.W)\n",
    "        db = np.zeros_like(self.b)\n",
    "\n",
    "        # compute and return gradients with respect to each parameter\n",
    "        # for the whole time series.\n",
    "        # Why are we not computing the gradient with respect to inputs (x)?\n",
    "        # A: because we are using only one layer, there is nothing to propagate through the network\n",
    "        grads = np.zeros_like(dh[0])\n",
    "        for dh_t, cache_t in reversed(list(zip(dh, cache))):\n",
    "            grads, dU_t, dW_t, db_t = self.rnn_step_backward(dh_t + grads, cache_t)\n",
    "            \n",
    "            dU += dU_t\n",
    "            dW += dW_t\n",
    "            db += db_t\n",
    "        \n",
    "        return np.clip(dU, -5, 5), np.clip(dW, -5, 5), np.clip(db, -5, 5)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        x -= np.max(x)\n",
    "        logits_exp = np.exp(x)\n",
    "        return logits_exp / np.sum(logits_exp, axis=1, keepdims=True)\n",
    "    \n",
    "    def output(self, h, V, c):\n",
    "        # Calculate the output \n",
    "        return np.dot(h, V) + c\n",
    "\n",
    "    def output_loss_and_grads(self, h, V, c, y):\n",
    "        # Calculate the loss of the network for each of the outputs\n",
    "\n",
    "        # h - hidden states of the network for each timestep. \n",
    "        #     the dimensionality of h is (batch size x sequence length x hidden size (the initial state is irrelevant for the output)\n",
    "        # V - the output projection matrix of dimension hidden size x vocabulary size\n",
    "        # c - the output bias of dimension vocabulary size x 1\n",
    "        # y - the true class distribution - a tensor of dimension \n",
    "        #     batch_size x sequence_length x vocabulary size - you need to do this conversion prior to\n",
    "        #     passing the argument. A fast way to create a one-hot vector from\n",
    "        #     an id could be something like the following code:\n",
    "\n",
    "        #   y[batch_id][timestep] = np.zeros((vocabulary_size, 1))\n",
    "        #   y[batch_id][timestep][batch_y[timestep]] = 1\n",
    "\n",
    "        #     where y might be a list or a dictionary.\n",
    "\n",
    "        loss, dh, dV, dc = None, None, None, None\n",
    "        \n",
    "        loss = 0\n",
    "        dhs = []\n",
    "        dV = np.zeros_like(self.V)\n",
    "        dc = np.zeros_like(self.c)\n",
    "        \n",
    "        batch_size = len(h)\n",
    "        \n",
    "        # calculate the output (o) - unnormalized log probabilities of classes\n",
    "        # calculate yhat - softmax of the output\n",
    "        # calculate the cross-entropy loss\n",
    "        # calculate the derivative of the cross-entropy softmax loss with respect to the output (o)\n",
    "        # calculate the gradients with respect to the output parameters V and c\n",
    "        # calculate the gradients with respect to the hidden layer h\n",
    "        \n",
    "        for sequence in range(self.sequence_length):\n",
    "            y_true = y[:, sequence, :]\n",
    "            h_t = h[:, sequence, :]\n",
    "            \n",
    "            o = self.output(h_t, V, c)\n",
    "            yhat = self.softmax(o)\n",
    "            \n",
    "            # cross entropy log loss\n",
    "            loss -= np.sum(np.log(yhat) * y_true) / batch_size\n",
    "            \n",
    "            dO = (yhat - y_true)\n",
    "            dV += np.dot(h_t.T, dO)\n",
    "            dc += np.sum(dO, axis=0)\n",
    "            \n",
    "            dhs.append(np.dot(dO, V.T))\n",
    "\n",
    "        return loss, dhs, dV, dc\n",
    "    \n",
    "    # Adagrad parameter update\n",
    "    def update_parameters(self, batch_size, dU, dW, db, dV, dc):\n",
    "        # update memory matrices\n",
    "        # perform the Adagrad update of parameters\n",
    "        eps = 1e-7\n",
    "        for x, dx, mem_x in zip([self.U, self.W, self.b, self.V, self.c], [dU, dW, db, dV, dc], [self.memory_U, self.memory_W, self.memory_b, self.memory_V, self.memory_c]):\n",
    "            mem_x += np.square(dx)\n",
    "            x -= self.learning_rate * dx / np.sqrt(mem_x + eps)\n",
    "            \n",
    "    def step(self, hs, x, y):\n",
    "        hs, caches = self.rnn_forward(x, hs, self.U, self.W, self.b)\n",
    "        loss, dh, dV, dc = self.output_loss_and_grads(hs, self.V, self.c, y)\n",
    "        dU, dW, db = self.rnn_backward(dh, caches)\n",
    "        \n",
    "        self.update_parameters(len(x), dU, dW, db, dV, dc)\n",
    "        \n",
    "        # return loss and the last state value\n",
    "        return loss, hs[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_language_model(dataset, max_epochs, hidden_size=100, sequence_length=30, learning_rate=1e-1, sample_every=100):\n",
    "    vocabulary_size = len(dataset.sorted_characters)\n",
    "    rnn = RNN(hidden_size=hidden_size, sequence_length=sequence_length, vocabulary_size=vocabulary_size, learning_rate=learning_rate) # initialize the recurrent network\n",
    "\n",
    "    current_epoch = 0 \n",
    "    batch = 0\n",
    "\n",
    "    h0 = np.zeros((dataset.batch_size, hidden_size))\n",
    "    average_loss = 0\n",
    "    \n",
    "    should_print = False\n",
    "    first_print = True\n",
    "    while current_epoch < max_epochs: \n",
    "        e, x, y = dataset.next_minibatch()\n",
    "        \n",
    "        if e: \n",
    "            current_epoch += 1\n",
    "            h0 = np.zeros((dataset.batch_size, hidden_size))\n",
    "            \n",
    "            current_batch = batch % dataset.num_batches\n",
    "            print(\"Epoch: %06d\" % (current_epoch), end=\"\\n\")\n",
    "            print(\"Average_loss: %.4f; Last batch loss: %.4f\" % (average_loss / batch, loss))\n",
    "            print(\"=====================================================\")\n",
    "            \n",
    "            should_print = True\n",
    "            \n",
    "        if first_print:\n",
    "            seed = \"HAN:\\nIs that good or bad?\\n\\n\"\n",
    "            sampled_data = sample(rnn, seed, 300, dataset)\n",
    "\n",
    "            print(''.join(sampled_data))\n",
    "            print() \n",
    "            first_print = False\n",
    "            \n",
    "\n",
    "        # One-hot transform the x and y batches\n",
    "        x_oh = dataset.one_hot(x)\n",
    "        y_oh = dataset.one_hot(y)\n",
    "\n",
    "        # Run the recurrent network on the current batch\n",
    "        # Since we are using windows of a short length of characters,\n",
    "        # the step function should return the hidden state at the end\n",
    "        # of the unroll. You should then use that hidden state as the\n",
    "        # input for the next minibatch. In this way, we artificially\n",
    "        # preserve context between batches.\n",
    "        loss, _ = rnn.step(h0, x_oh, y_oh)\n",
    "        average_loss += loss\n",
    "\n",
    "        if should_print: \n",
    "            # run sampling (2.2)\n",
    "            seed = \"HAN:\\nIs that good or bad?\\n\\n\"\n",
    "            sampled_data = sample(rnn, seed, 300, dataset)\n",
    "\n",
    "            print(''.join(sampled_data))\n",
    "            print() \n",
    "            should_print = False\n",
    "            \n",
    "        batch += 1\n",
    "        \n",
    "def sample(rnn, seed, sample_size, dataset):\n",
    "    h0 = np.zeros([1, rnn.hidden_size])\n",
    "    seed_oh = dataset.one_hot(dataset.encode(seed))\n",
    "    \n",
    "    sampled = []\n",
    "    \n",
    "    for seed_character in seed_oh:\n",
    "        h0, _ = rnn.rnn_step_forward(seed_character.reshape([1, -1]), h0, rnn.U, rnn.W, rnn.b)\n",
    "        sampled.append(np.argmax(seed_character))\n",
    "        \n",
    "    for i in range(len(seed), sample_size):\n",
    "        prev_out = np.array([sampled[-1]])\n",
    "        in_oh = dataset.one_hot(prev_out)\n",
    "        h0, _ = rnn.rnn_step_forward(in_oh, h0, rnn.U, rnn.W, rnn.b)\n",
    "        \n",
    "        probabilities = rnn.softmax(rnn.output(h0, rnn.V, rnn.c))\n",
    "        # use random choice based on the probabilities, otherwise there will be repeating phrases\n",
    "        out_char = np.random.choice(range(dataset.vocab_size), p=probabilities.ravel()) \n",
    "        sampled.append(out_char)\n",
    "  \n",
    "    return dataset.decode(sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the RNN: 24371\n",
      "Hidden size: 100\n",
      "Sequence length: 30\n",
      "Vocabulary size: 71\n",
      "\n",
      "\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "y!?xDq9i,wtpkO,:cltKt7O3aPuBB9MBNix3V5RkWnfEFd5KcpwDVipost6otHXwe?p,P5Yw6wyFXj'G7`XhEawPF.Au5pO6sYJenHrPzNlAJ6583uH\n",
      "slixlIIHU?:YHO`Wp.4'ovndTig'rk fELFfNREraX'jQK?Lk::dbIRmYCXe1o0f\n",
      "\n",
      "TtC\n",
      "VCvNlI3FaNhEgz!leB:liPdDoRK8a v0y,x\n",
      "1lv0nDnXvpfOApbOV1U.om6L`BUgjPJa6UfpbW!ccJSOZ,qkwKa\n",
      "\n",
      "Epoch: 000001\n",
      "Average_loss: 68.9526; Last batch loss: 57.8715\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "MECKARMqI'd courlived.\n",
      "\n",
      "VEUGOR:\n",
      "Mlen't buth, save.  Peely ertuve haves?\n",
      "\n",
      "T Re wo f as hipim toued joll, gomsing bo the bere fulg? I filed bat is uBse, low. .n.\n",
      "\n",
      "MANSOR:\n",
      "Gise son then Tid pucl wor leot andaod asinotr that Coull huve in.\n",
      "\n",
      "UKE:\n",
      "I dut there lack bimid inoubeer\n",
      "\n",
      "Epoch: 000002\n",
      "Average_loss: 64.8254; Last batch loss: 55.0992\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "GHNDUTE:\n",
      "Loan Car orvingcell won't about. Whyest a bare! I'le thind a got lanjy houve le.  tting, on ou do hin't...\n",
      "\n",
      "LADED:\n",
      "Where that tiser?\n",
      "\n",
      "YANPIU:\n",
      ".r..\n",
      "\n",
      "GUt, JOd wriuke ught qa.  We fut!..\n",
      "\n",
      "Roke to is wech theply, Loat? I know thave peace?\n",
      "\n",
      "CHORE:\n",
      "Wer ite.\n",
      "\n",
      "HAN:\n",
      "Eld. f\n",
      "\n",
      "Epoch: 000003\n",
      "Average_loss: 62.8760; Last batch loss: 53.5527\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DAVERGR I Sid ey this me bacten tomarsss thes, ba won't herent. . kelle any's kid you nos A by sver the feake watne thinctted you to tuik duttill to mareing the fichmise?\n",
      "\n",
      "LRIt afur the haik wannl trowner isy hink hes won't my.  Siss andingain.\n",
      "\n",
      "VADECHu:\n",
      "Thagr?\n",
      "\n",
      "LUKE:\n",
      "Whal\n",
      "\n",
      "Epoch: 000004\n",
      "Average_loss: 61.6673; Last batch loss: 52.8458\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "LUKE:\n",
      "Non'n Arny Bella ope a ceane to nocks you rowen lrem tomog datt you whath not Cald this the Gut'm and you donGOt, Jus eeca goagh, candy san to bo reare to don wild welt walke aren if wo like puld to diden ficcerpetwel bay con meds age sat.!\n",
      "\n",
      "JOl. I'm sammiont, che bi\n",
      "\n",
      "Epoch: 000005\n",
      "Average_loss: 60.8201; Last batch loss: 52.4421\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "GOt' HARETTARETCY:\n",
      "Dony Boupie istil. The I kid in.\n",
      "\n",
      "DED:\n",
      "Are seund?\n",
      "\n",
      "JERGOTERETN:\n",
      "Thas ok ficel to mumnow. He', I'm?\n",
      "\n",
      "CUKERAOR:\n",
      "We'l you coueenst... Detabe.\n",
      "\n",
      "DOLZO:\n",
      "Loing collorss men't enb!\n",
      "\n",
      "DEVER:\n",
      "Gou taresberst. And in... no lloo!  We mishaug giinssebe. Hoce.\n",
      "\n",
      "LENA:\n",
      "No\n",
      "\n",
      "Epoch: 000006\n",
      "Average_loss: 60.1784; Last batch loss: 52.0123\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "IGEN:\n",
      "Wha to who'll Kive io ane gid shis lecled faldy of dil my have locke orry a geak, is. Eve.\n",
      "\n",
      "CHENKE:\n",
      "How you qoud do'l as, Loog! What eal.\n",
      "\n",
      "DONNOE:\n",
      "We'p it Goallafin' pir?  nath tweady now sol lake you be's lenrid!\n",
      "\n",
      "LUKETTR:\n",
      "WhEDETT:\n",
      "Wherf thinksith aling baped got if\n",
      "\n",
      "Epoch: 000007\n",
      "Average_loss: 59.6612; Last batch loss: 51.6264\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "HEN:\n",
      "Lot wnort und, yow.  ale jo. The ...leh petey you kio cyon't wour hes he's sod on you.  Thingst mm to ploin, ritegla be that lemer well the net and trad...\n",
      "\n",
      "DR.... Highte. An breche chobreaiss have fabl you wiet say is reoget tay?\n",
      "\n",
      "LOIDvel. I don't Saged trazy to Doed\n",
      "\n",
      "Epoch: 000008\n",
      "Average_loss: 59.2267; Last batch loss: 51.2987\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DDY:\n",
      "So Dah, youre lat conke dithe, you.\n",
      "\n",
      "DONNIA:\n",
      "Coull, ap carve's andry. Cellives.\n",
      "\n",
      "JOt. That cour unts. I'vl can in you. Collliny wour pitef dowkl sures.\n",
      "\n",
      "KOR:\n",
      "You're the dighis Gid. Do ense his neythere we over?\n",
      "\n",
      "BUCK:\n",
      "You do cantil about of shey, guche ollary.w.\n",
      "\n",
      "DAN:\n",
      "\n",
      "Epoch: 000009\n",
      "Average_loss: 58.8524; Last batch loss: 51.0440\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DRENK:\n",
      "Lonkat plaese of in a MAw Vrye. . duldera of you deaty fordes beed stasemays.\n",
      "\n",
      "GOw AVIETTY:\n",
      "Ar in the bete you'd yit.\n",
      "\n",
      "CINDY:\n",
      "Ut't hige ticknothere dy thive tharried. Mwech sonecedss!\n",
      "\n",
      "DONNZO:\n",
      "The Aribo heret.\n",
      "\n",
      "LUKE:\n",
      "An I con't have luvery retid.\n",
      "\n",
      "LUKE:\n",
      "As it to cal\n",
      "\n",
      "Epoch: 000010\n",
      "Average_loss: 58.5244; Last batch loss: 50.7741\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "HAN:\n",
      "Dike butt! You wandy. We poeataund stitied explent.\n",
      "\n",
      "VTHENTERESI DEFFEOPOR, Mrabelee, the Ceded Couring heht fickan on? We came, Jenodes bille.\n",
      "\n",
      "DUKERIE:\n",
      "Tela  Urey gon the Fry!\n",
      "\n",
      "LUOk hus?  We how, all out almysy the ane? Your ond!\n",
      "\n",
      "CELE:\n",
      "We I'm' in he aherdanine.\n",
      "\n",
      "DO\n",
      "\n",
      "Epoch: 000011\n",
      "Average_loss: 58.2335; Last batch loss: 50.5182\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DER:\n",
      "You that yes to reckesede Somelyay don't pravy happlong so me.\n",
      "\n",
      "DOL:\n",
      "In'EDSTER:\n",
      "Lat bappa.\n",
      "\n",
      "IGOR:\n",
      "But these gid his all my?\n",
      "\n",
      "DITH:\n",
      "I know. Thop iny sime Jungarill homester eclizel geokidighem. They think?? Adgereones the goy rwieviling ofuthing you hiuching gicke. And\n",
      "\n",
      "Epoch: 000012\n",
      "Average_loss: 57.9715; Last batch loss: 50.3171\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "MDOMEROTO:\n",
      "Wien'll witly lich sole?\n",
      "\n",
      "MADATER:\n",
      "And his you can don't ligh the ceppae.\n",
      "\n",
      "LOIDOT.\n",
      "\n",
      "DONNIE:\n",
      "You, Con's wheblenegreced! MI buve kid Ohl.  Anse my.\n",
      "\n",
      "LAURA:\n",
      "We mothe that can it's thy ar, On it tere a vereastoud in have forline porren doing we coo.\n",
      "\n",
      "DONNIE:\n",
      "Trist g\n",
      "\n",
      "Epoch: 000013\n",
      "Average_loss: 57.7328; Last batch loss: 50.1651\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "HAN:\n",
      "Goentin' doc!\n",
      "\n",
      "MIDA:\n",
      "Ithhell ya hald en?\n",
      "\n",
      "HAN:\n",
      "Tha po's lhen?\n",
      "\n",
      "DEIG:\n",
      "And, of it your that?\n",
      "\n",
      "Dat.\n",
      "\n",
      "DONEY:\n",
      "They lownt povntillind. WI gonty. I'mr!\n",
      "\n",
      "DONNKE:\n",
      "Way you.. some trevengemes on you himopm, dony's have this ond in me teandimutery then laks betcaut GOLMERse me.\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 000014\n",
      "Average_loss: 57.5129; Last batch loss: 49.9450\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "HAN:\n",
      "What!\n",
      "\n",
      "COUGHA:\n",
      "They in it.\n",
      "\n",
      "DWI'EO MZA:\n",
      "Yeed you?\n",
      "\n",
      "CIVI THURMAN:\n",
      "Are?\n",
      "\n",
      "AUKE:\n",
      " Heact whem.\n",
      "\n",
      "OVERETHEDST:\n",
      "We foking up oary a tinfe to they cossar hume!\n",
      "\n",
      "DONDY:\n",
      "Gid, just gon barig the gotsroushee ouney rogess to the 20Ade gas.\n",
      "\n",
      "OUKE:\n",
      "Warlit, he for is an hing, the of t\n",
      "\n",
      "Epoch: 000015\n",
      "Average_loss: 57.3092; Last batch loss: 49.7050\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "GOE WBURDY:\n",
      "Goe, this your you. He folle old whyas.\n",
      "\n",
      "VUKR TEDDY:\n",
      "The Fer.\n",
      "\n",
      "VAD:\n",
      "Vein'. Jood?..\n",
      "\n",
      "GONZO:\n",
      "Who mees!\n",
      "\n",
      "TOMEWTURMY:\n",
      "Collmn.\n",
      "\n",
      "BENPI HTORE:\n",
      "We le thop this do the the makes, Se cearive an I'm ked cone to enery?\n",
      "\n",
      "LUKE:\n",
      "And the bewownt lese this is.. thed.  Forel. I \n",
      "\n",
      "Epoch: 000016\n",
      "Average_loss: 57.1190; Last batch loss: 49.3927\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "GROY:\n",
      "Of you kEA BEDDY:\n",
      "Cous not here to abounfown od afe.  ever tome an huld is the broms.  or the mothert to kid him mafing see of the have I good.  Dfortas. Where?\n",
      "\n",
      "DONNIE:\n",
      "Seeniod leld sive on, is cruthe sors. HOAG DOk:\n",
      "There?\n",
      "\n",
      "MAN:\n",
      "Why my tenotids tick'nch whay!\n",
      "\n",
      "EULA\n",
      "\n",
      "Epoch: 000017\n",
      "Average_loss: 56.9402; Last batch loss: 49.0821\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DONNIE:\n",
      "Hey, way go's hove we for forlio! in dny aipled Gool trugut soke ake noos sole then the nake of asted lowien't oo cheen Joiobin' long dock wanty something, Cat the there you telinu to to sure, what us thlly beckmaifent jongut why!  wift up!\n",
      "\n",
      "GELSECKATTTAMSN:\n",
      "What s\n",
      "\n",
      "Epoch: 000018\n",
      "Average_loss: 56.7714; Last batch loss: 48.8461\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "CHEDDY:\n",
      "Lee.\n",
      "\n",
      "LONDO:\n",
      "Wind home.  Wou cigt I dould bickun us A guann.\n",
      "\n",
      "GONZO:\n",
      "I'll sching wiky heap binibol Suakstaly evee?\n",
      "\n",
      "MENWOUTEY:\n",
      "Laibhing beterns dot.\n",
      "\n",
      "LEURA:\n",
      "Gock warm.\n",
      "\n",
      "DODRY:\n",
      "The ham agorna we to being think lunts how gob.\n",
      "\n",
      "WEDNS:\n",
      "I lent to che so to aily save to \n",
      "\n",
      "Epoch: 000019\n",
      "Average_loss: 56.6112; Last batch loss: 48.6229\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "IGHA:\n",
      "Hadbamedment.\n",
      "\n",
      "ELIZAMET:\n",
      "You tove a that no bowk! Therss.  Tide have as betede wulleger gote'n huviag. Lew. A do you'd so reve cepeen, to it's awen't has stutizuft! Bellach.\n",
      "\n",
      "EVERGOS, De.\n",
      "\n",
      "LANDO:\n",
      "Whirlt for, denolle the sugot to your sime, have woogdanishen if here I\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000020\n",
      "Average_loss: 56.4588; Last batch loss: 48.3902\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DONNBE:\n",
      "Wo havisedn, you lets. Delet ar! Fist in?\n",
      "\n",
      "DWIGHT:\n",
      "That me.\n",
      "\n",
      "DETOTHITE:\n",
      "Jast about but ereterzandate suredtan whey? I's not and of the roge on.  Patbanf huuge.\n",
      "\n",
      "GOTT:\n",
      "Govn't sturty het angh.  Cong.?\n",
      "\n",
      "DR. PBL:\n",
      "And yNun't coun so goand you'll mach. What's all you's V\n",
      "\n",
      "Epoch: 000021\n",
      "Average_loss: 56.3136; Last batch loss: 48.1362\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "GAN:\n",
      "Thanks him.\n",
      "\n",
      "MAN:\n",
      "Shhoy mice so tell to sound.\n",
      "\n",
      "LEIA:\n",
      "What it.\n",
      "\n",
      "BOELLIT:\n",
      " ENR. That Neppe now alrea wouly spalistemn.\n",
      "\n",
      "S INK:\n",
      "What roag!.. sheap have.\n",
      "\n",
      "REEPEUCMAR:\n",
      "It and are.\n",
      "\n",
      "HONG:\n",
      "You civer conute the won't let ore the corller rige, am though or the moers fraples t\n",
      "\n",
      "Epoch: 000022\n",
      "Average_loss: 56.1751; Last batch loss: 47.9021\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "GONDL:\n",
      "Go you.\n",
      "\n",
      "JEG. No. What he pentr tell me!\n",
      "\n",
      "GUNTERAY:\n",
      "We down't wain't mas.\n",
      "\n",
      "LEIA:\n",
      "Culd me Lif imyese sad ow a bumfeed you beath of abousicind.\n",
      "\n",
      "DUGIETE:\n",
      "He wild can teop.\n",
      "\n",
      "LUKE:\n",
      "Mugias?\n",
      "\n",
      "LOUGA:\n",
      "We've, wonky nife of a shis with umsers to sas!\n",
      "\n",
      "LUKE:\n",
      "He sorn'll a lidce\n",
      "\n",
      "Epoch: 000023\n",
      "Average_loss: 56.0426; Last batch loss: 47.7103\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "GUNGOOM:\n",
      "He nearark you was psing a ma it you can for.\n",
      "\n",
      "DONNIE:\n",
      "Leind bogrdo.\n",
      "\n",
      "DONNIE:\n",
      "Nuts. Some. I low's you May not amseverve bame petitally that's like srout rick curist here and.\n",
      "\n",
      "DELMAR:\n",
      "Lanks bewe will eal juve herged oge thac, it'r apken donk.\n",
      "\n",
      "LENROE WHERPYULLIEN:\n",
      "\n",
      "Epoch: 000024\n",
      "Average_loss: 55.9155; Last batch loss: 47.5391\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "BETT:\n",
      "Prey new.\n",
      "\n",
      "MINGER:\n",
      "I ccry do the to couplestlofted bay they just homate. My wark showicwioccee, in or? That Zat lick.\n",
      "\n",
      "REEPY:\n",
      "Ot's pors...okmino wotl can your aralicary, whernunaw give al.  Who cor!\n",
      "\n",
      "LUKE:\n",
      "Bes out 'oust, seed comed woust.\n",
      "\n",
      "GWHA:\n",
      "Where's ctums tonmu w\n",
      "\n",
      "Epoch: 000025\n",
      "Average_loss: 55.7934; Last batch loss: 47.3629\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "GREIZOR:\n",
      "In broue ling take baybetadone's forwor!\n",
      "\n",
      "DONNIE:\n",
      "I just goug sordy say to hese tiarsang prown.  I good obutt a geo him cleartine bickuld. Fuctue wousd regebomod unfelleref with mister?\n",
      "\n",
      "MONNSEODOR:\n",
      "OR Last erain I'm frryiop!\n",
      "\n",
      "DONNIE:\n",
      "Clark.\n",
      "\n",
      "GONZO:\n",
      "E.\n",
      "\n",
      "LEIA:\n",
      " tuy\n",
      "\n",
      "Epoch: 000026\n",
      "Average_loss: 55.6757; Last batch loss: 47.1995\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "YETR REDDY:\n",
      "If But?\n",
      "\n",
      "UMERYY:\n",
      "We feerter?\n",
      "\n",
      "HAT:\n",
      "My Ondawievoy.\n",
      " Oh, io baincing aup pithty, baibed dice kightiar, your here beentac!\n",
      "\n",
      "DDY:\n",
      "It off of bliatary donny will carnod tomade yom.\n",
      "\n",
      "LEIA:\n",
      "Where \n",
      " Lou buthing.  I'ntluy then somekno him vicen.\n",
      "\n",
      "MR MONTHERANTH:\n",
      "You Geir\n",
      "\n",
      "Epoch: 000027\n",
      "Average_loss: 55.5621; Last batch loss: 47.0157\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "BUMLAYL:\n",
      "Vo.  be stowigot a life!\n",
      "\n",
      "DUNPIE:\n",
      "That he's fapp, I gidn your for?\n",
      "\n",
      "DR. RLOROR:\n",
      "Whe for ald ive. Crazy, me ditna sh reay.\n",
      "\n",
      "JOONG GIE:\n",
      "An yevacuze jubumive tums me ond. Hight my just himin! Cour's a my hersn's we't ip be constal me's beene forick then' kaliss con't\n",
      "\n",
      "Epoch: 000028\n",
      "Average_loss: 55.4524; Last batch loss: 46.8652\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "PETR:\n",
      "Sow comna st anys coushe., Deachede?\n",
      "\n",
      "IGEO:\n",
      "Mr why like atiches?\n",
      "\n",
      "LOUNDY:\n",
      "What heredulf a culd is bigen is all treford trand thind sc?\n",
      "\n",
      "MAN:\n",
      "You pave cordy. They  no jeke.\n",
      "\n",
      "LUKE: LOA Sow is thell, sturs, your ald fuckurver this sugot it som you contber now to deen lo\n",
      "\n",
      "Epoch: 000029\n",
      "Average_loss: 55.3462; Last batch loss: 46.7935\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "GONZO:\n",
      "I lent to where!\n",
      "\n",
      "GRETCHEELMAR:\n",
      "Lon to pohboding coundurw to be this what Whasu prim all is coote. We cave. Save.'s, If her let in I've of think why of choech see Kid his wat the psouse wike minede.\n",
      "\n",
      "RUK:\n",
      "Sores! But is the offienter, Corneile wall it chere of is fru\n",
      "\n",
      "Epoch: 000030\n",
      "Average_loss: 55.2434; Last batch loss: 46.7638\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "PETE:\n",
      "Okear  Old!\n",
      "\n",
      "CHAND:\n",
      "My now hime lose. Bonwiting to do at in a my as willed apelo the bold dy. I tear.\n",
      "\n",
      "CHERD:\n",
      "Good ahpure of dol beching prelass to note se?  me you not thace in jus listwod sore mm..Rst throug stur. I've yourd.\n",
      "\n",
      "DY MAG, CHIMS. Whomar you're houscoull\n",
      "\n",
      "Epoch: 000031\n",
      "Average_loss: 55.1440; Last batch loss: 46.7297\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "RDELIEL?  fhous.\n",
      "\n",
      "RYELL:\n",
      "Thero so for wim fausm stor enound with the mins te fucking apes Magch ragbory liff a car my out my befow of live that Coorrnarely about lonestee a whogeted wom this guen looky have to the cadping you. Paga  ankibld and tr if as are ir.\n",
      "\n",
      "DONNIN:\n",
      "I \n",
      "\n",
      "Epoch: 000032\n",
      "Average_loss: 55.0478; Last batch loss: 46.6748\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DONNIE:\n",
      "Hoss!  I with Frond's t'm would tell, MA. I juntering.\n",
      "\n",
      "LEIA:\n",
      "Full grasan't everrow mac! Lene?..\n",
      "\n",
      "LEIA:\n",
      "What in sure!\n",
      "\n",
      "IGE. That care in trowelenatt would ho said, it's can you your talk go be lutthroble his, styen on to betnggis and do. it whed, Goin' gott what yo\n",
      "\n",
      "Epoch: 000033\n",
      "Average_loss: 54.9546; Last batch loss: 46.6108\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "LUKE:\n",
      " That gon we'll out for cop. This!\n",
      "\n",
      "MCOE:\n",
      "Thel.  You gofed they comes you?\n",
      "\n",
      "LUKE:\n",
      "That's we'le donges? do, Colle's thas if all mace lefe.  I basts evey thim! Where's fin' her same, but hird, wonlt mouthion bay the ffidmomy.\n",
      "\n",
      "DON:\n",
      "sorping of deans.\n",
      "\n",
      "EVEREN:\n",
      "Hike'cle s\n",
      "\n",
      "Epoch: 000034\n",
      "Average_loss: 54.8642; Last batch loss: 46.5369\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "HEN:\n",
      "Lou're do of take in the owh I'm stact of and Joove dicana fal moppenterlady that wealiou becon stappe.\n",
      "\n",
      "PECK:\n",
      "Thisk onitidenss youne.\n",
      "\n",
      "LUKE:\n",
      "It sourd. What it cleat?\n",
      "\n",
      "VADER:\n",
      "I rowkene to do yould arither uppashe, Geasts see ma not gotthuse Vetie out.\n",
      "\n",
      "DUKE:\n",
      "I'll the \n",
      "\n",
      "Epoch: 000035\n",
      "Average_loss: 54.7765; Last batch loss: 46.4515\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "HANROLAVERICK:\n",
      "I grecach get rurlon't iunst't shut one all pilliet.\n",
      "\n",
      "GRETCHERA:\n",
      "The mone probse sigut that mighind a light.\n",
      "\n",
      "DELMAR:\n",
      "He figrist your usplew.\n",
      "\n",
      "VIDER:\n",
      "Anas, citens my your wance onaty's it mide a frAdy out me day.\n",
      "\n",
      "CHERGIAN:\n",
      "Now ffoy? Rett a nat was Cower wah\n",
      "\n",
      "Epoch: 000036\n",
      "Average_loss: 54.6913; Last batch loss: 46.3569\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DONNIE:\n",
      "Hiugaidt to we cave is istare...\n",
      "\n",
      "BUDDY:\n",
      "Whe I'd wact he are er.\n",
      "\n",
      "DELMAR:\n",
      "I'm way!\n",
      "\n",
      "GONZO:\n",
      "Yebue him Go.\n",
      "\n",
      "PROFEIE:\n",
      "Go itt 've, the cadb My it.\n",
      "\n",
      "EDDIE:\n",
      "Sutues. Vat and right it preelor?\n",
      "\n",
      "LEIA:\n",
      "Thys has my ter to gikedn.\n",
      "\n",
      "GONZO:\n",
      "What this is way surlgeexen to Ragh of\n",
      "\n",
      "Epoch: 000037\n",
      "Average_loss: 54.6084; Last batch loss: 46.2443\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "CINDY:\n",
      "Hust... Kes.\n",
      "\n",
      "DONNIE:\n",
      "I heme oy of this is to do.\n",
      "\n",
      "HAN:\n",
      " The Rser come. Lictuin?\n",
      "\n",
      "JAMMYby I'll coon like Caster don't lose fuck you beradion.\n",
      "\n",
      "DONNIE:\n",
      "I lablis tonart amm. so hie the to to a must careus he'll, you won some gom. I'm not on the mopping I jeh grets mad\n",
      "\n",
      "Epoch: 000038\n",
      "Average_loss: 54.5276; Last batch loss: 46.1115\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "GOTE:\n",
      "I Can ake to tellmon one him.\n",
      "\n",
      "LUKE:\n",
      "You'd beft it do borterrer with tugst singe it.\n",
      "\n",
      "HROLON:\n",
      "Traste to Gell, I don't want to diderst you're the carss.\n",
      "\n",
      "DET'BEAS Hust and in mught. Ve lappeneer a wass. Where!  Til.\n",
      "\n",
      "EVERDSX:\n",
      "Why.\n",
      "\n",
      "MR..\n",
      "\n",
      "OVEROOT:\n",
      "What neto do veesn Ny\n",
      "\n",
      "Epoch: 000039\n",
      "Average_loss: 54.4488; Last batch loss: 45.9380\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "HAN:\n",
      "Yeaw, peaged shit you . Out dight.\n",
      "\n",
      "LEIA:\n",
      "That. You see?.. sak that with your ponitering my the way mary'n woners,  I themEd Py what's huprr he puttapope I wasterstalying don't. I worry cepff goed a hims. You pan a vous new, and stutlorne, set bue to becen't rogbed pr\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000040\n",
      "Average_loss: 54.3719; Last batch loss: 45.7929\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "GAN:\n",
      "Mre.\n",
      "\n",
      "LANDOF TER:\n",
      "Whow so wery?\n",
      "\n",
      "LUKE:\n",
      "ThemE, told I's may sempoosefes it I'll once.  I think burnde.\n",
      "\n",
      "LEFA:\n",
      "Reaves, I don't truster I mese out?\n",
      "\n",
      "LUKE:\n",
      "Pair Tirnch. You cumming. Gadbon good runger hese. HER... conty you at ham it.\n",
      "\n",
      "DYO PIL:\n",
      "At los? Well is forutalk.\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 000041\n",
      "Average_loss: 54.2968; Last batch loss: 45.6530\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "GRESPER:\n",
      "What gety, in back tondar.\n",
      "\n",
      "SCOANDR:\n",
      "I a corvelarbot pille noar. I've godiss thould I know my you licm.\n",
      "\n",
      "SANDY:\n",
      "Hew it momed reae hiull! The simest a day, acceed yound a sclous orred, whit everaiss to pepsesnticce wanconen thing. No.  focenstod olatetifowitele wil\n",
      "\n",
      "Epoch: 000042\n",
      "Average_loss: 54.2235; Last batch loss: 45.5321\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DGE:\n",
      "But a not beaste think.\n",
      "\n",
      "THENE:\n",
      "The mover if to will isker.  the do sirhtal take ald Dsey?\n",
      "\n",
      "LANDO:\n",
      "Oal!\n",
      "\n",
      "MAVERICK:\n",
      "I heapbous yeve you say I fatakpopre.  I know a lowes bettan, of toahimigeland.\n",
      "\n",
      "ESPER\n",
      "HEN:\n",
      "You're been we toary. Pens a is out.  ove pomater oued tell h\n",
      "\n",
      "Epoch: 000043\n",
      "Average_loss: 54.1518; Last batch loss: 45.4254\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "GOR, COL. What birny the lot in to. You'd our. It aspep be you now me.\n",
      "\n",
      "OVERY:\n",
      "Cher no when It'll thus.  Bloon decess tool up arould work, basted bod.\n",
      "\n",
      "LUKE:\n",
      "Doede.  I know. Tell hugction.\n",
      "\n",
      "BETTY:\n",
      "Goods look are objut fow what's lufe. No?\n",
      "\n",
      "CANDY:\n",
      "And you ston.\n",
      "\n",
      "SARAH:\n",
      "Don'\n",
      "\n",
      "Epoch: 000044\n",
      "Average_loss: 54.0817; Last batch loss: 45.3436\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "YOIN:\n",
      "I'm go so out, the mored?\n",
      "\n",
      "LEOA MURDOF:\n",
      "Tiplle ond harr ovel soop?\n",
      "\n",
      "DETMY:\n",
      "Now. Time being.\n",
      "\n",
      "DONNIE:\n",
      "ADATHAN HOER:\n",
      "How tongy for.\n",
      "\n",
      "Dot go expsiers thelo but.\n",
      "\n",
      "LAURE:\n",
      "I dickion of wrong birce. Mrint me! Let he'll gome.\n",
      "\n",
      "OVEUR:\n",
      "Sa!\n",
      "\n",
      "PROFEM:\n",
      "Three.\n",
      "\n",
      "EVERET:\n",
      "Let.\n",
      "\n",
      "FATHTE\n",
      "\n",
      "Epoch: 000045\n",
      "Average_loss: 54.0131; Last batch loss: 45.2708\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "SHOE:\n",
      "So tonoing bewith you tassent.\n",
      "\n",
      "PETE:\n",
      "Siat.  Time, aprmarten ours.\n",
      "\n",
      "EVERETT:\n",
      "YenN. INKOMOFF:\n",
      "Gyt Baybely.\n",
      "\n",
      "CALGER:\n",
      "Why yO his out have be faing it!\n",
      "\n",
      "IGHA:\n",
      "Handely his seavin's as cup treen decest!\n",
      "\n",
      "KORBEN:\n",
      "Cottw ango.\n",
      "\n",
      "HAN:\n",
      "You don't fight?\n",
      "\n",
      "COUDEY:\n",
      "Don't go me dirpe\n",
      "\n",
      "Epoch: 000046\n",
      "Average_loss: 53.9460; Last batch loss: 45.2150\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "PGH:\n",
      "Yes.  You dusntenars.\n",
      "\n",
      "HENRY:\n",
      "Utidate.\n",
      "\n",
      "BETTY:\n",
      "Look!\n",
      "\n",
      "KORBENT:\n",
      "No.\n",
      "\n",
      "SANDY:\n",
      "And held, your the mana. Whatee it.\n",
      "\n",
      "GRETCHEN:\n",
      "No. !  Thee!\n",
      "\n",
      "GOON:\n",
      "Strip.\n",
      "\n",
      "EVERETT:\n",
      "You're tayced but the himpeding here resion.  I was, asking you Look.\n",
      "\n",
      "DUKCH:\n",
      "I'll the know.\n",
      "\n",
      "ROK:\n",
      "No. Dirter\n",
      "\n",
      "Epoch: 000047\n",
      "Average_loss: 53.8803; Last batch loss: 45.1628\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "GONZO:\n",
      "Thenl. You gobto would owhad umpookin' to kerive.\n",
      "\n",
      "VADER:\n",
      "Apink Goed knownahted not like of hamp see dien.\n",
      "\n",
      "PLEPRE:\n",
      "Seacre. They know becaedysbinis ham tider tame Get hus a have go your to see it.\n",
      "\n",
      "DUKE:\n",
      "Would with roggld?\n",
      "\n",
      "DONNIE:\n",
      "Labraded?\n",
      "\n",
      "DR. PIN:\n",
      "He just's don'\n",
      "\n",
      "Epoch: 000048\n",
      "Average_loss: 53.8159; Last batch loss: 45.1175\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DR. PIN:\n",
      "Meva.\n",
      "\n",
      "DYLDOO:\n",
      "Deeew and let is do impels what grant. And I ronctater.\n",
      "\n",
      "GRETTY:\n",
      "Old got.  It talk zaventhole a lot'me that only.\n",
      "\n",
      "FRETHY:\n",
      "Wountary rese spean abiesn't devere and bred you hart tuke likno list make peen they intany pittarn with Deezysns once a man.\n",
      "\n",
      "\n",
      "Epoch: 000049\n",
      "Average_loss: 53.7529; Last batch loss: 45.0687\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DONNIE:\n",
      "Shoy...got wish toises yound ompon payble you on!  How do do your unces the got rerepedn't buin and are my pets on not reter dia won't not's ragpty arty she'd a to agche in tran!\n",
      "\n",
      "PENNY:\n",
      "Yes, How aboptene seathigh right. I'll could We Mose lame orralese?!\n",
      "\n",
      "ROKE:\n",
      "Th\n",
      "\n",
      "Epoch: 000050\n",
      "Average_loss: 53.6913; Last batch loss: 45.0277\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "BRY:\n",
      "I'm get here... get!\n",
      "\n",
      "LEELUMAN:\n",
      "I there timed he sonty dine... we're's out.\n",
      "\n",
      "HAN:\n",
      "Toull yound it.\n",
      "\n",
      "HODO:\n",
      "He spugcenone now.'! . Looker for a don't go Teck ton you're tordge.\n",
      "\n",
      "VADER:\n",
      "Not'roon you woind.\n",
      "\n",
      "DWIGHT:\n",
      "Gon't cade of two'll to than.\n",
      "\n",
      "DONNIE:\n",
      "What's good?\n",
      "\n",
      "DONN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset()\n",
    "dataset.preprocess(\"data/selected_conversations.txt\")\n",
    "dataset.create_minibatches(batch_size=5, sequence_length=30)\n",
    "\n",
    "run_language_model(dataset, 50, sequence_length=dataset.sequence_length)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

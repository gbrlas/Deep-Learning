{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. vježba: modeliranje nizova povratnim neuronskim mrežama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pdb\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadatak 1: učitavanje podataka i batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hardcoded paths - change if necessary\n",
    "root = 'data'\n",
    "\n",
    "# this one you need to download from the dataset\n",
    "full_dataset = 'data/movie_lines.txt'\n",
    "\n",
    "output_destination = 'data/selected_conversations.txt'\n",
    "movie_selection = 'data/selected_movies.txt'\n",
    "\n",
    "# separator used in the original dataset\n",
    "separator = ' +++$+++ '\n",
    "\n",
    "# movie ID file\n",
    "MOVIE_ID = 0\n",
    "\n",
    "# full conversation dataset file\n",
    "MOVIE_ID_FULL = 2\n",
    "# reverse indexing\n",
    "CHARACTER_NAME = -2\n",
    "CHARACTER_LINE = -1\n",
    "\n",
    "# keep just these characters for simplicity (and utf8 breaking)\n",
    "repl = r'[^A-Za-z0-9()\\,!\\?\\'\\`\\. ]'\n",
    "\n",
    "\n",
    "# regex replace\n",
    "def filter(string):\n",
    "    return re.sub(repl, '', string)\n",
    "\n",
    "\n",
    "# from a movie ID string (e.g. M134), output the number (134)\n",
    "def number_from_id(id):\n",
    "    return int(id[1:])\n",
    "\n",
    "\n",
    "# read just movie ID's, rest is for readability\n",
    "def read_selected(path_to_selected_movies):\n",
    "    selected_movies = set()\n",
    "\n",
    "    with open(path_to_selected_movies, 'r') as infile:\n",
    "        for line in infile:\n",
    "            parts = line.strip().split(separator)\n",
    "            selected_movies.add(parts[MOVIE_ID].strip())\n",
    "    return selected_movies\n",
    "\n",
    "\n",
    "# select and write to output file\n",
    "def select_and_write(path_to_full_dataset, path_to_output, selected_movies):\n",
    "    movies = {}\n",
    "\n",
    "    with open(path_to_full_dataset, 'r', encoding=\"ISO-8859-1\") as infile:\n",
    "\n",
    "        for line in infile:\n",
    "\n",
    "            parts = line.strip().split(separator)\n",
    "\n",
    "            if parts[MOVIE_ID_FULL].strip() not in selected_movies:\n",
    "                continue\n",
    "\n",
    "            # take data and transform to tuple\n",
    "            ID = parts[MOVIE_ID_FULL]\n",
    "            char_name = parts[CHARACTER_NAME]\n",
    "            char_line = parts[CHARACTER_LINE]\n",
    "\n",
    "            tup = (number_from_id(ID), char_name, char_line)\n",
    "\n",
    "            # add to map\n",
    "            if ID not in movies:\n",
    "                movies[ID] = []\n",
    "            movies[ID].append(tup)\n",
    "\n",
    "    with open(path_to_output, 'w') as out:\n",
    "        for movie in movies:\n",
    "            # sort by line number\n",
    "            dialogue = sorted(movies[movie], key=lambda t: t[0])\n",
    "            for n, name, text in dialogue:\n",
    "                out.write(filter(name) + ':\\n' + filter(text) + '\\n\\n')\n",
    "\n",
    "\n",
    "def main():\n",
    "    # uses hardcoded paths\n",
    "    selection = os.path.join(root, movie_selection)\n",
    "    selected_movies = read_selected(selection)\n",
    "\n",
    "    dataset = os.path.join(root, full_dataset)\n",
    "    output = os.path.join(root, output_destination)\n",
    "    select_and_write(dataset, output, selected_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def preprocess(self, input_file):\n",
    "        with open(input_file, \"r\") as f:\n",
    "            data = f.read()\n",
    "\n",
    "        # count and sort most frequent characters\n",
    "        characters, counts = np.unique(list(data), return_index=True)\n",
    "        self.sorted_characters = characters[np.argsort(-counts)]\n",
    "        self.vocab_size = len(self.sorted_characters)\n",
    "\n",
    "        # self.sorted chars contains just the characters ordered descending by frequency\n",
    "        self.char2id = dict(zip(self.sorted_characters, range(len(self.sorted_characters)))) \n",
    "        # reverse the mapping\n",
    "        self.id2char = {k:v for v,k in self.char2id.items()}\n",
    "        # convert the data to ids\n",
    "        self.x = np.array(list(map(self.char2id.get, data)))\n",
    "\n",
    "    def encode(self, sequence):\n",
    "        encoded_sequence = np.array([self.char2id[c] for c in sequence], dtype=np.int32)\n",
    "        return encoded_sequence\n",
    "\n",
    "    def decode(self, sequence):\n",
    "        decoded_sequence = [self.id2char[c] for c in sequence]\n",
    "        return decoded_sequence\n",
    "        \n",
    "    def create_minibatches(self, batch_size, sequence_length):\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.batch_index = 0\n",
    "        \n",
    "        self.num_batches = int(len(self.x) / (batch_size * sequence_length)) # calculate the number of batches\n",
    "        self.batches = np.zeros([self.num_batches, self.batch_size, self.sequence_length + 1], dtype=np.int32)  \n",
    "    \n",
    "        for batch in range(self.num_batches):\n",
    "            for index in range(self.batch_size):\n",
    "                start = batch * sequence_length + index * (self.num_batches * sequence_length)\n",
    "                end = start + self.sequence_length + 1 \n",
    "                self.batches[batch, index, :] = self.x[start : end]\n",
    "\n",
    "    def next_minibatch(self):        \n",
    "        if self.batch_index == self.num_batches:\n",
    "            self.batch_index = 0\n",
    "\n",
    "        batch = self.batches[self.batch_index, :, :]\n",
    "        self.batch_index += 1\n",
    "        \n",
    "        batch_x = batch[:, :-1]\n",
    "        batch_y = batch[:, 1:]\n",
    "        \n",
    "        return self.batch_index == self.num_batches, batch_x, batch_y\n",
    "    \n",
    "    def _as_one_hot(self, x, vocab):\n",
    "        n = len(x)\n",
    "        Yoh = np.zeros((n, vocab))\n",
    "        Yoh[np.arange(n), x] = 1\n",
    "        \n",
    "        return Yoh\n",
    "    \n",
    "    def one_hot(self, batch):\n",
    "        if batch.ndim == 1:\n",
    "            return self._as_one_hot(batch, self.vocab_size)\n",
    "        else:\n",
    "            return np.array([self._as_one_hot(b, self.vocab_size) for b in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T', 'o', 'm', 'o', 'r', 'r', 'o', 'w']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test 1\n",
    "dat = Dataset()\n",
    "dat.preprocess(\"data/selected_conversations.txt\")\n",
    "\n",
    "dat.decode(dat.encode(\"Tomorrow\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadatak 2: obična jednoslojna povratna neuronska mreža"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN():\n",
    "    # inicijalizacija parametara\n",
    "    def __init__(self, hidden_size, sequence_length, vocabulary_size, learning_rate):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.U = np.random.normal(size=[vocabulary_size, hidden_size], scale=1.0 / np.sqrt(hidden_size))  # ... input projection\n",
    "        self.W = np.random.normal(size=[hidden_size, hidden_size], scale=1.0 / np.sqrt(hidden_size))  # ... hidden-to-hidden projection\n",
    "        self.b = np.zeros([1, hidden_size])\n",
    "\n",
    "        self.V = np.random.normal(size=[hidden_size, vocabulary_size], scale=1.0 / np.sqrt(vocabulary_size)) # ... output projection\n",
    "        self.c = np.zeros([1, vocabulary_size]) # ... output bias\n",
    "\n",
    "        # memory of past gradients - rolling sum of squares for Adagrad\n",
    "        self.memory_U = np.zeros_like(self.U)\n",
    "        self.memory_W = np.zeros_like(self.W)\n",
    "        self.memory_V = np.zeros_like(self.V)\n",
    "        self.memory_b = np.zeros_like(self.b)\n",
    "        self.memory_c = np.zeros_like(self.c)\n",
    "        \n",
    "    def rnn_step_forward(self, x, h_prev, U, W, b):\n",
    "        # A single time step forward of a recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity.\n",
    "\n",
    "        # x - input data (minibatch size x input dimension)\n",
    "        # h_prev - previous hidden state (minibatch size x hidden size)\n",
    "        # U - input projection matrix (input dimension x hidden size)\n",
    "        # W - hidden to hidden projection matrix (hidden size x hidden size)\n",
    "        # b - bias of shape (hidden size x 1)\n",
    "\n",
    "        h_current, cache = None, None\n",
    "        h_current = np.tanh(np.dot(h_prev, W) + np.dot(x, U) + b)\n",
    "        cache = (W, x, h_prev, h_current)\n",
    "\n",
    "        # return the new hidden state and a tuple of values needed for the backward step\n",
    "\n",
    "        return h_current, cache\n",
    "\n",
    "\n",
    "    def rnn_forward(self, x, h0, U, W, b):\n",
    "        # Full unroll forward of the recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity\n",
    "\n",
    "        # x - input data for the whole time-series (minibatch size x sequence_length x input dimension)\n",
    "        # h0 - initial hidden state (minibatch size x hidden size)\n",
    "        # U - input projection matrix (input dimension x hidden size)\n",
    "        # W - hidden to hidden projection matrix (hidden size x hidden size)\n",
    "        # b - bias of shape (hidden size x 1)\n",
    "        \n",
    "        h, cache = None, None\n",
    "        hs = [h0]\n",
    "        caches = []\n",
    "        \n",
    "        for sequence in range(self.sequence_length):\n",
    "            data = x[:, sequence, :]\n",
    "            h_current, cache_current = self.rnn_step_forward(data, hs[-1], U, W, b)\n",
    "            \n",
    "            hs.append(h_current)\n",
    "            caches.append(cache_current)\n",
    "            \n",
    "        hs = np.array(hs[1:]).transpose((1, 0, 2))\n",
    "        \n",
    "        # return the hidden states for the whole time series (T+1) and a tuple of values needed for the backward step\n",
    "        \n",
    "        return hs, caches\n",
    "    \n",
    "    def rnn_step_backward(self, grad_next, cache):\n",
    "        # A single time step backward of a recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity.\n",
    "\n",
    "        # grad_next - upstream gradient of the loss with respect to the next hidden state and current output\n",
    "        # cache - cached information from the forward pass        \n",
    "        dh_prev, dU, dW, db = None, None, None, None\n",
    "        W, x, h_prev, h_current = cache\n",
    "        \n",
    "        # compute and return gradients with respect to each parameter\n",
    "        # HINT: you can use the chain rule to compute the derivative of the\n",
    "        # hyperbolic tangent function and use it to compute the gradient\n",
    "        # with respect to the remaining parameters\n",
    "        \n",
    "        # 65. i 66. slajd formule\n",
    "        da = grad_next * (1 - h_current**2)\n",
    "        \n",
    "        dh_prev = np.dot(da, W.T)\n",
    "        dU = np.dot(x.T, da)\n",
    "        dW = np.dot(h_prev.T, da)\n",
    "        db = np.sum(da, axis=0)\n",
    "\n",
    "        return dh_prev, dU, dW, db\n",
    "\n",
    "\n",
    "    def rnn_backward(self, dh, cache):\n",
    "        # Full unroll forward of the recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity\n",
    "\n",
    "        dU = np.zeros_like(self.U)\n",
    "        dW = np.zeros_like(self.W)\n",
    "        db = np.zeros_like(self.b)\n",
    "\n",
    "        # compute and return gradients with respect to each parameter\n",
    "        # for the whole time series.\n",
    "        # Why are we not computing the gradient with respect to inputs (x)?\n",
    "        grads = np.zeros_like(dh[0])\n",
    "        for dh_t, cache_t in reversed(list(zip(dh, cache))):\n",
    "            grads, dU_t, dW_t, db_t = self.rnn_step_backward(dh_t + grads, cache_t)\n",
    "            \n",
    "            dU += dU_t\n",
    "            dW += dW_t\n",
    "            db += db_t\n",
    "        \n",
    "        return np.clip(dU, -5, 5), np.clip(dW, -5, 5), np.clip(db, -5, 5)\n",
    "    \n",
    "    def output(self, h, V, c):\n",
    "        # Calculate the output probabilities of the network\n",
    "        return np.dot(h, V) + c\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        x -= np.max(x)\n",
    "        logits_exp = np.exp(x)\n",
    "        return logits_exp / np.sum(logits_exp, axis=1, keepdims=True)\n",
    "\n",
    "    def output_loss_and_grads(self, h, V, c, y):\n",
    "        # Calculate the loss of the network for each of the outputs\n",
    "\n",
    "        # h - hidden states of the network for each timestep. \n",
    "        #     the dimensionality of h is (batch size x sequence length x hidden size (the initial state is irrelevant for the output)\n",
    "        # V - the output projection matrix of dimension hidden size x vocabulary size\n",
    "        # c - the output bias of dimension vocabulary size x 1\n",
    "        # y - the true class distribution - a tensor of dimension \n",
    "        #     batch_size x sequence_length x vocabulary size - you need to do this conversion prior to\n",
    "        #     passing the argument. A fast way to create a one-hot vector from\n",
    "        #     an id could be something like the following code:\n",
    "\n",
    "        #   y[batch_id][timestep] = np.zeros((vocabulary_size, 1))\n",
    "        #   y[batch_id][timestep][batch_y[timestep]] = 1\n",
    "\n",
    "        #     where y might be a list or a dictionary.\n",
    "\n",
    "        loss, dh, dV, dc = None, None, None, None\n",
    "        \n",
    "        loss = 0\n",
    "        dhs = []\n",
    "        dV = np.zeros_like(self.V)\n",
    "        dc = np.zeros_like(self.c)\n",
    "        \n",
    "        batch_size = len(h)\n",
    "        \n",
    "        # calculate the output (o) - unnormalized log probabilities of classes\n",
    "        # calculate yhat - softmax of the output\n",
    "        # calculate the cross-entropy loss\n",
    "        # calculate the derivative of the cross-entropy softmax loss with respect to the output (o)\n",
    "        # calculate the gradients with respect to the output parameters V and c\n",
    "        # calculate the gradients with respect to the hidden layer h\n",
    "        \n",
    "        for sequence in range(self.sequence_length):\n",
    "            y_true = y[:, sequence, :]\n",
    "            h_t = h[:, sequence, :]\n",
    "            \n",
    "            o = self.output(h_t, V, c)\n",
    "            yhat = self.softmax(o)\n",
    "            \n",
    "            loss -= np.sum(np.log(yhat) * y_true) / batch_size\n",
    "            \n",
    "            dO = (yhat - y_true) / batch_size\n",
    "            dV += np.dot(h_t.T, dO)\n",
    "            dc += np.sum(dO, axis=0)\n",
    "            \n",
    "            dhs.append(np.dot(dO, V.T))\n",
    "\n",
    "        return loss, dhs, dV, dc\n",
    "    \n",
    "    # updates parameters\n",
    "    def update(self, batch_size, dU, dW, db, dV, dc):\n",
    "        # update memory matrices\n",
    "        # perform the Adagrad update of parameters\n",
    "        eps = 1e-7\n",
    "        for x, dx, mem_x in zip([self.U, self.W, self.b, self.V, self.c], [dU, dW, db, dV, dc], [self.memory_U, self.memory_W, self.memory_b, self.memory_V, self.memory_c]):\n",
    "            mem_x += np.square(dx)\n",
    "            x -= self.learning_rate * dx / np.sqrt(mem_x + eps)\n",
    "            \n",
    "    def step(self, hs, x, y):\n",
    "        hs, caches = self.rnn_forward(x, hs, self.U, self.W, self.b)\n",
    "        loss, dh, dV, dc = self.output_loss_and_grads(hs, self.V, self.c, y)\n",
    "        dU, dW, db = self.rnn_backward(dh, caches)\n",
    "        \n",
    "        self.update(len(x), dU, dW, db, dV, dc)\n",
    "        \n",
    "        return loss, hs[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_language_model(dataset, max_epochs, hidden_size=100, sequence_length=30, learning_rate=1e-1, sample_every=100):\n",
    "    vocabulary_size = len(dataset.sorted_characters)\n",
    "    rnn = RNN(hidden_size=hidden_size, sequence_length=sequence_length, vocabulary_size=vocabulary_size, learning_rate=learning_rate) # initialize the recurrent network\n",
    "\n",
    "    current_epoch = 0 \n",
    "    batch = 0\n",
    "\n",
    "    h0 = np.zeros((dataset.batch_size, hidden_size))\n",
    "    average_loss = 0\n",
    "    \n",
    "    should_print = False\n",
    "    while current_epoch < max_epochs: \n",
    "        e, x, y = dataset.next_minibatch()\n",
    "        \n",
    "        if e: \n",
    "            current_epoch += 1\n",
    "            h0 = np.zeros((dataset.batch_size, hidden_size))\n",
    "            \n",
    "            current_batch = batch % dataset.num_batches\n",
    "            print(\"Epoch: %06d\" % (current_epoch), end=\"\\n\")\n",
    "            print(\"Average_loss: %.4f; Last batch loss: %.4f\" % (average_loss / batch, loss))\n",
    "            print(\"=====================================================\")\n",
    "            \n",
    "            should_print = True\n",
    "\n",
    "        # One-hot transform the x and y batches\n",
    "        x_oh = dataset.one_hot(x)\n",
    "        y_oh = dataset.one_hot(y)\n",
    "\n",
    "        # Run the recurrent network on the current batch\n",
    "        # Since we are using windows of a short length of characters,\n",
    "        # the step function should return the hidden state at the end\n",
    "        # of the unroll. You should then use that hidden state as the\n",
    "        # input for the next minibatch. In this way, we artificially\n",
    "        # preserve context between batches.\n",
    "        loss, h0 = rnn.step(h0, x_oh, y_oh)\n",
    "        average_loss += loss\n",
    "\n",
    "        if batch % sample_every == 0: \n",
    "            # run sampling (2.2)\n",
    "            seed = \"HAN:\\nIs that good or bad?\\n\\n\"\n",
    "            sampled_data = sample(rnn, seed, 300, dataset)\n",
    "            \n",
    "            if should_print:\n",
    "                print(''.join(sampled_data))\n",
    "                print() \n",
    "                should_print = False\n",
    "            \n",
    "        batch += 1\n",
    "        \n",
    "def sample(rnn, seed, n_sample, dataset):\n",
    "    h0 = np.zeros([1, rnn.hidden_size])\n",
    "    seed_oh = dataset.one_hot(dataset.encode(seed))\n",
    "    \n",
    "    sampled = []\n",
    "    \n",
    "    for c_oh in seed_oh:\n",
    "        h0, _ = rnn.rnn_step_forward(c_oh.reshape([1, -1]), h0, rnn.U, rnn.W, rnn.b)\n",
    "        sampled.append(np.argmax(c_oh))\n",
    "        \n",
    "    for i in range(len(seed), n_sample):\n",
    "        prev_out = np.array([sampled[-1]])\n",
    "        in_oh = dataset.one_hot(prev_out)\n",
    "        h0, _ = rnn.rnn_step_forward(in_oh, h0, rnn.U, rnn.W, rnn.b)\n",
    "        \n",
    "        probabilities = rnn.softmax(rnn.output(h0, rnn.V, rnn.c))\n",
    "        out_char_oh = np.random.choice(range(dataset.vocab_size), p=probabilities.ravel()) \n",
    "        sampled.append(out_char_oh)\n",
    "  \n",
    "    return dataset.decode(sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000001\n",
      "Average_loss: 66.6664; Last batch loss: 69.4112\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "BECTORE:\n",
      "Wplrint thath! Mon\n",
      "\n",
      "\n",
      "DUELY:\n",
      "What'r pantiwer sous ten't... Yout sidlyt I'm a bit't be it ke me. No. TPING:\n",
      "Hut a a buactiniin.\n",
      "\n",
      "MERE FYou'lr tri, ...\n",
      "\n",
      "GOKE:\n",
      "Whe'll, at stalg. Wo conk way as to be bassed ceme sulgiks acke?\n",
      "\n",
      "MONND:\n",
      "Hente I. Doow ied andyomedurd be et\n",
      "\n",
      "Epoch: 000002\n",
      "Average_loss: 62.4517; Last batch loss: 65.9423\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DONKERSIN:\n",
      "I con thinve you wall bean, as Guexshede!\n",
      "\n",
      "INNIE:\n",
      "And Dake? HR. Hos?\n",
      "\n",
      "DONDO:\n",
      "Whagh be out what it the haof the lot guees beand of se to on you they leake ckagunnd mimesit'y wuy. be tety?\n",
      "\n",
      "DETE:\n",
      "The gupsting wertigem, I got him thito, a good. Lidts alle. Your wic\n",
      "\n",
      "Epoch: 000003\n",
      "Average_loss: 60.4021; Last batch loss: 65.3751\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DDE. Y:\n",
      "Ol no, stole there betran go all coonto! ber bive, ever.  Golyon.\n",
      "\n",
      "IVERETTh:\n",
      "Fo alit't it nigh the show if?\n",
      "\n",
      "DDONTH:\n",
      "Ary tua cwool on methang. Hlithing weasishote lit a fe  Chat's are.\n",
      "\n",
      "ERMINFETT:\n",
      "Whake it mine. you quen FIdd hove Eh, Thed.  st's ricain't of though\n",
      "\n",
      "Epoch: 000004\n",
      "Average_loss: 59.0672; Last batch loss: 64.7825\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DUK.MY:\n",
      "Ged't sence well sidk!\n",
      "\n",
      "JOE:\n",
      "There? You im?\n",
      "\n",
      "SADIA:\n",
      "Setrenoir you?\n",
      "\n",
      "EDDAR:\n",
      "Reel huveon, don't te lest ander ight things you.\n",
      "\n",
      "MONKEY:\n",
      "OhZ the dyone reckilld veaseds and be this thking. Paighon, Leforo. I've thiged forses.\n",
      "\n",
      "DOPERETE:\n",
      "Do in in's inliket it the way sh\n",
      "\n",
      "Epoch: 000005\n",
      "Average_loss: 58.0926; Last batch loss: 63.9169\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "GONDO:\n",
      "Ammpocnler... wain' look go It niging hat  this in Leeting ..E hand.\n",
      "\n",
      "LUKE:\n",
      "Chat's ontt.\n",
      "\n",
      "GONZO:\n",
      "Headeghthers the mopiot, it offoldmain tro.. I'm sors.\n",
      "\n",
      "DOVE:\n",
      "MIG T UDEOb..\n",
      "\n",
      "DOUGHWOM:\n",
      "He's as don't berig.\n",
      "\n",
      "COOCK:\n",
      "War kin. What waiteris. Bliget astiels your tho Mrefo\n",
      "\n",
      "Epoch: 000006\n",
      "Average_loss: 57.3249; Last batch loss: 63.8895\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "AVERMAN:\n",
      "Us Hanks Do like get Cont a nothink, bebart is priest be.  pkcon hy undin't bectupstind extting it lide tirmon, in fe troe all ab ue.\n",
      "\n",
      "MENDY:\n",
      "Bust sees, ECONNAMOR:\n",
      "Anking ip.\n",
      "\n",
      "LANDO:\n",
      "Here...\n",
      "\n",
      "DELOY:\n",
      "Af sed like of cusppicl rie by we rrobly vucnot in anling, I was \n",
      "\n",
      "Epoch: 000007\n",
      "Average_loss: 56.6961; Last batch loss: 63.3844\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "Than bus, lrocksocgeand bee enegh, went af on. You reaby. Aiss a bud Tarsent, dain, you. I don't you hunven! Leaveadid.\n",
      "\n",
      "JIKE:\n",
      "She int. I'll be the Ares. Oge it mey bacgen't a and but boryw, your hon. You'r, traytulilieve see hearilefer fromed wyinger sire, live sayched of\n",
      "\n",
      "Epoch: 000008\n",
      "Average_loss: 56.1684; Last batch loss: 62.9427\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "BUGEED:\n",
      "You're face acmong! Jonernafe, won a tood can't live can to know an on in Coinurture out Mon's shoes ewnee pelice that hes dartet hergaflermanips out bedven here's that some, bid.\n",
      "\n",
      "CINA:\n",
      "You an that kill. You leathis.  Fhey to blen?\n",
      "\n",
      "INGOR! DUGETT:\n",
      "How donn youp. P\n",
      "\n",
      "Epoch: 000009\n",
      "Average_loss: 55.7385; Last batch loss: 61.9755\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DAWKA D At a word sky. What.\n",
      "\n",
      "JOHE:\n",
      "ThI T2Es, I dod ane pion fant huld the muy that go I calling int some ove arath meat. We't'p jotthom wher ther, mion mitcing that attro. I'p for y's and mo ill soram the . boys for op us! You save, to to to his with shat my a unfertich. \n",
      "\n",
      "Epoch: 000010\n",
      "Average_loss: 55.3533; Last batch loss: 62.5441\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "GUKAN:\n",
      "It for bettiction?...\n",
      "\n",
      "STORMAT:\n",
      "And noy, you his with good pring that is.\n",
      "\n",
      "EDDOR:\n",
      "Yeshe Mabut? You defore that well, yian par put whosice, deto doiving a fugul arinss of...  Whis But are hands the flia ithowinger of thrie Alo, you'reirat!\n",
      "\n",
      "CTEVE:\n",
      "Criad. I combout mi\n",
      "\n",
      "Epoch: 000011\n",
      "Average_loss: 55.0093; Last batch loss: 61.7026\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "CORBET:\n",
      "You what way am girves time, the ellipe Mforricher. He wh gory about our one?\n",
      "\n",
      "GONZO:\n",
      "How engure.\n",
      "\n",
      "DON'LLUMER:\n",
      "Whisgo with in of Forthis my tho mand ! Engart., wiltsir letie, a.\n",
      "\n",
      "MADON:\n",
      "Maythornsce but yip a kilvesser... and metor we hea bous HORe. I coulde. Wh\n",
      "\n",
      "DO\n",
      "\n",
      "Epoch: 000012\n",
      "Average_loss: 54.7213; Last batch loss: 62.1253\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "TEXE:\n",
      "That's to mill a me and. If mong! Anycher talkss to and that's save thede's comes conder. It. I mastons you. Plio my!\n",
      "\n",
      "STAVE I RITN:\n",
      "Raperther everde.\n",
      "\n",
      "SSAPINAPE:\n",
      "You's that wodday ton.  Gour mist disgaduve dast. I've Gon't wanleelle but back.\n",
      "\n",
      "LEIA:\n",
      "I hank on to mac\n",
      "\n",
      "Epoch: 000013\n",
      "Average_loss: 54.4470; Last batch loss: 60.8339\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DOO:\n",
      "No, I longerns in obe you're Didn't turleneld sakis in any Minny?\n",
      "\n",
      "MR. Y'RAN!\n",
      "\n",
      "SPITHRY:\n",
      "What no!\n",
      "\n",
      "GARE:\n",
      "Antite cownytusies op you Conning on. You lesppet, you.\n",
      "\n",
      "LEICHAR:\n",
      "She Heniig geven doedy.\n",
      "\n",
      "MRISkNIS MAGHEROD:\n",
      "How bet'm just'ming sad that I carse mee Micks just it\n",
      "\n",
      "Epoch: 000014\n",
      "Average_loss: 54.1966; Last batch loss: 60.7308\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "MV. POMARON:\n",
      "What wh it be that were worreinid. Mr Very?\n",
      "\n",
      "DANETHIL:\n",
      "I nights... bive masty the ture here? Nst taget how that some it if six, your preter.\n",
      "\n",
      "HWIKER:\n",
      "But I dope. I nagNy, bastlesone, bioghectie!  THe Exgraakizy bet's to no propay nave riece inst. And ywire.\n",
      "\n",
      "Y\n",
      "\n",
      "Epoch: 000015\n",
      "Average_loss: 53.9675; Last batch loss: 60.5765\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "LULEOR:\n",
      "What?\n",
      "\n",
      "MGICHAE:\n",
      "DeI pleed hix asser usmance. I woonld whit, it Heen, himasiluce pounge I worlien fuck Somartent that.\n",
      "\n",
      "CINDY:\n",
      "Leagnet! That it. I there the mo tople a mapewn you didine E Mak Wimmerming!\n",
      "\n",
      "DOM:\n",
      "I don't man\n",
      "\n",
      "ELMINETY:\n",
      "I live, we've.\n",
      "\n",
      "DOTHER:\n",
      "Yes job d\n",
      "\n",
      "Epoch: 000016\n",
      "Average_loss: 53.7566; Last batch loss: 60.4220\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "RHEN:\n",
      "What?\n",
      "\n",
      "HAN:\n",
      "We well carestigios.\n",
      "\n",
      "JOE:\n",
      "No. Wes but retumast... I's 8\n",
      "Quy neer scougy for mester you haven't be lack woint I've frisetty wuse\n",
      "\n",
      "TRITNIE:\n",
      "So swally is. You teldinale, Conet in Pacpuse coule robleed fouling has to if pill I have to the kny retreading.  Vo\n",
      "\n",
      "Epoch: 000017\n",
      "Average_loss: 53.5618; Last batch loss: 60.2567\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "GUDA:\n",
      "So bie?  Duliticn.\n",
      "\n",
      "VOF:\n",
      "Hom.\n",
      "\n",
      "LEIA:\n",
      "Do up I and that at to aren Sether, us, Tomm same beme 'll.  What him, Mittay?\n",
      "\n",
      "KNRBED:\n",
      "I'm these is side befby he's we've sated oky, I fave to fio cay aatly is did uple. I'll fusencion excemsom. Hewly ovan the lixt?\n",
      "\n",
      "LANDY:\n",
      "But b\n",
      "\n",
      "Epoch: 000018\n",
      "Average_loss: 53.3811; Last batch loss: 60.0843\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "GHED:\n",
      "He toown't.\n",
      "\n",
      "LUKE:\n",
      "Lithing. LU3,?\n",
      "\n",
      "YWKALBODETTHURE, ECHASSORN:\n",
      "Eighte is ass, amseatis we galo start of be your shit.\n",
      "\n",
      "VADER:\n",
      "Well of aflang in and going to Rencthed moment.\n",
      "\n",
      "SARDON:\n",
      "You got ongon but at his haps eorded! Erens be pleconem.\n",
      "\n",
      "ELOOR:\n",
      "Aspalk you do some \n",
      "\n",
      "Epoch: 000019\n",
      "Average_loss: 53.2127; Last batch loss: 59.9864\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "ZORE:\n",
      "Willively a must giviing? Buninys?\n",
      "\n",
      "DONTY:\n",
      "What's to gool!\n",
      "\n",
      "CKADE:\n",
      "\n",
      "\n",
      "PERGHIN:\n",
      "I maon?\n",
      "\n",
      "GLETE:\n",
      "I got oud to wad the of?\n",
      "\n",
      "DOCK:\n",
      "Morfating elller.\n",
      "\n",
      "THED:\n",
      "Wh walt your back in!\n",
      "\n",
      "LUKE:\n",
      "Ohin.\n",
      "\n",
      "KORBEN:\n",
      "Well, I werk. Doon thelen't aid woite cestwey.\n",
      "\n",
      "LETT:\n",
      "Oh.\n",
      "\n",
      "TRUKE:\n",
      "Hed ta\n",
      "\n",
      "Epoch: 000020\n",
      "Average_loss: 53.0552; Last batch loss: 59.8899\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "AVERIA:\n",
      "Why? Lheaht? \n",
      "\n",
      "EDDY:\n",
      "Oh, woudd firsonly you the frounlas okaunee lew to dearty it I'm no... I'm drance bitte, oven yourn say?\n",
      "\n",
      "COkNILOU:\n",
      "Hello up Horfave, is we're says hill areft! It meen mestonhting any his neel cight you wall yat open on tell?\n",
      "\n",
      "LITE:\n",
      "Hens are yo\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000021\n",
      "Average_loss: 52.9073; Last batch loss: 59.6862\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "HAD:\n",
      "Thele dim. I lesper.\n",
      "\n",
      "DONNA:\n",
      "I've mencicw. I jadd anythertaok do I want on was a fuck crick?. neen a a feane octafe.\n",
      "\n",
      "DN:\n",
      "Brosanicg ease us one claine you.  Do, wild will.\n",
      "\n",
      "PINDO:\n",
      "I could haver?\n",
      "\n",
      "EDDY:\n",
      "Well Tilds didn't think right?  Peal you do rogure?\n",
      "\n",
      "AN:\n",
      "I lill.\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 000022\n",
      "Average_loss: 52.7680; Last batch loss: 59.6035\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "MUDDY:\n",
      "What with your about mownicus hoily want tilk aln percer my shit wett. This Thred the rebrert!.. don't we're hation. Pliald did He compat of this formapse a geten jadd to lig!\n",
      "\n",
      "DONNIE:\n",
      "Frightes of up.\n",
      "\n",
      "DONNIE:\n",
      "Than for come brong to rad on is your youp thought at st\n",
      "\n",
      "Epoch: 000023\n",
      "Average_loss: 52.6365; Last batch loss: 59.4965\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "HENGUROM:\n",
      "What comeshonde Tike wilk onland id. It an carkenilak, alo wain'..s like have.\n",
      "\n",
      "DRITHY:\n",
      "Ohath themafulyind has packin' nielly  you Use fuse wome flasher.\n",
      "\n",
      "RATTEN:\n",
      "En't shouldgit mean, for hill.\n",
      "\n",
      "EVERETTCFETHOF:\n",
      "Conticath do the woon, sagsly with fise. It meand, I\n",
      "\n",
      "Epoch: 000024\n",
      "Average_loss: 52.5123; Last batch loss: 59.3668\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "TCRITE:\n",
      "Of tive riech Hase funf can your hemactalus?\n",
      "\n",
      "CONGEWRY:\n",
      "like to hap and plecusivekay priur to going mice dilifill. I doncule and ened you of we detere. It's goncef restece.\n",
      "\n",
      "OVEUDD:\n",
      "What's thther as and.\n",
      "\n",
      "JEE:\n",
      "All Mocted up?\n",
      "\n",
      "DWICHAINA:\n",
      "I'm verrorly livrift my sutc\n",
      "\n",
      "Epoch: 000025\n",
      "Average_loss: 52.3944; Last batch loss: 59.3225\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "LUMAN:\n",
      "If is more that a me it in that is Did west all hown hong right Fromeding up.! done weret a methinger, that don't me wourd gent enchon' things sugh, that?\n",
      "\n",
      "MONNTY:\n",
      "I the womes it interstioser not and have sacts is chuse!\n",
      "\n",
      "BRUECORIE:\n",
      "Yeah I usglione.\n",
      "\n",
      "KORBEN:\n",
      "No woul\n",
      "\n",
      "Epoch: 000026\n",
      "Average_loss: 52.2824; Last batch loss: 59.2180\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "MEDERR:\n",
      "I'm they? See?\n",
      "\n",
      "DONNIE:\n",
      "We bellaes your fack it.  That compuie mee runver have he Weme, it, Cool, bepprce of the bissous be couch to cowno, it!\n",
      "\n",
      "HANNOF:\n",
      "Erentwalt seep. I can back tike your whik, me Done to fick.\n",
      "\n",
      "RAY:\n",
      "Ereff apace and it that is the goide.\n",
      "\n",
      "DEND:\n",
      "I\n",
      "\n",
      "Epoch: 000027\n",
      "Average_loss: 52.1759; Last batch loss: 59.1159\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "YWIGOR:\n",
      "I shall.\n",
      "\n",
      "MAMINK:\n",
      "I happes been aw I'm much gill willing he's it. Demban him doing the your has we're a staring mongana by wanft.  I the hadicg the onol to fant ooker him in happ?\n",
      "\n",
      "HAN:\n",
      "Eddom all you, Teating I'm  Chen the thlein't cuall.\n",
      "\n",
      "D14N:\n",
      "What or ex9wet. Loo\n",
      "\n",
      "Epoch: 000028\n",
      "Average_loss: 52.0742; Last batch loss: 58.9833\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "BUGOR!\n",
      "\n",
      "LUKE:\n",
      "I heperect lece ready in....\n",
      "\n",
      "LEyNKERVIERELL..\n",
      "\n",
      "KORBEN:\n",
      "Mr. Plight io torlment.\n",
      "\n",
      "PLIES!ERI DALTER, GHTRIMGOR:\n",
      "You're roob hut it it.orone about to this worve, for mone weat on mash. day out hond there!\n",
      "\n",
      "CITA:\n",
      "Empnol...  Licko.\n",
      "\n",
      "DWIGO:\n",
      "There'd. It's wellly, Mr\n",
      "\n",
      "Epoch: 000029\n",
      "Average_loss: 51.9789; Last batch loss: 59.0649\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "AULAY:\n",
      "Does. But Gynoter.\n",
      "\n",
      "LUKE:\n",
      "It case.\n",
      "\n",
      "CORETY:\n",
      "I that a.\n",
      "\n",
      "EDDY:\n",
      "Dooved tho will feore. I'm son that's can't going ond mund too lot can drang your into.\n",
      "\n",
      "DUKE:\n",
      "Hill Conna a further my fince not to is hard they donse's gonmy.\n",
      "\n",
      "DONNA:\n",
      "Of and thinka tere we could sight! Th\n",
      "\n",
      "Epoch: 000030\n",
      "Average_loss: 51.8861; Last batch loss: 58.8000\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "BLAND:\n",
      "What lown we't hale timn to in this sittring?\n",
      "\n",
      "VIDDY:\n",
      "Donnwoko y...\n",
      "\n",
      "LIZABERITE:\n",
      "We unvelanst you me not... look with.\n",
      "\n",
      "VINFE:\n",
      "So have me Dong I right. Aght cof at home that atent?\n",
      "\n",
      "TEREEN:\n",
      "Downied tomoted.  You're a nut for mair?\n",
      "\n",
      "DESMAN:\n",
      "Bho's we mactace afpese a \n",
      "\n",
      "Epoch: 000031\n",
      "Average_loss: 51.7970; Last batch loss: 58.6984\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "BERTHREELIN:\n",
      "That sowly, ficts. Gos man loor Roee grelly. .. youglrewh!\n",
      "\n",
      "DONNIE:\n",
      "Sripts come  in you will mottar i.\n",
      "\n",
      "CERMY:\n",
      "Wause coped It the coming come doing out. Mast, I Pive everytoredos like mon's will, sown? And carig copece prowly pocther agoof poin, ussont timy on\n",
      "\n",
      "Epoch: 000032\n",
      "Average_loss: 51.7116; Last batch loss: 58.5595\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "MR. PIGOR HOT SOUD TRUGGON:\n",
      "Loos use out anged instank, dice.\n",
      "\n",
      "DWIGHT:\n",
      "OkG Cooth only, I'm like's it... delpaiige my preby around. Dund new, Frouch about there we wueclame canneer on thing olF. Un for the Hare creall to ago not orah night is of crape woryted manted op the \n",
      "\n",
      "Epoch: 000033\n",
      "Average_loss: 51.6296; Last batch loss: 58.4475\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "GUNITA:\n",
      "Bust way it!\n",
      "\n",
      "EOPER:\n",
      "What here to I was the for at the finted evenshwow.\n",
      "\n",
      "GONZO:\n",
      "Lucking sela eversor?\n",
      "\n",
      "EVERETT:\n",
      "Is swiles He will hem, give... as thrues 're with have back tenor say a rig now bame, Ho so not,!\n",
      "\n",
      "EDDY:\n",
      "Winking hem.\n",
      "\n",
      "MAVERICK:\n",
      "Gon on. Hor rilg you lo\n",
      "\n",
      "Epoch: 000034\n",
      "Average_loss: 51.5603; Last batch loss: 60.2675\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DUGEE:\n",
      "AUMG\n",
      "DR. PERSSTY:\n",
      "Oh as your bazes. He way in know them come us. Hings the lank ok thim. I'm bemorilan' the know.\n",
      "\n",
      "DODOON:\n",
      "Chaid.\n",
      "\n",
      "CORE..:\n",
      "Herce.\n",
      "\n",
      "BUDDRIN:\n",
      "You nig to be think this dona?\n",
      "\n",
      "DOTE:\n",
      "That a donour?\n",
      "\n",
      "DEN:\n",
      "Wesh onay good seet.\n",
      "\n",
      "DONNOR:\n",
      "We the me hip, progen\n",
      "\n",
      "Epoch: 000035\n",
      "Average_loss: 51.4898; Last batch loss: 58.7215\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "PEVIELLITE:\n",
      "Wellsizumen!\n",
      "\n",
      "OPAICOLFASEAN:\n",
      "Were a bed I squf the righintwen Gudd who far?\n",
      "\n",
      "BWELLIE:\n",
      "I'm just mapla the atone, you're see was just must you'll Vered!\n",
      "\n",
      "COA:\n",
      "MCKERRINGERSTHRELBURGH:\n",
      "We'll sended on lot.\n",
      "\n",
      "MR. WULLIEBERMMY:\n",
      "I'm cust doiamonved Dias the has pered t\n",
      "\n",
      "Epoch: 000036\n",
      "Average_loss: 51.4203; Last batch loss: 60.3487\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DWILDY:\n",
      "Well you wanserer.\n",
      "\n",
      "MRBENTY:\n",
      "Chit compor bena... houndineed... there wuy, Ewnot I dyessical?\n",
      "\n",
      "LUKE:\n",
      "He eaced can' to get one pregaythecen panested yourfrete I nute dyn't there pooting to is time Hew, Exprog preter exil for map here.\n",
      "\n",
      "THRIELRYN:\n",
      "We kids engent me ci\n",
      "\n",
      "Epoch: 000037\n",
      "Average_loss: 51.3544; Last batch loss: 60.5295\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "THREEN:\n",
      "Hemed, not have I don't mesind fasp... Nonna?\n",
      "\n",
      "GONZOREBARE NOSE TORAHNEATOR:\n",
      "The wanced in his and owed a fuck onge pould any preally and tinst. Anit agains of I how have oillssald Dirdend, with you siguties been shindidio by mK. your the cone then.\n",
      "\n",
      "CFREFFEETE:\n",
      "Ho\n",
      "\n",
      "Epoch: 000038\n",
      "Average_loss: 51.2914; Last batch loss: 59.4234\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "LURAOR:\n",
      "Hey, stolsed God to going that seway, a driso.  Jave a char a cop newn birt, and to? Yeal you, Trey be losved don't a pally there? Whave sown to see.\n",
      "\n",
      "ELEY:\n",
      "Co so wail be to and renerss. Thid.\n",
      "\n",
      "LEEDOR:\n",
      "He's the owle, I'm ghy find The hisfuren, the grees fut. We cri\n",
      "\n",
      "Epoch: 000039\n",
      "Average_loss: 51.2309; Last batch loss: 59.3449\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "LERMCINNY:\n",
      "Get on that it alsets and Hack estan.re, Jon kint dows a veadd!  werleart you'll javion of a mowno a comething.\n",
      "\n",
      "DENMONDO:\n",
      "I wanch a velse take sodding. Se's me you lake your tir...\n",
      "\n",
      "JEFFREY:\n",
      "I just, them?\n",
      "\n",
      "CERNON:\n",
      "I can dotio we talk to can us Ond see on you'll\n",
      "\n",
      "Epoch: 000040\n",
      "Average_loss: 51.1716; Last batch loss: 59.3283\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "4VUDDY:\n",
      "It's got a Diving was our deniont oned... a cop counds gooding Mrriss, in just are.\n",
      "\n",
      "MI, RUMSOROS ORETT:\n",
      "Caring if a coble feel compucked yess man.\n",
      "\n",
      "DONNIE:\n",
      "He's they know have do you sometar of weron to twing pack of thes I fown got spowhionedm...'repen shit.\n",
      "\n",
      "YOK\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000041\n",
      "Average_loss: 51.1136; Last batch loss: 59.2898\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "46N:\n",
      "YeO, the baffir! N:\n",
      "Okt you imont. Is is to and us to damen it, Frow never?\n",
      "\n",
      "MONZOREFTEMEN:\n",
      "Aleves dour?\n",
      "\n",
      "JUND.\n",
      "\n",
      "MR. NIS.E:\n",
      "VIt., scuo live now that just sack couch to botul... you plar, Door!\n",
      "\n",
      "MOADO:\n",
      "It's to this his look orry bestoges and apotens I was onybosies en \n",
      "\n",
      "Epoch: 000042\n",
      "Average_loss: 51.0568; Last batch loss: 59.3576\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "AOOSTY:\n",
      "He watting 're and for himbpom She week to.\n",
      "\n",
      "CINSSE:\n",
      "Whroe, Frainy, is she Are ampsedles.  Now with do. Wo seann.\n",
      "\n",
      "LOUE:\n",
      "Not can''ll dayb.\n",
      "\n",
      "MIM:\n",
      "P mupp. That's pother.\n",
      "\n",
      "COWBOY:\n",
      "What I woulddycontitn.  Fut ond us being to just younet?\n",
      "\n",
      "SAID PER:\n",
      "Dattair, came munts \n",
      "\n",
      "Epoch: 000043\n",
      "Average_loss: 51.0020; Last batch loss: 59.1546\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "EVERETT:\n",
      "Thtou.\n",
      "\n",
      "CWOPPRAN:\n",
      "It's pahen't co retuy, thr.\n",
      "\n",
      "CONNINERY THURMANLERIS\n",
      "\n",
      "PETE:\n",
      "I here. It's lose. We're from soy.\n",
      "\n",
      "MR. BOY:\n",
      "No.\n",
      "\n",
      "VODA:\n",
      "Has just got it. IVEgot, If here.\n",
      "\n",
      "DUKN:\n",
      "Nar desco to no 'ven me even kind!\n",
      "\n",
      "RITE:\n",
      "I toow, Down why are Jefererlyboybose. This but \n",
      "\n",
      "Epoch: 000044\n",
      "Average_loss: 50.9491; Last batch loss: 60.0035\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DR...'Y:\n",
      "If.\n",
      "\n",
      "GONZO:\n",
      "Term do tos the on like on the gonna lackeve. You can't to pigtried ire think?\n",
      "\n",
      "COROS:\n",
      "Rey! You wass oud about you live of therest sudsieg workerteded to just I what?..\n",
      "\n",
      "LEIA:\n",
      "But et not.\n",
      "\n",
      "JOHN:\n",
      "Faved May use...\n",
      "\n",
      "EVERETT:\n",
      "What aboet the thele gotthol h\n",
      "\n",
      "Epoch: 000045\n",
      "Average_loss: 50.8971; Last batch loss: 59.8503\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DOM:\n",
      "He were dion, Soreda woule I gather. You werriness fight?\n",
      "\n",
      "CHETEC MOTY:\n",
      "Dine... I'll geal that's till get?!\n",
      "\n",
      "PRIA:\n",
      "And bettative are conth a him!\n",
      "\n",
      "VADER:\n",
      "All to him scourdy baved alcoh me, Wheretytin' the cen Oke?\n",
      "\n",
      "AVEUR:\n",
      "My hee sickiniby say it myst these dunve feer \n",
      "\n",
      "Epoch: 000046\n",
      "Average_loss: 50.8462; Last batch loss: 59.8171\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "PEN:\n",
      "Oh..What this once you do your that and give! There!\n",
      "\n",
      "LUKE:\n",
      "Baftly I 5.\n",
      "\n",
      "CORAND:\n",
      "How, ghey?\n",
      "\n",
      "ALLIAA:\n",
      "No, frerden se okay... change, of and for roty out is and?\n",
      "\n",
      "MR..THEN:\n",
      "And cran with speldey!\n",
      "\n",
      "EVERETT:\n",
      "How, up?\n",
      "\n",
      "IWK ECHENT:\n",
      "They're getch angmat! Consbie'.   Ran't so\n",
      "\n",
      "Epoch: 000047\n",
      "Average_loss: 50.7966; Last batch loss: 59.7597\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DUKERESA:\n",
      " Milansa with we do.\n",
      "\n",
      "DONNIE:\n",
      "You're gotdo?  the Fremor no old reallion' you rean fhour.\n",
      "\n",
      "HAN:\n",
      "You tilkin't like okay to be!\n",
      "\n",
      "PETE:\n",
      "Shots.\n",
      "\n",
      "MAVERIC:\n",
      "What for meloby are that Kip... ONE SHET.. BRUMAN:\n",
      "An ory.. it's a futetirede littar not fake they where mone, No \n",
      "\n",
      "Epoch: 000048\n",
      "Average_loss: 50.7482; Last batch loss: 59.7580\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "\n",
      "PRIESE:\n",
      "Jespowing to bly.\n",
      "\n",
      "GOOSE:\n",
      "If a langer?  Butare teriff on the Wesp I'll goth in to be?\n",
      "\n",
      "LUKE:\n",
      "Your rool a jost. He?!\n",
      "\n",
      "YOOBOO:\n",
      "That's beed on betruty boter. He aront you them Is disty you koot fradatit... it. Where badly chestloping a side bonirul, is them to them a\n",
      "\n",
      "Epoch: 000049\n",
      "Average_loss: 50.7010; Last batch loss: 59.7626\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "BEOSTER:\n",
      "Dant to pilliselves... I guel hours been no see melorest stiel you foinsoys...\n",
      "\n",
      "AVOVIK:\n",
      "Why night press...wall can of nothors to calar to one... araus they reater. IS All get hero ingath.\n",
      "\n",
      "EEPITTH:\n",
      "Luke?\n",
      "\n",
      "DUNNICY:\n",
      "Lught. Tvien the way.\n",
      "\n",
      "STEMSE:\n",
      "Oh!\n",
      "\n",
      "ILGHAIL:\n",
      "Dooy.\n",
      "\n",
      "Epoch: 000050\n",
      "Average_loss: 50.6549; Last batch loss: 59.6904\n",
      "=====================================================\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset()\n",
    "dataset.preprocess(\"data/selected_conversations.txt\")\n",
    "dataset.create_minibatches(batch_size=5, sequence_length=30)\n",
    "rnn = run_language_model(dataset, 50, sequence_length=dataset.sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. vježba: modeliranje nizova povratnim neuronskim mrežama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pdb\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadatak 1: učitavanje podataka i batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hardcoded paths - change if necessary\n",
    "root = 'data'\n",
    "\n",
    "# this one you need to download from the dataset\n",
    "full_dataset = 'data/movie_lines.txt'\n",
    "\n",
    "output_destination = 'data/selected_conversations.txt'\n",
    "movie_selection = 'data/selected_movies.txt'\n",
    "\n",
    "# separator used in the original dataset\n",
    "separator = ' +++$+++ '\n",
    "\n",
    "# movie ID file\n",
    "MOVIE_ID = 0\n",
    "\n",
    "# full conversation dataset file\n",
    "MOVIE_ID_FULL = 2\n",
    "# reverse indexing\n",
    "CHARACTER_NAME = -2\n",
    "CHARACTER_LINE = -1\n",
    "\n",
    "# keep just these characters for simplicity (and utf8 breaking)\n",
    "repl = r'[^A-Za-z0-9()\\,!\\?\\'\\`\\. ]'\n",
    "\n",
    "\n",
    "# regex replace\n",
    "def filter(string):\n",
    "    return re.sub(repl, '', string)\n",
    "\n",
    "\n",
    "# from a movie ID string (e.g. M134), output the number (134)\n",
    "def number_from_id(id):\n",
    "    return int(id[1:])\n",
    "\n",
    "\n",
    "# read just movie ID's, rest is for readability\n",
    "def read_selected(path_to_selected_movies):\n",
    "    selected_movies = set()\n",
    "\n",
    "    with open(path_to_selected_movies, 'r') as infile:\n",
    "        for line in infile:\n",
    "            parts = line.strip().split(separator)\n",
    "            selected_movies.add(parts[MOVIE_ID].strip())\n",
    "    return selected_movies\n",
    "\n",
    "\n",
    "# select and write to output file\n",
    "def select_and_write(path_to_full_dataset, path_to_output, selected_movies):\n",
    "    movies = {}\n",
    "\n",
    "    with open(path_to_full_dataset, 'r', encoding=\"ISO-8859-1\") as infile:\n",
    "\n",
    "        for line in infile:\n",
    "\n",
    "            parts = line.strip().split(separator)\n",
    "\n",
    "            if parts[MOVIE_ID_FULL].strip() not in selected_movies:\n",
    "                continue\n",
    "\n",
    "            # take data and transform to tuple\n",
    "            ID = parts[MOVIE_ID_FULL]\n",
    "            char_name = parts[CHARACTER_NAME]\n",
    "            char_line = parts[CHARACTER_LINE]\n",
    "\n",
    "            tup = (number_from_id(ID), char_name, char_line)\n",
    "\n",
    "            # add to map\n",
    "            if ID not in movies:\n",
    "                movies[ID] = []\n",
    "            movies[ID].append(tup)\n",
    "\n",
    "    with open(path_to_output, 'w') as out:\n",
    "        for movie in movies:\n",
    "            # sort by line number\n",
    "            dialogue = sorted(movies[movie], key=lambda t: t[0])\n",
    "            for n, name, text in dialogue:\n",
    "                out.write(filter(name) + ':\\n' + filter(text) + '\\n\\n')\n",
    "\n",
    "\n",
    "def main():\n",
    "    # uses hardcoded paths\n",
    "    selection = os.path.join(root, movie_selection)\n",
    "    selected_movies = read_selected(selection)\n",
    "\n",
    "    dataset = os.path.join(root, full_dataset)\n",
    "    output = os.path.join(root, output_destination)\n",
    "    select_and_write(dataset, output, selected_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def preprocess(self, input_file):\n",
    "        with open(input_file, \"r\") as f:\n",
    "            data = f.read()\n",
    "\n",
    "        # count and sort most frequent characters\n",
    "        characters, counts = np.unique(list(data), return_index=True)\n",
    "        self.sorted_characters = characters[np.argsort(-counts)]\n",
    "        self.vocab_size = len(self.sorted_characters)\n",
    "\n",
    "        # self.sorted chars contains just the characters ordered descending by frequency\n",
    "        self.char2id = dict(zip(self.sorted_characters, range(len(self.sorted_characters)))) \n",
    "        # reverse the mapping\n",
    "        self.id2char = {k:v for v,k in self.char2id.items()}\n",
    "        # convert the data to ids\n",
    "        self.x = np.array(list(map(self.char2id.get, data)))\n",
    "\n",
    "    def encode(self, sequence):\n",
    "        encoded_sequence = np.array([self.char2id[c] for c in sequence], dtype=np.int32)\n",
    "        return encoded_sequence\n",
    "\n",
    "    def decode(self, sequence):\n",
    "        decoded_sequence = [self.id2char[c] for c in sequence]\n",
    "        return decoded_sequence\n",
    "        \n",
    "    def create_minibatches(self, batch_size, sequence_length):\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.batch_index = 0\n",
    "        \n",
    "        self.num_batches = int(len(self.x) / (batch_size * sequence_length)) # calculate the number of batches\n",
    "        self.batches = np.zeros([self.num_batches, self.batch_size, self.sequence_length + 1], dtype=np.int32)  \n",
    "    \n",
    "        for batch in range(self.num_batches):\n",
    "            for index in range(self.batch_size):\n",
    "                start = batch * sequence_length + index * (self.num_batches * sequence_length)\n",
    "                end = start + self.sequence_length + 1 \n",
    "                self.batches[batch, index, :] = self.x[start : end]\n",
    "\n",
    "    def next_minibatch(self):        \n",
    "        if self.batch_index == self.num_batches:\n",
    "            self.batch_index = 0\n",
    "\n",
    "        batch = self.batches[self.batch_index, :, :]\n",
    "        self.batch_index += 1\n",
    "        \n",
    "        batch_x = batch[:, :-1]\n",
    "        batch_y = batch[:, 1:]\n",
    "        \n",
    "        return self.batch_index == self.num_batches, batch_x, batch_y\n",
    "    \n",
    "    def _as_one_hot(self, x, vocab):\n",
    "        n = len(x)\n",
    "        Yoh = np.zeros((n, vocab))\n",
    "        Yoh[np.arange(n), x] = 1\n",
    "        \n",
    "        return Yoh\n",
    "    \n",
    "    def one_hot(self, batch):\n",
    "        if batch.ndim == 1:\n",
    "            return self._as_one_hot(batch, self.vocab_size)\n",
    "        else:\n",
    "            return np.array([self._as_one_hot(b, self.vocab_size) for b in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T', 'o', 'm', 'o', 'r', 'r', 'o', 'w']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test 1\n",
    "dat = Dataset()\n",
    "dat.preprocess(\"data/selected_conversations.txt\")\n",
    "\n",
    "dat.decode(dat.encode(\"Tomorrow\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadatak 2: obična jednoslojna povratna neuronska mreža"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN():\n",
    "    # inicijalizacija parametara\n",
    "    def __init__(self, hidden_size, sequence_length, vocabulary_size, learning_rate):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.U = np.random.normal(size=[vocabulary_size, hidden_size], scale=1.0 / np.sqrt(hidden_size))  # ... input projection\n",
    "        self.W = np.random.normal(size=[hidden_size, hidden_size], scale=1.0 / np.sqrt(hidden_size))  # ... hidden-to-hidden projection\n",
    "        self.b = np.zeros([1, hidden_size])\n",
    "\n",
    "        self.V = np.random.normal(size=[hidden_size, vocabulary_size], scale=1.0 / np.sqrt(vocabulary_size)) # ... output projection\n",
    "        self.c = np.zeros([1, vocabulary_size]) # ... output bias\n",
    "\n",
    "        # memory of past gradients - rolling sum of squares for Adagrad\n",
    "        self.memory_U = np.zeros_like(self.U)\n",
    "        self.memory_W = np.zeros_like(self.W)\n",
    "        self.memory_V = np.zeros_like(self.V)\n",
    "        self.memory_b = np.zeros_like(self.b)\n",
    "        self.memory_c = np.zeros_like(self.c)\n",
    "        \n",
    "    def rnn_step_forward(self, x, h_prev, U, W, b):\n",
    "        # A single time step forward of a recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity.\n",
    "\n",
    "        # x - input data (minibatch size x input dimension)\n",
    "        # h_prev - previous hidden state (minibatch size x hidden size)\n",
    "        # U - input projection matrix (input dimension x hidden size)\n",
    "        # W - hidden to hidden projection matrix (hidden size x hidden size)\n",
    "        # b - bias of shape (hidden size x 1)\n",
    "\n",
    "        h_current, cache = None, None\n",
    "        h_current = np.tanh(np.dot(h_prev, W) + np.dot(x, U) + b)\n",
    "        cache = (W, x, h_prev, h_current)\n",
    "\n",
    "        # return the new hidden state and a tuple of values needed for the backward step\n",
    "\n",
    "        return h_current, cache\n",
    "\n",
    "\n",
    "    def rnn_forward(self, x, h0, U, W, b):\n",
    "        # Full unroll forward of the recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity\n",
    "\n",
    "        # x - input data for the whole time-series (minibatch size x sequence_length x input dimension)\n",
    "        # h0 - initial hidden state (minibatch size x hidden size)\n",
    "        # U - input projection matrix (input dimension x hidden size)\n",
    "        # W - hidden to hidden projection matrix (hidden size x hidden size)\n",
    "        # b - bias of shape (hidden size x 1)\n",
    "        \n",
    "        h, cache = None, None\n",
    "        hs = [h0]\n",
    "        caches = []\n",
    "        \n",
    "        for sequence in range(self.sequence_length):\n",
    "            data = x[:, sequence, :]\n",
    "            h_current, cache_current = self.rnn_step_forward(data, hs[-1], U, W, b)\n",
    "            \n",
    "            hs.append(h_current)\n",
    "            caches.append(cache_current)\n",
    "            \n",
    "        hs = np.array(hs[1:]).transpose((1, 0, 2))\n",
    "        \n",
    "        # return the hidden states for the whole time series (T+1) and a tuple of values needed for the backward step\n",
    "        \n",
    "        return hs, caches\n",
    "    \n",
    "    def rnn_step_backward(self, grad_next, cache):\n",
    "        # A single time step backward of a recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity.\n",
    "\n",
    "        # grad_next - upstream gradient of the loss with respect to the next hidden state and current output\n",
    "        # cache - cached information from the forward pass        \n",
    "        dh_prev, dU, dW, db = None, None, None, None\n",
    "        W, x, h_prev, h_current = cache\n",
    "        \n",
    "        # compute and return gradients with respect to each parameter\n",
    "        # HINT: you can use the chain rule to compute the derivative of the\n",
    "        # hyperbolic tangent function and use it to compute the gradient\n",
    "        # with respect to the remaining parameters\n",
    "        \n",
    "        # 65. i 66. slajd formule\n",
    "        da = grad_next * (1 - h_current**2)\n",
    "        \n",
    "        dh_prev = np.dot(da, W.T)\n",
    "        dU = np.dot(x.T, da)\n",
    "        dW = np.dot(h_prev.T, da)\n",
    "        db = np.sum(da, axis=0)\n",
    "\n",
    "        return dh_prev, dU, dW, db\n",
    "\n",
    "\n",
    "    def rnn_backward(self, dh, cache):\n",
    "        # Full unroll forward of the recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity\n",
    "\n",
    "        dU = np.zeros_like(self.U)\n",
    "        dW = np.zeros_like(self.W)\n",
    "        db = np.zeros_like(self.b)\n",
    "\n",
    "        # compute and return gradients with respect to each parameter\n",
    "        # for the whole time series.\n",
    "        # Why are we not computing the gradient with respect to inputs (x)?\n",
    "        grads = np.zeros_like(dh[0])\n",
    "        for dh_t, cache_t in reversed(list(zip(dh, cache))):\n",
    "            grads, dU_t, dW_t, db_t = self.rnn_step_backward(dh_t + grads, cache_t)\n",
    "            \n",
    "            dU += dU_t\n",
    "            dW += dW_t\n",
    "            db += db_t\n",
    "        \n",
    "        return np.clip(dU, -5, 5), np.clip(dW, -5, 5), np.clip(db, -5, 5)\n",
    "    \n",
    "    def output(self, h, V, c):\n",
    "        # Calculate the output probabilities of the network\n",
    "        return np.dot(h, V) + c\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        x -= np.max(x)\n",
    "        logits_exp = np.exp(x)\n",
    "        return logits_exp / np.sum(logits_exp, axis=1, keepdims=True)\n",
    "\n",
    "    def output_loss_and_grads(self, h, V, c, y):\n",
    "        # Calculate the loss of the network for each of the outputs\n",
    "\n",
    "        # h - hidden states of the network for each timestep. \n",
    "        #     the dimensionality of h is (batch size x sequence length x hidden size (the initial state is irrelevant for the output)\n",
    "        # V - the output projection matrix of dimension hidden size x vocabulary size\n",
    "        # c - the output bias of dimension vocabulary size x 1\n",
    "        # y - the true class distribution - a tensor of dimension \n",
    "        #     batch_size x sequence_length x vocabulary size - you need to do this conversion prior to\n",
    "        #     passing the argument. A fast way to create a one-hot vector from\n",
    "        #     an id could be something like the following code:\n",
    "\n",
    "        #   y[batch_id][timestep] = np.zeros((vocabulary_size, 1))\n",
    "        #   y[batch_id][timestep][batch_y[timestep]] = 1\n",
    "\n",
    "        #     where y might be a list or a dictionary.\n",
    "\n",
    "        loss, dh, dV, dc = None, None, None, None\n",
    "        \n",
    "        loss = 0\n",
    "        dhs = []\n",
    "        dV = np.zeros_like(self.V)\n",
    "        dc = np.zeros_like(self.c)\n",
    "        \n",
    "        batch_size = len(h)\n",
    "        \n",
    "        # calculate the output (o) - unnormalized log probabilities of classes\n",
    "        # calculate yhat - softmax of the output\n",
    "        # calculate the cross-entropy loss\n",
    "        # calculate the derivative of the cross-entropy softmax loss with respect to the output (o)\n",
    "        # calculate the gradients with respect to the output parameters V and c\n",
    "        # calculate the gradients with respect to the hidden layer h\n",
    "        \n",
    "        for sequence in range(self.sequence_length):\n",
    "            y_true = y[:, sequence, :]\n",
    "            h_t = h[:, sequence, :]\n",
    "            \n",
    "            o = self.output(h_t, V, c)\n",
    "            yhat = self.softmax(o)\n",
    "            \n",
    "            loss -= np.sum(np.log(yhat) * y_true) / batch_size\n",
    "            \n",
    "            dO = (yhat - y_true) / batch_size\n",
    "            dV += np.dot(h_t.T, dO)\n",
    "            dc += np.sum(dO, axis=0)\n",
    "            \n",
    "            dhs.append(np.dot(dO, V.T))\n",
    "\n",
    "        return loss, dhs, dV, dc\n",
    "    \n",
    "    # Adagrad parameter update\n",
    "    def update(self, batch_size, dU, dW, db, dV, dc):\n",
    "        # update memory matrices\n",
    "        # perform the Adagrad update of parameters\n",
    "        eps = 1e-7\n",
    "        for x, dx, mem_x in zip([self.U, self.W, self.b, self.V, self.c], [dU, dW, db, dV, dc], [self.memory_U, self.memory_W, self.memory_b, self.memory_V, self.memory_c]):\n",
    "            mem_x += np.square(dx)\n",
    "            x -= self.learning_rate * dx / np.sqrt(mem_x + eps)\n",
    "            \n",
    "    def step(self, hs, x, y):\n",
    "        hs, caches = self.rnn_forward(x, hs, self.U, self.W, self.b)\n",
    "        loss, dh, dV, dc = self.output_loss_and_grads(hs, self.V, self.c, y)\n",
    "        dU, dW, db = self.rnn_backward(dh, caches)\n",
    "        \n",
    "        self.update(len(x), dU, dW, db, dV, dc)\n",
    "        \n",
    "        return loss, hs[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_language_model(dataset, max_epochs, hidden_size=100, sequence_length=30, learning_rate=1e-1, sample_every=100):\n",
    "    vocabulary_size = len(dataset.sorted_characters)\n",
    "    rnn = RNN(hidden_size=hidden_size, sequence_length=sequence_length, vocabulary_size=vocabulary_size, learning_rate=learning_rate) # initialize the recurrent network\n",
    "\n",
    "    current_epoch = 0 \n",
    "    batch = 0\n",
    "\n",
    "    h0 = np.zeros((dataset.batch_size, hidden_size))\n",
    "    average_loss = 0\n",
    "    \n",
    "    should_print = False\n",
    "    while current_epoch < max_epochs: \n",
    "        e, x, y = dataset.next_minibatch()\n",
    "        \n",
    "        if e: \n",
    "            current_epoch += 1\n",
    "            h0 = np.zeros((dataset.batch_size, hidden_size))\n",
    "            \n",
    "            current_batch = batch % dataset.num_batches\n",
    "            print(\"Epoch: %06d\" % (current_epoch), end=\"\\n\")\n",
    "            print(\"Average_loss: %.4f; Last batch loss: %.4f\" % (average_loss/batch, loss))\n",
    "            print(\"=====================================================\")\n",
    "            \n",
    "            should_print = True\n",
    "\n",
    "        # One-hot transform the x and y batches\n",
    "        x_oh = dataset.one_hot(x)\n",
    "        y_oh = dataset.one_hot(y)\n",
    "\n",
    "        # Run the recurrent network on the current batch\n",
    "        # Since we are using windows of a short length of characters,\n",
    "        # the step function should return the hidden state at the end\n",
    "        # of the unroll. You should then use that hidden state as the\n",
    "        # input for the next minibatch. In this way, we artificially\n",
    "        # preserve context between batches.\n",
    "        loss, h0 = rnn.step(h0, x_oh, y_oh)\n",
    "        average_loss += loss\n",
    "\n",
    "        if batch % sample_every == 0: \n",
    "            # run sampling (2.2)\n",
    "            seed = \"HAN:\\nIs that good or bad?\\n\\n\"\n",
    "            sampled_data = sample(rnn, seed, 300, dataset)\n",
    "            \n",
    "            if should_print:\n",
    "                print(''.join(sampled_data))\n",
    "                print() \n",
    "                should_print = False\n",
    "            \n",
    "        batch += 1\n",
    "        \n",
    "def sample(rnn, seed, n_sample, dataset):\n",
    "    h0 = np.zeros([1, rnn.hidden_size])\n",
    "    seed_oh = dataset.one_hot(dataset.encode(seed))\n",
    "    \n",
    "    sampled = []\n",
    "    \n",
    "    for c_oh in seed_oh:\n",
    "        h0, _ = rnn.rnn_step_forward(c_oh.reshape([1, -1]), h0, rnn.U, rnn.W, rnn.b)\n",
    "        sampled.append(np.argmax(c_oh))\n",
    "        \n",
    "    for i in range(len(seed), n_sample):\n",
    "        prev_out = np.array([sampled[-1]])\n",
    "        in_oh = dataset.one_hot(prev_out)\n",
    "        h0, _ = rnn.rnn_step_forward(in_oh, h0, rnn.U, rnn.W, rnn.b)\n",
    "        \n",
    "        probas = rnn.softmax(rnn.output(h0, rnn.V, rnn.c))\n",
    "        out_char_oh = np.random.choice(range(dataset.vocab_size), p=probas.ravel()) \n",
    "        sampled.append(out_char_oh)\n",
    "  \n",
    "    return dataset.decode(sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-140-a4adf098dd0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/selected_conversations.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_minibatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_language_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-139-9e45217fa9a3>\u001b[0m in \u001b[0;36mrun_language_model\u001b[0;34m(dataset, max_epochs, hidden_size, sequence_length, learning_rate, sample_every)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# input for the next minibatch. In this way, we artificially\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# preserve context between batches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_oh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_oh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0maverage_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-138-76f0dad91291>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, hs, x, y)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_loss_and_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0mdU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-138-76f0dad91291>\u001b[0m in \u001b[0;36moutput_loss_and_grads\u001b[0;34m(self, h, V, c, y)\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mdO\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mdV\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0mdc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0mdhs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = Dataset()\n",
    "dataset.preprocess(\"data/selected_conversations.txt\")\n",
    "dataset.create_minibatches(batch_size=5, sequence_length=30)\n",
    "rnn = run_language_model(dataset, 50, sequence_length=dataset.sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

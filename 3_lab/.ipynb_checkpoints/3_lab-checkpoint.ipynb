{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. vježba: modeliranje nizova povratnim neuronskim mrežama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pdb\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadatak 1: učitavanje podataka i batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hardcoded paths - change if necessary\n",
    "root = 'data'\n",
    "\n",
    "# this one you need to download from the dataset\n",
    "full_dataset = 'data/movie_lines.txt'\n",
    "\n",
    "output_destination = 'data/selected_conversations.txt'\n",
    "movie_selection = 'data/selected_movies.txt'\n",
    "\n",
    "# separator used in the original dataset\n",
    "separator = ' +++$+++ '\n",
    "\n",
    "# movie ID file\n",
    "MOVIE_ID = 0\n",
    "\n",
    "# full conversation dataset file\n",
    "MOVIE_ID_FULL = 2\n",
    "# reverse indexing\n",
    "CHARACTER_NAME = -2\n",
    "CHARACTER_LINE = -1\n",
    "\n",
    "# keep just these characters for simplicity (and utf8 breaking)\n",
    "repl = r'[^A-Za-z0-9()\\,!\\?\\'\\`\\. ]'\n",
    "\n",
    "\n",
    "# regex replace\n",
    "def filter(string):\n",
    "    return re.sub(repl, '', string)\n",
    "\n",
    "\n",
    "# from a movie ID string (e.g. M134), output the number (134)\n",
    "def number_from_id(id):\n",
    "    return int(id[1:])\n",
    "\n",
    "\n",
    "# read just movie ID's, rest is for readability\n",
    "def read_selected(path_to_selected_movies):\n",
    "    selected_movies = set()\n",
    "\n",
    "    with open(path_to_selected_movies, 'r') as infile:\n",
    "        for line in infile:\n",
    "            parts = line.strip().split(separator)\n",
    "            selected_movies.add(parts[MOVIE_ID].strip())\n",
    "    return selected_movies\n",
    "\n",
    "\n",
    "# select and write to output file\n",
    "def select_and_write(path_to_full_dataset, path_to_output, selected_movies):\n",
    "    movies = {}\n",
    "\n",
    "    with open(path_to_full_dataset, 'r', encoding=\"ISO-8859-1\") as infile:\n",
    "\n",
    "        for line in infile:\n",
    "\n",
    "            parts = line.strip().split(separator)\n",
    "\n",
    "            if parts[MOVIE_ID_FULL].strip() not in selected_movies:\n",
    "                continue\n",
    "\n",
    "            # take data and transform to tuple\n",
    "            ID = parts[MOVIE_ID_FULL]\n",
    "            char_name = parts[CHARACTER_NAME]\n",
    "            char_line = parts[CHARACTER_LINE]\n",
    "\n",
    "            tup = (number_from_id(ID), char_name, char_line)\n",
    "\n",
    "            # add to map\n",
    "            if ID not in movies:\n",
    "                movies[ID] = []\n",
    "            movies[ID].append(tup)\n",
    "\n",
    "    with open(path_to_output, 'w') as out:\n",
    "        for movie in movies:\n",
    "            # sort by line number\n",
    "            dialogue = sorted(movies[movie], key=lambda t: t[0])\n",
    "            for n, name, text in dialogue:\n",
    "                out.write(filter(name) + ':\\n' + filter(text) + '\\n\\n')\n",
    "\n",
    "\n",
    "def main():\n",
    "    # uses hardcoded paths\n",
    "    selection = os.path.join(root, movie_selection)\n",
    "    selected_movies = read_selected(selection)\n",
    "\n",
    "    dataset = os.path.join(root, full_dataset)\n",
    "    output = os.path.join(root, output_destination)\n",
    "    select_and_write(dataset, output, selected_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def preprocess(self, input_file):\n",
    "        with open(input_file, \"r\") as f:\n",
    "            data = f.read()\n",
    "\n",
    "        # count and sort most frequent characters\n",
    "        characters, counts = np.unique(list(data), return_index=True)\n",
    "        # self.sorted chars contains just the characters ordered descending by frequency\n",
    "        self.sorted_characters = characters[np.argsort(-counts)]\n",
    "        self.vocab_size = len(self.sorted_characters)\n",
    "\n",
    "        self.char2id = dict(zip(self.sorted_characters, range(len(self.sorted_characters)))) \n",
    "        # reverse the mapping\n",
    "        self.id2char = {k:v for v,k in self.char2id.items()}\n",
    "        # convert the data to ids\n",
    "        self.x = np.array(list(map(self.char2id.get, data)))\n",
    "\n",
    "    def encode(self, sequence):\n",
    "        encoded_sequence = np.array([self.char2id[c] for c in sequence])\n",
    "        return encoded_sequence\n",
    "\n",
    "    def decode(self, sequence):\n",
    "        decoded_sequence = [self.id2char[c] for c in sequence]\n",
    "        return decoded_sequence\n",
    "        \n",
    "    # creates minibatches with batch_size * sequence length letters\n",
    "    def create_minibatches(self, batch_size, sequence_length):\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.batch_index = 0\n",
    "        \n",
    "        self.num_batches = int(len(self.x) / (batch_size * sequence_length)) # calculate the number of batches\n",
    "        self.batches = np.zeros([self.num_batches, self.batch_size, self.sequence_length + 1], dtype=np.int32)  \n",
    "    \n",
    "        for minibatch_index in range(self.num_batches):\n",
    "            for sequence_index in range(self.batch_size):\n",
    "                start = minibatch_index * sequence_length + sequence_index * (self.num_batches * sequence_length)\n",
    "                end = start + self.sequence_length + 1 \n",
    "                self.batches[minibatch_index, sequence_index, :] = self.x[start : end]\n",
    "                \n",
    "        print(self.decode(self.batches[0][0]))\n",
    "        print(self.decode(self.batches[0][1]))\n",
    "\n",
    "    def next_minibatch(self):        \n",
    "        if self.batch_index == self.num_batches:\n",
    "            self.batch_index = 0\n",
    "\n",
    "        batch = self.batches[self.batch_index, :, :]\n",
    "        self.batch_index += 1\n",
    "        \n",
    "        batch_x = batch[:, :-1]\n",
    "        batch_y = batch[:, 1:]\n",
    "        \n",
    "        return self.batch_index == self.num_batches, batch_x, batch_y\n",
    "    \n",
    "    def _as_one_hot(self, x, vocab):\n",
    "        n = len(x)\n",
    "        Yoh = np.zeros((n, vocab))\n",
    "        Yoh[np.arange(n), x] = 1\n",
    "        \n",
    "        return Yoh\n",
    "    \n",
    "    def one_hot(self, batch):\n",
    "        if batch.ndim == 1:\n",
    "            return self._as_one_hot(batch, self.vocab_size)\n",
    "        else:\n",
    "            return np.array([self._as_one_hot(b, self.vocab_size) for b in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D', 'a', 'y']\n",
      "['T', 'o', 'm', 'o', 'r', 'r', 'o', 'w']\n",
      "['S', 'H', 'I', 'R', 'L', 'E', 'Y', ':', '\\n', 'J', 'a', 'c', 'k', ',', ' ', 'i', 's', 'n', \"'\", 't', ' ', 't', 'h', 'a', 't', ' ', 'F', 'r', 'e', 'd', ' ']\n",
      "[' ', 'B', 'l', 'i', 'f', 'f', 'e', 'r', 't', ' ', 'o', 'v', 'e', 'r', ' ', 't', 'h', 'e', 'r', 'e', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 'b', 'l', 'u']\n"
     ]
    }
   ],
   "source": [
    "# test 1\n",
    "dat = Dataset()\n",
    "dat.preprocess(\"primjer.txt\")\n",
    "\n",
    "encoded = dat.encode(\"Day\")\n",
    "decoded = dat.decode(encoded)\n",
    "#print(dat.one_hot(encoded))\n",
    "print(decoded)\n",
    "print(dat.decode(dat.encode(\"Tomorrow\")))\n",
    "\n",
    "dat.create_minibatches(5, 30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadatak 2: obična jednoslojna povratna neuronska mreža"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN():\n",
    "    # inicijalizacija parametara\n",
    "    def __init__(self, hidden_size, sequence_length, vocabulary_size, learning_rate):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        print(\"Number of parameters in the RNN: \" + str(2*vocabulary_size*hidden_size + hidden_size + hidden_size*hidden_size + vocabulary_size))\n",
    "        print(\"Hidden size: \" + str(hidden_size))\n",
    "        print(\"Sequence length: \" + str(sequence_length))\n",
    "        print(\"Vocabulary size: \" + str(vocabulary_size))\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        self.U = np.random.normal(size=[vocabulary_size, hidden_size], scale=1.0 / np.sqrt(hidden_size))  # ... input projection\n",
    "        self.W = np.random.normal(size=[hidden_size, hidden_size], scale=1.0 / np.sqrt(hidden_size))  # ... hidden-to-hidden projection\n",
    "        self.b = np.zeros([1, hidden_size])\n",
    "\n",
    "        self.V = np.random.normal(size=[hidden_size, vocabulary_size], scale=1.0 / np.sqrt(vocabulary_size)) # ... output projection\n",
    "        self.c = np.zeros([1, vocabulary_size]) # ... output bias\n",
    "\n",
    "        # memory of past gradients - rolling sum of squares for Adagrad\n",
    "        self.memory_U = np.zeros_like(self.U)\n",
    "        self.memory_W = np.zeros_like(self.W)\n",
    "        self.memory_V = np.zeros_like(self.V)\n",
    "        self.memory_b = np.zeros_like(self.b)\n",
    "        self.memory_c = np.zeros_like(self.c)\n",
    "        \n",
    "    def rnn_step_forward(self, x, h_prev, U, W, b):\n",
    "        # A single time step forward of a recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity.\n",
    "\n",
    "        # x - input data (minibatch size x input dimension)\n",
    "        # h_prev - previous hidden state (minibatch size x hidden size)\n",
    "        # U - input projection matrix (input dimension x hidden size)\n",
    "        # W - hidden to hidden projection matrix (hidden size x hidden size)\n",
    "        # b - bias of shape (hidden size x 1)\n",
    "\n",
    "        h_current, cache = None, None\n",
    "        h_current = np.tanh(np.dot(h_prev, W) + np.dot(x, U) + b)\n",
    "        cache = (W, x, h_prev, h_current)\n",
    "\n",
    "        # return the new hidden state and a tuple of values needed for the backward step\n",
    "\n",
    "        return h_current, cache\n",
    "\n",
    "\n",
    "    def rnn_forward(self, x, h0, U, W, b):\n",
    "        # Full unroll forward of the recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity\n",
    "\n",
    "        # x - input data for the whole time-series (minibatch size x sequence_length x input dimension)\n",
    "        # h0 - initial hidden state (minibatch size x hidden size)\n",
    "        # U - input projection matrix (input dimension x hidden size)\n",
    "        # W - hidden to hidden projection matrix (hidden size x hidden size)\n",
    "        # b - bias of shape (hidden size x 1)\n",
    "        \n",
    "        h, cache = None, None\n",
    "        hs = [h0]\n",
    "        caches = []\n",
    "        \n",
    "        for sequence in range(self.sequence_length):\n",
    "            data = x[:, sequence, :]\n",
    "            h_current, cache_current = self.rnn_step_forward(data, hs[-1], U, W, b)\n",
    "            \n",
    "            hs.append(h_current)\n",
    "            caches.append(cache_current)\n",
    "            \n",
    "        # transposes the hs matrix according to indexes\n",
    "        hs = np.array(hs[1:]).transpose((1, 0, 2))\n",
    "        \n",
    "        # return the hidden states for the whole time series (T+1) and a tuple of values needed for the backward step\n",
    "        return hs, caches\n",
    "    \n",
    "    def rnn_step_backward(self, grad_next, cache):\n",
    "        # A single time step backward of a recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity.\n",
    "\n",
    "        # grad_next - upstream gradient of the loss with respect to the next hidden state and current output\n",
    "        # cache - cached information from the forward pass        \n",
    "        dh_prev, dU, dW, db = None, None, None, None\n",
    "        W, x, h_prev, h_current = cache\n",
    "        \n",
    "        # compute and return gradients with respect to each parameter\n",
    "        # HINT: you can use the chain rule to compute the derivative of the\n",
    "        # hyperbolic tangent function and use it to compute the gradient\n",
    "        # with respect to the remaining parameters\n",
    "        \n",
    "        # 65. i 66. slajd formule\n",
    "        \n",
    "        # gradijent tangensa hiperbolnog\n",
    "        d_tanh = 1 - h_current**2\n",
    "        \n",
    "        da = grad_next * (d_tanh)\n",
    "        \n",
    "        # gradijenti na parametre\n",
    "        dU = np.dot(x.T, da)\n",
    "        dW = np.dot(h_prev.T, da)\n",
    "        db = np.sum(da, axis=0)\n",
    "        \n",
    "        # gradijenti na ulaze\n",
    "        dh_prev = np.dot(da, W.T)\n",
    "\n",
    "        return dh_prev, dU, dW, db\n",
    "\n",
    "\n",
    "    def rnn_backward(self, dh, cache):\n",
    "        # Full unroll forward of the recurrent neural network with a \n",
    "        # hyperbolic tangent nonlinearity\n",
    "\n",
    "        dU = np.zeros_like(self.U)\n",
    "        dW = np.zeros_like(self.W)\n",
    "        db = np.zeros_like(self.b)\n",
    "\n",
    "        # compute and return gradients with respect to each parameter\n",
    "        # for the whole time series.\n",
    "        # Why are we not computing the gradient with respect to inputs (x)?\n",
    "        # A: because we are using only one layer, there is nothing to propagate through the network\n",
    "        grads = np.zeros_like(dh[0])\n",
    "        for dh_t, cache_t in reversed(list(zip(dh, cache))):\n",
    "            grads, dU_t, dW_t, db_t = self.rnn_step_backward(dh_t + grads, cache_t)\n",
    "            \n",
    "            dU += dU_t\n",
    "            dW += dW_t\n",
    "            db += db_t\n",
    "        \n",
    "        return np.clip(dU, -5, 5), np.clip(dW, -5, 5), np.clip(db, -5, 5)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        x -= np.max(x)\n",
    "        logits_exp = np.exp(x)\n",
    "        return logits_exp / np.sum(logits_exp, axis=1, keepdims=True)\n",
    "    \n",
    "    def output(self, h, V, c):\n",
    "        # Calculate the output \n",
    "        return np.dot(h, V) + c\n",
    "\n",
    "    def output_loss_and_grads(self, h, V, c, y):\n",
    "        # Calculate the loss of the network for each of the outputs\n",
    "\n",
    "        # h - hidden states of the network for each timestep. \n",
    "        #     the dimensionality of h is (batch size x sequence length x hidden size (the initial state is irrelevant for the output)\n",
    "        # V - the output projection matrix of dimension hidden size x vocabulary size\n",
    "        # c - the output bias of dimension vocabulary size x 1\n",
    "        # y - the true class distribution - a tensor of dimension \n",
    "        #     batch_size x sequence_length x vocabulary size - you need to do this conversion prior to\n",
    "        #     passing the argument. A fast way to create a one-hot vector from\n",
    "        #     an id could be something like the following code:\n",
    "\n",
    "        #   y[batch_id][timestep] = np.zeros((vocabulary_size, 1))\n",
    "        #   y[batch_id][timestep][batch_y[timestep]] = 1\n",
    "\n",
    "        #     where y might be a list or a dictionary.\n",
    "\n",
    "        loss, dh, dV, dc = None, None, None, None\n",
    "        \n",
    "        loss = 0\n",
    "        dhs = []\n",
    "        dV = np.zeros_like(self.V)\n",
    "        dc = np.zeros_like(self.c)\n",
    "        \n",
    "        batch_size = len(h)\n",
    "        \n",
    "        # calculate the output (o) - unnormalized log probabilities of classes\n",
    "        # calculate yhat - softmax of the output\n",
    "        # calculate the cross-entropy loss\n",
    "        # calculate the derivative of the cross-entropy softmax loss with respect to the output (o)\n",
    "        # calculate the gradients with respect to the output parameters V and c\n",
    "        # calculate the gradients with respect to the hidden layer h\n",
    "        \n",
    "        for sequence in range(self.sequence_length):\n",
    "            y_true = y[:, sequence, :]\n",
    "            h_t = h[:, sequence, :]\n",
    "            \n",
    "            o = self.output(h_t, V, c)\n",
    "            yhat = self.softmax(o)\n",
    "            \n",
    "            loss -= np.sum(np.log(yhat) * y_true) / batch_size\n",
    "            \n",
    "            dO = (yhat - y_true) / batch_size\n",
    "            dV += np.dot(h_t.T, dO)\n",
    "            dc += np.sum(dO, axis=0)\n",
    "            \n",
    "            dhs.append(np.dot(dO, V.T))\n",
    "\n",
    "        return loss, dhs, dV, dc\n",
    "    \n",
    "    # Adagrad parameter update\n",
    "    def update_parameters(self, batch_size, dU, dW, db, dV, dc):\n",
    "        # update memory matrices\n",
    "        # perform the Adagrad update of parameters\n",
    "        eps = 1e-7\n",
    "        for x, dx, mem_x in zip([self.U, self.W, self.b, self.V, self.c], [dU, dW, db, dV, dc], [self.memory_U, self.memory_W, self.memory_b, self.memory_V, self.memory_c]):\n",
    "            mem_x += np.square(dx)\n",
    "            x -= self.learning_rate * dx / np.sqrt(mem_x + eps)\n",
    "            \n",
    "    def step(self, hs, x, y):\n",
    "        hs, caches = self.rnn_forward(x, hs, self.U, self.W, self.b)\n",
    "        loss, dh, dV, dc = self.output_loss_and_grads(hs, self.V, self.c, y)\n",
    "        dU, dW, db = self.rnn_backward(dh, caches)\n",
    "        \n",
    "        self.update_parameters(len(x), dU, dW, db, dV, dc)\n",
    "        \n",
    "        # return loss and the last state value\n",
    "        return loss, hs[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_language_model(dataset, max_epochs, hidden_size=100, sequence_length=30, learning_rate=1e-1, sample_every=100):\n",
    "    vocabulary_size = len(dataset.sorted_characters)\n",
    "    rnn = RNN(hidden_size=hidden_size, sequence_length=sequence_length, vocabulary_size=vocabulary_size, learning_rate=learning_rate) # initialize the recurrent network\n",
    "\n",
    "    current_epoch = 0 \n",
    "    batch = 0\n",
    "\n",
    "    h0 = np.zeros((dataset.batch_size, hidden_size))\n",
    "    average_loss = 0\n",
    "    \n",
    "    should_print = False\n",
    "    while current_epoch < max_epochs: \n",
    "        e, x, y = dataset.next_minibatch()\n",
    "        \n",
    "        if e: \n",
    "            current_epoch += 1\n",
    "            h0 = np.zeros((dataset.batch_size, hidden_size))\n",
    "            \n",
    "            current_batch = batch % dataset.num_batches\n",
    "            print(\"Epoch: %06d\" % (current_epoch), end=\"\\n\")\n",
    "            print(\"Average_loss: %.4f; Last batch loss: %.4f\" % (average_loss / batch, loss))\n",
    "            print(\"=====================================================\")\n",
    "            \n",
    "            should_print = True\n",
    "\n",
    "        # One-hot transform the x and y batches\n",
    "        x_oh = dataset.one_hot(x)\n",
    "        y_oh = dataset.one_hot(y)\n",
    "\n",
    "        # Run the recurrent network on the current batch\n",
    "        # Since we are using windows of a short length of characters,\n",
    "        # the step function should return the hidden state at the end\n",
    "        # of the unroll. You should then use that hidden state as the\n",
    "        # input for the next minibatch. In this way, we artificially\n",
    "        # preserve context between batches.\n",
    "        loss, _ = rnn.step(h0, x_oh, y_oh)\n",
    "        average_loss += loss\n",
    "\n",
    "        if should_print: \n",
    "            # run sampling (2.2)\n",
    "    \n",
    "            seed = \"HAN:\\nIs that good or bad?\\n\\n\"\n",
    "            sampled_data = sample(rnn, seed, 300, dataset)\n",
    "\n",
    "            print(''.join(sampled_data))\n",
    "            print() \n",
    "            should_print = False\n",
    "            \n",
    "        batch += 1\n",
    "        \n",
    "def sample(rnn, seed, sample_size, dataset):\n",
    "    h0 = np.zeros([1, rnn.hidden_size])\n",
    "    seed_oh = dataset.one_hot(dataset.encode(seed))\n",
    "    \n",
    "    sampled = []\n",
    "    \n",
    "    for seed_character in seed_oh:\n",
    "        h0, _ = rnn.rnn_step_forward(seed_character.reshape([1, -1]), h0, rnn.U, rnn.W, rnn.b)\n",
    "        sampled.append(np.argmax(seed_character))\n",
    "        \n",
    "    for i in range(len(seed), sample_size):\n",
    "        prev_out = np.array([sampled[-1]])\n",
    "        in_oh = dataset.one_hot(prev_out)\n",
    "        h0, _ = rnn.rnn_step_forward(in_oh, h0, rnn.U, rnn.W, rnn.b)\n",
    "        \n",
    "        probabilities = rnn.softmax(rnn.output(h0, rnn.V, rnn.c))\n",
    "        out_char = np.random.choice(range(dataset.vocab_size), p=probabilities.ravel()) \n",
    "        sampled.append(out_char)\n",
    "  \n",
    "    return dataset.decode(sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S', 'H', 'I', 'R', 'L', 'E', 'Y', ':', '\\n', 'J', 'a', 'c', 'k', ',', ' ', 'i', 's', 'n', \"'\", 't', ' ', 't', 'h', 'a', 't', ' ', 'F', 'r', 'e', 'd', ' ']\n",
      "[' ', 'B', 'l', 'i', 'f', 'f', 'e', 'r', 't', ' ', 'o', 'v', 'e', 'r', ' ', 't', 'h', 'e', 'r', 'e', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 'b', 'l', 'u']\n",
      "Number of parameters in the RNN: 24371\n",
      "Hidden size: 100\n",
      "Sequence length: 30\n",
      "Vocabulary size: 71\n",
      "\n",
      "\n",
      "Epoch: 000001\n",
      "Average_loss: 60.1321; Last batch loss: 54.1029\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DONNIE:\n",
      ":'ve p3ereny hist to to me here?\n",
      "\n",
      "DR.\n",
      "NMION:\n",
      "Peritser\n",
      "\n",
      "\n",
      "R.... The nom guts ouly mu. worel ae his?.. It's Ip are?\n",
      "\n",
      "DR.'N\n",
      "ER:\n",
      "Thoan.. Then ine weres... Hann ipes. Dourestal to cupray,.. hit meer ele.\n",
      "\n",
      "DRTTHHE:\n",
      "Leing afe to thele ha me ro doght er ous the themlis?.\n",
      "\n",
      "D\n",
      "\n",
      "Epoch: 000002\n",
      "Average_loss: 57.0275; Last batch loss: 53.7912\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DDINmEllysull kink.\n",
      "\n",
      "DR. THURMAB:\n",
      "He me my at's susury alk remested ou he Sou'sh heries me tall a yourr now.. He. That or. THuthe ffing ou'f roolis.\n",
      "\n",
      "DR. THUMMA:\n",
      ":\n",
      "Gofaled gren thing he cepapel there as. Where a otiot we theshioo?\n",
      "\n",
      "DR. TJUMME:\n",
      "Wow on wate woly as sussw pan\n",
      "\n",
      "Epoch: 000003\n",
      "Average_loss: 55.5888; Last batch loss: 53.6590\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "RORE:\n",
      "Hone to is punlee?\n",
      "\n",
      "DONNIE:\n",
      "I'm you a mangitur ou leme thit wan wolen jusse thy prool...  thif. Oh prays tell a chit hasee? Dh?\n",
      "\n",
      "DDRVOY:\n",
      "Disot and be ons of it we wil in mut'se ov wo hiy peuctate cacifed ito ther an of on to is mut?\n",
      "\n",
      "DR. THURYexmoun ald.\n",
      "\n",
      "DR. THURMAN\n",
      "\n",
      "Epoch: 000004\n",
      "Average_loss: 54.7027; Last batch loss: 53.5545\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "DR.  OLMME:\n",
      "I's to Tho you the on... wirs?\n",
      "\n",
      "DONNIE:\n",
      "Hee, Do.. row came like prelpy.\n",
      "\n",
      "DURON:\n",
      "AMMAN:\n",
      "The to pacher hy do mo coui sow is Mo goise ked horimboul to aig there ald af the heraln.\n",
      "\n",
      "DONNIE:\n",
      "What dow wape the?\n",
      "\n",
      "ROMAN:\n",
      "That all the wing Ted with are whas me, they Son\n",
      "\n",
      "Epoch: 000005\n",
      "Average_loss: 54.0722; Last batch loss: 53.1324\n",
      "=====================================================\n",
      "HAN:\n",
      "Is that good or bad?\n",
      "\n",
      "GORMMITOFF:\n",
      "A DORNIE:\n",
      "Whhes ho the you retind nead be ousinb to so willl Thore with  I a wisings.  so be exsip.\n",
      "\n",
      "SIELA:\n",
      "So red ouds. I hime ccout a be nocke to of of do the her mols us that's pomes?. will you going iapct.\n",
      "\n",
      "DR. THURMAN:\n",
      "Wid Feald it he goof themquiglt'm It \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-a4adf098dd0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/selected_conversations.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_minibatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_language_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-da7c9e146581>\u001b[0m in \u001b[0;36mrun_language_model\u001b[0;34m(dataset, max_epochs, hidden_size, sequence_length, learning_rate, sample_every)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# input for the next minibatch. In this way, we artificially\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# preserve context between batches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_oh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_oh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0maverage_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-8e00350f60bc>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, hs, x, y)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_loss_and_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mdU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-8e00350f60bc>\u001b[0m in \u001b[0;36mrnn_backward\u001b[0;34m(self, dh, cache)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdh_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_t\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdU_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_step_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdh_t\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mdU\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdU_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-8e00350f60bc>\u001b[0m in \u001b[0;36mrnn_step_backward\u001b[0;34m(self, grad_next, cache)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mdU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mdW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_prev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m# gradijenti na ulaze\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/Anaconda/anaconda/lib/python3.5/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         return _methods._sum(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 1848\u001b[0;31m                             out=out, **kwargs)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/Anaconda/anaconda/lib/python3.5/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_prod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = Dataset()\n",
    "dataset.preprocess(\"data/selected_conversations.txt\")\n",
    "dataset.create_minibatches(batch_size=5, sequence_length=30)\n",
    "rnn = run_language_model(dataset, 50, sequence_length=dataset.sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
